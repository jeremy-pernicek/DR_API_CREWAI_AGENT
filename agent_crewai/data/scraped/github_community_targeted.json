[
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "quickstart.py",
    "file_name": "quickstart.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# type: ignore\nimport argparse\nimport json\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nif sys.version_info[0] < 3 or (sys.version_info[0] >= 3 and sys.version_info[1] < 9):\n    print(\"Must be using Python version 3.9 or higher\")\n    exit(1)\n\nwork_dir = Path(os.path.dirname(__file__))\ndot_env_file = Path(work_dir / \".env\")\nvenv_dir = work_dir / \".venv\"\n\n\ndef is_datarobot_codespace():\n    return os.getenv(\"DATAROBOT_NOTEBOOK_IMAGE\") is not None\n\n\ndef check_pulumi_installed():\n    try:\n        subprocess.check_call(\n            [\"pulumi\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Is pulumi installed? If not, please go to `https://www.pulumi.com/docs/iac/download-install/`\"\n        )\n        exit(1)\n\n\ndef check_pulumi_login():\n    try:\n        subprocess.check_call(\n            [\"pulumi\", \"whoami\", \"-j\", \"--non-interactive\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT,\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Please login to pulumi and rerun. Use `pulumi login --local` to log in locally or `pulumi login` to login to pulumi cloud with an API token\"\n        )\n        exit(1)\n\n\ndef check_dotenv_exists():\n    if not dot_env_file.exists():\n        print(\n            \"Could not find `.env`. Please rename the file `.env.template` and fill in your details\"\n        )\n        exit(1)\n\n\ndef is_windows():\n    return os.name == \"nt\"\n\n\ndef get_activate_command():\n    if is_datarobot_codespace():\n        return []\n    if is_conda_environment():\n        if is_windows():\n            activate_cmd = [\"conda\", \"activate\", f\"{venv_dir}\", \"&&\"]\n        else:\n            # see https://github.com/conda/conda/issues/7980\n            activate_cmd = [\n                \"eval\",\n                '\"$(conda shell.bash hook)\"',\n                \"&&\",\n                \"conda\",\n                \"activate\",\n                f\"{str(venv_dir)}\",\n                \"&&\",\n            ]\n\n    else:\n        # Regular venv activation\n        if is_windows():\n            activate_script = str(venv_dir / \"Scripts\" / \"activate.bat\")\n            activate_cmd = [\"call\", f\"{activate_script}\", \"&&\"]\n        else:\n            activate_script = str(venv_dir / \"bin\" / \"activate\")\n            activate_cmd = [\n                \"source\",\n                f\"{activate_script}\",\n                \"&&\",\n            ]\n    return activate_cmd\n\n\ndef is_conda_environment():\n    return os.environ.get(\"CONDA_DEFAULT_ENV\") is not None\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Infrastructure management script\")\n    parser.add_argument(\"stack_name\", help=\"Stack name to use\")\n    parser.add_argument(\n        \"--action\",\n        choices=[\"up\", \"destroy\"],\n        default=\"up\",\n        required=False,\n        help=\"Action to perform (up or destroy)\",\n    )\n    return parser.parse_args()\n\n\ndef get_python_executable():\n    if is_conda_environment():\n        return shutil.which(\"python\")\n    return sys.executable\n\n\ndef run_subprocess_in_venv(command: list[str]):\n    if is_windows():\n        full_cmd = get_activate_command() + command\n        # shell = True, otherwise CMD complains it can't find the file\n        result = subprocess.run(\n            \" \".join(full_cmd),\n            check=True,\n            cwd=work_dir,\n            shell=True,\n            capture_output=True,\n            text=True,\n        )\n    else:\n        full_cmd = [\"bash\", \"-c\", \" \".join(get_activate_command() + command)]\n        print(full_cmd)\n        result = subprocess.run(\n            full_cmd,\n            check=True,\n            cwd=work_dir,\n            capture_output=True,\n            text=True,\n        )\n\n    if result.returncode != 0:\n        print(f\"Error running command: {result.stderr}\")\n        raise subprocess.CalledProcessError(\n            result.returncode, result.args, result.stdout, result.stderr\n        )\n\n\ndef create_virtual_environment() -> None:\n    if not venv_dir.exists():\n        if is_conda_environment():\n            print(\"Creating conda environment...\")\n            subprocess.run(\n                [\n                    \"conda\",\n                    \"create\",\n                    \"--prefix\",\n                    str(venv_dir),\n                    \"python=3.11\",\n                    \"pip\",\n                    \"-y\",\n                ],\n                check=True,\n                cwd=work_dir,\n            )\n        else:\n            # Regular venv creation\n            python_executable = get_python_executable()\n            subprocess.run(\n                [python_executable, \"-m\", \"venv\", \".venv\"],\n                check=True,\n                cwd=work_dir,\n            )\n\n\ndef setup_virtual_environment() -> None:\n    \"\"\"Create and configure a virtual environment in a cross-platform manner.\"\"\"\n    print(\"Preparing virtual environment...\")\n\n    try:\n        # Handle activation and package installation\n\n        try:\n            run_subprocess_in_venv([\"pip\", \"install\", \"-U\", \"uv\"])\n            if is_datarobot_codespace():\n                os.system(\"uv pip install $(pip freeze | grep ipykernel)\")\n            # Install requirements using uv\n            run_subprocess_in_venv([\"uv\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n        except Exception as e:\n            print(f\"UV installation/usage failed: {e}\")\n            print(\"Falling back to pip\")\n\n            run_subprocess_in_venv([\"pip\", \"install\", \"-r\", \"requirements.txt\"])\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during virtual environment setup: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected error during virtual environment setup: {e}\")\n        raise\n\n\ndef load_dotenv():\n    with open(\".env\") as f:\n        content = f.read()\n\n    pattern = r\"\"\"\n        (?:^|\\n)         # Must start at beginning of string or after newline\n        (?![\\s#])        # Negative lookahead: next char cannot be whitespace or #\n        ([A-Za-z_]\\w*)   # Key: start with letter/underscore, then word chars\n        =\\s*             # Equals with optional whitespace after\n        (?:\n            '(.*?)'      # Single quoted (group 2)\n            |\"(.*?)\"     # Double quoted (group 3)\n            |([^\\n]+)    # Unquoted: everything until newline (group 4)\n        )\n    \"\"\"\n\n    env_vars = {}\n    for match in re.finditer(pattern, content, re.MULTILINE | re.DOTALL | re.VERBOSE):\n        key = match.group(1).strip()\n\n        if match.group(2) is not None:  # Single quoted\n            value = match.group(2).strip()\n        elif match.group(3) is not None:  # Double quoted\n            value = match.group(3).strip()\n        else:  # Unquoted\n            value = match.group(4)\n            if \" #\" in value:  # Only strip comments after space\n                value = value.split(\" #\", 1)[0]\n            value = value.strip()\n\n        env_vars[key] = value\n\n    os.environ.update(env_vars)\n    return env_vars\n\n\ndef run_pulumi_command(command: list, work_dir: Path, env_vars: dict):\n    \"\"\"Run a Pulumi command using shell activation with PTY support.\"\"\"\n    cmd_str = \" \".join(command)\n    try:\n        if is_windows():\n            os.system(f\"{' '.join(get_activate_command())}{cmd_str}\")\n        else:\n            os.system(f\"bash -c '{' '.join(get_activate_command())}{cmd_str}'\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        sys.exit(1)\n\n\ndef setup_pulumi_config(work_dir: Path, stack_name: str, env_vars: dict):\n    stack_select = [\"pulumi\", \"stack\", \"select\", stack_name, \"--create\"]\n\n    # Run commands\n    run_pulumi_command(stack_select, work_dir, env_vars)\n\n\ndef print_app_url():\n    try:\n        pulumi_output = subprocess.check_output([\"pulumi\", \"stack\", \"output\", \"-j\"])\n        pulumi_output_dict = json.loads(pulumi_output)\n        application_id = pulumi_output_dict[\"DATAROBOT_APPLICATION_ID\"]\n        datarobot_endpoint = os.environ[\"DATAROBOT_ENDPOINT\"]\n        url = f\"{datarobot_endpoint.rstrip('/').replace('api/v2', '')}custom_applications/{application_id}/\"\n        print(\"\\n\\n\")\n        print(\"=\" * 80)\n        print(f\"\\n    Your app is ready! Application URL:\\n\\n    {url}\\n\")\n        print(\"=\" * 80)\n    except Exception as e:\n        print(e)\n\n\ndef main():\n    args = parse_args()\n    if args.stack_name == \"YOUR_PROJECT_NAME\":\n        print(\"Please use a different project name\")\n        sys.exit(0)\n    check_dotenv_exists()\n    # Load environment variables\n    env_vars = load_dotenv()\n\n    check_pulumi_installed()\n    check_pulumi_login()\n\n    # Skip venv setup in Codespaces or if explicitly requested\n    if not is_datarobot_codespace():\n        create_virtual_environment()\n    setup_virtual_environment()\n    # Setup Pulumi configuration\n    setup_pulumi_config(work_dir, args.stack_name, env_vars)\n\n    # Refresh the stack\n    print(\"\\nRefreshing stack...\")\n    run_pulumi_command(\n        [\"pulumi\", \"refresh\", \"--yes\", \"--skip-pending-creates\"], work_dir, env_vars\n    )\n\n    if args.action == \"destroy\":\n        print(\"\\nDestroying stack...\")\n        run_pulumi_command([\"pulumi\", \"destroy\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack destroy complete\")\n    else:\n        print(\"\\nCreate/update stack...\")\n        run_pulumi_command([\"pulumi\", \"up\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack update complete\")\n\n        print_app_url()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/quickstart.py",
    "size": 10067,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "datarobot"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "utils/api.py",
    "file_name": "api.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import annotations\n\nimport ast\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport tempfile\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom types import ModuleType, TracebackType\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Type,\n    TypeVar,\n    cast,\n)\n\nimport datarobot as dr\nimport instructor\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport polars as pl\nimport psutil\nimport scipy\nimport sklearn\nimport statsmodels as sm\nfrom datarobot.client import RESTClientObject\nfrom joblib import Memory\nfrom openai import AsyncOpenAI\nfrom openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\nfrom openai.types.chat.chat_completion_system_message_param import (\n    ChatCompletionSystemMessageParam,\n)\nfrom openai.types.chat.chat_completion_user_message_param import (\n    ChatCompletionUserMessageParam,\n)\nfrom plotly.subplots import make_subplots\nfrom pydantic import ValidationError\n\nsys.path.append(os.path.dirname(os.path.realpath(__file__)))\nfrom utils import prompts, tools\nfrom utils.analyst_db import AnalystDB, DataSourceType\nfrom utils.code_execution import (\n    InvalidGeneratedCode,\n    MaxReflectionAttempts,\n    execute_python,\n    reflect_code_generation_errors,\n)\nfrom utils.data_cleansing_helpers import (\n    add_summary_statistics,\n    process_column,\n)\nfrom utils.database_helpers import get_external_database\nfrom utils.logging_helper import get_logger, log_api_call\nfrom utils.resources import LLMDeployment\nfrom utils.schema import (\n    AnalysisError,\n    AnalystChatMessage,\n    AnalystDataset,\n    BusinessAnalysisGeneration,\n    ChartGenerationExecutionResult,\n    ChatRequest,\n    CleansedDataset,\n    CodeGeneration,\n    Component,\n    DatabaseAnalysisCodeGeneration,\n    DataDictionary,\n    DataDictionaryColumn,\n    DataRegistryDataset,\n    DictionaryGeneration,\n    DownloadedRegistryDataset,\n    EnhancedQuestionGeneration,\n    GetBusinessAnalysisMetadata,\n    GetBusinessAnalysisRequest,\n    GetBusinessAnalysisResult,\n    QuestionListGeneration,\n    RunAnalysisRequest,\n    RunAnalysisResult,\n    RunAnalysisResultMetadata,\n    RunChartsRequest,\n    RunChartsResult,\n    RunDatabaseAnalysisRequest,\n    RunDatabaseAnalysisResult,\n    RunDatabaseAnalysisResultMetadata,\n    Tool,\n    ValidatedQuestion,\n)\n\nlogger = get_logger()\nlogging.getLogger(\"openai\").setLevel(logging.WARNING)\nlogging.getLogger(\"openai.http_client\").setLevel(logging.WARNING)\n\nVALUE_ERROR_MESSAGE = \"Input data cannot be empty (no dataset provided)\"\nDEFAULT_LLM_GATEWAY_MODEL = \"azure/gpt-4o\"\nDEFAULT_LLM_GATEWAY_MODEL_SMALL = \"azure/gpt-4o-mini\"\n\n\ndef log_memory() -> None:\n    process = psutil.Process()\n    memory = process.memory_info().rss / 1024 / 1024  # MB\n    logger.info(f\"Memory usage: {memory:.2f} MB\")\n\n\n@functools.cache\ndef initialize_deployment() -> tuple[RESTClientObject, str]:\n    \"\"\"Initialize either LLM Gateway or DataRobot-hosted LLM deployment based on environment settings and credential priority.\"\"\"\n    try:\n        dr_client = dr.Client()\n        chat_agent_deployment_id = LLMDeployment().id\n        if chat_agent_deployment_id is None:\n            raise ValueError(\n                \"LLM Deployment ID is required but not found. Please check your infrastructure setup.\"\n            )\n        deployment_chat_base_url = (\n            f\"{dr_client.endpoint.rstrip('/')}/deployments/{chat_agent_deployment_id}/\"\n        )\n        logger.info(\n            f\"Using the DataRobot-hosted LLM deployment (configured at infrastructure time) at: {deployment_chat_base_url}\"\n        )\n        return dr_client, deployment_chat_base_url\n\n    except ValidationError as e:\n        raise ValueError(\n            \"Unable to load Deployment ID.\"\n            \"If running locally, verify you have selected the correct \"\n            \"stack and that it is active using `pulumi stack output`. \"\n            \"If running in DataRobot, verify your runtime parameters have been set correctly.\"\n        ) from e\n\n\nclass AsyncLLMClient:\n    async def __aenter__(self) -> instructor.AsyncInstructor:\n        dr_client, deployment_base_url = initialize_deployment()\n        self.openai_client = AsyncOpenAI(\n            api_key=dr_client.token,\n            base_url=deployment_base_url,\n            timeout=90,\n            max_retries=2,\n        )\n        self.client = instructor.from_openai(\n            self.openai_client, mode=instructor.Mode.MD_JSON\n        )\n        return self.client\n\n    async def __aexit__(\n        self,\n        exc_type: Type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.openai_client.close()  # Properly close the client\n\n\nALTERNATIVE_LLM_BIG = \"datarobot-deployed-llm\"\nALTERNATIVE_LLM_SMALL = \"datarobot-deployed-llm\"\nDICTIONARY_BATCH_SIZE = 10\nMAX_REGISTRY_DATASET_SIZE = 400e6  # aligns to 400MB set in streamlit config.toml\nDISK_CACHE_LIMIT_BYTES = 512e6\nDICTIONARY_PARALLEL_BATCH_SIZE = 2\nDICTIONARY_TIMEOUT = 45.0\n\n_memory = Memory(tempfile.gettempdir(), verbose=0)\n_memory.clear(warn=False)  # clear cache on startup\n\nT = TypeVar(\"T\")\n\n\ndef cache(f: T) -> T:\n    \"\"\"Cache function and coroutine results to disk using joblib.\"\"\"\n    cached_f = _memory.cache(f)\n\n    if asyncio.iscoroutinefunction(f):\n\n        async def awrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = await cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"\n                )\n            return result\n\n        return cast(T, awrapper)\n    else:\n\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"  # type: ignore[attr-defined]\n                )\n            return result\n\n        return cast(T, wrapper)\n\n\n# This can be large as we are not storing the actual datasets in memory, just metadata\ndef list_registry_datasets(limit: int = 100) -> list[DataRegistryDataset]:\n    \"\"\"\n    Fetch datasets from Data Registry with specified limit\n\n    Args:\n        limit: int\n        Datasets to retrieve. Max value: 100\n    \"\"\"\n\n    url = f\"datasets?limit={limit}\"\n\n    # Get all datasets and manually limit the results\n    datasets = dr.client.get_client().get(url).json()[\"data\"]\n\n    return [\n        DataRegistryDataset(\n            id=ds[\"datasetId\"],\n            name=ds[\"name\"],\n            created=(\n                ds[\"creationDate\"][:10] if \"creationDate\" in ds else \"N/A\"  # %Y-%m-%d\n            ),\n            size=(\n                f\"{ds['datasetSize'] / (1024 * 1024):.1f} MB\"\n                if \"datasetSize\" in ds\n                else \"N/A\"\n            ),\n        )\n        for ds in datasets\n    ]\n\n\nasync def download_registry_datasets(\n    dataset_ids: list[str], analyst_db: AnalystDB\n) -> list[DownloadedRegistryDataset]:\n    \"\"\"Load selected datasets as pandas DataFrames\n\n    Args:\n        *args: list of dataset IDs to download\n\n    Returns:\n        list[AnalystDataset]: Dictionary of dataset names and data\n    \"\"\"\n    downloaded_datasets = []\n    datasets = [dr.Dataset.get(id_) for id_ in dataset_ids]\n    if (\n        sum([ds.size for ds in datasets if ds.size is not None])\n        > MAX_REGISTRY_DATASET_SIZE\n    ):\n        raise ValueError(\n            f\"The requested Data Registry datasets must total <= {int(MAX_REGISTRY_DATASET_SIZE)} bytes\"\n        )\n\n    result_datasets: list[AnalystDataset] = []\n    for dataset in datasets:\n        try:\n            df = dataset.get_as_dataframe()\n            result_datasets.append(AnalystDataset(name=dataset.name, data=df))\n            logger.info(f\"Successfully downloaded {dataset.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to read dataset {dataset.name}: {str(e)}\")\n            downloaded_datasets.append(\n                DownloadedRegistryDataset(name=dataset.name, error=str(e))\n            )\n            continue\n    for result_dataset in result_datasets:\n        await analyst_db.register_dataset(\n            result_dataset, DataSourceType.REGISTRY, dataset.size or 0\n        )\n        downloaded_datasets.append(DownloadedRegistryDataset(name=result_dataset.name))\n    return downloaded_datasets\n\n\nasync def _get_dictionary_batch(\n    columns: list[str], df: pl.DataFrame, batch_size: int = 5\n) -> list[DataDictionaryColumn]:\n    \"\"\"Process a batch of columns to get their descriptions\"\"\"\n\n    # Get sample data and stats for just these columns\n    # Convert timestamps to ISO format strings for JSON serialization\n    try:\n        logger.debug(f\"Processing batch of {len(columns)} columns\")\n        sample_data = {}\n        logger.debug(\"Converting datetime columns to ISO format\")\n        num_samples = 10\n        for col in columns:\n            if df[col].dtype.is_temporal():\n                # Convert timestamps to ISO format strings\n                sample_data[col] = (\n                    df.select(\n                        pl.col(col)\n                        .cast(pl.Datetime)\n                        .map_elements(\n                            lambda x: x.isoformat() if x is not None else None\n                        )\n                    )\n                    .head(num_samples)\n                    .to_dict()\n                )\n            else:\n                # For non-datetime columns, just take the samples as is\n                sample_data[col] = df.select(pl.col(col)).head(num_samples).to_dict()\n\n        # Handle numeric summary\n        numeric_summary = {}\n        logger.debug(\"Calculating numeric summaries\")\n        for col in columns:\n            if df[col].dtype.is_numeric():\n                desc = df[col].describe()\n                numeric_summary[col] = desc.to_dict()\n\n        # Get categories for non-numeric columns\n        categories = []\n        logger.debug(\"Getting categories for non-numeric columns\")\n        for column in columns:\n            if not df[column].dtype.is_numeric():\n                try:\n                    value_counts = (\n                        df[column].sample(n=1000, seed=42).value_counts().head(10)\n                    )\n                    # Convert any timestamp values to strings\n                    if df[column].dtype.is_temporal():\n                        value_counts[column] = value_counts[column].cast(pl.String)\n                    categories.append({column: value_counts[column].to_list()})\n                except Exception:\n                    continue\n\n        # Create messages for OpenAI\n        messages: list[ChatCompletionMessageParam] = [\n            ChatCompletionSystemMessageParam(\n                role=\"system\", content=prompts.SYSTEM_PROMPT_GET_DICTIONARY\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Data:\\n{sample_data}\\n\"\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Statistical Summary:\\n{numeric_summary}\\n\"\n            ),\n        ]\n\n        if categories:\n            messages.append(\n                ChatCompletionUserMessageParam(\n                    role=\"user\", content=f\"Categorical Values:\\n{categories}\\n\"\n                )\n            )\n        logger.debug(\n            f\"total_characters: {len(''.join([str(msg) for msg in messages]))}\"\n        )\n        # Get descriptions from OpenAI\n        async with AsyncLLMClient() as client:\n            completion: DictionaryGeneration = await client.chat.completions.create(\n                response_model=DictionaryGeneration,\n                model=ALTERNATIVE_LLM_SMALL,\n                messages=messages,\n            )\n\n        # Convert to dictionary format\n        descriptions = completion.to_dict()\n\n        # Only return descriptions for requested columns\n        return [\n            DataDictionaryColumn(\n                column=col,\n                description=descriptions.get(col, \"No description available\"),\n                data_type=str(df[col].dtype),\n            )\n            for col in columns\n        ]\n\n    except ValueError as e:\n        logger.error(f\"Invalid dictionary response: {str(e)}\")\n        return [\n            DataDictionaryColumn(\n                column=col,\n                description=\"No valid description available\",\n                data_type=str(df[col].dtype),\n            )\n            for col in columns\n        ]\n\n\n@log_api_call\nasync def get_dictionary(dataset: AnalystDataset) -> DataDictionary:\n    \"\"\"Process a single dataset with parallel column batch processing\"\"\"\n\n    try:\n        logger.info(f\"Processing dataset {dataset.name} init\")\n        # Convert JSON to DataFrame\n        df_full = dataset.to_df()\n        df = df_full.sample(n=min(10000, len(df_full)), seed=42)\n\n        # Add debug logging\n        logger.info(f\"Processing dataset {dataset.name} with shape {df.shape}\")\n\n        # Handle empty dataset\n        if df.is_empty():\n            logger.warning(f\"Dataset {dataset.name} is empty\")\n            return DataDictionary(\n                name=dataset.name,\n                column_descriptions=[],\n            )\n\n        # Split columns into batches\n        column_batches = [\n            list(df.columns[i : i + DICTIONARY_BATCH_SIZE])\n            for i in range(0, len(df.columns), DICTIONARY_BATCH_SIZE)\n        ]\n        logger.info(\n            f\"Created {len(column_batches)} batches for {len(df.columns)} columns\"\n        )\n\n        # Create a semaphore to limit concurrent tasks to 2\n        sem = asyncio.Semaphore(DICTIONARY_PARALLEL_BATCH_SIZE)\n\n        async def throttled_get_dictionary_batch(\n            batch: list[str],\n        ) -> list[DataDictionaryColumn]:\n            try:\n                async with sem:\n                    return await asyncio.wait_for(\n                        _get_dictionary_batch(batch, df, DICTIONARY_BATCH_SIZE),\n                        timeout=DICTIONARY_TIMEOUT,\n                    )\n            except asyncio.TimeoutError:\n                logger.warning(f\"Timeout processing batch: {batch}\")\n                return [\n                    DataDictionaryColumn(\n                        column=col,\n                        description=\"No Description Available\",\n                        data_type=str(df[col].dtype),\n                    )\n                    for col in batch\n                ]\n            except Exception as e:\n                logger.error(f\"Error processing batch {batch}: {str(e)}\")\n                return [\n                    DataDictionaryColumn(\n                        column=col,\n                        description=\"No Description Available\",\n                        data_type=str(df[col].dtype),\n                    )\n                    for col in batch\n                ]\n\n        tasks = [throttled_get_dictionary_batch(batch) for batch in column_batches]\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        # Filter out any exceptions and flatten results\n        dictionary: list[DataDictionaryColumn] = []\n        for result in results:\n            if isinstance(result, BaseException):\n                logger.error(f\"Task failed with error: {str(result)}\")\n                continue\n            dictionary.extend(result)\n\n        logger.info(\n            f\"Created dictionary with {len(dictionary)} entries for dataset {dataset.name}\"\n        )\n\n        return DataDictionary(\n            name=dataset.name,\n            column_descriptions=dictionary,\n        )\n\n    except Exception:\n        return DataDictionary(\n            name=dataset.name,\n            column_descriptions=[\n                DataDictionaryColumn(\n                    column=c,\n                    data_type=str(dataset.to_df()[c].dtype),\n                    description=\"No Description Available\",\n                )\n                for c in dataset.columns\n            ],\n        )\n\n\ndef _validate_question_feasibility(\n    question: str, available_columns: list[str]\n) -> ValidatedQuestion | None:\n    \"\"\"Validate if a question can be answered with available data\n\n    Checks if common data elements mentioned in the question exist in columns\n    \"\"\"\n    # Convert question and columns to lowercase for matching\n    question_lower = question.lower()\n    columns_lower = [col.lower() for col in available_columns]\n\n    # Extract potential column references from question\n    words = set(re.findall(r\"\\b\\w+\\b\", question_lower))\n\n    # Find matches and missing terms\n    found_columns = [col for col in columns_lower if any(word in col for word in words)]\n\n    is_valid = len(found_columns) > 0\n    if is_valid:\n        return ValidatedQuestion(\n            question=question,\n        )\n    return None\n\n\n@log_api_call\nasync def suggest_questions(\n    datasets: list[AnalystDataset], max_columns: int = 40\n) -> list[ValidatedQuestion]:\n    \"\"\"Generate and validate suggested analysis questions\n\n    Args:\n        dictionary: DataFrame containing data dictionary\n        max_columns: Maximum number of columns to include in prompt\n\n    Returns:\n        Dict containing:\n            - questions: list of validated question objects\n            - metadata: Dictionary of processing information\n    \"\"\"\n    # Validate input\n    dictionary = sum(\n        [\n            DataDictionary.from_analyst_df(\n                ds.to_df(),\n                column_descriptions=f\"Column from dataset {ds.name}\",\n            ).column_descriptions\n            for ds in datasets\n        ],\n        [],\n    )\n\n    if len(dictionary) < 1:\n        raise ValueError(\"Dictionary DataFrame cannot be empty\")\n\n    # Limit columns for OpenAI prompt\n    total_columns = len(dictionary)\n    if total_columns > max_columns:\n        # Take first and last 20 columns\n        half_max = max_columns // 2\n        first_half = dictionary[:half_max]\n        last_half = dictionary[-half_max:]\n\n        # Remove any duplicates\n        dictionary = first_half + last_half\n\n        # deduplicate\n        dictionary = list({item.column: item for item in dictionary}.values())\n\n    # Convert dictionary to format expected by OpenAI\n    dict_data = {\n        \"columns\": [d.column for d in dictionary],\n        \"descriptions\": [d.description for d in dictionary],\n        \"data_types\": [d.data_type for d in dictionary],\n    }\n\n    # Create OpenAI messages\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\", content=prompts.SYSTEM_PROMPT_SUGGEST_A_QUESTION\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Dictionary:\\n{json.dumps(dict_data)}\"\n        ),\n    ]\n    async with AsyncLLMClient() as client:\n        completion: QuestionListGeneration = await client.chat.completions.create(\n            response_model=QuestionListGeneration,\n            model=ALTERNATIVE_LLM_SMALL,\n            messages=messages,\n        )\n\n    available_columns = dict_data[\"columns\"]\n    validated_questions: list[ValidatedQuestion] = []\n\n    for question in completion.questions:\n        validated_question = _validate_question_feasibility(question, available_columns)\n        if validated_question is not None:\n            validated_questions.append(validated_question)\n\n    return validated_questions\n\n\ndef find_imports(module: ModuleType) -> list[str]:\n    \"\"\"\n    Get top-level third-party imports from a Python module.\n\n    Args:\n        module: Python module object to analyze\n\n    Returns:\n        list of third-party package names\n\n    Example:\n        >>> import my_module\n        >>> imports = find_third_party_imports(my_module)\n        >>> print(imports)  # ['pandas', 'numpy', 'requests']\n    \"\"\"\n    try:\n        # Get the source code of the module\n        source = inspect.getsource(module)\n        tree = ast.parse(source)\n\n        stdlib_modules = set(sys.stdlib_module_names)\n        third_party = set()\n\n        # Only look at top-level imports\n        for node in tree.body:\n            if isinstance(node, ast.Import):\n                for name in node.names:\n                    module_name = name.name.split(\".\")[0]\n                    if module_name not in stdlib_modules:\n                        third_party.add(module_name)\n\n            elif isinstance(node, ast.ImportFrom):\n                if node.module is None:\n                    continue\n                module_name = node.module.split(\".\")[0]\n                if module_name not in stdlib_modules:\n                    third_party.add(module_name)\n\n        return sorted(third_party)\n    except Exception:\n        return []\n\n\ndef get_tools() -> list[Tool]:\n    try:\n        # find all functions defined in the tools module\n        tool_functions = [func for func in dir(tools) if callable(getattr(tools, func))]\n\n        # find the function signatures and doc strings\n        tools_list = []\n        for func_name in tool_functions:\n            func = getattr(tools, func_name)\n            signature = inspect.signature(func)\n            docstring = inspect.getdoc(func)\n            tools_list.append(\n                Tool(\n                    name=func_name,\n                    signature=str(signature),\n                    docstring=docstring,\n                    function=func,\n                )\n            )\n        return tools_list\n    except Exception:\n        return []\n\n\nasync def _generate_run_charts_python_code(\n    request: RunChartsRequest,\n    validation_error: InvalidGeneratedCode | None = None,\n) -> str:\n    df = request.dataset.to_df().to_pandas()\n    question = request.question\n    dataframe_metadata = {\n        \"shape\": {\"rows\": int(df.shape[0]), \"columns\": int(df.shape[1])},\n        \"statistics\": df.describe(include=\"all\").to_dict(),\n        \"dtypes\": df.dtypes.astype(str).to_dict(),\n    }\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\",\n            content=prompts.SYSTEM_PROMPT_PLOTLY_CHART,\n        ),\n        ChatCompletionUserMessageParam(role=\"user\", content=f\"Question: {question}\"),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Metadata:\\n{dataframe_metadata}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data top 25 rows:\\n{df.head(25).to_string()}\"\n        ),\n    ]\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n\n    # Get response based on model mode\n    async with AsyncLLMClient() as client:\n        response: CodeGeneration = await client.chat.completions.create(\n            response_model=CodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0,\n            messages=messages,\n        )\n    return response.code\n\n\nasync def _generate_run_analysis_python_code(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n    validation_error: InvalidGeneratedCode | None = None,\n    attempt: int = 0,\n) -> str:\n    \"\"\"\n    Generate Python analysis code based on JSON data and question.\n\n    Parameters:\n    - request: RunAnalysisRequest containing data and question\n    - validation_errors: Past validation errors to include in prompt\n\n    Returns:\n    - Generated code\n    \"\"\"\n    # Convert dictionary data structure to list of columns for all datasets\n    logger.info(\"Starting code gen\")\n\n    all_columns = []\n    all_descriptions = []\n    all_data_types = []\n\n    dictionaries = [\n        await analyst_db.get_data_dictionary(name) for name in request.dataset_names\n    ]\n    for dictionary in dictionaries:\n        if dictionary is None:\n            continue\n        for entry in dictionary.column_descriptions:\n            all_columns.append(f\"{dictionary.name}.{entry.column}\")\n            all_descriptions.append(entry.description)\n            all_data_types.append(entry.data_type)\n\n    # Create dictionary format for prompt\n    dictionary_data = {\n        \"columns\": all_columns,\n        \"descriptions\": all_descriptions,\n        \"data_types\": all_data_types,\n    }\n\n    # Get sample data and shape info for all datasets\n    all_samples = []\n    all_shapes = []\n\n    logger.debug(f\"datasets: {request.dataset_names}\")\n    for dataset_name in request.dataset_names:\n        try:\n            dataset = (await analyst_db.get_cleansed_dataset(dataset_name)).to_df()\n        except Exception:\n            dataset = (await analyst_db.get_dataset(dataset_name)).to_df()\n        all_shapes.append(\n            f\"{dataset_name}: {dataset.shape[0]} rows x {dataset.shape[1]} columns\"\n        )\n        # Limit sample to 10 rows\n        sample_df = dataset.head(10)\n        all_samples.append(f\"{dataset_name}:\\n{sample_df}\")\n\n    shape_info = \"\\n\".join(all_shapes)\n    sample_data = \"\\n\\n\".join(all_samples)\n    logger.debug(\"Assembling messages\")\n    # Create messages for OpenAI\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\", content=prompts.SYSTEM_PROMPT_PYTHON_ANALYST\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Business Question: {request.question}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Shapes:\\n{shape_info}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Sample Data:\\n{sample_data}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\",\n            content=f\"Data Dictionary:\\n{json.dumps(dictionary_data)}\",\n        ),\n    ]\n\n    tools_list = get_tools()\n    if len(tools_list) > 0:\n        messages.append(\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=\"If it helps the analysis, you can optionally use following functions:\\n\"\n                + \"\\n\".join([str(t) for t in tools_list]),\n            )\n        )\n\n    logger.debug(f\"total_characters: {len(''.join([str(msg) for msg in messages]))}\")\n    # Add error context if available\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n        if attempt > 2:\n            messages.append(\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Convert the dataframe to pandas!\",\n                )\n            )\n    logger.info(\"Running Code Gen\")\n    logger.debug(messages)\n    async with AsyncLLMClient() as client:\n        completion: CodeGeneration = await client.chat.completions.create(\n            response_model=CodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0.1,\n            messages=messages,\n            max_retries=10,\n        )\n    logger.info(\"Code Gen complete\")\n    return completion.code\n\n\nasync def cleanse_dataframe(dataset: AnalystDataset) -> CleansedDataset:\n    \"\"\"Clean and standardize multiple pandas DataFrames in parallel.\n\n    Args:\n        datasets: List of AnalystDataset objects to clean\n    Returns:\n        List of CleansedDataset objects containing cleaned data and reports\n    Raises:\n        ValueError: If a dataset is empty\n    \"\"\"\n\n    if dataset.to_df().is_empty():\n        raise ValueError(f\"Dataset {dataset.name} is empty\")\n\n    df = dataset.to_df()\n    sample_df = df.sample(min(100, len(df)))\n\n    results = []\n    for col in df.columns:\n        results.append(process_column(df, col, sample_df))\n\n    # Create new DataFrame from processed columns\n    new_columns = {}\n    reports = []\n\n    for new_name, series, report in results:\n        new_columns[new_name] = series\n        reports.append(report)\n\n    cleaned_df = pl.DataFrame(new_columns)\n    add_summary_statistics(cleaned_df, reports)\n\n    return CleansedDataset(\n        dataset=AnalystDataset(\n            name=dataset.name,\n            data=cleaned_df,\n        ),\n        cleaning_report=reports,\n    )\n\n\n@log_api_call\nasync def rephrase_message(messages: ChatRequest) -> str:\n    \"\"\"Process chat messages history and return a new question\n\n    Args:\n        messages: list of message dictionaries with 'role' and 'content' fields\n\n    Returns:\n        Dict[str, str]: Dictionary containing response content\n    \"\"\"\n    # Convert messages to string format for prompt\n    messages_str = \"\\n\".join(\n        [f\"{msg['role']}: {msg['content']}\" for msg in messages.messages]\n    )\n\n    prompt_messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            content=prompts.SYSTEM_PROMPT_REPHRASE_MESSAGE,\n            role=\"system\",\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Message History:\\n{messages_str}\",\n            role=\"user\",\n        ),\n    ]\n    async with AsyncLLMClient() as client:\n        completion: EnhancedQuestionGeneration = await client.chat.completions.create(\n            response_model=EnhancedQuestionGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            messages=prompt_messages,\n        )\n\n    return completion.enhanced_user_message\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_charts(\n    request: RunChartsRequest,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunChartsResult:\n    \"\"\"Generate and validate chart code with retry logic\"\"\"\n    # Create messages for OpenAI\n    start_time = datetime.now()\n\n    if not request.dataset:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    df = request.dataset.to_df().to_pandas()\n    if exception_history is None:\n        exception_history = []\n\n    code = await _generate_run_charts_python_code(\n        request, next(iter(exception_history[::-1]), None)\n    )\n    try:\n        result = execute_python(\n            modules={\n                \"pd\": pd,\n                \"np\": np,\n                \"go\": go,\n                \"pl\": pl,\n                \"scipy\": scipy,\n            },\n            functions={\n                \"make_subplots\": make_subplots,\n            },\n            expected_function=\"create_charts\",\n            code=code,\n            input_data=df,\n            output_type=ChartGenerationExecutionResult,\n            allowed_modules={\n                \"pandas\",\n                \"numpy\",\n                \"plotly\",\n                \"scipy\",\n                \"datetime\",\n                \"polars\",\n            },\n        )\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=code, exception=e)\n\n    duration = datetime.now() - start_time\n\n    return RunChartsResult(\n        status=\"success\",\n        code=code,\n        fig1_json=result.fig1.to_json(),\n        fig2_json=result.fig2.to_json(),\n        metadata=RunAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history) + 1,\n        ),\n    )\n\n\n@log_api_call\nasync def run_charts(request: RunChartsRequest) -> RunChartsResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    try:\n        chart_result = await _run_charts(request)\n        return chart_result\n    except ValidationError:\n        return RunChartsResult(\n            status=\"error\", metadata=RunAnalysisResultMetadata(duration=0, attempts=1)\n        )\n    except MaxReflectionAttempts as e:\n        return RunChartsResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n\n\n@log_api_call\nasync def get_business_analysis(\n    request: GetBusinessAnalysisRequest,\n) -> GetBusinessAnalysisResult:\n    \"\"\"\n    Generate business analysis based on data and question.\n\n    Parameters:\n    - request: BusinessAnalysisRequest containing data and question\n\n    Returns:\n    - Dictionary containing analysis components\n    \"\"\"\n    try:\n        # Convert JSON data to DataFrame for analysis\n        start = datetime.now()\n\n        df = request.dataset.to_df().to_pandas()\n\n        # Get first 1000 rows as CSV with quoted values for context\n        df_csv = df.head(750).to_csv(index=False, quoting=1)\n\n        # Create messages for OpenAI\n        messages: list[ChatCompletionMessageParam] = [\n            ChatCompletionSystemMessageParam(\n                role=\"system\", content=prompts.SYSTEM_PROMPT_BUSINESS_ANALYSIS\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=f\"Business Question: {request.question}\",\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Analyzed Data:\\n{df_csv}\"\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=f\"Data Dictionary:\\n{request.dictionary.model_dump_json()}\",\n            ),\n        ]\n        async with AsyncLLMClient() as client:\n            completion: BusinessAnalysisGeneration = (\n                await client.chat.completions.create(\n                    response_model=BusinessAnalysisGeneration,\n                    model=ALTERNATIVE_LLM_BIG,\n                    temperature=0.1,\n                    messages=messages,\n                )\n            )\n        duration = (datetime.now() - start).total_seconds()\n        # Ensure all response fields are present\n        metadata = GetBusinessAnalysisMetadata(\n            duration=duration,\n            question=request.question,\n            rows_analyzed=len(df),\n            columns_analyzed=len(df.columns),\n        )\n        return GetBusinessAnalysisResult(\n            status=\"success\",\n            **completion.model_dump(),\n            metadata=metadata,\n        )\n\n    except Exception as e:\n        msg = type(e).__name__ + f\": {str(e)}\"\n        logger.error(f\"Error in get_business_analysis: {msg}\")\n        return GetBusinessAnalysisResult(\n            status=\"error\",\n            metadata=GetBusinessAnalysisMetadata(exception_str=msg),\n            additional_insights=\"\",\n            follow_up_questions=[],\n            bottom_line=\"\",\n        )\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_analysis(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunAnalysisResult:\n    start_time = datetime.now()\n\n    if not request.dataset_names:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    if exception_history is None:\n        exception_history = []\n    logger.info(f\"Running analysis (attempt {len(exception_history)})\")\n    code = await _generate_run_analysis_python_code(\n        request,\n        analyst_db,\n        next(iter(exception_history[::-1]), None),\n        attempt=len(exception_history),\n    )\n    logger.info(\"Code generated, preparing execution\")\n    dataframes: dict[str, pl.DataFrame] = {}\n\n    for dataset_name in request.dataset_names:\n        try:\n            dataset = (\n                await analyst_db.get_cleansed_dataset(dataset_name, max_rows=None)\n            ).to_df()\n        except Exception:\n            dataset = (\n                await analyst_db.get_dataset(dataset_name, max_rows=None)\n            ).to_df()\n        dataframes[dataset_name] = dataset\n    functions = {}\n    tool_functions = get_tools()\n    for tool in tool_functions:\n        functions[tool.name] = tool.function\n    try:\n        logger.info(\"Executing\")\n        result = execute_python(\n            modules={\n                \"pd\": pd,\n                \"np\": np,\n                \"sm\": sm,\n                \"pl\": pl,\n                \"scipy\": scipy,\n                \"sklearn\": sklearn,\n            },\n            functions=functions,\n            expected_function=\"analyze_data\",\n            code=code,\n            input_data=dataframes,\n            output_type=AnalystDataset,\n            allowed_modules={\n                \"pandas\",\n                \"numpy\",\n                \"scipy\",\n                \"sklearn\",\n                \"statsmodels\",\n                \"datetime\",\n                \"polars\",\n                *find_imports(tools),\n            },\n        )\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=code, exception=e)\n    logger.info(\"Execution done\")\n    duration = datetime.now() - start_time\n    return RunAnalysisResult(\n        status=\"success\",\n        code=code,\n        dataset=result,\n        metadata=RunAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history) + 1,\n            datasets_analyzed=len(dataframes),\n            total_rows_analyzed=sum(\n                len(df) for df in dataframes.values() if not df.is_empty()\n            ),\n            total_columns_analyzed=sum(\n                len(df.columns) for df in dataframes.values() if not df.is_empty()\n            ),\n        ),\n    )\n\n\n@log_api_call\nasync def run_analysis(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n) -> RunAnalysisResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    logger.debug(\"Entering run_analysis\")\n    log_memory()\n    try:\n        return await _run_analysis(request, analyst_db=analyst_db)\n    except MaxReflectionAttempts as e:\n        return RunAnalysisResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n    except ValueError as e:\n        return RunAnalysisResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=0,\n                attempts=1,\n                exception=AnalysisError.from_value_error(e),\n            ),\n        )\n\n\nasync def _generate_database_analysis_code(\n    request: RunDatabaseAnalysisRequest,\n    analyst_db: AnalystDB,\n    validation_error: InvalidGeneratedCode | None = None,\n) -> str:\n    \"\"\"\n    Generate Snowflake SQL analysis code based on data samples and question.\n\n    Parameters:\n    - request: DatabaseAnalysisRequest containing data samples and question\n\n    Returns:\n    - Dictionary containing generated code and description\n    \"\"\"\n\n    # Convert dictionary data structure to list of columns for all tables\n    dictionaries = [\n        await analyst_db.get_data_dictionary(name) for name in request.dataset_names\n    ]\n    all_tables_info = [d.model_dump(mode=\"json\") for d in dictionaries if d is not None]\n\n    # Get sample data for all tables\n    all_samples = []\n\n    for table in request.dataset_names:\n        df = (await analyst_db.get_dataset(table)).to_df().to_pandas()\n\n        sample_str = f\"Table: {table}\\n{df.head(10).to_string()}\"\n        all_samples.append(sample_str)\n\n    # Create messages for OpenAI\n    messages: list[ChatCompletionMessageParam] = [\n        get_external_database().get_system_prompt(),\n        ChatCompletionUserMessageParam(\n            content=f\"Business Question: {request.question}\",\n            role=\"user\",\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Sample Data:\\n{chr(10).join(all_samples)}\", role=\"user\"\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Data Dictionary:\\n{json.dumps(all_tables_info)}\", role=\"user\"\n        ),\n    ]\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n\n    # Get response from OpenAI\n    async with AsyncLLMClient() as client:\n        completion = await client.chat.completions.create(\n            response_model=DatabaseAnalysisCodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0.1,\n            messages=messages,\n        )\n\n    return completion.code\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_database_analysis(\n    request: RunDatabaseAnalysisRequest,\n    analyst_db: AnalystDB,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunDatabaseAnalysisResult:\n    start_time = datetime.now()\n    if not request.dataset_names:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    if exception_history is None:\n        exception_history = []\n\n    sql_code = await _generate_database_analysis_code(\n        request, analyst_db, next(iter(exception_history[::-1]), None)\n    )\n    try:\n        results = get_external_database().execute_query(query=sql_code)\n        results = cast(list[dict[str, Any]], results)\n        duration = datetime.now() - start_time\n\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=sql_code, exception=e)\n    return RunDatabaseAnalysisResult(\n        status=\"success\",\n        code=sql_code,\n        dataset=AnalystDataset(\n            data=results,\n        ),\n        metadata=RunDatabaseAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history),\n            datasets_analyzed=len(request.dataset_names),\n            # total_columns_analyzed=sum(len(ds.columns) for ds in request.datasets),\n        ),\n    )\n\n\n@log_api_call\nasync def run_database_analysis(\n    request: RunDatabaseAnalysisRequest, analyst_db: AnalystDB\n) -> RunDatabaseAnalysisResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    try:\n        return await _run_database_analysis(request, analyst_db)\n    except MaxReflectionAttempts as e:\n        return RunDatabaseAnalysisResult(\n            status=\"error\",\n            metadata=RunDatabaseAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n    except ValueError as e:\n        return RunDatabaseAnalysisResult(\n            status=\"error\",\n            metadata=RunDatabaseAnalysisResultMetadata(\n                duration=0,\n                attempts=1,\n                exception=AnalysisError.from_value_error(e),\n            ),\n        )\n\n\n# Type definitions\n@dataclass\nclass AnalysisGenerationError:\n    message: str\n    original_error: BaseException | None = None\n\n\nasync def execute_business_analysis_and_charts(\n    analysis_result: RunAnalysisResult | RunDatabaseAnalysisResult,\n    enhanced_message: str,\n    enable_chart_generation: bool = True,\n    enable_business_insights: bool = True,\n) -> tuple[\n    RunChartsResult | BaseException | None,\n    GetBusinessAnalysisResult | BaseException | None,\n]:\n    analysis_result.dataset = cast(AnalystDataset, analysis_result.dataset)\n    # Prepare both requests\n    chart_request = RunChartsRequest(\n        dataset=analysis_result.dataset,\n        question=enhanced_message,\n    )\n\n    business_request = GetBusinessAnalysisRequest(\n        dataset=analysis_result.dataset,\n        dictionary=DataDictionary.from_analyst_df(analysis_result.dataset.to_df()),\n        question=enhanced_message,\n    )\n\n    if enable_chart_generation and enable_business_insights:\n        # Run both analyses concurrently\n        result = await asyncio.gather(\n            run_charts(chart_request),\n            get_business_analysis(business_request),\n            return_exceptions=True,\n        )\n\n        return (result[0], result[1])\n    elif enable_chart_generation:\n        charts_result = await run_charts(chart_request)\n        return charts_result, None\n    else:\n        business_result = await get_business_analysis(business_request)\n        return None, business_result\n\n\nasync def run_complete_analysis(\n    chat_request: ChatRequest,\n    data_source: DataSourceType,\n    datasets_names: list[str],\n    analyst_db: AnalystDB,\n    chat_id: str,\n    message_id: str,\n    enable_chart_generation: bool = True,\n    enable_business_insights: bool = True,\n) -> AsyncGenerator[Component | AnalysisGenerationError, None]:\n    user_message = await analyst_db.get_chat_message(message_id=message_id)\n    if user_message is None or user_message.role != \"user\":\n        yield AnalysisGenerationError(\"Message not found\")\n\n        return\n    # Get enhanced message\n    try:\n        logger.info(\"Getting rephrased question...\")\n        enhanced_message = await rephrase_message(chat_request)\n        logger.info(\"Getting rephrased question done\")\n\n        yield enhanced_message\n\n    except ValidationError:\n        user_message.error = \"LLM Error, please retry\"\n        user_message.in_progress = False\n        await analyst_db.update_chat_message(\n            message_id=message_id,\n            message=user_message,\n        )\n        yield AnalysisGenerationError(user_message.error)\n\n        return\n\n    assistant_message = AnalystChatMessage(\n        role=\"assistant\",\n        content=enhanced_message,\n        components=[EnhancedQuestionGeneration(enhanced_user_message=enhanced_message)],\n    )\n\n    user_message.in_progress = False\n    await analyst_db.update_chat_message(\n        message_id=message_id,\n        message=user_message,\n    )\n    await analyst_db.add_chat_message(chat_id=chat_id, message=assistant_message)\n    # Run main analysis\n    logger.info(\"Start main analysis\")\n    try:\n        is_database = data_source == DataSourceType.DATABASE\n        logger.info(\"Getting analysis result...\")\n        log_memory()\n\n        if is_database:\n            analysis_result: (\n                RunAnalysisResult | RunDatabaseAnalysisResult\n            ) = await run_database_analysis(\n                RunDatabaseAnalysisRequest(\n                    dataset_names=datasets_names,\n                    question=enhanced_message,\n                ),\n                analyst_db,\n            )\n        else:\n            analysis_result = await run_analysis(\n                RunAnalysisRequest(\n                    dataset_names=datasets_names,\n                    question=enhanced_message,\n                ),\n                analyst_db,\n            )\n\n        log_memory()\n        logger.info(\"Getting analysis result done\")\n\n        if isinstance(analysis_result, BaseException):\n            error_message = f\"Error running initial analysis. Try rephrasing: {str(analysis_result)}\"\n            assistant_message.in_progress = False\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n            return\n\n        yield analysis_result\n\n        assistant_message.components.append(analysis_result)\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n    except Exception as e:\n        error_message = f\"Error running initial analysis. Try rephrasing: {str(e)}\"\n        assistant_message.in_progress = False\n        assistant_message.error = error_message\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n        yield AnalysisGenerationError(error_message)\n\n        return\n\n    # Only proceed with additional analysis if we have valid initial results\n    if not (\n        analysis_result\n        and analysis_result.dataset\n        and (enable_chart_generation or enable_business_insights)\n    ):\n        assistant_message.in_progress = False\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n        return\n\n    # Run concurrent analyses\n    try:\n        charts_result, business_result = await execute_business_analysis_and_charts(\n            analysis_result,\n            enhanced_message,\n            enable_business_insights=enable_business_insights,\n            enable_chart_generation=enable_chart_generation,\n        )\n\n        # Handle chart results\n        if isinstance(charts_result, BaseException):\n            error_message = \"Error generating charts\"\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n        elif charts_result is not None:\n            assistant_message.components.append(charts_result)\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield charts_result\n\n        # Handle business analysis results\n        if isinstance(business_result, BaseException):\n            error_message = \"Error generating business insights\"\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n        elif business_result is not None:\n            assistant_message.components.append(business_result)\n            assistant_message.in_progress = False\n\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield business_result\n\n    except Exception as e:\n        error_message = f\"Error setting up additional analysis: {str(e)}\"\n        assistant_message.in_progress = False\n        assistant_message.error = error_message\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n        yield AnalysisGenerationError(error_message)\n\n\nasync def process_data_and_update_state(\n    new_dataset_names: list[str],\n    analyst_db: AnalystDB,\n    data_source: str | DataSourceType,\n) -> AsyncGenerator[str, None]:\n    \"\"\"Process datasets and yield progress updates asynchronously.\"\"\"\n    # Start processing and yield initial message\n    logger.info(\"Starting data processing\")\n    log_memory()\n    yield \"Starting data processing\"\n\n    # Handle data cleansing based on the source\n    # Convert string data_source to DataSourceType if needed\n    data_source_type = (\n        data_source\n        if isinstance(data_source, DataSourceType)\n        else DataSourceType(data_source)\n    )\n    if data_source_type != DataSourceType.DATABASE:\n        try:\n            logger.info(\"Cleansing datasets\")\n            yield \"Cleansing datasets\"\n            for analysis_dataset_name in new_dataset_names:\n                analysis_dataset = await analyst_db.get_dataset(\n                    analysis_dataset_name, max_rows=None\n                )\n                cleansed_dataset = await cleanse_dataframe(analysis_dataset)\n                await analyst_db.register_dataset(\n                    cleansed_dataset, data_source=DataSourceType.GENERATED\n                )\n                yield f\"Cleansed dataset: {analysis_dataset_name}\"\n                del cleansed_dataset\n                del analysis_dataset\n                log_memory()\n\n            logger.info(\"Cleansing datasets complete\")\n            yield \"Cleansing datasets complete\"\n            log_memory()\n        except Exception:\n            logger.error(\"Data processing failed\", exc_info=True)\n            yield \"Data processing failed\"\n            raise\n    else:\n        pass\n\n    # Generate data dictionaries\n    logger.info(\"Data processing successful, generating dictionaries\")\n    yield \"Data processing successful, generating dictionaries\"\n    log_memory()\n    try:\n        for analysis_dataset_name in new_dataset_names:\n            try:\n                existing_dictionary = await analyst_db.get_data_dictionary(\n                    analysis_dataset_name\n                )\n                logger.info(\n                    f\"Found existing dictionary for dataset: {analysis_dataset_name}\"\n                )\n                if existing_dictionary is not None:\n                    continue\n\n            except Exception:\n                pass\n            logger.info(f\"Creating dictionary for dataset: {analysis_dataset_name}\")\n            analysis_dataset = await analyst_db.get_dataset(analysis_dataset_name)\n            new_dictionary = await get_dictionary(analysis_dataset)\n            logger.info(new_dictionary.to_application_df())\n            del analysis_dataset\n            await analyst_db.register_data_dictionary(new_dictionary)\n            logger.info(f\"Registered dictionary for dataset: {analysis_dataset_name}\")\n            yield f\"Registered data dictionary: {analysis_dataset_name}\"\n            log_memory()\n            continue\n    except Exception:\n        logger.error(\"Failed to generate data dictionaries\", exc_info=True)\n        yield \"Failed to generate data dictionaries\"\n        raise\n    log_memory()\n    # Final completion message\n    yield \"Processing complete\"\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/utils/api.py",
    "size": 55548,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "openai",
      "streamlit",
      "datarobot",
      "deployment",
      "llm"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "utils/tools.py",
    "file_name": "tools.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Add additional tools that can be used by the analysis code execution. Remember to include the necessary imports and provide a docstring and signature for each function.\n# signature and docstring will be provided to the LLM in the prompt.\n# Uncomment the examples below to get started.\n\n\n# import datarobot as dr\n# import pandas as pd\n# from datarobot_predict.deployment import predict\n\n# def calculate_summary(df: pd.DataFrame) -> pd.DataFrame:\n#     \"\"\"\n#     Calculate summary statistics for a DataFrame.\n\n#     Args:\n#         df (pd.DataFrame): Input DataFrame.\n\n#     Returns:\n#         pd.DataFrame: Summary statistics including count, mean, std, min, max, and percentiles.\n#     \"\"\"\n#     description = df.describe(percentiles=[0.2, 0.4, 0.6, 0.8])\n#     return description\n\n\n# def filter_data(df: pd.DataFrame, column: str, value: float) -> pd.DataFrame:\n#     \"\"\"\n#     Filter DataFrame based on a condition.\n#     Args:\n#         df (pd.DataFrame): Input DataFrame.\n#         column (str): Column to apply the filter on.\n#         value (float): Value to compare against in the filter.\n#     Returns:\n#         pd.DataFrame: Filtered DataFrame where the specified column's values are greater than the given value.\n#     \"\"\"\n#     filtered_df = df[df[column] > value]\n#     return filtered_df\n\n\n# def call_datarobot_deployment(df: pd.DataFrame, deployment_id: str) -> pd.DataFrame:\n#     \"\"\"\n#     Call a DataRobot deployment to get predictions.\n\n#     Args:\n#         df (pd.DataFrame): Input DataFrame with features for prediction.\n#         deployment_id (str): ID of the DataRobot deployment to use for predictions.\n\n#     Returns:\n#         pd.DataFrame: DataFrame containing the predictions from DataRobot. The prediction column is named 'predictions'.\n#     \"\"\"\n#     deployment = dr.Deployment.get(deployment_id)  # type: ignore[attr-defined]\n#     prediction_response: pd.DataFrame = predict(\n#         deployment=deployment, data_frame=df\n#     ).dataframe\n\n#     prediction_response.columns = [\n#         c.replace(\"_PREDICTION\", \"\")\n#         for c in prediction_response.columns  # type: ignore[assignment]\n#     ]\n\n#     if deployment.model is not None:\n#         target_column = deployment.model.get(\"target_name\")\n#         if target_column:\n#             prediction_response[\"predictions\"] = prediction_response[target_column]\n\n#     return prediction_response\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/utils/tools.py",
    "size": 2968,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "deployment",
      "datarobot",
      "llm"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "infra/settings_generative.py",
    "file_name": "settings_generative.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import annotations\n\nimport datarobot as dr\nimport pulumi\nimport pulumi_datarobot as datarobot\nfrom datarobot_pulumi_utils.pulumi.stack import PROJECT_NAME\nfrom datarobot_pulumi_utils.schema.custom_models import (\n    CustomModelArgs,\n    DeploymentArgs,\n    RegisteredModelArgs,\n)\nfrom datarobot_pulumi_utils.schema.exec_envs import RuntimeEnvironments\nfrom datarobot_pulumi_utils.schema.llms import (\n    LLMBlueprintArgs,\n    LLMs,\n    LLMSettings,\n    PlaygroundArgs,\n)\n\nfrom utils.schema import LLMDeploymentSettings\n\nLLM = LLMs.AZURE_OPENAI_GPT_4_O\n\ncustom_model_args = CustomModelArgs(\n    resource_name=f\"Generative Analyst Custom Model [{PROJECT_NAME}]\",\n    name=\"Generative Analyst Assistant\",  # built-in QA app uses this as the AI's name\n    target_name=LLMDeploymentSettings().target_feature_name,\n    target_type=dr.enums.TARGET_TYPE.TEXT_GENERATION,\n    replicas=2,\n    base_environment_id=RuntimeEnvironments.PYTHON_312_MODERATIONS.value.id,\n    opts=pulumi.ResourceOptions(delete_before_replace=True),\n)\n\nregistered_model_args = RegisteredModelArgs(\n    resource_name=f\"Generative Analyst Registered Model [{PROJECT_NAME}]\",\n)\n\n\ndeployment_args = DeploymentArgs(\n    resource_name=f\"Generative Analyst Deployment [{PROJECT_NAME}]\",\n    label=f\"Generative Analyst Deployment [{PROJECT_NAME}]\",\n    association_id_settings=datarobot.DeploymentAssociationIdSettingsArgs(\n        column_names=[\"association_id\"],\n        auto_generate_id=False,\n        required_in_prediction_requests=True,\n    ),\n    predictions_data_collection_settings=datarobot.DeploymentPredictionsDataCollectionSettingsArgs(\n        enabled=True,\n    ),\n    predictions_settings=(\n        datarobot.DeploymentPredictionsSettingsArgs(min_computes=0, max_computes=2)\n    ),\n)\n\nplayground_args = PlaygroundArgs(\n    resource_name=f\"Generative Analyst Playground [{PROJECT_NAME}]\",\n)\n\nllm_blueprint_args = LLMBlueprintArgs(\n    resource_name=f\"Generative Analyst LLM Blueprint [{PROJECT_NAME}]\",\n    llm_id=LLM.name,\n    llm_settings=LLMSettings(\n        max_completion_length=2048,\n        temperature=0.1,\n    ),\n)\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/infra/settings_generative.py",
    "size": 2692,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "deployment",
      "datarobot",
      "llm",
      "openai"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "README.md",
    "file_name": "README.md",
    "file_type": "markdown",
    "content": "# Talk to My Data  \n\n**Talk to My Data** delivers a seamless **talk-to-your-data** experience, transforming files, spreadsheets, and cloud data into actionable insights. Simply upload data, connect to Snowflake or BigQuery, or access datasets from DataRobot's Data Registry. Then, ask a question, and the agent recommends business analyses, generating **charts, tables, and even code** to help you interpret the results.\n\nThis intuitive experience is designed for **scalability and flexibility**, ensuring that whether you're working with a few thousand rows or billions, your data analysis remains **fast, efficient, and insightful**.\n\n\n> [!WARNING]\n> Application templates are intended to be starting points that provide guidance on how to develop, serve, and maintain AI applications.\n> They require a developer or data scientist to adapt and modify them for their business requirements before being put into production.\n\n![Using the \"Talk to My Data\" agent](https://s3.us-east-1.amazonaws.com/datarobot_public/drx/recipe_gifs/launch_gifs/talktomydata.gif)\n\n\n## Table of contents\n1. [Setup](#setup)\n2. [Architecture overview](#architecture-overview)\n3. [Why build AI Apps with DataRobot app templates?](#why-build-ai-apps-with-datarobot-app-templates)\n4. [Data privacy](#data-privacy)\n5. [Make changes](#make-changes)\n   - [Change the frontend](#change-the-frontend)\n   - [Change the LLM](#change-the-llm)\n   - [Change the database](#change-the-database)\n      * [Snowflake](#snowflake)\n      * [BigQuery](#bigquery)\n6. [Tools](#tools)\n7. [Share results](#share-results)\n8. [Delete all provisioned resources](#delete-all-provisioned-resources)\n9. [Setup for advanced users](#setup-for-advanced-users)\n\n## Setup\nPlease check out this [Talk To My Data walkthrough](https://docs.datarobot.com/en/docs/get-started/gs-dr5/talk-data-walk.html).\n\nBefore proceeding, ensure you have access to the required credentials and services. This template is pre-configured to use an Azure OpenAI endpoint and Snowflake Database credentials. To run the template as-is, you will need access to Azure OpenAI (leverages `gpt-4o` by default).\n\n**DataRobot Codespaces users:** If you opened this template from the [Application Templates gallery](https://docs.datarobot.com/en/docs/workbench/wb-apps/app-templates/index.html#application-templates), you can **skip steps 1 and 2**. If you created a fresh codespace, you can **skip step 1** but still need to **clone the repository (step 2)**.\n\n**For local development,** follow all of the following steps:\n\n1. If `pulumi` is not already installed, install the CLI following instructions [here](https://www.pulumi.com/docs/iac/download-install/).\n   After installing for the first time, restart your terminal and run:\n\n   ```bash\n   pulumi login --local  # omit --local to use Pulumi Cloud (requires separate account)\n   ```\n\n2. Clone the template repository\n\n   ```bash\n   git clone https://github.com/datarobot-community/talk-to-my-data-agent.git\n   cd talk-to-my-data-agent\n   ```\n\n3. Rename the file `.env.template` to `.env` in the root directory of the repo and populate your credentials.\n\n   ```bash\n   DATAROBOT_API_TOKEN=...\n   DATAROBOT_ENDPOINT=...  # e.g. https://app.datarobot.com/api/v2\n   OPENAI_API_KEY=...\n   OPENAI_API_VERSION=...  # e.g. 2024-02-01\n   OPENAI_API_BASE=...  # e.g. https://your_org.openai.azure.com/\n   OPENAI_API_DEPLOYMENT_ID=...  # e.g. gpt-4o\n   PULUMI_CONFIG_PASSPHRASE=...  # Required. Choose your own alphanumeric passphrase to be used for encrypting pulumi config\n   FRONTEND_TYPE=...  # Optional. Default is \"react\", set to \"streamlit\" to use Streamlit frontend\n   USE_DATAROBOT_LLM_GATEWAY=...  # Optional. Set to \"true\" to use DataRobot LLM Gateway with consumption based pricing instead of using your own LLM credentials\n   ```\n\n   Use the following resources to locate the required credentials:\n   - **DataRobot API Token**: Refer to the *Create a DataRobot API Key* section of the [DataRobot API Quickstart docs](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#create-a-datarobot-api-key).\n   - **DataRobot Endpoint**: Refer to the *Retrieve the API Endpoint* section of the same [DataRobot API Quickstart docs](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#retrieve-the-api-endpoint).\n   - **LLM Endpoint and API Key**: Refer to the [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cjavascript-keyless%2Ctypescript-keyless%2Cpython-new&pivots=programming-language-python#retrieve-key-and-endpoint).\n\n4. In a terminal, run:\n\n   ```bash\n   python quickstart.py YOUR_PROJECT_NAME  # Windows users may have to use `py` instead of `python`\n   ```\n   \n   **What does `quickstart.py` do?**\n   \n   The quickstart script automates the entire setup process for you:\n   - Creates and activates a Python virtual environment\n   - Installs all required dependencies (using `uv` for faster installation, falling back to `pip`)\n   - Loads your `.env` configuration\n   - Sets up the Pulumi stack with your project name\n   - Runs `pulumi up` to deploy your application\n   - Displays your application URL when complete\n   \n   This single command replaces all the manual steps described in the [advanced setup section](#setup-for-advanced-users).\n   \n   Python 3.10 - 3.12 are supported\n\n\nAdvanced users desiring control over virtual environment creation, dependency installation, environment variable setup\nand `pulumi` invocation see [here](#setup-for-advanced-users).\n\n\n## Template development\n\nThe Talk to My Data agent supports two frontend options:\n- **React** (default): a modern JavaScript-based frontend with enhanced UI features which uses [FastAPI Backend](app_backend/README.md). See the [React Frontend Development Guide](app_frontend/README.md)\n- **Streamlit:** A Python-based frontend with a simple interface. See the [Streamlit Frontend Development Guide](frontend/README.md)\n\nTo change the frontend:\n1. In `.env`: Set `FRONTEND_TYPE=\"streamlit\"` to use the Streamlit frontend instead of the default React.\n2. Run the following to update your stack (Or run `python quickstart.py` for easier setup)\n   ```bash\n   source set_env.sh  # On Windows use `set_env.bat`\n   pulumi up\n   ```\n\n## Architecture overview\n\n![image](https://s3.us-east-1.amazonaws.com/datarobot_public/drx/ttmd2-schematic.jpg)\n\n\nApp templates contain three families of complementary logic:\n\n- **AI logic**: Necessary to service AI requests and produce predictions and completions.\n  ```\n  deployment_*/  # Chat agent model\n  ```\n- **App Logic**: Necessary for user consumption; whether via a hosted front-end or integrating into an external consumption layer.\n  ```\n  frontend/  # Streamlit frontend\n  app_frontend/  # React frontend alternative with the api located in app_backend\n  utils/  # App business logic & runtime helpers\n  ```\n- **Operational Logic**: Necessary to activate DataRobot assets.\n  ```\n  infra/__main__.py  # Pulumi program for configuring DataRobot to serve and monitor AI and app logic\n  infra/  # Settings for resources and assets created in DataRobot\n  ```\n\n## Why build AI Apps with DataRobot app templates?\n\nApp Templates transform your AI projects from notebooks to production-ready applications. Too often, getting models into production means rewriting code, juggling credentials, and coordinating with multiple tools and teams just to make simple changes. DataRobot's composable AI apps framework eliminates these bottlenecks, letting you spend more time experimenting with your ML and app logic and less time wrestling with plumbing and deployment.\n- Start building in minutes: Deploy complete AI applications instantly, then customize the AI logic or the front-end independently (no architectural rewrites needed).\n- Keep working your way: Data scientists keep working in notebooks, developers in IDEs, and configs stay isolated. Update any piece without breaking others.\n- Iterate with confidence: Make changes locally and deploy with confidence. Spend less time writing and troubleshooting plumbing and more time improving your app.\n\nEach template provides an end-to-end AI architecture, from raw inputs to deployed application, while remaining highly customizable for specific business requirements.\n\n## Data privacy\nYour data privacy is important to us. Data handling is governed by the DataRobot [Privacy Policy](https://www.datarobot.com/privacy/), please review before using your own data with DataRobot.\n\n\n## Make changes\n\n### Change the LLM\n\n1. Modify the `LLM` setting in `infra/settings_generative.py` by changing `LLM=LLMs.AZURE_OPENAI_GPT_4_O` to any other LLM from the `LLMs` object.\n     - Trial users: Please set `LLM=LLMs.AZURE_OPENAI_GPT_4_O_MINI` since GPT-4o is not supported in the trial. Use the `OPENAI_API_DEPLOYMENT_ID` in `.env` to override which model is used in your Azure organization. You'll still see GPT 4o-mini in the playground, but the deployed app will use the provided Azure deployment.\n2. To use an existing TextGen model or deployment:\n      - In `infra/settings_generative.py`: Set `LLM=LLMs.DEPLOYED_LLM`.\n      - In `.env`: Set either the `TEXTGEN_REGISTERED_MODEL_ID` or the `TEXTGEN_DEPLOYMENT_ID`\n      - In `.env`: Set `CHAT_MODEL_NAME` to the model name expected by the deployment (e.g. \"claude-3-7-sonnet-20250219\" for an anthropic deployment,\"datarobot-deployed-llm\" for NIM models )\n      - (Optional) In `utils/api.py`: `ALTERNATIVE_LLM_BIG` and `ALTERNATIVE_LLM_SMALL` can be used for fine-grained control over which LLM is used for different tasks.\n\n### Use [DataRobot LLM Gateway](https://docs.datarobot.com/en/docs/gen-ai/genai-code/dr-llm-gateway.html)\n\nThe application supports using the DataRobot LLM Gateway instead of bringing your own LLM credentials.\n\n#### **Credential Priority**\n\nThe application follows this priority order for LLM selection:\n\n1. **OpenAI Credentials** (Highest Priority) - If `OPENAI_API_KEY`, `OPENAI_API_BASE`, etc. are provided in `.env`, they will always be used regardless of the `USE_DATAROBOT_LLM_GATEWAY` setting\n2. **LLM Gateway** - If `USE_DATAROBOT_LLM_GATEWAY=true` and no OpenAI credentials are provided\n\n#### **Setup**\n\n**Important**: Remove or comment out `OPENAI_*` environment variables to use DataRobot's LLM Gateway\n\n1. In `.env`: Set `USE_DATAROBOT_LLM_GATEWAY=true` \n2. Run `pulumi up` to update your stack (Or run `python quickstart.py` for easier setup)\n   ```bash\n   source set_env.sh  # On Windows use `set_env.bat`\n   pulumi up\n   ```\n\n#### **When LLM Gateway is enabled:**\n- No hardcoded LLM credentials (OpenAI keys) are required in your `.env` file\n- The LLM Gateway provides a unified interface to multiple LLM providers through DataRobot in production\n- You can pick from the catalog and change the model `LLM` in `infra/settings_generative.py`\n- It will use a DataRobot Guarded RAG Deployment and LLM Blueprint for that selected model\n\n**Note**: LLM Gateway mode requires consumption based pricing is enabled for your DataRobot account as is evidenced by the `ENABLE_LLM_GATEWAY` feature flag.\nContact your administrator if this feature is not available.\n\n1. In `.env`: If not using an existing TextGen model or deployment, provide the required credentials dependent on your choice.\n2. Run `pulumi up` to update your stack (Or run `python quickstart.py` for easier setup)\n      ```bash\n      source set_env.sh  # On Windows use `set_env.bat`\n      pulumi up\n      ```\n\n> **⚠️ Availability information:**\n> Using a NIM model requires custom model GPU inference, a premium feature. You will experience errors by using this type of model without the feature enabled. Contact your DataRobot representative or administrator for information on enabling this feature.\n\n### Change the database\n\n#### Snowflake\n\nTo add Snowflake support:\n\n1. Modify the `DATABASE_CONNECTION_TYPE` setting in `infra/settings_database.py` by changing `DATABASE_CONNECTION_TYPE = \"no_database\"` to `DATABASE_CONNECTION_TYPE = \"snowflake\"`.\n2. Provide snowflake credentials in `.env` by either setting `SNOWFLAKE_USER` and `SNOWFLAKE_PASSWORD` or by setting `SNOWFLAKE_KEY_PATH` to a file containing the key. The key file should be a `*.p8` private key file. (see [Snowflake Documentation](https://docs.snowflake.com/en/user-guide/key-pair-auth))\n3. Fill out the remaining snowflake connection settings in `.env` (refer to `.env.template` for more details)\n4. Run `pulumi up` to update your stack (Or run `python quickstart.py` for easier setup)\n      ```bash\n      source set_env.sh  # On Windows use `set_env.bat`\n      pulumi up\n      ```\n \n#### BigQuery\n\nThe Talk to my Data Agent supports connecting to BigQuery.\n1. Modify the `DATABASE_CONNECTION_TYPE` setting in `infra/settings_database.py` by changing `DATABASE_CONNECTION_TYPE = \"no_database\"` to `DATABASE_CONNECTION_TYPE = \"bigquery\"`. \n2. Provide the required google credentials in `.env` dependent on your choice.  Ensure that GOOGLE_DB_SCHEMA is also populated in `.env`.\n3. Run `pulumi up` to update your stack (Or run `python quickstart.py` for easier setup)\n      ```bash\n      source set_env.sh  # On Windows use `set_env.bat`\n      pulumi up\n      ```\n\n#### SAP Datasphere\n\nThe Talk to my Data Agent supports connecting to SAP Datasphere.\n1. Modify the `DATABASE_CONNECTION_TYPE` setting in `infra/settings_database.py` by changing `DATABASE_CONNECTION_TYPE = \"no_database\"` to `DATABASE_CONNECTION_TYPE = \"sap\"`. \n2. Provide the required SAP credentials in `.env`.\n3. Run `pulumi up` to update your stack (Or run `python quickstart.py` for easier setup)\n      ```bash\n      source set_env.sh  # On Windows use `set_env.bat`\n      pulumi up\n      ```\n\n## Tools\n\nYou can help the data analyst python agent by providing tools that can assist with data analysis tasks. For that, define functions in `utils/tools.py`. The function will be made available inside the code execution environment of the agent. The name, docstring and signature will be provided to the agent inside the prompt.\n\n## Share results\n\n1. Log into the DataRobot application.\n2. Navigate to **Registry > Applications**.\n3. Navigate to the application you want to share, open the actions menu, and select **Share** from the dropdown.\n\n## Delete all provisioned resources\n\n```bash\npulumi down\n```\n\n## Setup for advanced users\nFor manual control over the setup process adapt the following steps for MacOS/Linux to your environment:\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nsource set_env.sh\npulumi stack init YOUR_PROJECT_NAME\npulumi up \n```\ne.g. for Windows/conda/cmd.exe this would be:\n```bash\nconda create --prefix .venv pip\nconda activate .\\.venv\npip install -r requirements.txt\nset_env.bat\npulumi stack init YOUR_PROJECT_NAME\npulumi up\n```\nFor projects that will be maintained, DataRobot recommends forking the repo so upstream fixes and improvements can be merged in the future.\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/README.md",
    "size": 14936,
    "description": "Markdown: README",
    "tags": []
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "infra/__init__.py",
    "file_name": "__init__.py",
    "file_type": "python",
    "content": "",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/infra/__init__.py",
    "size": 0,
    "description": "Python: __init__",
    "tags": []
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "utils/__init__.py",
    "file_name": "__init__.py",
    "file_type": "python",
    "content": "",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/utils/__init__.py",
    "size": 0,
    "description": "Python: __init__",
    "tags": []
  },
  {
    "repo_name": "guarded-rag-assistant",
    "file_path": "README.md",
    "file_name": "README.md",
    "file_type": "markdown",
    "content": "# Guarded RAG Assistant\n\nThe guarded RAG assistant is an easily customizable recipe for building a RAG-powered chatbot. \n\nIn addition to creating a hosted, shareable user interface, the guarded RAG assistant provides:\n\n* Business logic and LLM-based guardrails.\n* A predictive secondary model that evaluates response quality.\n* GenAI-focused [custom metrics][custom-metrics].\n* DataRobot MLOps hosting, monitoring, and governing the individual back-end deployments.\n\n> [!WARNING]\n> Application templates are intended to be starting points that provide guidance on how to develop, serve, and maintain AI applications.\n> They require a developer or data scientist to adapt and modify them for their business requirements before being put into production.\n\n![Using the Guarded RAG Assistant](https://s3.amazonaws.com/datarobot_public/drx/recipe_gifs/launch_gifs/guardedraghq-small.gif)\n\n[custom-metrics]: https://docs.datarobot.com/en/docs/workbench/nxt-console/nxt-monitoring/nxt-custom-metrics.html\n\n## Table of contents\n1. [Setup](#setup)\n2. [Architecture overview](#architecture-overview)\n3. [Why build AI Apps with DataRobot app templates?](#why-build-ai-apps-with-datarobot-app-templates)\n4. [Make changes](#make-changes)\n   - [Change the RAG documents](#change-the-rag-documents)\n   - [Change the LLM](#change-the-llm)\n   - [Change the RAG prompt](#change-the-RAG-prompt)\n   - [Custom front-end](#fully-custom-front-end)\n   - [Custom RAG logic](#fully-custom-rag-chunking-vectorization-and-retrieval)\n5. [Share results](#share-results)\n6. [Delete all provisioned resources](#delete-all-provisioned-resources)\n7. [Setup for advanced users](#setup-for-advanced-users)\n8. [Data Privacy](#data-privacy)\n\n\n## Setup\n\n> [!IMPORTANT]\n> If you are running this template in a DataRobot codespace, `pulumi` is already configured and the repo is automatically cloned;\n> skip to **Step 3**.\n1. If `pulumi` is not already installed, install the CLI following instructions [here](https://www.pulumi.com/docs/iac/download-install/). \n   After installing for the first time, restart your terminal and run:\n   ```bash\n   pulumi login --local  # omit --local to use Pulumi Cloud (requires separate account)\n   ```\n\n2. Clone the template repository.\n\n   ```bash\n   git clone https://github.com/datarobot-community/guarded-rag-assistant.git\n   cd guarded-rag-assistant\n   ```\n\n3. Rename the file `.env.template` to `.env` in the root directory of the repo and populate your credentials.\n   This template is pre-configured to use an Azure OpenAI endpoint. If you wish to use a different LLM provider, modifications to the code will be [necessary](#change-the-llm).\n\n   Please refer to the documentation inside `.env.template`\n\n4. In a terminal, run:\n   ```bash\n   python quickstart.py YOUR_PROJECT_NAME  # Windows users may have to use `py` instead of `python`\n   ```\n   Python 3.9+ is required.\n\nAdvanced users desiring control over virtual environment creation, dependency installation, environment variable setup,\nand `pulumi` invocation see [the advanced setup instructions](#setup-for-advanced-users).\n\n\n## Architecture overview\n\n![Guarded RAG architecture](https://s3.amazonaws.com/datarobot_public/drx/recipe_gifs/rag_architecture.svg)\n\nApp templates contain three families of complementary logic. For Guarded RAG you can [opt-in](#make-changes) to fully \ncustom RAG logic and a fully custom frontend or utilize DR's off the shelf offerings:\n\n- **AI logic**: Necessary to service AI requests and produce predictions and completions.\n  ```\n  deployment_*/  # Predictive model scoring logic, RAG completion logic (DIY RAG)\n  notebooks/  # Document chunking, VDB creation logic (DIY RAG)\n  ```\n- **App Logic**: Necessary for user consumption; whether via a hosted front-end or integrating into an external consumption layer.\n  ```\n  frontend/  # Streamlit frontend (DIY frontend)\n  docsassist/  # App business logic & runtime helpers (DIY front-end)\n  ```\n- **Operational Logic**: Necessary to activate DataRobot assets.\n  ```\n  infra/  # Settings for resources and assets to be created in DataRobot\n  infra/__main__.py  # Pulumi program for configuring DataRobot to serve and monitor AI and App logic\n  ```\n\n## Why build AI Apps with DataRobot app templates?\n\nApp Templates transform your AI projects from notebooks to production-ready applications. Too often, getting models into production means rewriting code, juggling credentials, and coordinating with multiple tools & teams just to make simple changes. DataRobot's composable AI apps framework eliminates these bottlenecks, letting you spend more time experimenting with your ML and app logic and less time wrestling with plumbing and deployment.\n\n- Start building in minutes: Deploy complete AI applications instantly, then customize the AI logic or the front-end independently (no architectural rewrites needed).\n- Keep working your way: Data scientists keep working in notebooks, developers in IDEs, and configs stay isolated. Update any piece without breaking others.\n- Iterate with confidence: Make changes locally and deploy with confidence. Spend less time writing and troubleshooting plumbing and more time improving your app.\n\nEach template provides an end-to-end AI architecture, from raw inputs to deployed application, while remaining highly customizable for specific business requirements.\n\n## Make changes\n\n### Change the RAG documents\n\n1. Replace `assets/datarobot_english_documentation_docsassist.zip` with a new zip file containing .pdf, .docx,\n   .md, or .txt documents ([example alternative docs here](https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/acme_corp_company_policies_source_business_victoria_templates.zip)).\n3. Update the `rag_documents` setting in `infra/settings_main.py` to specify the local path to the\n   new zip file.\n4. Run `pulumi up` to update your stack.\n   ```bash\n   source set_env.sh  # On windows use `set_env.bat`\n   pulumi up\n   ```\n\n### Change the LLM\n\n1. Modify the `LLM` setting in `infra/settings_generative.py` by changing `LLM=GlobalLLM.AZURE_OPENAI_GPT_4_O_MINI` to any other LLM from the `GlobalLLM` object. \n     - Trial users: Please set `LLM=GlobalLLM.AZURE_OPENAI_GPT_4_O_MINI` since GPT-4o is not supported in the trial. Use the `OPENAI_API_DEPLOYMENT_ID` in `.env` to override which model is used in your azure organisation. You'll still see GPT 4o-mini in the playground, but the deployed app will use the provided azure deployment.  \n2. To use an existing TextGen model or deployment:\n      - In `infra/settings_generative.py`: Set `LLM=GlobalLLM.DEPLOYED_LLM`.\n      - In `.env`: Set either the `TEXTGEN_REGISTERED_MODEL_ID` or the `TEXTGEN_DEPLOYMENT_ID`\n      - In `.env`: Set `CHAT_MODEL_NAME` to the model name expected by the deployment (e.g. \"claude-3-7-sonnet-20250219\" for an anthropic deployment, \"datarobot-deployed-llm\" for NIM models )\n3. In `.env`: If not using an existing TextGen model or deployment, provide the required credentials dependent on your choice.\n4. Run `pulumi up` to update your stack (Or rerun your quickstart).\n      ```bash\n      source set_env.sh  # On windows use `set_env.bat`\n      pulumi up\n      ```\n\n\n> **⚠️ Availability information:**  \n> Using a NIM model requires custom model GPU inference, a premium feature. You will experience errors by using this type of model without the feature enabled. Contact your DataRobot representative or administrator for information on enabling this feature.\n### Change the RAG prompt\n\n1. Modify the `system_prompt` variable in `infra/settings_generative.py` with your desired prompt. \n2. If using [fully custom RAG logic](#fully-custom-rag-chunking-vectorization-and-retrieval), instead please change the `stuff_prompt` variable in `notebooks/build_rag.ipynb`.\n\n### Fully custom front-end\n\n1. Edit `infra/settings_main.py` and update `application_type` to `ApplicationType.DIY`\n   - Optionally, update `APP_LOCALE` in `docsassist/i18n.py` to toggle the language.\n     Supported locales are Japanese and English, with English set as the default.\n2. Run `pulumi up` to update your stack with the example custom Streamlit frontend:\n   ```bash\n   source set_env.sh  # On windows use `set_env.bat`\n   pulumi up\n   ```\n3. After provisioning the stack at least once, you can also edit and test the Streamlit\n   front-end locally using `streamlit run app.py` from the `frontend/` directory (don't\n   forget to initialize your environment using `set_env`).\n   ```bash\n   source set_env.sh  # On windows use `set_env.bat`\n   cd frontend\n   streamlit run app.py\n   ```\n   \n\n### Fully custom RAG chunking, vectorization, and retrieval\n1. Install additional requirements (e.g. FAISS, HuggingFace).\n   ```bash\n   source set_env.sh  # On windows use `set_env.bat`\n   pip install -r requirements-extra.txt\n   ```\n2. Edit `infra/settings_main.py` and update `rag_type` to `RAGType.DIY`.\n3. Run `pulumi up` to update your stack with the example custom RAG logic.\n   ```bash\n   source set_env.sh  # On windows use `set_env.bat`\n   pulumi up\n   ```\n4. Edit `notebooks/build_rag.ipynb` to customize the doc chunking, vectorization logic.\n5. Edit `deployment_diy_rag/custom.py` to customize the retrieval logic & LLM call.\n6. Run `pulumi up` to update your stack.\n   ```bash\n   source set_env.sh  # On windows use `set_env.bat`\n   pulumi up\n   ```\n## Share results\n\n1. Log into the DataRobot application.\n2. Navigate to **Registry > Applications**.\n3. Navigate to the application you want to share, open the actions menu, and select **Share** from the dropdown.\n\n## Delete all provisioned resources\n```bash\npulumi down\n```\n\n## Setup for advanced users\nFor manual control over the setup process adapt the following steps for MacOS/Linux to your environent:\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nsource set_env.sh\npulumi stack init YOUR_PROJECT_NAME\npulumi up\n```\ne.g. for Windows/conda/cmd.exe this would be:\n```bash\nconda create --prefix .venv pip\nconda activate .\\.venv\npip install -r requirements.txt\nset_env.bat\npulumi stack init YOUR_PROJECT_NAME\npulumi up\n```\nFor projects that will be maintained, DataRobot recommends forking the repo so upstream fixes and improvements can be merged in the future.\n\n## Data Privacy\nYour data privacy is important to us. Data handling is governed by the DataRobot [Privacy Policy](https://www.datarobot.com/privacy/), please review before using your own data with DataRobot.\n",
    "url": "https://github.com/datarobot-community/guarded-rag-assistant/blob/main/README.md",
    "size": 10444,
    "description": "Markdown: README",
    "tags": []
  },
  {
    "repo_name": "forecast-assistant",
    "file_path": "README.md",
    "file_name": "README.md",
    "file_type": "markdown",
    "content": "# Forecast assistant\n\nThe forecast assistant is a customizable application template for building AI-powered forecasts. In addition to creating a hosted and shareable user interface, the forecast assistant provides: \n\n* Best-in-class predictive model training and deployment using DataRobot forecasting.\n* An intelligent explanation of factors driving the forecast that are uniquely derived for any series at any time.\n\n> [!WARNING]\n> Application templates are intended to be starting points that provide guidance on how to develop, serve, and maintain AI applications.\n> They require a developer or data scientist to adapt and modify them to meet business requirements before being put into production.\n\n![Using forecastic](https://s3.amazonaws.com/datarobot_public/drx/recipe_gifs/launch_gifs/forecast-assistant-smallest.gif)\n\n## Table of contents\n1. [Setup](#setup)\n2. [Architecture overview](#architecture-overview)\n3. [Why build AI Apps with DataRobot app templates?](#why-build-ai-apps-with-datarobot-app-templates)\n4. [Make changes](#make-changes)\n   - [Change the data and how the model is trained](#change-the-data-and-how-the-model-is-trained)\n   - [Disable the LLM](#disable-the-llm)\n   - [Change the LLM](#change-the-llm)\n   - [Change the front-end](#change-the-front-end)\n   - [Change the language in the front-end](#change-the-language-in-the-front-end)\n5. [Share results](#share-results)\n6. [Delete all resources](#delete-all-provisioned-resources)\n7. [Setup for advanced users](#setup-for-advanced-users)\n8. [Data privacy](#data-privacy)\n\n## Setup\n\n> [!IMPORTANT]  \n> If you are running this template in a DataRobot codespace, `pulumi` is already configured and the repository is automatically cloned.\n> Skip to **Step 3**.\n\n1. If `pulumi` is not already installed, install the CLI following instructions [here](https://www.pulumi.com/docs/iac/download-install/). \n   After installing `pulumi` for the first time, restart your terminal and run:\n   ```bash\n   pulumi login --local  # omit --local to use Pulumi Cloud (requires separate account)\n   ```\n\n2. Clone the template repository.\n\n   ```bash\n   git clone https://github.com/datarobot-community/forecast-assistant.git\n   cd forecast-assistant\n   ```\n\n3. Rename the file `.env.template` to `.env` in the root directory of the repo and populate your credentials.\n   \n   [Optional] If you want to use the GenAI functionality of the app, follow the instructions in `.env` to supply LLM credentials.\n   \n4. In a terminal, run the following command:\n   \n   ```bash\n   python quickstart.py YOUR_PROJECT_NAME  # Windows users may have to use `py` instead of `python`\n   ```\n   Python 3.9+ is required.\n\nAdvanced users who want to control virtual environment creation, dependency installation, environment variable setup,\nand `pulumi` invocation, see [the advanced setup instructions](#setup-for-advanced-users).\n\n\n## Architecture overview\n![Forecast assistant](https://s3.us-east-1.amazonaws.com/datarobot_public/drx/recipe_gifs/forecast-assistant-diagram.svg)\n\nApp Templates contain three families of complementary logic. For this template, you can [opt-in](#make-changes) to fully \ncustom AI logic and a fully custom front-end or utilize DataRobot's off-the-shelf offerings:\n\n- **AI logic**: Necessary to service AI requests, generate predictions, and manage predictive models.\n  ```\n  notebooks/  # Model training logic, scoring data prep logic\n  ```\n- **App logic**: Necessary for user consumption, whether via a hosted front-end or integrating into an external consumption layer.\n  ```\n  frontend/  # Streamlit frontend\n  forecastic/  # App biz logic & runtime helpers\n  ```\n- **Operational logic**: Necessary to turn on all DataRobot assets.\n  ```\n  infra/  # Settings for resources and assets to be created in DataRobot\n  infra/__main__.py  # Pulumi program for configuring DataRobot to serve and monitor AI and App logic\n  ```\n\n\n## Why build AI Apps with DataRobot app templates?\n\nApp templates transform your AI projects from notebooks to production-ready applications. Too often, getting models into production means rewriting code, juggling credentials, and coordinating with multiple tools and teams just to make simple changes. DataRobot's composable AI apps framework eliminates these bottlenecks, letting you spend more time experimenting with your ML and app logic and less time wrestling with plumbing and deployment.\n\n- Start building in minutes: Deploy complete AI applications instantly, then customize AI logic or front-end independently - no architectural rewrites needed.\n- Keep working your way: Data scientists keep working in notebooks, developers in IDEs, and configs stay isolated - update any piece without breaking others.\n- Iterate with confidence: Make changes locally and deploy with confidence - spend less time writing and troubleshooting plumbing, more time improving your app.\n\nEach template provides an end-to-end AI architecture, from raw inputs to deployed application, while remaining highly customizable for specific business requirements.\n\n## Make changes\n\n### Change the data and how the model is trained\n1. Edit the following two notebooks:\n   - `notebooks/train_model.ipynb`: Handles training data ingest and preparation and model training settings.\n   - `notebooks/prep_scoring_data.ipynb`: Handles scoring data preparation (the data used to show forecasts in the front-end).\n   \n   The last cell of each notebook is required, as it writes outputs needed for the rest of the pipeline.\n\n**Recent improvements in `train_model.ipynb`:**\n- **Dual-mode operation**: The notebook now supports both training new models and using existing deployments\n- **Automatic metadata extraction**: When using an existing deployment, the notebook automatically extracts model metadata (target, datetime partition column, etc.)\n- **Flexible feature configuration**: Easy configuration of known-in-advance features for what-if analysis\n- **Error handling**: Improved error handling with fallback mechanisms for missing model metadata\n\n2. Run the revised notebooks.\n3. Run `pulumi up` to update your stack with these changes.\n```bash\nsource set_env.sh  # On windows use `set_env.bat`\npulumi up\n```  \n4. For a forecasting app that is continuously updated, consider running `prep_scoring_data.ipynb` on a schedule.\n\n### Disable the LLM\nIn `infra/settings_generative.py`: Set `LLM=None` to disable any generative output altogether.\n\n### Use an existing forecast deployment\n\nTo use an existing forecast deployment instead of creating a new one:\n\n1. In `.env`: Set `FORECAST_DEPLOYMENT_ID` to the ID of your existing deployment\n2. Run `pulumi up` to update your stack with the existing deployment\n   ```bash\n   source set_env.sh  # On windows use `set_env.bat`\n   pulumi up\n   ```\n\n> **⚠️ Note:** When using an existing deployment:\n> - The script will skip creating batch prediction jobs and retraining policies  \n> - The `train_model.ipynb` notebook will skip training and extract metadata from the existing model\n> - You may need to adjust the `feature_settings_config` in the notebook to match your model's known-in-advance features\n\n**Files that need modification for existing deployments:**\n\nWhen using an existing deployment, you may need to modify these files to match your model's configuration:\n\n1. **`notebooks/train_model.ipynb`** - Update the `feature_settings_config` to match your model's known-in-advance features:\n   ```python\n   feature_settings_config=[\n       FeatureSettingConfig(feature_name=\"Your_Feature_Name\", known_in_advance=True),\n       # Add other known-in-advance features from your model\n   ]\n   ```\n\n2. **`notebooks/prep_scoring_data.ipynb`** - Ensure your scoring data preparation matches the data format expected by your existing model\n\n3. **`forecastic/schema.py`** - Update app settings if your model has different features or requirements\n\n**What happens when using an existing deployment:**\n\n- **Model Training**: Completely skipped - no new model is trained\n- **Data Ingestion**: Skipped - uses existing model's training data\n- **Metadata Extraction**: The notebook extracts target, datetime partition column, and other model metadata from your existing deployment\n- **Resource Creation**: Only creates the application frontend and LLM components (if enabled)\n- **Batch Prediction**: Not created (you'll need to set up your own if needed)\n- **Retraining Policy**: Not created (you'll need to set up your own if needed)\n\n### Change the LLM\n\n1. Modify the `LLM` setting in `infra/settings_generative.py` by changing `LLM=LLMs.AZURE_OPENAI_GPT_4_O_MINI` to any other LLM from the `LLMs` object. \n     - Trial users: Please set `LLM=LLMs.AZURE_OPENAI_GPT_4_O_MINI` since GPT-4o is not supported in the trial. Use the `OPENAI_API_DEPLOYMENT_ID` in `.env` to override which model is used in your azure organisation. You'll still see GPT 4o-mini in the playground, but the deployed app will use the provided azure deployment.  \n2. To use an existing TextGen model or deployment:\n      - In `infra/settings_generative.py`: Set `LLM=LLMs.DEPLOYED_LLM`.\n      - In `.env`: Set either the `TEXTGEN_REGISTERED_MODEL_ID` or the `TEXTGEN_DEPLOYMENT_ID`\n      - In `.env`: Set `CHAT_MODEL_NAME` to the model name expected by the deployment (e.g. \"claude-3-7-sonnet-20250219\" for an anthropic deployment, \"datarobot-deployed-llm\" for NIM models ) \n3. In `.env`: If not using an existing TextGen model or deployment, provide the required credentials dependent on your choice.\n4. Run `pulumi up` to update your stack (Or rerun your quickstart).\n      ```bash\n      source set_env.sh  # On windows use `set_env.bat`\n      pulumi up\n      ```\n\n> **⚠️ Availability information:**  \n> Using a NIM model requires custom model GPU inference, a premium feature. You will experience errors by using this type of model without the feature enabled. Contact your DataRobot representative or administrator for information on enabling this feature.\n\n### Change the front-end\n1. Ensure you have already run `pulumi up` at least once (to provision the time series deployment).\n2. Streamlit assets are in `frontend/` and can be edited. After provisioning the stack\n   at least once, you can also test the front-end locally using `streamlit run app.py` from the\n   `frontend/` directory (don't forget to initialize your environment using `source set_env.sh`).\n```bash\nsource set_env.sh  # On windows use `set_env.bat`\ncd frontend\nstreamlit run app.py\n```\n3. Run `pulumi up` again to update your stack with the changes.\n```bash\nsource set_env.sh  # On windows use `set_env.bat`\npulumi up\n```\n\n#### Change the language in the front-end\nOptionally, you can set the application locale in `forecastic/i18n.py`, e.g. `APP_LOCALE = LanguageCode.JA`. Supported locales are Japanese and English, with English set as the default.\n\n#### Application resources\nThe application now supports inheriting resource configurations from the Application Source. When the Application Source is created, the system automatically fetches its resource settings (replicas, memory, CPU) via the DataRobot API and applies them to the Custom Application.\n\n**How it works:**\n1. The Application Source is created with its resource configuration\n2. The system fetches the source's resource details using `application_source.id`\n3. These resources are automatically applied to the Custom Application\n\n**Environment variables required:**\n- `DATAROBOT_ENDPOINT`: Your DataRobot API endpoint\n- `DATAROBOT_API_TOKEN`: Your DataRobot API token\n\n**Fallback behavior:**\n- If resources cannot be fetched from the Application Source, the system falls back to DataRobot's automatic resource allocation\n- Error messages are logged as warnings, ensuring deployment continues successfully\n\n### Environment Variables\n\nThe following environment variables can be configured in your `.env` file:\n\n**Required for all deployments:**\n- `DATAROBOT_ENDPOINT`: Your DataRobot API endpoint (e.g., `https://app.datarobot.com`)\n- `DATAROBOT_API_TOKEN`: Your DataRobot API token\n\n**Optional for existing deployments:**\n- `FORECAST_DEPLOYMENT_ID`: ID of an existing forecast deployment to reuse instead of creating a new one\n- `TEXTGEN_REGISTERED_MODEL_ID`: ID of an existing registered model for LLM functionality\n- `TEXTGEN_DEPLOYMENT_ID`: ID of an existing LLM deployment for LLM functionality\n- `CHAT_MODEL_NAME`: Model name for LLM deployments (e.g., \"claude-3-7-sonnet-20250219\", \"datarobot-deployed-llm\")\n\n**Optional for LLM providers:**\n- `OPENAI_API_KEY`: OpenAI API key (for OpenAI LLMs)\n- `OPENAI_API_DEPLOYMENT_ID`: Azure OpenAI deployment ID (for Azure OpenAI)\n- `ANTHROPIC_API_KEY`: Anthropic API key (for Claude models)\n- `GOOGLE_API_KEY`: Google API key (for Google LLMs)\n\n**Optional for advanced configuration:**\n- `DATAROBOT_DEFAULT_USE_CASE`: Use case ID to associate with the project\n\n## Share results\n1. Log into the DataRobot application.\n2. Navigate to **Registry > Applications**.\n3. Navigate to the application you want to share, open the actions menu, and select **Share** from the dropdown.\n\n## Delete all provisioned resources\n```bash\npulumi down\n```\nThen run the jupyter notebook `notebooks/delete_non_pulumi_assets.ipynb`.\n\n## Setup for advanced users\nFor manual control over the setup process, adapt the following steps for MacOS/Linux to your environent:\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nsource set_env.sh\npulumi stack init YOUR_PROJECT_NAME\npulumi up \n```\ne.g., for Windows/conda/cmd.exe the previous example would change to the following:\n```bash\nconda create --prefix .venv pip\nconda activate .\\.venv\npip install -r requirements.txt\nset_env.bat\npulumi stack init YOUR_PROJECT_NAME\npulumi up \n```\nFor projects that will be maintained, DataRobot recommends forking the repo so upstream fixes and improvements can be merged in the future.\n\n## Data privacy\nYour data privacy is important to DataRobot. Data handling is governed by the DataRobot [Privacy Policy](https://www.datarobot.com/privacy/). Review the policy before using your own data with DataRobot.\n",
    "url": "https://github.com/datarobot-community/forecast-assistant/blob/main/README.md",
    "size": 14070,
    "description": "Markdown: README",
    "tags": []
  }
]