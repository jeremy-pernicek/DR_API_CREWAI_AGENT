[
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "quickstart.py",
    "file_name": "quickstart.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# type: ignore\nimport argparse\nimport json\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nif sys.version_info[0] < 3 or (sys.version_info[0] >= 3 and sys.version_info[1] < 9):\n    print(\"Must be using Python version 3.9 or higher\")\n    exit(1)\n\nwork_dir = Path(os.path.dirname(__file__))\ndot_env_file = Path(work_dir / \".env\")\nvenv_dir = work_dir / \".venv\"\n\n\ndef is_datarobot_codespace():\n    return os.getenv(\"DATAROBOT_NOTEBOOK_IMAGE\") is not None\n\n\ndef check_pulumi_installed():\n    try:\n        subprocess.check_call(\n            [\"pulumi\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Is pulumi installed? If not, please go to `https://www.pulumi.com/docs/iac/download-install/`\"\n        )\n        exit(1)\n\n\ndef check_pulumi_login():\n    try:\n        subprocess.check_call(\n            [\"pulumi\", \"whoami\", \"-j\", \"--non-interactive\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT,\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Please login to pulumi and rerun. Use `pulumi login --local` to log in locally or `pulumi login` to login to pulumi cloud with an API token\"\n        )\n        exit(1)\n\n\ndef check_dotenv_exists():\n    if not dot_env_file.exists():\n        print(\n            \"Could not find `.env`. Please rename the file `.env.template` and fill in your details\"\n        )\n        exit(1)\n\n\ndef is_windows():\n    return os.name == \"nt\"\n\n\ndef get_activate_command():\n    if is_datarobot_codespace():\n        return []\n    if is_conda_environment():\n        if is_windows():\n            activate_cmd = [\"conda\", \"activate\", f\"{venv_dir}\", \"&&\"]\n        else:\n            # see https://github.com/conda/conda/issues/7980\n            activate_cmd = [\n                \"eval\",\n                '\"$(conda shell.bash hook)\"',\n                \"&&\",\n                \"conda\",\n                \"activate\",\n                f\"{str(venv_dir)}\",\n                \"&&\",\n            ]\n\n    else:\n        # Regular venv activation\n        if is_windows():\n            activate_script = str(venv_dir / \"Scripts\" / \"activate.bat\")\n            activate_cmd = [\"call\", f\"{activate_script}\", \"&&\"]\n        else:\n            activate_script = str(venv_dir / \"bin\" / \"activate\")\n            activate_cmd = [\n                \"source\",\n                f\"{activate_script}\",\n                \"&&\",\n            ]\n    return activate_cmd\n\n\ndef is_conda_environment():\n    return os.environ.get(\"CONDA_DEFAULT_ENV\") is not None\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Infrastructure management script\")\n    parser.add_argument(\"stack_name\", help=\"Stack name to use\")\n    parser.add_argument(\n        \"--action\",\n        choices=[\"up\", \"destroy\"],\n        default=\"up\",\n        required=False,\n        help=\"Action to perform (up or destroy)\",\n    )\n    return parser.parse_args()\n\n\ndef get_python_executable():\n    if is_conda_environment():\n        return shutil.which(\"python\")\n    return sys.executable\n\n\ndef run_subprocess_in_venv(command: list[str]):\n    if is_windows():\n        full_cmd = get_activate_command() + command\n        # shell = True, otherwise CMD complains it can't find the file\n        result = subprocess.run(\n            \" \".join(full_cmd),\n            check=True,\n            cwd=work_dir,\n            shell=True,\n            capture_output=True,\n            text=True,\n        )\n    else:\n        full_cmd = [\"bash\", \"-c\", \" \".join(get_activate_command() + command)]\n        print(full_cmd)\n        result = subprocess.run(\n            full_cmd,\n            check=True,\n            cwd=work_dir,\n            capture_output=True,\n            text=True,\n        )\n\n    if result.returncode != 0:\n        print(f\"Error running command: {result.stderr}\")\n        raise subprocess.CalledProcessError(\n            result.returncode, result.args, result.stdout, result.stderr\n        )\n\n\ndef create_virtual_environment() -> None:\n    if not venv_dir.exists():\n        if is_conda_environment():\n            print(\"Creating conda environment...\")\n            subprocess.run(\n                [\n                    \"conda\",\n                    \"create\",\n                    \"--prefix\",\n                    str(venv_dir),\n                    \"python=3.11\",\n                    \"pip\",\n                    \"-y\",\n                ],\n                check=True,\n                cwd=work_dir,\n            )\n        else:\n            # Regular venv creation\n            python_executable = get_python_executable()\n            subprocess.run(\n                [python_executable, \"-m\", \"venv\", \".venv\"],\n                check=True,\n                cwd=work_dir,\n            )\n\n\ndef setup_virtual_environment() -> None:\n    \"\"\"Create and configure a virtual environment in a cross-platform manner.\"\"\"\n    print(\"Preparing virtual environment...\")\n\n    try:\n        # Handle activation and package installation\n\n        try:\n            run_subprocess_in_venv([\"pip\", \"install\", \"-U\", \"uv\"])\n            if is_datarobot_codespace():\n                os.system(\"uv pip install $(pip freeze | grep ipykernel)\")\n            # Install requirements using uv\n            run_subprocess_in_venv([\"uv\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n        except Exception as e:\n            print(f\"UV installation/usage failed: {e}\")\n            print(\"Falling back to pip\")\n\n            run_subprocess_in_venv([\"pip\", \"install\", \"-r\", \"requirements.txt\"])\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during virtual environment setup: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected error during virtual environment setup: {e}\")\n        raise\n\n\ndef load_dotenv():\n    with open(\".env\") as f:\n        content = f.read()\n\n    pattern = r\"\"\"\n        (?:^|\\n)         # Must start at beginning of string or after newline\n        (?![\\s#])        # Negative lookahead: next char cannot be whitespace or #\n        ([A-Za-z_]\\w*)   # Key: start with letter/underscore, then word chars\n        =\\s*             # Equals with optional whitespace after\n        (?:\n            '(.*?)'      # Single quoted (group 2)\n            |\"(.*?)\"     # Double quoted (group 3)\n            |([^\\n]+)    # Unquoted: everything until newline (group 4)\n        )\n    \"\"\"\n\n    env_vars = {}\n    for match in re.finditer(pattern, content, re.MULTILINE | re.DOTALL | re.VERBOSE):\n        key = match.group(1).strip()\n\n        if match.group(2) is not None:  # Single quoted\n            value = match.group(2).strip()\n        elif match.group(3) is not None:  # Double quoted\n            value = match.group(3).strip()\n        else:  # Unquoted\n            value = match.group(4)\n            if \" #\" in value:  # Only strip comments after space\n                value = value.split(\" #\", 1)[0]\n            value = value.strip()\n\n        env_vars[key] = value\n\n    os.environ.update(env_vars)\n    return env_vars\n\n\ndef run_pulumi_command(command: list, work_dir: Path, env_vars: dict):\n    \"\"\"Run a Pulumi command using shell activation with PTY support.\"\"\"\n    cmd_str = \" \".join(command)\n    try:\n        if is_windows():\n            os.system(f\"{' '.join(get_activate_command())}{cmd_str}\")\n        else:\n            os.system(f\"bash -c '{' '.join(get_activate_command())}{cmd_str}'\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        sys.exit(1)\n\n\ndef setup_pulumi_config(work_dir: Path, stack_name: str, env_vars: dict):\n    stack_select = [\"pulumi\", \"stack\", \"select\", stack_name, \"--create\"]\n\n    # Run commands\n    run_pulumi_command(stack_select, work_dir, env_vars)\n\n\ndef print_app_url():\n    try:\n        pulumi_output = subprocess.check_output([\"pulumi\", \"stack\", \"output\", \"-j\"])\n        pulumi_output_dict = json.loads(pulumi_output)\n        application_id = pulumi_output_dict[\"DATAROBOT_APPLICATION_ID\"]\n        datarobot_endpoint = os.environ[\"DATAROBOT_ENDPOINT\"]\n        url = f\"{datarobot_endpoint.rstrip('/').replace('api/v2', '')}custom_applications/{application_id}/\"\n        print(\"\\n\\n\")\n        print(\"=\" * 80)\n        print(f\"\\n    Your app is ready! Application URL:\\n\\n    {url}\\n\")\n        print(\"=\" * 80)\n    except Exception as e:\n        print(e)\n\n\ndef main():\n    args = parse_args()\n    if args.stack_name == \"YOUR_PROJECT_NAME\":\n        print(\"Please use a different project name\")\n        sys.exit(0)\n    check_dotenv_exists()\n    # Load environment variables\n    env_vars = load_dotenv()\n\n    check_pulumi_installed()\n    check_pulumi_login()\n\n    # Skip venv setup in Codespaces or if explicitly requested\n    if not is_datarobot_codespace():\n        create_virtual_environment()\n    setup_virtual_environment()\n    # Setup Pulumi configuration\n    setup_pulumi_config(work_dir, args.stack_name, env_vars)\n\n    # Refresh the stack\n    print(\"\\nRefreshing stack...\")\n    run_pulumi_command(\n        [\"pulumi\", \"refresh\", \"--yes\", \"--skip-pending-creates\"], work_dir, env_vars\n    )\n\n    if args.action == \"destroy\":\n        print(\"\\nDestroying stack...\")\n        run_pulumi_command([\"pulumi\", \"destroy\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack destroy complete\")\n    else:\n        print(\"\\nCreate/update stack...\")\n        run_pulumi_command([\"pulumi\", \"up\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack update complete\")\n\n        print_app_url()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/quickstart.py",
    "size": 10067,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "datarobot"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "utils/api.py",
    "file_name": "api.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import annotations\n\nimport ast\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport tempfile\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom types import ModuleType, TracebackType\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Type,\n    TypeVar,\n    cast,\n)\n\nimport datarobot as dr\nimport instructor\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport polars as pl\nimport psutil\nimport scipy\nimport sklearn\nimport statsmodels as sm\nfrom datarobot.client import RESTClientObject\nfrom joblib import Memory\nfrom openai import AsyncOpenAI\nfrom openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\nfrom openai.types.chat.chat_completion_system_message_param import (\n    ChatCompletionSystemMessageParam,\n)\nfrom openai.types.chat.chat_completion_user_message_param import (\n    ChatCompletionUserMessageParam,\n)\nfrom plotly.subplots import make_subplots\nfrom pydantic import ValidationError\n\nsys.path.append(os.path.dirname(os.path.realpath(__file__)))\nfrom utils import prompts, tools\nfrom utils.analyst_db import AnalystDB, DataSourceType\nfrom utils.code_execution import (\n    InvalidGeneratedCode,\n    MaxReflectionAttempts,\n    execute_python,\n    reflect_code_generation_errors,\n)\nfrom utils.data_cleansing_helpers import (\n    add_summary_statistics,\n    process_column,\n)\nfrom utils.database_helpers import get_external_database\nfrom utils.logging_helper import get_logger, log_api_call\nfrom utils.resources import LLMDeployment\nfrom utils.schema import (\n    AnalysisError,\n    AnalystChatMessage,\n    AnalystDataset,\n    BusinessAnalysisGeneration,\n    ChartGenerationExecutionResult,\n    ChatRequest,\n    CleansedDataset,\n    CodeGeneration,\n    Component,\n    DatabaseAnalysisCodeGeneration,\n    DataDictionary,\n    DataDictionaryColumn,\n    DataRegistryDataset,\n    DictionaryGeneration,\n    DownloadedRegistryDataset,\n    EnhancedQuestionGeneration,\n    GetBusinessAnalysisMetadata,\n    GetBusinessAnalysisRequest,\n    GetBusinessAnalysisResult,\n    QuestionListGeneration,\n    RunAnalysisRequest,\n    RunAnalysisResult,\n    RunAnalysisResultMetadata,\n    RunChartsRequest,\n    RunChartsResult,\n    RunDatabaseAnalysisRequest,\n    RunDatabaseAnalysisResult,\n    RunDatabaseAnalysisResultMetadata,\n    Tool,\n    ValidatedQuestion,\n)\n\nlogger = get_logger()\nlogging.getLogger(\"openai\").setLevel(logging.WARNING)\nlogging.getLogger(\"openai.http_client\").setLevel(logging.WARNING)\n\nVALUE_ERROR_MESSAGE = \"Input data cannot be empty (no dataset provided)\"\nDEFAULT_LLM_GATEWAY_MODEL = \"azure/gpt-4o\"\nDEFAULT_LLM_GATEWAY_MODEL_SMALL = \"azure/gpt-4o-mini\"\n\n\ndef log_memory() -> None:\n    process = psutil.Process()\n    memory = process.memory_info().rss / 1024 / 1024  # MB\n    logger.info(f\"Memory usage: {memory:.2f} MB\")\n\n\n@functools.cache\ndef initialize_deployment() -> tuple[RESTClientObject, str]:\n    \"\"\"Initialize either LLM Gateway or DataRobot-hosted LLM deployment based on environment settings and credential priority.\"\"\"\n    try:\n        dr_client = dr.Client()\n        chat_agent_deployment_id = LLMDeployment().id\n        if chat_agent_deployment_id is None:\n            raise ValueError(\n                \"LLM Deployment ID is required but not found. Please check your infrastructure setup.\"\n            )\n        deployment_chat_base_url = (\n            f\"{dr_client.endpoint.rstrip('/')}/deployments/{chat_agent_deployment_id}/\"\n        )\n        logger.info(\n            f\"Using the DataRobot-hosted LLM deployment (configured at infrastructure time) at: {deployment_chat_base_url}\"\n        )\n        return dr_client, deployment_chat_base_url\n\n    except ValidationError as e:\n        raise ValueError(\n            \"Unable to load Deployment ID.\"\n            \"If running locally, verify you have selected the correct \"\n            \"stack and that it is active using `pulumi stack output`. \"\n            \"If running in DataRobot, verify your runtime parameters have been set correctly.\"\n        ) from e\n\n\nclass AsyncLLMClient:\n    async def __aenter__(self) -> instructor.AsyncInstructor:\n        dr_client, deployment_base_url = initialize_deployment()\n        self.openai_client = AsyncOpenAI(\n            api_key=dr_client.token,\n            base_url=deployment_base_url,\n            timeout=90,\n            max_retries=2,\n        )\n        self.client = instructor.from_openai(\n            self.openai_client, mode=instructor.Mode.MD_JSON\n        )\n        return self.client\n\n    async def __aexit__(\n        self,\n        exc_type: Type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.openai_client.close()  # Properly close the client\n\n\nALTERNATIVE_LLM_BIG = \"datarobot-deployed-llm\"\nALTERNATIVE_LLM_SMALL = \"datarobot-deployed-llm\"\nDICTIONARY_BATCH_SIZE = 10\nMAX_REGISTRY_DATASET_SIZE = 400e6  # aligns to 400MB set in streamlit config.toml\nDISK_CACHE_LIMIT_BYTES = 512e6\nDICTIONARY_PARALLEL_BATCH_SIZE = 2\nDICTIONARY_TIMEOUT = 45.0\n\n_memory = Memory(tempfile.gettempdir(), verbose=0)\n_memory.clear(warn=False)  # clear cache on startup\n\nT = TypeVar(\"T\")\n\n\ndef cache(f: T) -> T:\n    \"\"\"Cache function and coroutine results to disk using joblib.\"\"\"\n    cached_f = _memory.cache(f)\n\n    if asyncio.iscoroutinefunction(f):\n\n        async def awrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = await cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"\n                )\n            return result\n\n        return cast(T, awrapper)\n    else:\n\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"  # type: ignore[attr-defined]\n                )\n            return result\n\n        return cast(T, wrapper)\n\n\n# This can be large as we are not storing the actual datasets in memory, just metadata\ndef list_registry_datasets(limit: int = 100) -> list[DataRegistryDataset]:\n    \"\"\"\n    Fetch datasets from Data Registry with specified limit\n\n    Args:\n        limit: int\n        Datasets to retrieve. Max value: 100\n    \"\"\"\n\n    url = f\"datasets?limit={limit}\"\n\n    # Get all datasets and manually limit the results\n    datasets = dr.client.get_client().get(url).json()[\"data\"]\n\n    return [\n        DataRegistryDataset(\n            id=ds[\"datasetId\"],\n            name=ds[\"name\"],\n            created=(\n                ds[\"creationDate\"][:10] if \"creationDate\" in ds else \"N/A\"  # %Y-%m-%d\n            ),\n            size=(\n                f\"{ds['datasetSize'] / (1024 * 1024):.1f} MB\"\n                if \"datasetSize\" in ds\n                else \"N/A\"\n            ),\n        )\n        for ds in datasets\n    ]\n\n\nasync def download_registry_datasets(\n    dataset_ids: list[str], analyst_db: AnalystDB\n) -> list[DownloadedRegistryDataset]:\n    \"\"\"Load selected datasets as pandas DataFrames\n\n    Args:\n        *args: list of dataset IDs to download\n\n    Returns:\n        list[AnalystDataset]: Dictionary of dataset names and data\n    \"\"\"\n    downloaded_datasets = []\n    datasets = [dr.Dataset.get(id_) for id_ in dataset_ids]\n    if (\n        sum([ds.size for ds in datasets if ds.size is not None])\n        > MAX_REGISTRY_DATASET_SIZE\n    ):\n        raise ValueError(\n            f\"The requested Data Registry datasets must total <= {int(MAX_REGISTRY_DATASET_SIZE)} bytes\"\n        )\n\n    result_datasets: list[AnalystDataset] = []\n    for dataset in datasets:\n        try:\n            df = dataset.get_as_dataframe()\n            result_datasets.append(AnalystDataset(name=dataset.name, data=df))\n            logger.info(f\"Successfully downloaded {dataset.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to read dataset {dataset.name}: {str(e)}\")\n            downloaded_datasets.append(\n                DownloadedRegistryDataset(name=dataset.name, error=str(e))\n            )\n            continue\n    for result_dataset in result_datasets:\n        await analyst_db.register_dataset(\n            result_dataset, DataSourceType.REGISTRY, dataset.size or 0\n        )\n        downloaded_datasets.append(DownloadedRegistryDataset(name=result_dataset.name))\n    return downloaded_datasets\n\n\nasync def _get_dictionary_batch(\n    columns: list[str], df: pl.DataFrame, batch_size: int = 5\n) -> list[DataDictionaryColumn]:\n    \"\"\"Process a batch of columns to get their descriptions\"\"\"\n\n    # Get sample data and stats for just these columns\n    # Convert timestamps to ISO format strings for JSON serialization\n    try:\n        logger.debug(f\"Processing batch of {len(columns)} columns\")\n        sample_data = {}\n        logger.debug(\"Converting datetime columns to ISO format\")\n        num_samples = 10\n        for col in columns:\n            if df[col].dtype.is_temporal():\n                # Convert timestamps to ISO format strings\n                sample_data[col] = (\n                    df.select(\n                        pl.col(col)\n                        .cast(pl.Datetime)\n                        .map_elements(\n                            lambda x: x.isoformat() if x is not None else None\n                        )\n                    )\n                    .head(num_samples)\n                    .to_dict()\n                )\n            else:\n                # For non-datetime columns, just take the samples as is\n                sample_data[col] = df.select(pl.col(col)).head(num_samples).to_dict()\n\n        # Handle numeric summary\n        numeric_summary = {}\n        logger.debug(\"Calculating numeric summaries\")\n        for col in columns:\n            if df[col].dtype.is_numeric():\n                desc = df[col].describe()\n                numeric_summary[col] = desc.to_dict()\n\n        # Get categories for non-numeric columns\n        categories = []\n        logger.debug(\"Getting categories for non-numeric columns\")\n        for column in columns:\n            if not df[column].dtype.is_numeric():\n                try:\n                    value_counts = (\n                        df[column].sample(n=1000, seed=42).value_counts().head(10)\n                    )\n                    # Convert any timestamp values to strings\n                    if df[column].dtype.is_temporal():\n                        value_counts[column] = value_counts[column].cast(pl.String)\n                    categories.append({column: value_counts[column].to_list()})\n                except Exception:\n                    continue\n\n        # Create messages for OpenAI\n        messages: list[ChatCompletionMessageParam] = [\n            ChatCompletionSystemMessageParam(\n                role=\"system\", content=prompts.SYSTEM_PROMPT_GET_DICTIONARY\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Data:\\n{sample_data}\\n\"\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Statistical Summary:\\n{numeric_summary}\\n\"\n            ),\n        ]\n\n        if categories:\n            messages.append(\n                ChatCompletionUserMessageParam(\n                    role=\"user\", content=f\"Categorical Values:\\n{categories}\\n\"\n                )\n            )\n        logger.debug(\n            f\"total_characters: {len(''.join([str(msg) for msg in messages]))}\"\n        )\n        # Get descriptions from OpenAI\n        async with AsyncLLMClient() as client:\n            completion: DictionaryGeneration = await client.chat.completions.create(\n                response_model=DictionaryGeneration,\n                model=ALTERNATIVE_LLM_SMALL,\n                messages=messages,\n            )\n\n        # Convert to dictionary format\n        descriptions = completion.to_dict()\n\n        # Only return descriptions for requested columns\n        return [\n            DataDictionaryColumn(\n                column=col,\n                description=descriptions.get(col, \"No description available\"),\n                data_type=str(df[col].dtype),\n            )\n            for col in columns\n        ]\n\n    except ValueError as e:\n        logger.error(f\"Invalid dictionary response: {str(e)}\")\n        return [\n            DataDictionaryColumn(\n                column=col,\n                description=\"No valid description available\",\n                data_type=str(df[col].dtype),\n            )\n            for col in columns\n        ]\n\n\n@log_api_call\nasync def get_dictionary(dataset: AnalystDataset) -> DataDictionary:\n    \"\"\"Process a single dataset with parallel column batch processing\"\"\"\n\n    try:\n        logger.info(f\"Processing dataset {dataset.name} init\")\n        # Convert JSON to DataFrame\n        df_full = dataset.to_df()\n        df = df_full.sample(n=min(10000, len(df_full)), seed=42)\n\n        # Add debug logging\n        logger.info(f\"Processing dataset {dataset.name} with shape {df.shape}\")\n\n        # Handle empty dataset\n        if df.is_empty():\n            logger.warning(f\"Dataset {dataset.name} is empty\")\n            return DataDictionary(\n                name=dataset.name,\n                column_descriptions=[],\n            )\n\n        # Split columns into batches\n        column_batches = [\n            list(df.columns[i : i + DICTIONARY_BATCH_SIZE])\n            for i in range(0, len(df.columns), DICTIONARY_BATCH_SIZE)\n        ]\n        logger.info(\n            f\"Created {len(column_batches)} batches for {len(df.columns)} columns\"\n        )\n\n        # Create a semaphore to limit concurrent tasks to 2\n        sem = asyncio.Semaphore(DICTIONARY_PARALLEL_BATCH_SIZE)\n\n        async def throttled_get_dictionary_batch(\n            batch: list[str],\n        ) -> list[DataDictionaryColumn]:\n            try:\n                async with sem:\n                    return await asyncio.wait_for(\n                        _get_dictionary_batch(batch, df, DICTIONARY_BATCH_SIZE),\n                        timeout=DICTIONARY_TIMEOUT,\n                    )\n            except asyncio.TimeoutError:\n                logger.warning(f\"Timeout processing batch: {batch}\")\n                return [\n                    DataDictionaryColumn(\n                        column=col,\n                        description=\"No Description Available\",\n                        data_type=str(df[col].dtype),\n                    )\n                    for col in batch\n                ]\n            except Exception as e:\n                logger.error(f\"Error processing batch {batch}: {str(e)}\")\n                return [\n                    DataDictionaryColumn(\n                        column=col,\n                        description=\"No Description Available\",\n                        data_type=str(df[col].dtype),\n                    )\n                    for col in batch\n                ]\n\n        tasks = [throttled_get_dictionary_batch(batch) for batch in column_batches]\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        # Filter out any exceptions and flatten results\n        dictionary: list[DataDictionaryColumn] = []\n        for result in results:\n            if isinstance(result, BaseException):\n                logger.error(f\"Task failed with error: {str(result)}\")\n                continue\n            dictionary.extend(result)\n\n        logger.info(\n            f\"Created dictionary with {len(dictionary)} entries for dataset {dataset.name}\"\n        )\n\n        return DataDictionary(\n            name=dataset.name,\n            column_descriptions=dictionary,\n        )\n\n    except Exception:\n        return DataDictionary(\n            name=dataset.name,\n            column_descriptions=[\n                DataDictionaryColumn(\n                    column=c,\n                    data_type=str(dataset.to_df()[c].dtype),\n                    description=\"No Description Available\",\n                )\n                for c in dataset.columns\n            ],\n        )\n\n\ndef _validate_question_feasibility(\n    question: str, available_columns: list[str]\n) -> ValidatedQuestion | None:\n    \"\"\"Validate if a question can be answered with available data\n\n    Checks if common data elements mentioned in the question exist in columns\n    \"\"\"\n    # Convert question and columns to lowercase for matching\n    question_lower = question.lower()\n    columns_lower = [col.lower() for col in available_columns]\n\n    # Extract potential column references from question\n    words = set(re.findall(r\"\\b\\w+\\b\", question_lower))\n\n    # Find matches and missing terms\n    found_columns = [col for col in columns_lower if any(word in col for word in words)]\n\n    is_valid = len(found_columns) > 0\n    if is_valid:\n        return ValidatedQuestion(\n            question=question,\n        )\n    return None\n\n\n@log_api_call\nasync def suggest_questions(\n    datasets: list[AnalystDataset], max_columns: int = 40\n) -> list[ValidatedQuestion]:\n    \"\"\"Generate and validate suggested analysis questions\n\n    Args:\n        dictionary: DataFrame containing data dictionary\n        max_columns: Maximum number of columns to include in prompt\n\n    Returns:\n        Dict containing:\n            - questions: list of validated question objects\n            - metadata: Dictionary of processing information\n    \"\"\"\n    # Validate input\n    dictionary = sum(\n        [\n            DataDictionary.from_analyst_df(\n                ds.to_df(),\n                column_descriptions=f\"Column from dataset {ds.name}\",\n            ).column_descriptions\n            for ds in datasets\n        ],\n        [],\n    )\n\n    if len(dictionary) < 1:\n        raise ValueError(\"Dictionary DataFrame cannot be empty\")\n\n    # Limit columns for OpenAI prompt\n    total_columns = len(dictionary)\n    if total_columns > max_columns:\n        # Take first and last 20 columns\n        half_max = max_columns // 2\n        first_half = dictionary[:half_max]\n        last_half = dictionary[-half_max:]\n\n        # Remove any duplicates\n        dictionary = first_half + last_half\n\n        # deduplicate\n        dictionary = list({item.column: item for item in dictionary}.values())\n\n    # Convert dictionary to format expected by OpenAI\n    dict_data = {\n        \"columns\": [d.column for d in dictionary],\n        \"descriptions\": [d.description for d in dictionary],\n        \"data_types\": [d.data_type for d in dictionary],\n    }\n\n    # Create OpenAI messages\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\", content=prompts.SYSTEM_PROMPT_SUGGEST_A_QUESTION\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Dictionary:\\n{json.dumps(dict_data)}\"\n        ),\n    ]\n    async with AsyncLLMClient() as client:\n        completion: QuestionListGeneration = await client.chat.completions.create(\n            response_model=QuestionListGeneration,\n            model=ALTERNATIVE_LLM_SMALL,\n            messages=messages,\n        )\n\n    available_columns = dict_data[\"columns\"]\n    validated_questions: list[ValidatedQuestion] = []\n\n    for question in completion.questions:\n        validated_question = _validate_question_feasibility(question, available_columns)\n        if validated_question is not None:\n            validated_questions.append(validated_question)\n\n    return validated_questions\n\n\ndef find_imports(module: ModuleType) -> list[str]:\n    \"\"\"\n    Get top-level third-party imports from a Python module.\n\n    Args:\n        module: Python module object to analyze\n\n    Returns:\n        list of third-party package names\n\n    Example:\n        >>> import my_module\n        >>> imports = find_third_party_imports(my_module)\n        >>> print(imports)  # ['pandas', 'numpy', 'requests']\n    \"\"\"\n    try:\n        # Get the source code of the module\n        source = inspect.getsource(module)\n        tree = ast.parse(source)\n\n        stdlib_modules = set(sys.stdlib_module_names)\n        third_party = set()\n\n        # Only look at top-level imports\n        for node in tree.body:\n            if isinstance(node, ast.Import):\n                for name in node.names:\n                    module_name = name.name.split(\".\")[0]\n                    if module_name not in stdlib_modules:\n                        third_party.add(module_name)\n\n            elif isinstance(node, ast.ImportFrom):\n                if node.module is None:\n                    continue\n                module_name = node.module.split(\".\")[0]\n                if module_name not in stdlib_modules:\n                    third_party.add(module_name)\n\n        return sorted(third_party)\n    except Exception:\n        return []\n\n\ndef get_tools() -> list[Tool]:\n    try:\n        # find all functions defined in the tools module\n        tool_functions = [func for func in dir(tools) if callable(getattr(tools, func))]\n\n        # find the function signatures and doc strings\n        tools_list = []\n        for func_name in tool_functions:\n            func = getattr(tools, func_name)\n            signature = inspect.signature(func)\n            docstring = inspect.getdoc(func)\n            tools_list.append(\n                Tool(\n                    name=func_name,\n                    signature=str(signature),\n                    docstring=docstring,\n                    function=func,\n                )\n            )\n        return tools_list\n    except Exception:\n        return []\n\n\nasync def _generate_run_charts_python_code(\n    request: RunChartsRequest,\n    validation_error: InvalidGeneratedCode | None = None,\n) -> str:\n    df = request.dataset.to_df().to_pandas()\n    question = request.question\n    dataframe_metadata = {\n        \"shape\": {\"rows\": int(df.shape[0]), \"columns\": int(df.shape[1])},\n        \"statistics\": df.describe(include=\"all\").to_dict(),\n        \"dtypes\": df.dtypes.astype(str).to_dict(),\n    }\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\",\n            content=prompts.SYSTEM_PROMPT_PLOTLY_CHART,\n        ),\n        ChatCompletionUserMessageParam(role=\"user\", content=f\"Question: {question}\"),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Metadata:\\n{dataframe_metadata}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data top 25 rows:\\n{df.head(25).to_string()}\"\n        ),\n    ]\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n\n    # Get response based on model mode\n    async with AsyncLLMClient() as client:\n        response: CodeGeneration = await client.chat.completions.create(\n            response_model=CodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0,\n            messages=messages,\n        )\n    return response.code\n\n\nasync def _generate_run_analysis_python_code(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n    validation_error: InvalidGeneratedCode | None = None,\n    attempt: int = 0,\n) -> str:\n    \"\"\"\n    Generate Python analysis code based on JSON data and question.\n\n    Parameters:\n    - request: RunAnalysisRequest containing data and question\n    - validation_errors: Past validation errors to include in prompt\n\n    Returns:\n    - Generated code\n    \"\"\"\n    # Convert dictionary data structure to list of columns for all datasets\n    logger.info(\"Starting code gen\")\n\n    all_columns = []\n    all_descriptions = []\n    all_data_types = []\n\n    dictionaries = [\n        await analyst_db.get_data_dictionary(name) for name in request.dataset_names\n    ]\n    for dictionary in dictionaries:\n        if dictionary is None:\n            continue\n        for entry in dictionary.column_descriptions:\n            all_columns.append(f\"{dictionary.name}.{entry.column}\")\n            all_descriptions.append(entry.description)\n            all_data_types.append(entry.data_type)\n\n    # Create dictionary format for prompt\n    dictionary_data = {\n        \"columns\": all_columns,\n        \"descriptions\": all_descriptions,\n        \"data_types\": all_data_types,\n    }\n\n    # Get sample data and shape info for all datasets\n    all_samples = []\n    all_shapes = []\n\n    logger.debug(f\"datasets: {request.dataset_names}\")\n    for dataset_name in request.dataset_names:\n        try:\n            dataset = (await analyst_db.get_cleansed_dataset(dataset_name)).to_df()\n        except Exception:\n            dataset = (await analyst_db.get_dataset(dataset_name)).to_df()\n        all_shapes.append(\n            f\"{dataset_name}: {dataset.shape[0]} rows x {dataset.shape[1]} columns\"\n        )\n        # Limit sample to 10 rows\n        sample_df = dataset.head(10)\n        all_samples.append(f\"{dataset_name}:\\n{sample_df}\")\n\n    shape_info = \"\\n\".join(all_shapes)\n    sample_data = \"\\n\\n\".join(all_samples)\n    logger.debug(\"Assembling messages\")\n    # Create messages for OpenAI\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\", content=prompts.SYSTEM_PROMPT_PYTHON_ANALYST\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Business Question: {request.question}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Shapes:\\n{shape_info}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Sample Data:\\n{sample_data}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\",\n            content=f\"Data Dictionary:\\n{json.dumps(dictionary_data)}\",\n        ),\n    ]\n\n    tools_list = get_tools()\n    if len(tools_list) > 0:\n        messages.append(\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=\"If it helps the analysis, you can optionally use following functions:\\n\"\n                + \"\\n\".join([str(t) for t in tools_list]),\n            )\n        )\n\n    logger.debug(f\"total_characters: {len(''.join([str(msg) for msg in messages]))}\")\n    # Add error context if available\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n        if attempt > 2:\n            messages.append(\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Convert the dataframe to pandas!\",\n                )\n            )\n    logger.info(\"Running Code Gen\")\n    logger.debug(messages)\n    async with AsyncLLMClient() as client:\n        completion: CodeGeneration = await client.chat.completions.create(\n            response_model=CodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0.1,\n            messages=messages,\n            max_retries=10,\n        )\n    logger.info(\"Code Gen complete\")\n    return completion.code\n\n\nasync def cleanse_dataframe(dataset: AnalystDataset) -> CleansedDataset:\n    \"\"\"Clean and standardize multiple pandas DataFrames in parallel.\n\n    Args:\n        datasets: List of AnalystDataset objects to clean\n    Returns:\n        List of CleansedDataset objects containing cleaned data and reports\n    Raises:\n        ValueError: If a dataset is empty\n    \"\"\"\n\n    if dataset.to_df().is_empty():\n        raise ValueError(f\"Dataset {dataset.name} is empty\")\n\n    df = dataset.to_df()\n    sample_df = df.sample(min(100, len(df)))\n\n    results = []\n    for col in df.columns:\n        results.append(process_column(df, col, sample_df))\n\n    # Create new DataFrame from processed columns\n    new_columns = {}\n    reports = []\n\n    for new_name, series, report in results:\n        new_columns[new_name] = series\n        reports.append(report)\n\n    cleaned_df = pl.DataFrame(new_columns)\n    add_summary_statistics(cleaned_df, reports)\n\n    return CleansedDataset(\n        dataset=AnalystDataset(\n            name=dataset.name,\n            data=cleaned_df,\n        ),\n        cleaning_report=reports,\n    )\n\n\n@log_api_call\nasync def rephrase_message(messages: ChatRequest) -> str:\n    \"\"\"Process chat messages history and return a new question\n\n    Args:\n        messages: list of message dictionaries with 'role' and 'content' fields\n\n    Returns:\n        Dict[str, str]: Dictionary containing response content\n    \"\"\"\n    # Convert messages to string format for prompt\n    messages_str = \"\\n\".join(\n        [f\"{msg['role']}: {msg['content']}\" for msg in messages.messages]\n    )\n\n    prompt_messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            content=prompts.SYSTEM_PROMPT_REPHRASE_MESSAGE,\n            role=\"system\",\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Message History:\\n{messages_str}\",\n            role=\"user\",\n        ),\n    ]\n    async with AsyncLLMClient() as client:\n        completion: EnhancedQuestionGeneration = await client.chat.completions.create(\n            response_model=EnhancedQuestionGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            messages=prompt_messages,\n        )\n\n    return completion.enhanced_user_message\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_charts(\n    request: RunChartsRequest,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunChartsResult:\n    \"\"\"Generate and validate chart code with retry logic\"\"\"\n    # Create messages for OpenAI\n    start_time = datetime.now()\n\n    if not request.dataset:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    df = request.dataset.to_df().to_pandas()\n    if exception_history is None:\n        exception_history = []\n\n    code = await _generate_run_charts_python_code(\n        request, next(iter(exception_history[::-1]), None)\n    )\n    try:\n        result = execute_python(\n            modules={\n                \"pd\": pd,\n                \"np\": np,\n                \"go\": go,\n                \"pl\": pl,\n                \"scipy\": scipy,\n            },\n            functions={\n                \"make_subplots\": make_subplots,\n            },\n            expected_function=\"create_charts\",\n            code=code,\n            input_data=df,\n            output_type=ChartGenerationExecutionResult,\n            allowed_modules={\n                \"pandas\",\n                \"numpy\",\n                \"plotly\",\n                \"scipy\",\n                \"datetime\",\n                \"polars\",\n            },\n        )\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=code, exception=e)\n\n    duration = datetime.now() - start_time\n\n    return RunChartsResult(\n        status=\"success\",\n        code=code,\n        fig1_json=result.fig1.to_json(),\n        fig2_json=result.fig2.to_json(),\n        metadata=RunAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history) + 1,\n        ),\n    )\n\n\n@log_api_call\nasync def run_charts(request: RunChartsRequest) -> RunChartsResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    try:\n        chart_result = await _run_charts(request)\n        return chart_result\n    except ValidationError:\n        return RunChartsResult(\n            status=\"error\", metadata=RunAnalysisResultMetadata(duration=0, attempts=1)\n        )\n    except MaxReflectionAttempts as e:\n        return RunChartsResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n\n\n@log_api_call\nasync def get_business_analysis(\n    request: GetBusinessAnalysisRequest,\n) -> GetBusinessAnalysisResult:\n    \"\"\"\n    Generate business analysis based on data and question.\n\n    Parameters:\n    - request: BusinessAnalysisRequest containing data and question\n\n    Returns:\n    - Dictionary containing analysis components\n    \"\"\"\n    try:\n        # Convert JSON data to DataFrame for analysis\n        start = datetime.now()\n\n        df = request.dataset.to_df().to_pandas()\n\n        # Get first 1000 rows as CSV with quoted values for context\n        df_csv = df.head(750).to_csv(index=False, quoting=1)\n\n        # Create messages for OpenAI\n        messages: list[ChatCompletionMessageParam] = [\n            ChatCompletionSystemMessageParam(\n                role=\"system\", content=prompts.SYSTEM_PROMPT_BUSINESS_ANALYSIS\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=f\"Business Question: {request.question}\",\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Analyzed Data:\\n{df_csv}\"\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=f\"Data Dictionary:\\n{request.dictionary.model_dump_json()}\",\n            ),\n        ]\n        async with AsyncLLMClient() as client:\n            completion: BusinessAnalysisGeneration = (\n                await client.chat.completions.create(\n                    response_model=BusinessAnalysisGeneration,\n                    model=ALTERNATIVE_LLM_BIG,\n                    temperature=0.1,\n                    messages=messages,\n                )\n            )\n        duration = (datetime.now() - start).total_seconds()\n        # Ensure all response fields are present\n        metadata = GetBusinessAnalysisMetadata(\n            duration=duration,\n            question=request.question,\n            rows_analyzed=len(df),\n            columns_analyzed=len(df.columns),\n        )\n        return GetBusinessAnalysisResult(\n            status=\"success\",\n            **completion.model_dump(),\n            metadata=metadata,\n        )\n\n    except Exception as e:\n        msg = type(e).__name__ + f\": {str(e)}\"\n        logger.error(f\"Error in get_business_analysis: {msg}\")\n        return GetBusinessAnalysisResult(\n            status=\"error\",\n            metadata=GetBusinessAnalysisMetadata(exception_str=msg),\n            additional_insights=\"\",\n            follow_up_questions=[],\n            bottom_line=\"\",\n        )\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_analysis(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunAnalysisResult:\n    start_time = datetime.now()\n\n    if not request.dataset_names:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    if exception_history is None:\n        exception_history = []\n    logger.info(f\"Running analysis (attempt {len(exception_history)})\")\n    code = await _generate_run_analysis_python_code(\n        request,\n        analyst_db,\n        next(iter(exception_history[::-1]), None),\n        attempt=len(exception_history),\n    )\n    logger.info(\"Code generated, preparing execution\")\n    dataframes: dict[str, pl.DataFrame] = {}\n\n    for dataset_name in request.dataset_names:\n        try:\n            dataset = (\n                await analyst_db.get_cleansed_dataset(dataset_name, max_rows=None)\n            ).to_df()\n        except Exception:\n            dataset = (\n                await analyst_db.get_dataset(dataset_name, max_rows=None)\n            ).to_df()\n        dataframes[dataset_name] = dataset\n    functions = {}\n    tool_functions = get_tools()\n    for tool in tool_functions:\n        functions[tool.name] = tool.function\n    try:\n        logger.info(\"Executing\")\n        result = execute_python(\n            modules={\n                \"pd\": pd,\n                \"np\": np,\n                \"sm\": sm,\n                \"pl\": pl,\n                \"scipy\": scipy,\n                \"sklearn\": sklearn,\n            },\n            functions=functions,\n            expected_function=\"analyze_data\",\n            code=code,\n            input_data=dataframes,\n            output_type=AnalystDataset,\n            allowed_modules={\n                \"pandas\",\n                \"numpy\",\n                \"scipy\",\n                \"sklearn\",\n                \"statsmodels\",\n                \"datetime\",\n                \"polars\",\n                *find_imports(tools),\n            },\n        )\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=code, exception=e)\n    logger.info(\"Execution done\")\n    duration = datetime.now() - start_time\n    return RunAnalysisResult(\n        status=\"success\",\n        code=code,\n        dataset=result,\n        metadata=RunAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history) + 1,\n            datasets_analyzed=len(dataframes),\n            total_rows_analyzed=sum(\n                len(df) for df in dataframes.values() if not df.is_empty()\n            ),\n            total_columns_analyzed=sum(\n                len(df.columns) for df in dataframes.values() if not df.is_empty()\n            ),\n        ),\n    )\n\n\n@log_api_call\nasync def run_analysis(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n) -> RunAnalysisResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    logger.debug(\"Entering run_analysis\")\n    log_memory()\n    try:\n        return await _run_analysis(request, analyst_db=analyst_db)\n    except MaxReflectionAttempts as e:\n        return RunAnalysisResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n    except ValueError as e:\n        return RunAnalysisResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=0,\n                attempts=1,\n                exception=AnalysisError.from_value_error(e),\n            ),\n        )\n\n\nasync def _generate_database_analysis_code(\n    request: RunDatabaseAnalysisRequest,\n    analyst_db: AnalystDB,\n    validation_error: InvalidGeneratedCode | None = None,\n) -> str:\n    \"\"\"\n    Generate Snowflake SQL analysis code based on data samples and question.\n\n    Parameters:\n    - request: DatabaseAnalysisRequest containing data samples and question\n\n    Returns:\n    - Dictionary containing generated code and description\n    \"\"\"\n\n    # Convert dictionary data structure to list of columns for all tables\n    dictionaries = [\n        await analyst_db.get_data_dictionary(name) for name in request.dataset_names\n    ]\n    all_tables_info = [d.model_dump(mode=\"json\") for d in dictionaries if d is not None]\n\n    # Get sample data for all tables\n    all_samples = []\n\n    for table in request.dataset_names:\n        df = (await analyst_db.get_dataset(table)).to_df().to_pandas()\n\n        sample_str = f\"Table: {table}\\n{df.head(10).to_string()}\"\n        all_samples.append(sample_str)\n\n    # Create messages for OpenAI\n    messages: list[ChatCompletionMessageParam] = [\n        get_external_database().get_system_prompt(),\n        ChatCompletionUserMessageParam(\n            content=f\"Business Question: {request.question}\",\n            role=\"user\",\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Sample Data:\\n{chr(10).join(all_samples)}\", role=\"user\"\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Data Dictionary:\\n{json.dumps(all_tables_info)}\", role=\"user\"\n        ),\n    ]\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n\n    # Get response from OpenAI\n    async with AsyncLLMClient() as client:\n        completion = await client.chat.completions.create(\n            response_model=DatabaseAnalysisCodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0.1,\n            messages=messages,\n        )\n\n    return completion.code\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_database_analysis(\n    request: RunDatabaseAnalysisRequest,\n    analyst_db: AnalystDB,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunDatabaseAnalysisResult:\n    start_time = datetime.now()\n    if not request.dataset_names:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    if exception_history is None:\n        exception_history = []\n\n    sql_code = await _generate_database_analysis_code(\n        request, analyst_db, next(iter(exception_history[::-1]), None)\n    )\n    try:\n        results = get_external_database().execute_query(query=sql_code)\n        results = cast(list[dict[str, Any]], results)\n        duration = datetime.now() - start_time\n\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=sql_code, exception=e)\n    return RunDatabaseAnalysisResult(\n        status=\"success\",\n        code=sql_code,\n        dataset=AnalystDataset(\n            data=results,\n        ),\n        metadata=RunDatabaseAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history),\n            datasets_analyzed=len(request.dataset_names),\n            # total_columns_analyzed=sum(len(ds.columns) for ds in request.datasets),\n        ),\n    )\n\n\n@log_api_call\nasync def run_database_analysis(\n    request: RunDatabaseAnalysisRequest, analyst_db: AnalystDB\n) -> RunDatabaseAnalysisResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    try:\n        return await _run_database_analysis(request, analyst_db)\n    except MaxReflectionAttempts as e:\n        return RunDatabaseAnalysisResult(\n            status=\"error\",\n            metadata=RunDatabaseAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n    except ValueError as e:\n        return RunDatabaseAnalysisResult(\n            status=\"error\",\n            metadata=RunDatabaseAnalysisResultMetadata(\n                duration=0,\n                attempts=1,\n                exception=AnalysisError.from_value_error(e),\n            ),\n        )\n\n\n# Type definitions\n@dataclass\nclass AnalysisGenerationError:\n    message: str\n    original_error: BaseException | None = None\n\n\nasync def execute_business_analysis_and_charts(\n    analysis_result: RunAnalysisResult | RunDatabaseAnalysisResult,\n    enhanced_message: str,\n    enable_chart_generation: bool = True,\n    enable_business_insights: bool = True,\n) -> tuple[\n    RunChartsResult | BaseException | None,\n    GetBusinessAnalysisResult | BaseException | None,\n]:\n    analysis_result.dataset = cast(AnalystDataset, analysis_result.dataset)\n    # Prepare both requests\n    chart_request = RunChartsRequest(\n        dataset=analysis_result.dataset,\n        question=enhanced_message,\n    )\n\n    business_request = GetBusinessAnalysisRequest(\n        dataset=analysis_result.dataset,\n        dictionary=DataDictionary.from_analyst_df(analysis_result.dataset.to_df()),\n        question=enhanced_message,\n    )\n\n    if enable_chart_generation and enable_business_insights:\n        # Run both analyses concurrently\n        result = await asyncio.gather(\n            run_charts(chart_request),\n            get_business_analysis(business_request),\n            return_exceptions=True,\n        )\n\n        return (result[0], result[1])\n    elif enable_chart_generation:\n        charts_result = await run_charts(chart_request)\n        return charts_result, None\n    else:\n        business_result = await get_business_analysis(business_request)\n        return None, business_result\n\n\nasync def run_complete_analysis(\n    chat_request: ChatRequest,\n    data_source: DataSourceType,\n    datasets_names: list[str],\n    analyst_db: AnalystDB,\n    chat_id: str,\n    message_id: str,\n    enable_chart_generation: bool = True,\n    enable_business_insights: bool = True,\n) -> AsyncGenerator[Component | AnalysisGenerationError, None]:\n    user_message = await analyst_db.get_chat_message(message_id=message_id)\n    if user_message is None or user_message.role != \"user\":\n        yield AnalysisGenerationError(\"Message not found\")\n\n        return\n    # Get enhanced message\n    try:\n        logger.info(\"Getting rephrased question...\")\n        enhanced_message = await rephrase_message(chat_request)\n        logger.info(\"Getting rephrased question done\")\n\n        yield enhanced_message\n\n    except ValidationError:\n        user_message.error = \"LLM Error, please retry\"\n        user_message.in_progress = False\n        await analyst_db.update_chat_message(\n            message_id=message_id,\n            message=user_message,\n        )\n        yield AnalysisGenerationError(user_message.error)\n\n        return\n\n    assistant_message = AnalystChatMessage(\n        role=\"assistant\",\n        content=enhanced_message,\n        components=[EnhancedQuestionGeneration(enhanced_user_message=enhanced_message)],\n    )\n\n    user_message.in_progress = False\n    await analyst_db.update_chat_message(\n        message_id=message_id,\n        message=user_message,\n    )\n    await analyst_db.add_chat_message(chat_id=chat_id, message=assistant_message)\n    # Run main analysis\n    logger.info(\"Start main analysis\")\n    try:\n        is_database = data_source == DataSourceType.DATABASE\n        logger.info(\"Getting analysis result...\")\n        log_memory()\n\n        if is_database:\n            analysis_result: (\n                RunAnalysisResult | RunDatabaseAnalysisResult\n            ) = await run_database_analysis(\n                RunDatabaseAnalysisRequest(\n                    dataset_names=datasets_names,\n                    question=enhanced_message,\n                ),\n                analyst_db,\n            )\n        else:\n            analysis_result = await run_analysis(\n                RunAnalysisRequest(\n                    dataset_names=datasets_names,\n                    question=enhanced_message,\n                ),\n                analyst_db,\n            )\n\n        log_memory()\n        logger.info(\"Getting analysis result done\")\n\n        if isinstance(analysis_result, BaseException):\n            error_message = f\"Error running initial analysis. Try rephrasing: {str(analysis_result)}\"\n            assistant_message.in_progress = False\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n            return\n\n        yield analysis_result\n\n        assistant_message.components.append(analysis_result)\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n    except Exception as e:\n        error_message = f\"Error running initial analysis. Try rephrasing: {str(e)}\"\n        assistant_message.in_progress = False\n        assistant_message.error = error_message\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n        yield AnalysisGenerationError(error_message)\n\n        return\n\n    # Only proceed with additional analysis if we have valid initial results\n    if not (\n        analysis_result\n        and analysis_result.dataset\n        and (enable_chart_generation or enable_business_insights)\n    ):\n        assistant_message.in_progress = False\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n        return\n\n    # Run concurrent analyses\n    try:\n        charts_result, business_result = await execute_business_analysis_and_charts(\n            analysis_result,\n            enhanced_message,\n            enable_business_insights=enable_business_insights,\n            enable_chart_generation=enable_chart_generation,\n        )\n\n        # Handle chart results\n        if isinstance(charts_result, BaseException):\n            error_message = \"Error generating charts\"\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n        elif charts_result is not None:\n            assistant_message.components.append(charts_result)\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield charts_result\n\n        # Handle business analysis results\n        if isinstance(business_result, BaseException):\n            error_message = \"Error generating business insights\"\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n        elif business_result is not None:\n            assistant_message.components.append(business_result)\n            assistant_message.in_progress = False\n\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield business_result\n\n    except Exception as e:\n        error_message = f\"Error setting up additional analysis: {str(e)}\"\n        assistant_message.in_progress = False\n        assistant_message.error = error_message\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n        yield AnalysisGenerationError(error_message)\n\n\nasync def process_data_and_update_state(\n    new_dataset_names: list[str],\n    analyst_db: AnalystDB,\n    data_source: str | DataSourceType,\n) -> AsyncGenerator[str, None]:\n    \"\"\"Process datasets and yield progress updates asynchronously.\"\"\"\n    # Start processing and yield initial message\n    logger.info(\"Starting data processing\")\n    log_memory()\n    yield \"Starting data processing\"\n\n    # Handle data cleansing based on the source\n    # Convert string data_source to DataSourceType if needed\n    data_source_type = (\n        data_source\n        if isinstance(data_source, DataSourceType)\n        else DataSourceType(data_source)\n    )\n    if data_source_type != DataSourceType.DATABASE:\n        try:\n            logger.info(\"Cleansing datasets\")\n            yield \"Cleansing datasets\"\n            for analysis_dataset_name in new_dataset_names:\n                analysis_dataset = await analyst_db.get_dataset(\n                    analysis_dataset_name, max_rows=None\n                )\n                cleansed_dataset = await cleanse_dataframe(analysis_dataset)\n                await analyst_db.register_dataset(\n                    cleansed_dataset, data_source=DataSourceType.GENERATED\n                )\n                yield f\"Cleansed dataset: {analysis_dataset_name}\"\n                del cleansed_dataset\n                del analysis_dataset\n                log_memory()\n\n            logger.info(\"Cleansing datasets complete\")\n            yield \"Cleansing datasets complete\"\n            log_memory()\n        except Exception:\n            logger.error(\"Data processing failed\", exc_info=True)\n            yield \"Data processing failed\"\n            raise\n    else:\n        pass\n\n    # Generate data dictionaries\n    logger.info(\"Data processing successful, generating dictionaries\")\n    yield \"Data processing successful, generating dictionaries\"\n    log_memory()\n    try:\n        for analysis_dataset_name in new_dataset_names:\n            try:\n                existing_dictionary = await analyst_db.get_data_dictionary(\n                    analysis_dataset_name\n                )\n                logger.info(\n                    f\"Found existing dictionary for dataset: {analysis_dataset_name}\"\n                )\n                if existing_dictionary is not None:\n                    continue\n\n            except Exception:\n                pass\n            logger.info(f\"Creating dictionary for dataset: {analysis_dataset_name}\")\n            analysis_dataset = await analyst_db.get_dataset(analysis_dataset_name)\n            new_dictionary = await get_dictionary(analysis_dataset)\n            logger.info(new_dictionary.to_application_df())\n            del analysis_dataset\n            await analyst_db.register_data_dictionary(new_dictionary)\n            logger.info(f\"Registered dictionary for dataset: {analysis_dataset_name}\")\n            yield f\"Registered data dictionary: {analysis_dataset_name}\"\n            log_memory()\n            continue\n    except Exception:\n        logger.error(\"Failed to generate data dictionaries\", exc_info=True)\n        yield \"Failed to generate data dictionaries\"\n        raise\n    log_memory()\n    # Final completion message\n    yield \"Processing complete\"\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/utils/api.py",
    "size": 55548,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "openai",
      "streamlit",
      "datarobot",
      "deployment",
      "llm"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "utils/tools.py",
    "file_name": "tools.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Add additional tools that can be used by the analysis code execution. Remember to include the necessary imports and provide a docstring and signature for each function.\n# signature and docstring will be provided to the LLM in the prompt.\n# Uncomment the examples below to get started.\n\n\n# import datarobot as dr\n# import pandas as pd\n# from datarobot_predict.deployment import predict\n\n# def calculate_summary(df: pd.DataFrame) -> pd.DataFrame:\n#     \"\"\"\n#     Calculate summary statistics for a DataFrame.\n\n#     Args:\n#         df (pd.DataFrame): Input DataFrame.\n\n#     Returns:\n#         pd.DataFrame: Summary statistics including count, mean, std, min, max, and percentiles.\n#     \"\"\"\n#     description = df.describe(percentiles=[0.2, 0.4, 0.6, 0.8])\n#     return description\n\n\n# def filter_data(df: pd.DataFrame, column: str, value: float) -> pd.DataFrame:\n#     \"\"\"\n#     Filter DataFrame based on a condition.\n#     Args:\n#         df (pd.DataFrame): Input DataFrame.\n#         column (str): Column to apply the filter on.\n#         value (float): Value to compare against in the filter.\n#     Returns:\n#         pd.DataFrame: Filtered DataFrame where the specified column's values are greater than the given value.\n#     \"\"\"\n#     filtered_df = df[df[column] > value]\n#     return filtered_df\n\n\n# def call_datarobot_deployment(df: pd.DataFrame, deployment_id: str) -> pd.DataFrame:\n#     \"\"\"\n#     Call a DataRobot deployment to get predictions.\n\n#     Args:\n#         df (pd.DataFrame): Input DataFrame with features for prediction.\n#         deployment_id (str): ID of the DataRobot deployment to use for predictions.\n\n#     Returns:\n#         pd.DataFrame: DataFrame containing the predictions from DataRobot. The prediction column is named 'predictions'.\n#     \"\"\"\n#     deployment = dr.Deployment.get(deployment_id)  # type: ignore[attr-defined]\n#     prediction_response: pd.DataFrame = predict(\n#         deployment=deployment, data_frame=df\n#     ).dataframe\n\n#     prediction_response.columns = [\n#         c.replace(\"_PREDICTION\", \"\")\n#         for c in prediction_response.columns  # type: ignore[assignment]\n#     ]\n\n#     if deployment.model is not None:\n#         target_column = deployment.model.get(\"target_name\")\n#         if target_column:\n#             prediction_response[\"predictions\"] = prediction_response[target_column]\n\n#     return prediction_response\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/utils/tools.py",
    "size": 2968,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "deployment",
      "datarobot",
      "llm"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "infra/settings_generative.py",
    "file_name": "settings_generative.py",
    "file_type": "python",
    "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import annotations\n\nimport datarobot as dr\nimport pulumi\nimport pulumi_datarobot as datarobot\nfrom datarobot_pulumi_utils.pulumi.stack import PROJECT_NAME\nfrom datarobot_pulumi_utils.schema.custom_models import (\n    CustomModelArgs,\n    DeploymentArgs,\n    RegisteredModelArgs,\n)\nfrom datarobot_pulumi_utils.schema.exec_envs import RuntimeEnvironments\nfrom datarobot_pulumi_utils.schema.llms import (\n    LLMBlueprintArgs,\n    LLMs,\n    LLMSettings,\n    PlaygroundArgs,\n)\n\nfrom utils.schema import LLMDeploymentSettings\n\nLLM = LLMs.AZURE_OPENAI_GPT_4_O\n\ncustom_model_args = CustomModelArgs(\n    resource_name=f\"Generative Analyst Custom Model [{PROJECT_NAME}]\",\n    name=\"Generative Analyst Assistant\",  # built-in QA app uses this as the AI's name\n    target_name=LLMDeploymentSettings().target_feature_name,\n    target_type=dr.enums.TARGET_TYPE.TEXT_GENERATION,\n    replicas=2,\n    base_environment_id=RuntimeEnvironments.PYTHON_312_MODERATIONS.value.id,\n    opts=pulumi.ResourceOptions(delete_before_replace=True),\n)\n\nregistered_model_args = RegisteredModelArgs(\n    resource_name=f\"Generative Analyst Registered Model [{PROJECT_NAME}]\",\n)\n\n\ndeployment_args = DeploymentArgs(\n    resource_name=f\"Generative Analyst Deployment [{PROJECT_NAME}]\",\n    label=f\"Generative Analyst Deployment [{PROJECT_NAME}]\",\n    association_id_settings=datarobot.DeploymentAssociationIdSettingsArgs(\n        column_names=[\"association_id\"],\n        auto_generate_id=False,\n        required_in_prediction_requests=True,\n    ),\n    predictions_data_collection_settings=datarobot.DeploymentPredictionsDataCollectionSettingsArgs(\n        enabled=True,\n    ),\n    predictions_settings=(\n        datarobot.DeploymentPredictionsSettingsArgs(min_computes=0, max_computes=2)\n    ),\n)\n\nplayground_args = PlaygroundArgs(\n    resource_name=f\"Generative Analyst Playground [{PROJECT_NAME}]\",\n)\n\nllm_blueprint_args = LLMBlueprintArgs(\n    resource_name=f\"Generative Analyst LLM Blueprint [{PROJECT_NAME}]\",\n    llm_id=LLM.name,\n    llm_settings=LLMSettings(\n        max_completion_length=2048,\n        temperature=0.1,\n    ),\n)\n",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/infra/settings_generative.py",
    "size": 2692,
    "description": "Copyright 2024 DataRobot, Inc.",
    "tags": [
      "deployment",
      "datarobot",
      "llm",
      "openai"
    ]
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "infra/__init__.py",
    "file_name": "__init__.py",
    "file_type": "python",
    "content": "",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/infra/__init__.py",
    "size": 0,
    "description": "Python: __init__",
    "tags": []
  },
  {
    "repo_name": "talk-to-my-data-agent",
    "file_path": "utils/__init__.py",
    "file_name": "__init__.py",
    "file_type": "python",
    "content": "",
    "url": "https://github.com/datarobot-community/talk-to-my-data-agent/blob/main/utils/__init__.py",
    "size": 0,
    "description": "Python: __init__",
    "tags": []
  }
]