[
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "DataPrep/dataprep_functions.py",
    "file_name": "dataprep_functions.py",
    "file_type": "python",
    "content": "__author__ = 'Callum Finlayson'\n\nimport requests,json,re\nfrom collections import OrderedDict\n\n# (1) Get Library Name and schema from LibraryID\ndef get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_schema_dict\n\n# (1a) Get Library Name and schema from LibraryID (and Version)\ndef get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    library_name = \"\"\n    library_schema_dict = {}\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources.get('name')\n        library_schema_dict = jdata_datasources.get('schema')\n    return library_name,library_schema_dict\n\n\n# (2) Get all of the datasources from Paxata that are tagged with \"tag\"\ndef get_tagged_library_items(auth_token,paxata_url,tag):\n    tagged_datasets = []\n    get_tags_request = (paxata_url + \"/rest/library/tags\")\n    get_tags_response = requests.get(get_tags_request, auth=auth_token, verify=False)\n    if (get_tags_response.ok):\n        AllTagsDatasetsJson = json.loads(get_tags_response.content)\n        i=0\n        number_of_datasets = len(AllTagsDatasetsJson)\n        while i < number_of_datasets:\n            if (AllTagsDatasetsJson[i].get('name') == tag):\n                tagged_datasets.append(AllTagsDatasetsJson[i].get('dataFileId'), AllTagsDatasetsJson[i].get('name'))\n            i += 1\n    else:\n        print(\"bad request> \" + get_tags_response.status_code)\n    return tagged_datasets\n\n# (3) POST Library data from Paxata and load it into a JSON structure\ndef get_paxata_library_data(auth_token,paxata_url,library_dataset_id):\n    post_request = (paxata_url + \"/rest/datasource/exports/local/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData\n\n# (4) Get the Name of a Library from it's DatasetID\ndef get_name_of_datasource(auth_token_source,paxata_url_source,libraryId):\n    url_request = (paxata_url_source + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataDataSources = json.loads(my_response.content)\n        libraryName = jDataDataSources[0].get('name')\n    return libraryName\n\n# (5) Get all the Projects that have been described with a specific \"description_tag\"\ndef get_all_project_information(auth_token_source, paxata_url_source, description_tag):\n    Package_Tagged_Projects = []\n    package_counter = 0\n    max_num_of_projects = 0\n    ProjectNames = []\n    url_request = (paxata_url_source + \"/rest/projects\")\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataProjectIds = json.loads(my_response.content)\n        for item in jDataProjectIds:\n            if description_tag == jDataProjectIds[package_counter].get('description'):\n                ProjectNames.append(jDataProjectIds[package_counter].get('name'))\n                Package_Tagged_Projects.append(jDataProjectIds[package_counter].get('projectId'))\n                max_num_of_projects += 1\n            package_counter += 1\n    return Package_Tagged_Projects\n\n# (6) Post a file to the Paxata library (Paxata will guess how to parse it), return the new libraryId\ndef post_file_to_paxata_library(auth_token_target,paxata_url_target, new_file_name):\n    new_libraryId = \"\"\n    if new_file_name is None:\n        print(\"File doesn't exist??\")\n    else:\n        ds = str(new_file_name)\n        print(\"Uploading \\\"\" + str(new_file_name) + \"\\\" to Library\")\n        sourcetype = {'source': 'local'}\n        files = {'data': open(ds, 'rb')}\n        dataset_upload_response = \"\"\n        try:\n            dataset_upload_response = requests.post(paxata_url_target + \"/rest/datasource/imports/local\", data=sourcetype,\n                                                  files=files, auth=auth_token_target)\n        except:\n            print(\"Connection error. Please validate the URL provided: \" + paxata_url_target.url)\n        if not (dataset_upload_response.ok):\n            print(\"Couldn't upload the library data. Status Code = \" + str(dataset_upload_response.status_code))\n        else:\n            jDataDataSources = json.loads(dataset_upload_response.content)\n            new_libraryId = jDataDataSources.get('dataFileId')\n    return new_libraryId\n\n# (7) Check if a Project Name exists and return it's ID\ndef check_if_a_project_exists(auth_token,paxata_url,project_name):\n    projectId = \"\"\n    url_request = (paxata_url + \"/rest/projects?name=\" + project_name)\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        jdata_new_project_response = json.loads(my_response.content)\n        if (not jdata_new_project_response):\n            projectId = 0\n        else:\n            projectId = jdata_new_project_response[0]['projectId']\n    else:\n        my_response.raise_for_status()\n    return projectId\n\n# (8) Delete a project based on ID (TEST THIS), not sure if i can access the content directly\ndef delete_a_project_if_it_exists(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/projects/\" + str(projectId))\n    my_response = requests.delete(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", my_response.content.get('name'), \"\\\" deleted.\")\n    else:\n        my_response.raise_for_status()\n\n# (9) Run a Project and publish the answerset to the library\ndef run_a_project(auth_token,paxata_url,projectId):\n    post_request = (paxata_url + \"/rest/project/publish?projectId=\" + projectId + \"&all=true\")\n    postResponse = requests.post(post_request, auth=auth_token, verify=False)\n    if (postResponse.ok):\n        print(\"Project Run - \", projectId)\n    else:\n        print(\"Something went wrong with POST call \", str(postResponse))\n    # I need to investigate the below, sometimes postResponse.content is a dict, sometimes a list, hence the two below trys\n    try:\n        AnswersetId = json.loads(postResponse.content)[0].get('dataFileId')\n    except(AttributeError):\n        AnswersetId = json.loads(postResponse.content).get('dataFileId', 0)\n    return AnswersetId\n\n# (10) Get the script of a projectId\ndef get_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        json_of_empty_project = json.loads(my_response.content)\n    else:\n        json_of_empty_project = 0\n        my_response.raise_for_status()\n    #the below return has an index of 0 to only return the latest version\n    return json_of_empty_project[0]\n\n# (11) Replace values in a json file (useful for updating Paxata project scripts)\ndef replace_json_values(json_file,oldvalue,newvalue):\n    fileinput = json.dumps(json_file)\n    newfileoutput = []\n    newfile = \"\"\n    newfile = re.sub(oldvalue, newvalue, fileinput.rstrip())\n    newfileoutput = json.loads(newfile)\n    return newfileoutput\n\n# (12) Create a new (empty) Paxata project. Will return the projectId\ndef create_a_new_project(auth_token_target,paxata_url_target,Project_Name):\n    projectId = \"\"\n    url_request = (paxata_url_target + \"/rest/projects?name=\" + Project_Name)\n    my_response = requests.post(url_request,auth=auth_token_target , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", Project_Name ,\"\\\" created.\")\n        jdata_new_project_response = json.loads(my_response.content)\n        projectId = jdata_new_project_response['projectId']\n    else:\n        if my_response.status_code == 409:\n            print(\"Project Already Exists\")\n        else:\n            my_response.raise_for_status()\n    return projectId\n\n# (13) Update an existing Project with a new script file (this is not recommended)\ndef update_project_with_new_script(auth_token,paxata_url,final_updated_json_script,projectId,working_path):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(final_updated_json_script)}\n    my_response = requests.put(url_request, data=s, auth=auth_token, verify=False)\n    if (not my_response.ok):\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        with open(working_path + '/invalid_script_dump.json', 'w') as f:\n            json.dump(final_updated_json_script, f)\n        my_response.raise_for_status()\n\n# (14) Get an existing Project's script file\ndef get_new_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId + \"&version=\" + \"0\")\n    myResponse = requests.get(url_request, auth=auth_token, verify=False)\n    if (myResponse.ok):\n        # Loads (Load String) takes a Json file and converts into python data structure (dict or list, depending on JSON)\n        json_of_empty_project = json.loads(myResponse.content)\n    else:\n        json_of_empty_project = 0\n        myResponse.raise_for_status()\n    return(json_of_empty_project)\n\n# (15) Delete Library Data from LibraryID\ndef delete_library_item(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.delete(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        library_name = jdata_datasources.get('name')\n    return library_name\n\n# (16) Export/POST a libraryItem(Answerset) to a target\ndef post_paxata_library_data(auth_token,paxata_url,library_dataset_id,pax_datasourceId,pax_connectorId):\n    post_request = (paxata_url + \"/rest/datasource/exports/\" + pax_datasourceId +\"/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData\n\n# (17) Get DatasourceId and ConnectorId from Name of the Datasource\ndef get_datasource_id_from_name(auth_token,paxata_url,datasource_name):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == datasource_name:\n                pax_datasourceId = jdata_datasources[0].get('dataSourceId')\n                pax_connectorId = jdata_datasources[0].get('connectorId')\n            row_count +=1\n    return pax_datasourceId,pax_connectorId\n\n# (18) Get the UserID From the REST API Token\ndef get_user_from_token(auth_token, paxata_url, resttoken):\n    url_request = (paxata_url + \"/rest/users?authToken=true\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('authToken') == resttoken:\n                pax_name = jdata_datasources[row_count].get('name')\n                pax_userId = jdata_datasources[row_count].get('userId')\n            row_count += 1\n    return pax_userId, pax_name\n\n# (19) Check that an Export/POST to an external source has been completed (version 2.22)\ndef get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token)\n    if (get_response.ok):\n        jdata_datasources = json.loads(get_response.content)\n        print(\"Succesfully have the exportId status\")\n        exportIdStatus = jdata_datasources[0].get('exportId')\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return exportIdStatus\n\n# (20) Check that an Export/POST to an external source has been completed (all versions)\ndef get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token, verify=False)\n    if (get_response.ok):\n        # In version 2.22 postResponse.content is a dict, prior to that it is a list which i need to manually iterate through, hence the two below trys\n        print('.')\n        jdata_datasources = json.loads(get_response.content)\n        try:\n            exportIdState = jdata_datasources.get('state')\n            exporttimeStarted = jdata_datasources.get('timeStarted')\n            exporttimeFinished = jdata_datasources.get('timeFinished')\n        except(AttributeError):\n            row_count = 0\n            for row in jdata_datasources:\n                if jdata_datasources[row_count].get('exportId') == pax_exportId:\n                    exportIdState = jdata_datasources[row_count].get('state')\n                    exporttimeStarted = jdata_datasources[row_count].get('timeStarted')\n                    exporttimeFinished = jdata_datasources[row_count].get('timeFinished')\n                row_count += 1\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return(exportIdState,exporttimeStarted,exporttimeFinished)\n\n# (21) Get the datasetId of a Library from it's Name\ndef get_id_of_datasource(auth_token,paxata_url,dataset_name):\n    url_request = (paxata_url + \"/library/data/\")\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    dataFileId = 0\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == dataset_name:\n                dataFileId = jdata_datasources[row_count].get('dataFileId')\n            row_count +=1\n    return dataFileId\n\n\n# (22) Get all of the datasources (ordered_ for a tenant)\ndef get_datasource_configs(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    myResponse = requests.get(url_request, auth=authorization_token, verify=True)\n    if (myResponse.ok):\n        json_of_datasource_configs = json.loads(myResponse.content)\n    else:\n        json_of_datasource_configs = 0\n        myResponse.raise_for_status()\n    dict_of_datasources = {}\n    for item in json_of_datasource_configs:\n#        dict_of_datasources[item.get('connectorId')] = item.get('name')\n        dict_of_datasources[item.get('name')] = item.get('dataSourceId')\n\n    dict_of_datasources['0'] = ' - No Connector - Data already exists in Paxata - '\n    #returning a sorted dictionary\n    return(OrderedDict(sorted(dict_of_datasources.items(), key=lambda kv:(kv[0].lower(),kv[1]))))\n\n# (23) Get All columns names for a library item\ndef get_library_item_metadata(authorization_token,paxata_url,dataFileID):\n    url_request = (paxata_url + \"/rest/library/data/\"+ dataFileID)\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns\n\n# (24) Get ALl users on a tenant\ndef get_users_on_tenant(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/users\")\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns\n\n# (25) Extract key values out of a json file\ndef extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n\n# (26) Get the Library items of a Project\ndef get_libraryIds_of_lenses_exported_by_project(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/project/publish?projectId=\"+ projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items\n\n\n# (27) Get the information of where a =Library item was exported (ie which external system)\ndef get_info_of_exported_library_items(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/exports\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    json_of_library_items = []\n    if(my_response.ok):\n        library_export_info = json.loads(my_response.content)\n        for item in library_export_info:\n            if item.get('dataFileId') == libraryId:\n                i+=1\n                print(item.get('destination'))\n                print(\"Data Export - \" ,str(i) + \" for libraryId(\" ,libraryId + \")\")\n                print(\"Filename = \" , item.get('destination').get('name'))\n                print(\"Path = \" , item.get('destination').get('itemPath'))\n                print(\"ConnectorID = \" , item.get('destination').get('connectorId') + \"\\n\")\n                json_of_library_items.append(str(item.get('destination')))\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items\n\n\n# (28) Update an existing Project's script file\ndef update_project_with_new_script(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result\n\n# (29) Update an existing Project's Datasource file\ndef update_project_with_new_dataset(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=datasets&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result\n\n# (30) get_name_latest_version_and_schema_of_datasource\ndef get_name_latest_version_and_schema_of_datasource(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request, auth=auth_token, verify=True)\n    library_name = \"\"\n    library_schema_dict = \"\"\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_version = jdata_datasources[0].get('version')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_version,library_schema_dict\n",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/DataPrep/dataprep_functions.py",
    "size": 21427,
    "description": "(1) Get Library Name and schema from LibraryID",
    "tags": []
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Modeling/Python/run_specifc_blueprints.py",
    "file_name": "run_specifc_blueprints.py",
    "file_type": "python",
    "content": "#Author: Thodoris Petropoulos\n\ndef run_specific_blueprints(project_object, search_term, featurelist_id = None):\n    \"\"\"Runs all of the blueprints that match the search term use provides\n        Input:\n        - project_object <DataRobot Project> (Your DataRobot project)\n        - search_term <string> (What to search for in the name of the Blueprint. e.g: \"Gradient\") \n        - featurelist_id <DataRobot Featurelist id> (Optional parameter to specify featurelist to use)\n    \"\"\"\n\n    blueprints = project_object.get_blueprints()\n    models_to_run = [blueprint for blueprint in blueprints if blueprint.model_type == search_term]\n    for model in models_to_run:\n        project_object.train(model, sample_pct = 80, featurelist_id=featurelist_id)\n\n    while len(project_object.get_all_jobs()) > 0:\n    time.sleep(1)\n    pass\n        \n    ",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Modeling/Python/run_specifc_blueprints.py",
    "size": 837,
    "description": "Author: Thodoris Petropoulos",
    "tags": [
      "modeling"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_clone_project.py",
    "file_name": "ts_clone_project.py",
    "file_type": "python",
    "content": "#Author: Katy Chow Haynie\n\n#Make sure you are connected to DataRobot Client.\n\n\n#This function will help you create a copy of a TS project using the same exact settings. Manipulate as you see fit.\n\nimport datarobot as dr\n\ndef clone_ts_project(pid):\n    \"\"\"This function will copy a DataRobot TS project with the same settings\n        Input:\n         - pid <str> the id of the project you want to copy\n         \n        Manipulate this function as you see fit in case you dont want a complete 1:1 copy.\n    \"\"\"\n    p = dr.Project.get(pid)\n    c_p = p.clone_project('Clone of {}'.format(p.project_name))\n    c_pid = c_p.id\n    \n    #Get datetimePartitioning data\n    #This will include calendar, backtesting and known-in-advance features.\n    dtp = dr.DatetimePartitioning.get(pid)\n    dtp_spec_for_clone = dtp.to_specification()\n    \n    #Fix the datetime_partition_column which will have an ' (actual)' string appended to it.\n    dtp_spec_for_clone.datetime_partition_column = dtp_spec_for_clone.datetime_partition_column.replace(' (actual)','')\n    \n    ##Place changes below##\n    #Manipulate dtp_spec_for_clone as you see fit (you can directly change its attribites)\n    \n    ##\n    \n    c_p.set_target(target = 'Sales',\n                       partitioning_method = dtp_spec_for_clone,\n                       mode = 'auto',\n                       worker_count = -1\n                  )\n    \n##Usage##\n#clone_ts_project('YOUR_PROJECT_ID')",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_clone_project.py",
    "size": 1438,
    "description": "Author: Katy Chow Haynie",
    "tags": [
      "datarobot-api"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_clustering.py",
    "file_name": "ts_clustering.py",
    "file_type": "python",
    "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport datetime as dt\nimport operator\n\nimport datarobot as dr\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy.spatial.distance import cdist, pdist, squareform\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom statsmodels.tsa.stattools import pacf\n\nfrom ts_metrics import *\nfrom ts_modeling import create_dr_project\nfrom ts_projects import get_preds_and_actuals\n\n\n####################\n# Series Clustering\n####################\n\n\ndef _split_series(df, series_id, target, by='quantiles', cuts=5, split_col='Cluster'):\n    \"\"\"\n    Split series into clusters by rank or quantile  of average target value\n\n    by: str\n        Rank or quantiles\n    cuts: int\n        Number of clusters\n    split_col: str\n        Name of new column\n\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    group = df.groupby([series_id]).mean()\n\n    if by == 'quantiles':\n        group[split_col] = pd.qcut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    elif by == 'rank':\n        group[split_col] = pd.cut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    else:\n        raise ValueError(f'{by} is not a supported value. Must be set to either quantiles or rank')\n\n    df = df.merge(\n        group[split_col], how='left', left_on=series_id, right_index=True, validate='many_to_one'\n    )\n\n    df[split_col] = df[split_col].astype('str')\n    n_clusters = len(df[split_col].unique())\n    mapper_clusters = {k: v for (k, v) in zip(df[split_col].unique(), range(1, n_clusters + 1))}\n    df[split_col] = df[split_col].map(mapper_clusters)\n\n    return df.reset_index(drop=True)\n\n\ndef _get_pacf_coefs(df, col, nlags, alpha, scale, scale_method):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    df: pandas df\n    col: str\n        Series name\n    nlags: int\n        Number of AR coefficients to include in pacf\n    alpha: float\n        Cutoff value for p-values to determine statistical significance\n    scale: boolean\n        Whether to standardize input data\n    scale_method: str\n        Choose from 'min_max' or 'normalize'\n\n    Returns:\n    --------\n    List of AR(n) coefficients\n\n    \"\"\"\n    if scale:\n        if scale_method == 'min_max':\n            df = df.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)), axis=0)\n        elif scale_method == 'normalize':\n            df = df.apply(lambda x: (x - np.mean(x)) / np.std(x), axis=0)\n        else:\n            raise ValueError(\n                f'{scale_method} is not a supported value. scale_method must be set to either min_max or normalize'\n            )\n\n    # if df[col].dropna().shape[0] == 0:\n    #     print(col, df[col].dropna())\n    # print('Running PAC...')\n    clf = pacf(df[col].dropna(), method='ols', nlags=nlags, alpha=alpha)\n    if alpha:\n        coefs = clf[0][1:]\n        zero_in_interval = [not i[0] < 0 < i[1] for i in clf[1][1:]]\n        adj_coefs = [c if z else 0.0 for c, z in zip(coefs, zero_in_interval)]\n        return adj_coefs\n    else:\n        coefs = clf[1:]\n        return coefs\n\ndef _get_optimal_n_clusters(df, n_series, max_clusters, plot=True):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    Get the number of clusters that results in the max silhouette score\n\n    Returns:\n    --------\n    int\n\n    \"\"\"\n    clusters = list(np.arange(min(max_clusters, n_series)) + 2)[:-1]\n    print(f'Testing {clusters[0]} to {clusters[-1]} clusters')\n    scores = {}\n    d = []\n    for c in clusters:\n        kmean = KMeans(n_clusters=c).fit(df)\n        d.append(sum(np.min(cdist(df, kmean.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n        preds = kmean.predict(df)\n        score = silhouette_score(df, preds, metric='euclidean')\n        scores[c] = score\n        print(f'For n_clusters = {c}, silhouette score is {score}')\n\n    n_clusters = max(scores.items(), key=operator.itemgetter(1))[0]\n    best_score = scores[n_clusters]\n    print(f'optimal n_clusters = {n_clusters}, max silhouette score is {best_score}')\n\n    if max_clusters > 2:\n        if plot:\n            fig = px.line(x=clusters, y=d)\n            fig.update_layout(height=500, width=750, title_text='Kmeans Optimal Number of Clusters')\n            fig.update_xaxes(title='Number of Clusters', range=[clusters[0], clusters[-1]])\n            fig.update_yaxes(title='Distortion')\n            fig.show()\n\n    return n_clusters\n\n\ndef add_cluster_labels(\n    df,\n    ts_settings,\n    method,\n    nlags=None,\n    scale=True,\n    scale_method='min_max',\n    alpha=0.05,\n    split_method=None,\n    n_clusters=None,\n    max_clusters=None,\n    plot=True,\n):\n    \"\"\"\n    Calculates series clusters and appends a column of cluster labels to the input df. This will only work on regularly spaced time series datasets.\n\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    method: type of clustering technique: must choose from either pacf, correlation, or target\n    nlags: int (Optional)\n        Number of AR(n) lags. Only applies to PACF method\n    scale: boolean (Optional)\n        Only applies to PACF method\n    scale_method: str (Optiona)\n        Choose between normalize (subtract the mean and divide by the std) or min_max (subtract the min and divide by the range)\n    split_method: str (Optional)\n        Choose between rank and quanitles. Only applies to target method\n    n_clusters: int\n        Number of clusters to create. If None, defaults to maximum silhouette score\n    max_clusters: int\n        Maximum number of clusters to create. If None, default to the number of series - 1\n\n    Returns:\n    --------\n    Updated pandas df with a new column 'Cluster' of clusters labels\n            -silhouette score per cluster:\n            (The best value is 1 and the worst value is -1. Values near 0 indicate overlapping\n            clusters. Negative values generally indicate that a sample has been assigned to the\n            wrong cluster.)\n            -plot of distortion per cluster\n    \"\"\"\n    target = ts_settings['target']\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    series = df[series_id].unique()\n    n_series = len(series)\n\n    if max_clusters is None:\n        max_clusters = n_series - 1\n\n    assert (\n        1 < max_clusters < n_series\n    ), 'max_clusters must be greater than 1 and less than or equal to the number of unique series -1'\n\n    if n_clusters:\n        assert (\n            1 < n_clusters <= max_clusters\n        ), f'n_clusters must be greater than 1 and less than {max_clusters}'\n\n    c = df.pivot(index=date_col, columns=series_id, values=target)\n\n    if method == 'pacf':\n        d = pd.DataFrame(\n            [_get_pacf_coefs(c, x, nlags, alpha, scale, scale_method) for x in c.columns]\n        )  # ignore missing values\n        d.index = c.columns\n        distances = pdist(d, 'minkowski', p=2)  # 1 for manhattan distance and 2 for euclidean\n        dist_matrix = squareform(distances)\n        dist_df = pd.DataFrame(dist_matrix)\n        dist_df.columns = series\n        dist_df.index = dist_df.columns\n\n    elif method == 'correlation':\n        dist_df = c.corr(method='pearson')\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=1)\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    elif method == 'target':\n        if split_method is not None:\n            if n_clusters:\n                cuts = n_clusters\n            else:\n                cuts = max_clusters\n\n            new_df = _split_series(df, series_id, target, by=split_method, cuts=cuts)\n            return new_df  # exit function\n        else:\n            dist_df = df.groupby(series_id).agg({target: 'mean'})\n\n    else:\n        raise ValueError(\n            f'{method} is not a supported value. Must be set to either pacf, correlation, or target'\n        )\n\n    # Find optimal number of clulsters is n_clusters is not specified\n    if n_clusters is None:\n        n_clusters = _get_optimal_n_clusters(\n            df=dist_df, n_series=n_series, max_clusters=max_clusters, plot=plot\n        )\n\n    kmeans = KMeans(n_clusters).fit(dist_df)\n    labels = kmeans.predict(dist_df)\n\n    df_clusters = (\n        pd.concat([pd.Series(series), pd.Series(labels)], axis=1)\n        .sort_values(by=1)\n        .reset_index(drop=True)\n    )\n    df_clusters.columns = [series_id, 'Cluster']\n\n    df_w_cluster_labels = df.merge(df_clusters, how='left', on=series_id)\n\n    return df_w_cluster_labels.reset_index(drop=True)\n\n\ndef plot_clusters(df, ts_settings, split_col='Cluster', max_sample_size=50000):\n    \"\"\"\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    col: cluster_id columns\n\n    Returns:\n    --------\n    Plotly bar plot\n\n    \"\"\"\n    assert split_col in df.columns, f'{split_col} must be a column in the df'\n\n    date_col = ts_settings['date_col']\n    target = ts_settings['target']\n    series_id = ts_settings['series_id']\n\n    n_clusters = len(df[split_col].unique())\n\n    if df.shape[0] > max_sample_size:  # limit the data points displayed in the charts to reduce lag\n        df = df.sample(n=max_sample_size).reset_index(drop=True)\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[split_col, date_col], inplace=True)\n    df_agg = df.groupby([split_col, date_col]).agg({target: 'mean'}).reset_index()\n    groups = df_agg.groupby([split_col])\n    fig = make_subplots(rows=n_clusters, cols=1)\n\n    a = 1\n    for name, group in groups:\n        n_series = len(df.loc[df[split_col] == name, series_id].unique())\n        fig.append_trace(\n            go.Line(\n                x=group[date_col], y=group[target], name=f'{split_col}={name} - {n_series} Series'\n            ),\n            row=a,\n            col=1,\n        )\n        a += 1\n\n    fig.update_layout(height=1000, width=1000, title_text=\"Cluster Subplots\")\n    fig.show()\n\n\n",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_clustering.py",
    "size": 10020,
    "description": "Authors: Justin Swansburg, Mark Philip",
    "tags": [
      "datarobot-api",
      "predictions"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_create_project_advanced.py",
    "file_name": "ts_create_project_advanced.py",
    "file_type": "python",
    "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport datetime as dt\nimport time\n\nimport datarobot as dr\nimport numpy as np\nimport pandas as pd\n\nfrom ts_data_quality import get_timestep #ts_data_quality is a helper function that exists within this Repo\n\n\n###################\n# Project Creation\n###################\n\n\ndef create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project\n\n\ndef create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects\n\n\ndef wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_create_project_advanced.py",
    "size": 10104,
    "description": "Authors: Justin Swansburg, Mark Philip",
    "tags": [
      "datarobot-api",
      "project-creation",
      "time-series"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_data_quality_check.py",
    "file_name": "ts_data_quality_check.py",
    "file_type": "python",
    "content": "#Authors:  Mark Philip, Justin Swansburg\n\nimport datetime as dt\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport statsmodels.api as sm\n\n\n###################################\n# Time Series Data Quality Checks\n###################################\n\n\nclass DataQualityCheck:\n    \"\"\"\n    A class used to capture summary stats and data quality checks prior to uploading time series data to DataRobot\n    Attributes:\n    -----------\n    df : DataFrame\n        time series data, including a date column and target variable at a minimum\n    settings : dict\n        definitions of date_col, target_col, series_id and time series parameters\n    stats : dict\n        summary statistics generated from `calc_summary_stats`\n    duplicate_dates : int\n        duplicate dates in the time series date_col\n    series_timesteps : series\n        steps between time units for each series_id\n    series_max_gap : series\n        maximum time gap per series\n    series_lenth : series\n        length of each series_id\n    series_pct : series\n        percent of series with complete time steps\n    irregular : boolean\n        True if df contains irregular time series data\n    series_negative_target_pct : float\n        Percent of target values that are negative\n    Methods:\n    --------\n    calc_summary_stats(settings, df)\n        generates a dictionary of summary statistics\n    calc_time_steps(settings, df)\n        calculate time steps per series_id\n    hierarchical_check(settings, df)\n        check if time series data passes heirarchical check\n    zero_inflated_check(settings, df)\n        check if target value contains zeros\n    negative_values_check(settings, df)\n        check if target value contains negative values\n    time_steps_gap_check(settings, df)\n        check if any series has missing time steps\n    irregular_check(settings, df)\n        check is time series data irregular\n    \"\"\"\n\n    def __init__(self, df, ts_settings):\n        self.df = df\n        self.settings = ts_settings\n        self.stats = None\n        self.duplicate_dates = None\n        self.series_time_steps = None\n        self.series_length = None\n        self.series_pct = None\n        self.irregular = None\n        self.series_negative_target_pct = None\n        self.project_time_unit = None\n        self.project_time_step = None\n        self.calc_summary_stats()\n        self.calc_time_steps()\n        self.run_all_checks()\n\n    def calc_summary_stats(self):\n        \"\"\"\n        Analyze time series data to perform checks and gather summary statistics prior to modeling.\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df[date_col] = pd.to_datetime(df[date_col])\n        df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n        # Create dictionary of helpful statistics\n        stats = dict()\n\n        stats['rows'] = df.shape[0]\n        stats['columns'] = df.shape[1]\n        stats['min_' + str(target)] = df[target].min()\n        stats['max_' + str(target)] = df[target].max()\n        stats['series'] = len(df[series_id].unique())\n        stats['start_date'] = df[date_col].min()\n        stats['end_date'] = df[date_col].max()\n        stats['timespan'] = stats['end_date'] - stats['start_date']\n        stats['median_timestep'] = df.groupby([series_id])[date_col].diff().median()\n        stats['min_timestep'] = df.groupby([series_id])[date_col].diff().min()\n        stats['max_timestep'] = df.groupby([series_id])[date_col].diff().max()\n\n        # create data for histogram of series lengths\n        stats['series_length'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.max() - x.min())\n            / stats['median_timestep']\n        )\n\n        # calculate max gap per series\n        stats['series_max_gap'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max())\n            / stats['median_timestep']\n        )\n\n        self.stats = stats\n\n    def calc_percent_missing(self, missing_value=np.nan):\n        \"\"\"\n        Calculate percentage of rows where target is np.nan\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if np.isnan(missing_value):\n            percent_missing = sum(np.isnan(df[target])) / len(df)\n        else:\n            percent_missing = sum(df[target] == missing_value) / len(df)\n\n        self.stats['percent_missing'] = percent_missing\n        print('{:0.2f}% of the rows are missing a target value'.format(percent_missing * 100))\n\n    def get_zero_inflated_series(self, cutoff=0.99):\n        \"\"\"\n        Identify series where the target is 0.0 in more than x% of the rows\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        assert 0 < cutoff <= 1.0, 'cutoff must be between 0 and 1'\n\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n        series = df[df >= cutoff].index.values\n\n        pct = len(series) / self.stats['series']\n\n        print(\n            '{:0.2f}% series have zeros in more than {:0.2f}% or more of the rows'.format(\n                pct * 100, cutoff * 100\n            )\n        )\n\n    def calc_time_steps(self):\n        \"\"\"\n        Calculate timesteps per series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # create data for histogram of timestep\n        series_timesteps = df.groupby([series_id])[date_col].diff() / self.stats['median_timestep']\n        self.series_time_steps = series_timesteps\n\n    def hierarchical_check(self):\n        \"\"\"\n        Calculate percentage of series that appear on each timestep\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # Test if series passes the hierarchical check\n        series_pct = df.groupby([date_col])[series_id].apply(\n            lambda x: x.count() / self.stats['series']\n        )\n        if np.where(series_pct > 0.95, 1, 0).mean() > 0.95:\n            self.stats['passes_hierarchical_check'] = True\n            print(\n                'Data passes hierarchical check! DataRobot hierarchical blueprints will run if you enable cross series features.'\n            )\n        else:\n            print('Data fails hierarchical check! No hierarchical blueprints will run.')\n            self.stats['passes_hierarchical_check'] = False\n\n        self.series_pct = series_pct\n\n    def zero_inflated_check(self):\n        \"\"\"\n        Check if minimum target value is 0.0\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if min(df[target]) == 0:\n            self.stats['passes_zero_inflated_check'] = False\n            print('The minimum target value is zero. Zero-Inflated blueprints will run.')\n        else:\n            self.stats['passes_zero_inflated_check'] = True\n            print('Minimum target value is <> 0. Zero-inflated blueprints will not run.')\n\n    def negative_values_check(self):\n        \"\"\"\n        Check if any series contain negative values. If yes, identify and call out which series by id.\n        \"\"\"\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df['target_sign'] = np.sign(df[target])\n\n        try:\n            # Get percent of series that have at least one negative value\n            any_series_negative = (\n                df.groupby([series_id])['target_sign'].value_counts().unstack()[-1]\n            )\n            series_negative_target_pct = np.sign(any_series_negative).sum() / len(\n                df[series_id].unique()\n            )\n            df.drop('target_sign', axis=1, inplace=True)\n            self.stats['passes_negative_values_check'] = False\n\n            print(\n                '{0:.2f}% of series have at least one negative {1} value.'.format(\n                    (round(series_negative_target_pct * 100), 2), target\n                )\n            )\n\n            # Identify which series have negative values\n            # print('{} contain negative values. Consider creating a seperate project for these series.'.format(any_series_negative[any_series_negative == 1].index.values))\n        except:\n            series_negative_target_pct = 0\n            self.stats['passes_negative_values_check'] = True\n            print('No negative values are contained in {}.'.format(target))\n\n        self.series_negative_target_pct = series_negative_target_pct\n\n    def new_series_check(self):\n        \"\"\"\n        Check if any series start after the the minimum datetime\n        \"\"\"\n        min_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].min()\n        new_series = min_dates > self.stats['start_date'] + dt.timedelta(days=30)\n\n        if new_series.sum() == 0:\n            self.stats['series_introduced_over_time'] = False\n            print('No new series were introduced after the start of the training data')\n        else:\n            self.stats['series_introduced_over_time'] = True\n            print(\n                'Warning: You may encounter new series at prediction time. \\n {0:.2f}% of the series appeared after the start of the training data'.format(\n                    round(new_series.mean() * 100, 0)\n                )\n            )\n\n    def old_series_check(self):\n        \"\"\"\n        Check if any series end before the maximum datetime\n        \"\"\"\n        max_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].max()\n        old_series = max_dates < self.stats['end_date'] - dt.timedelta(days=30)\n\n        if old_series.sum() == 0:\n            self.stats['series_removed_over_time'] = False\n            print('No series were removed before the end of the training data')\n        else:\n            self.stats['series_removed_over_time'] = True\n            print(\n                'Warning: You may encounter fewer series at prediction time. \\n {0:.2f}% of the series were removed before the end of the training data'.format(\n                    round(old_series.mean() * 100, 0)\n                )\n            )\n\n    def leading_or_trailing_zeros_check(self, threshold=5, drop=True):\n        \"\"\"\n        Check for contain consecutive zeros at the beginning or end of each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        new_df = remove_leading_and_trailing_zeros(\n            df,\n            series_id,\n            date_col,\n            target,\n            leading_threshold=threshold,\n            trailing_threshold=threshold,\n            drop=drop,\n        )\n\n        if new_df.shape[0] < df.shape[0]:\n            print(f'Warning: Leading and trailing zeros detected within series')\n        else:\n            print(f'No leading or trailing zeros detected within series')\n\n    def duplicate_dates_check(self):\n        \"\"\"\n        Check for duplicate datetimes within each series\n        \"\"\"\n\n        duplicate_dates = self.df.groupby([self.settings['series_id'], self.settings['date_col']])[\n            self.settings['date_col']\n        ].count()\n        duplicate_dates = duplicate_dates[duplicate_dates > 1]\n        if len(duplicate_dates) == 0:\n            print(f'No duplicate timestamps detected within any series')\n            self.stats['passes_duplicate_timestamp_check'] = True\n        else:\n            print('Warning: Data contains duplicate timestamps within series!')\n            self.stats['passes_duplicate_timestamp_check'] = False\n\n    def time_steps_gap_check(self):\n        \"\"\"\n        Check for missing timesteps within each series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n        gap_size = self.stats['median_timestep']\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # check is series has any missing time steps\n        self.stats['pct_series_w_gaps'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max()) > gap_size\n        ).mean()\n\n        print(\n            '{0:.2f}% of series have at least one missing time step.'.format(\n                round(self.stats['pct_series_w_gaps'] * 100), 2\n            )\n        )\n\n    def _get_spacing(self, df, project_time_unit):\n        \"\"\"\n        Helper function for self.irregular_check()\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        project_time_unit = self.project_time_unit\n        ts_settings = self.settings\n        date_col = ts_settings['date_col']\n        series_id = ts_settings['series_id']\n\n        df['indicator'] = 1\n        df = fill_missing_dates(df=df, ts_settings=ts_settings)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        sums = df.groupby([series_id, project_time_unit])['indicator'].sum()\n        counts = df.groupby([series_id, project_time_unit])['indicator'].agg(\n            lambda x: x.fillna(0).count()\n        )\n\n        pcts = sums / counts\n\n        irregular = pcts.reset_index(drop=True) < 0.8\n        irregular = irregular[irregular]\n\n        return irregular\n\n    def irregular_check(self, plot=False):\n        \"\"\"\n        Check for irregular spacing within each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        df = self.df.copy()\n\n        # first cast date column to a pandas datetime type\n        df[date_col] = pd.to_datetime(df[date_col])\n\n        project_time_unit, project_time_step = get_timestep(self.df, self.settings)\n\n        self.project_time_unit = project_time_unit\n        self.project_time_step = project_time_step\n\n        print('Project Timestep: ', project_time_step, ' ', project_time_unit)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        # Plot histogram of timesteps\n        time_unit_counts = df[project_time_unit].value_counts()\n\n        if plot:\n            time_unit_percent = time_unit_counts / sum(time_unit_counts.values)\n\n            fig = px.bar(\n                time_unit_percent,\n                x=time_unit_percent.index,\n                y=time_unit_percent.values,\n                title=f'Percentage of records per {project_time_unit}',\n            )\n            fig.update_xaxes(title=project_time_unit)\n            fig.update_yaxes(title='Percentage')\n            fig.show()\n\n        # Detect uncommon time steps\n        # If time bin has less than 30% of most common bin then it is an uncommon time bin\n        uncommon_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) < 0.3].index\n        )\n        common_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) >= 0.3].index\n        )\n\n        if len(uncommon_time_bins) > 0:\n            print(f'Uncommon {project_time_unit}s:', uncommon_time_bins)\n        else:\n            print('There are no uncommon time steps')\n\n        # Detect irregular series\n        df = df.loc[df[project_time_unit].isin(common_time_bins), :]\n        irregular_series = self._get_spacing(df, project_time_unit)\n\n        if len(irregular_series) > 0:\n            print(\n                'Series are irregularly spaced. Projects will only be able to run in row-based mode!'\n            )\n            self.stats['passes_irregular_check'] = False\n        else:\n            self.stats['passes_irregular_check'] = True\n            print(\n                'Timesteps are regularly spaced. You will be able to run projects in either time-based or row-based mode'\n            )\n\n    def detect_periodicity(self, alpha=0.05):\n        \"\"\"\n        Calculate project-level periodicity\n        \"\"\"\n\n        timestep = self.project_time_unit\n        df = self.df\n        target = self.settings['target']\n        date_col = self.settings['date_col']\n        metric = self.settings['metric']\n\n        metrics = {\n            'LogLoss': sm.families.Binomial(),\n            'RMSE': sm.families.Gaussian(),\n            'Poisson Deviance': sm.families.Poisson(),\n            'Gamma Deviance': sm.families.Gamma(),\n        }\n\n        periodicity = {\n            'moh': 'hourly',\n            'hod': 'daily',\n            'dow': 'weekly',\n            'dom': 'monthly',\n            'month': 'yearly',\n        }\n\n        try:\n            loss = metrics[metric]\n        except KeyError:\n            loss = metrics['RMSE']\n\n        # Instantiate a glm with the default link function.\n        df[date_col] = pd.to_datetime(df[date_col])\n        df = df.loc[np.isfinite(df[target]), :].copy()\n\n        df['moh'] = df[date_col].dt.minute\n        df['hod'] = df[date_col].dt.hour\n        df['dow'] = df[date_col].dt.dayofweek\n        df['dom'] = df[date_col].dt.day\n        df['month'] = df[date_col].dt.month\n\n        if timestep == 'minute':\n            inputs = ['moh', 'hod', 'dow', 'dom', 'month']\n        elif timestep == 'hour':\n            inputs = ['hod', 'dow', 'dom', 'month']\n        elif timestep == 'day':\n            inputs = ['dow', 'dom', 'month']\n        elif timestep == 'week':\n            inputs = ['month']\n        else:\n            raise ValueError('timestep has to be either minute, hour, day, week, or month')\n\n        output = []\n        for i in inputs:\n            x = pd.DataFrame(df[i])\n            y = df[target]\n\n            x = pd.get_dummies(x.astype('str'), drop_first=True)\n            x['const'] = 1\n\n            clf = sm.GLM(endog=y, exog=x, family=loss)\n            model = clf.fit()\n\n            if any(model.pvalues[:-1] <= alpha):\n                output.append(periodicity[i])\n                # print(f'Detected periodicity: {periodicity[i]}')\n                # return periodicity[i]\n\n        if len(output) > 0:\n            print(f'Detected periodicity: {output}')\n        else:\n            print('No periodicity detected')\n\n    def run_all_checks(self):\n        \"\"\"\n        Runner function to run all data checks in one call\n        \"\"\"\n        print('Running all data quality checks...\\n')\n\n        series = self.stats['series']\n        start_date = self.stats['start_date']\n        end_date = self.stats['end_date']\n        rows = self.stats['rows']\n        cols = self.stats['columns']\n\n        print(f'There are {rows} rows and {cols} columns')\n        print(f'There are {series} series')\n        print(f'The data spans from  {start_date} to {end_date}')\n\n        self.hierarchical_check()\n        self.zero_inflated_check()\n        self.new_series_check()\n        self.old_series_check()\n        self.duplicate_dates_check()\n        self.leading_or_trailing_zeros_check()\n        self.time_steps_gap_check()\n        self.calc_percent_missing()\n        self.get_zero_inflated_series()\n        self.irregular_check()\n        self.detect_periodicity()\n\n\ndef get_timestep(df, ts_settings):\n    \"\"\"\n    Calculate the project-level timestep\n    Returns:\n    --------\n    project_time_unit: minute, hour, day, week, or month\n    project_time_step: int\n    Examples:\n    --------\n    '1 days'\n    '4 days'\n    '1 week'\n    '2 months'\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    # Cast date column to a pandas datetime type and sort df\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n    # Calculate median timestep\n    deltas = df.groupby([series_id])[date_col].diff().reset_index(drop=True)\n    median_timestep = deltas.apply(lambda x: x.total_seconds()).median()\n\n    # Logic to detect project time step and time unit\n    if (60 <= median_timestep < 3600) & (median_timestep % 60 == 0):\n        project_time_unit = 'minute'\n        project_time_step = int(median_timestep / 60)\n        df['minute'] = df[date_col].dt.minute\n    elif (3600 <= median_timestep < 86400) & (median_timestep % 3600 == 0):\n        project_time_unit = 'hour'\n        project_time_step = int(median_timestep / 3600)\n        df['hour'] = df[date_col].dt.hour\n    elif (86400 <= median_timestep < 604800) & (median_timestep % 86400 == 0):\n        project_time_unit = 'day'\n        project_time_step = int(median_timestep / 86400)\n        df['day'] = df[date_col].dt.strftime('%A')\n    elif (604800 <= median_timestep < 2.628e6) & (median_timestep % 604800 == 0):\n        project_time_unit = 'week'\n        project_time_step = int(median_timestep / 604800)\n        df['week'] = df[date_col].dt.week\n    elif (median_timestep >= 2.628e6) & (median_timestep % 2.628e6 == 0):\n        project_time_unit = 'month'\n        project_time_step = int(median_timestep / 2.628e6)\n        df['month'] = df[date_col].dt.month\n    else:\n        raise ValueError(f'{median_timestep} seconds is not a supported timestep')\n\n    # print('Project Timestep: 1', project_time_unit)\n\n    return project_time_unit, project_time_step\n\n\ndef _reindex_dates(group, freq):\n    \"\"\"\n    Helper function for fill_missing_dates()\n    \"\"\"\n    date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    group = group.reindex(date_range)\n    return group\n\n\ndef fill_missing_dates(df, ts_settings, freq=None):\n    \"\"\"\n    Insert rows with np.nan targets for series with missing timesteps between the series start and end dates\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    freq: project time unit and timestep\n    Returns:\n    --------\n    pandas df with inserted rows\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    if freq is None:\n        mapper = {'minute': 'min', 'hour': 'H', 'day': 'D', 'week': 'W', 'month': 'M'}\n        project_time_unit, project_time_step = get_timestep(df, ts_settings)\n        freq = str(project_time_step) + mapper[project_time_unit]\n\n    df = (\n        df.set_index(date_col)\n        .groupby(series_id)\n        .apply(_reindex_dates, freq)\n        .rename_axis((series_id, date_col))\n        .drop(series_id, axis=1)\n        .reset_index()\n    )\n\n    return df.reset_index(drop=True)\n\n\ndef _remove_leading_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    min_date = df_non_zero[date_col].min()\n    df_begin = df[df[date_col] < min_date]\n    if df_begin[target].dropna().shape[0] >= threshold or pd.isnull(min_date):\n        if drop:\n            if pd.isnull(min_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] >= min_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(min_date) or row[date_col] < min_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df\n\n\ndef _remove_trailing_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    max_date = df_non_zero[date_col].max()\n    df_end = df[df[date_col] > max_date]\n    if df_end[target].dropna().shape[0] >= threshold or pd.isnull(max_date):\n        if drop:\n            if pd.isnull(max_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] <= max_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(max_date) or row[date_col] > max_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df\n\n\ndef remove_leading_and_trailing_zeros(\n    df, series_id, date_col, target, leading_threshold=5, trailing_threshold=5, drop=False\n):\n    \"\"\"\n    Remove excess zeros at the beginning or end of series\n    df: pandas df\n    leading_threshold: minimum number of consecutive zeros at the beginning of a series before rows are dropped\n    trailing_threshold: minimum number of consecutive zeros at the end of series before rows are dropped\n    drop: specifies whether to drop the zeros or set them to np.nan\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_leading_zeros, date_col, target, leading_threshold, drop)\n        .reset_index(drop=True)\n    )\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_trailing_zeros, date_col, target, trailing_threshold, drop)\n        .reset_index(drop=True)\n    )\n\n    return df.reset_index(drop=True)\n\n\n#####################\n# Data Visualization\n#####################\n\n\ndef _cut_series_by_rank(df, ts_settings, n=1, top=True):\n    df_agg = df.groupby(ts_settings['series_id']).mean()\n    selected_series_names = (\n        df_agg.sort_values(by=ts_settings['target'], ascending=top).tail(n).index.values\n    )\n\n    return selected_series_names\n\n\ndef _cut_series_by_quantile(df, ts_settings, quantile=0.95, top=True):\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df_agg = df.groupby(series_id).mean()\n\n    if top:\n        selected_series_names = df_agg[\n            df_agg[target] >= df_agg[target].quantile(quantile)\n        ].index.values\n    else:\n        selected_series_names = df_agg[\n            df_agg[target] <= df_agg[target].quantile(quantile)\n        ].index.values\n\n    return selected_series_names\n\n\ndef plot_series_average(df, settings):\n    date_col = settings['date_col']\n    target = settings['target']\n\n    # Average of all series over time\n    df_agg = df.groupby(date_col).mean()\n    df_agg['Date'] = pd.to_datetime(df_agg.index.values)\n\n    fig = px.line(df_agg, x='Date', y=target)\n    fig.update_layout(title_text='Average of all Series')\n    fig.show()\n\n\ndef plot_individual_series(df, ts_settings, n=None, top=True):\n    \"\"\"\n    Plot individual series on the same chart\n    n: (int) number of series to plot\n    top: (boolean) whether to select the top n largest or smallest series ranked by average target value\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    if n is None:\n        n = len(df[series_id].unique())\n\n    series = _cut_series_by_rank(df, ts_settings, n=n, top=top)\n    df_subset = df[df[series_id].isin(series)]\n\n    fig = px.line(df_subset, x=date_col, y=target, color=df_subset[series_id])\n    fig.update_layout(title_text='Top Series By Target Over Time')\n    fig.show()\n",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_data_quality_check.py",
    "size": 27776,
    "description": "Authors:  Mark Philip, Justin Swansburg",
    "tags": [
      "predictions"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_fill_dates_per_series.py",
    "file_name": "ts_fill_dates_per_series.py",
    "file_type": "python",
    "content": "#Author: Thodoris Petropoulos\n\nimport pandas as pd\n\ndef fill_missing_dates(group, date_col, freq = 'D', per_group_imputation = False):\n    \"\"\"This function can be used together with apply to fill in missing dates per group will fill missing dates per time series group. values per time series group follow the full script to see how you\n    Input:\n        - group <grouped pandas DataFrame> (No need to specify anything. It will work with .apply method)\n        - date_col <string> (Column that represents time)\n        - freq <string> Frequency of data imputation (\"D\" for daily, \"W\" for weekly, \"M\" for monthly)\n        - per_group_imputation <BOOLEAN> (If True, then the min and max value of dates will be specified by the individual series.\n                                          If False, then the min and max value of dates will be specified by the whole dataset).\n    \"\"\"\n    if per_group_imputation == True:\n        date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    else: \n        date_range = pd.date_range(df[date_col].min(), df[date_col].max(), freq=freq)\n    group = group.reindex(date_range)\n    return group\n\n##########\n#Usage\n##########s\n\n#To use the function above: \ndataframe = dataframe.set_index(date_col).groupby('series_id').apply(fill_missing_dates).rename_axis(('series_id',date_col)).drop('series_id', 1).reset_index()\n\n#Impute missing values of target feature with 0\ndataframe['target'].fillna(0,inplace=True)\n\n#Forward fill on the categorical features if needed (depending on dataset)\ndm_imputed.update(dm_imputed.groupby('series_id')[categorical_features].ffill())\n\n#Backward fill on the categorical features if needed (depending on dataset)\ndm_imputed.update(dm_imputed.groupby('series_id')[categorical_features].bfill())\n",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_fill_dates_per_series.py",
    "size": 1783,
    "description": "Author: Thodoris Petropoulos",
    "tags": []
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_ion_cannon.py",
    "file_name": "ts_ion_cannon.py",
    "file_type": "python",
    "content": "#Author: Lukas Innig\n\n#Make sure you are connected to DataRobot and have a completed TS project.\n\nimport datarobot as dr\nfrom datarobot.errors import ClientError\n\nclass TimeSeriesIonCannon(dr.Project):\n    \"\"\" This class takes as input a DataRobot Object and initiates a brute force search to increase accuracy.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        all_models = self.get_models()\n        self.supported_metrics = all_models[0].metrics.keys()\n        self.training_duration = [m for m in all_models if m.training_duration][0].training_duration\n    sort_order = {'MASE': False,\n     'FVE Poisson': True,\n     \"Theil's U\": False,\n     'RMSE': False,\n     'FVE Gamma': True,\n     'R Squared': True,\n     'Gamma Deviance': False,\n     'FVE Tweedie': True,\n     'MAE': False,\n     'SMAPE': True,\n     'MAPE': True,\n     'Gini Norm': True,\n     'Tweedie Deviance': False,\n     'Poisson Deviance': False,\n     'RMSLE': False}\n    \n    @classmethod\n    def aim(cls, *args, **kwargs):\n        return super().get(*args, **kwargs)\n    \n    def get_models_sorted(self, partition='validation', metric='RMSE', model_type_filter = ['']):\n        if partition not in ['backtesting', 'holdout', 'validation']:\n            raise ValueError(f\"Partition {partition} not in ['backtesting', 'holdout', 'validation']\")\n        if partition == 'holdout' and not self.holdout_unlocked:\n            print(\"Holdout not unlocked!\")\n            return []\n        if metric not in self.supported_metrics:\n            raise ValueError(f'Metric {metric} not supported')\n        reverse = self.sort_order.get(metric)\n        return sorted([m for m in self.get_datetime_models() \n                       if metric in m.metrics \n                       and m.metrics[metric][partition] \n                       and any([f in m.model_type for f in model_type_filter])], \n                      key=lambda m: m.metrics[metric][partition], reverse=reverse)\n    \n    def calculate_backtests(self, models):\n        def score_backtests(m):\n            try: \n                return m.score_backtests()\n            except ClientError as e:\n                return None\n        jobs = [score_backtests(m) for m in models]\n        [job.wait_for_completion() for job in jobs if job]\n    \n    def identify_best_featurelist(self):\n        best_models = self.get_models_sorted('backtesting')\n        if not best_models:\n            print('calculate some backtests')\n        featurelists = [m.featurelist_id for m in best_models[:20] if 'Blender' not in m.model_type]\n        reduced_fl = [fl for fl in featurelists if 'Reduced' in fl.name]\n        other_fl = [fl for fl in featurelists if 'Reduced' not in fl.name]\n        return reduced_fl + other_fl[:1]\n    \n    def run_all_blueprints(self, featurelist, training_duration=None, \n                           model_type_filter=['Mean', 'Eureqa', 'Keras', 'VARMAX']):\n        if not training_duration:\n            training_duration = self.training_duration\n        def train_blueprint(bp, fl):\n            try:\n                return self.train_datetime(bp.id, fl.id, training_duration=training_duration)\n            except ClientError as e:\n                print(e)\n                return None\n        bps = [bp for bp in self.get_blueprints() if all([f not in bp.model_type for f in model_type_filter])]\n        jobs = [train_blueprint(bp, featurelist) for bp in bps]\n        [job.wait_for_completion() for job in jobs if job]\n    def run_blenders(self):\n        def blend(model_ids, blender_method):\n            try:\n                return self.blend(model_ids, blender_method)\n            except ClientError as e:\n                print(e)\n                return None\n        best_models = self.get_models_sorted('backtesting')\n        best_models = [m for m in best_models if 'Blender' not in m.model_type]\n        jobs = []\n        for n in [3, 5, 7]:\n            for blender_method in [dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_AVG, \n                                   dr.enums.BLENDER_METHOD.AVERAGE,\n                                   dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_ENET]:\n                jobs.append(blend([m.id for m in best_models[:n]], blender_method=blender_method))\n        blender_models = [j.get_result_when_complete() for j in jobs if j]\n        blender_models = [dr.DatetimeModel.get(self.id, bm.id) for bm in blender_models]\n        return blender_models\n    def shoot(self):\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        fls = self.identify_best_featurelist()\n        for fl in fls:\n            self.run_all_blueprints(fl)\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        self.run_blenders()\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n\n\n##USAGE##\n#cannon = TimeSeriesIonCannon.aim('YOUR_PROJECT_ID')\n#cannon.shoot()",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_ion_cannon.py",
    "size": 4904,
    "description": "Author: Lukas Innig",
    "tags": [
      "datarobot-api",
      "time-series"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_metrics.py",
    "file_name": "ts_metrics.py",
    "file_type": "python",
    "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n\n#These functions can be used to calculate common evaluation metrics\n\nimport numpy as np\n\n\n#####################\n# Evaluation Metrics\n#####################\n\ndef mae(act, pred, weight=None):\n    \"\"\"\n    MAE = Mean Absolute Error = mean( abs(act - pred) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    ad = np.abs(d)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        ad = ad * weight / weight.mean()\n    mae = ad.mean()\n\n    if np.isnan(mae):\n        return np.finfo(np.float64).max\n    else:\n        return mae\n\n\ndef mape(act, pred, nan='ignore'):\n\n    # ignore NAN (drop rows), do nothing, replace Nan with 0\n    if nan not in ['ignore', 'set_to_zero', 'error']:\n        raise ValueError(f'{nan} must be either ignore, set_to_zero, or error')\n\n    act, pred = np.array(act), np.array(pred)\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(act - pred)\n    d = act\n    ape = n / d\n\n    if nan == 'set_to_zero':\n        ape[~np.isfinite(ape)] = 0\n    elif nan == 'ignore':\n        ape = ape[np.isfinite(ape)]\n\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape\n\n\ndef smape(act, pred):\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(pred - act)\n    d = (np.abs(pred) + np.abs(act)) / 2\n    ape = n / d\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape\n\n\ndef rmse(act, pred, weight=None):\n    \"\"\"\n    RMSE = Root Mean Squared Error = sqrt( mean( (act - pred)**2 ) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    sd = np.power(d, 2)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        sd = sd * weight / weight.mean()\n    mse = sd.mean()\n    rmse = np.sqrt(mse)\n\n    if np.isnan(rmse):\n        return np.finfo(np.float64).max\n    else:\n        return rmse\n\n\ndef gamma_loss(act, pred, weight=None):\n    \"\"\"Gamma deviance\"\"\"\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, eps)  # ensure actuals are strictly positive\n    d = 2 * (-np.log(act / pred) + (act - pred) / pred)\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)\n\n\ndef tweedie_loss(act, pred, weight=None, p=1.5):\n    \"\"\"tweedie deviance for p = 1.5 only\"\"\"\n\n    if p <= 1 or p >= 2:\n        raise ValueError('p equal to %s is not supported' % p)\n\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are not negative\n    d = (\n        (act ** (2.0 - p)) / ((1 - p) * (2 - p))\n        - (act * (pred ** (1 - p))) / (1 - p)\n        + (pred ** (2 - p)) / (2 - p)\n    )\n    d = 2 * d\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)\n\n\ndef poisson_loss(act, pred, weight=None):\n    \"\"\"\n        Poisson Deviance = 2*(act*log(act/pred)-(act-pred))\n        ONLY WORKS FOR POSITIVE RESPONSES\n    \"\"\"\n    if len(pred.shape) > 1:\n        pred = pred.ravel()\n    pred = np.maximum(pred, 1e-8)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are non-negative\n    d = np.zeros(len(act))\n    d[act == 0] = pred[act == 0]\n    cond = act > 0\n    d[cond] = act[cond] * np.log(act[cond] / pred[cond]) - (act[cond] - pred[cond])\n    d = d * 2\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        d = d * weight / weight.mean()\n    return d.mean()",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_metrics.py",
    "size": 3935,
    "description": "Author: Justin Swansburg, Mark Philip",
    "tags": [
      "predictions"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_modeling.py",
    "file_name": "ts_modeling.py",
    "file_type": "python",
    "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n\n#This function will help you create DataRobot Time series projects.\n\nimport datetime as dt\nimport time\n\nimport datarobot as dr\nimport numpy as np\nimport pandas as pd\n\nfrom ts_data_quality_check import get_timestep\n\n\n###################\n# Project Creation\n###################\n\n\ndef create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project\n\n\ndef create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects\n\n\ndef wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_modeling.py",
    "size": 10163,
    "description": "Author: Justin Swansburg, Mark Philip",
    "tags": [
      "datarobot-api",
      "project-creation",
      "time-series"
    ]
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_preprocessing.py",
    "file_name": "ts_preprocessing.py",
    "file_type": "python",
    "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport numpy as np\n\n\n#####################\n# Preprocessing Funcs\n#####################\n\n\ndef dataset_reduce_memory(df):\n    \"\"\"\n    Recast numerics to lower precision\n    \"\"\"\n    for c in df.select_dtypes(include=['float64']).columns:\n        df[c] = df[c].astype(np.float32)\n    for c in df.select_dtypes(include=['int64']).columns:\n        df[c] = df[c].astype(np.int32)\n    return df\n\n\ndef create_series_id(df, cols_to_concat, convert=True):\n    \"\"\"\n    Concatenate columns\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    df = df[cols_to_concat].copy()\n    non_strings = [c for c in df[cols_to_concat] if df[c].dtype != 'object']\n\n    if len(non_strings) > 0:\n        if convert:\n            df[non_strings] = df[non_strings].applymap(str)\n        else:\n            raise TypeError(\"columns must all be type str\")\n\n    df['series_id'] = df[cols_to_concat].apply(lambda x: '_'.join(x), axis=1)\n    return df['series_id']\n\n\ndef _create_cross_series_feature(df, group, col, func):\n    col_name = col + '_' + func\n    df.loc[:, col_name] = df.groupby(group)[col].transform(func)\n    return df\n\n\ndef create_cross_series_features(df, group, cols, funcs):\n    \"\"\"\n    Create custom aggregations across groups\n    Returns:\n    --------\n    pandas df with new cross series features\n    Example:\n    --------\n    df_agg = create_cross_series_features(df,\n                                          group=[date_col,'Cluster'],\n                                          cols=[target,'feat_1'],\n                                          funcs=['mean','std'])\n    \"\"\"\n    for c in cols:\n        for f in funcs:\n            df = _create_cross_series_feature(df, group, c, f)\n    return df.reset_index(drop=True)\n\n\ndef get_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    \"\"\"\n    Identify series where the target is 0.0 in more than x% of the rows\n    Returns:\n    --------\n    List of series\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n    series = df[df >= cutoff].index.values\n\n    return series\n\n\ndef drop_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    series_id = ts_settings['series_id']\n\n    series_to_drop = get_zero_inflated_series(df, ts_settings, cutoff=cutoff)\n\n    if len(series_to_drop) > 0:\n        print('Dropping ', len(series_to_drop), ' zero-inflated series')\n        df = df.loc[~df[series_id].isin(series_to_drop), :].reset_index(drop=True)\n        print('Remaining series: ', len(df[series_id].unique()))\n    else:\n        print('There are no zero-inflated series to drop')\n\n    return df\n\n\ndef sample_series(df, series_id, date_col, target, x=1, method='random', **kwargs):\n    \"\"\"\n    Sample series\n    x: percent of series to sample\n    random: sample x% of the series at random\n    target: sample the largest x% of series\n    timespan: sample the top x% of series with the longest histories\n    \"\"\"\n    if (x > 1) | (x < 0):\n        raise ValueError('x must be between 0 and 1')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series = round(x * len(df[series_id].unique()))\n\n    if method == 'random':\n        series_to_keep = np.random.choice(df[series_id].values, size=series)\n\n    elif method == 'target':\n        series_to_keep = (\n            df.groupby([series_id])[target]\n            .mean()\n            .sort_values(ascending=False)\n            .reset_index()\n            .loc[0:series, series_id]\n        )\n\n    elif method == 'timespan':\n        max_timespan = df[date_col].max() - df[date_col].min()\n        series_timespans = (\n            df.groupby([series_id])[date_col]\n            .apply(lambda x: x.max() - x.min())\n            .sort_values(ascending=False)\n            .reset_index()\n        )\n        series_to_keep = series_timespans.loc[0:series, series_id]\n        if kwargs.get('full_timespan'):\n            series_to_keep = series_timespans.loc[series_timespans == max_timespan, series_id]\n\n    else:\n        raise ValueError('Method not supported. Must be either random, target, or timespan')\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n\n    return sampled_df.reset_index(drop=True)\n\n\ndef drop_series_w_gaps(df, series_id, date_col, target, max_gap=1, output_dropped_series=False):\n    \"\"\"\n    Sample series\n    max_gap: number of timesteps\n    \"\"\"\n    if not isinstance(max_gap, int):\n        raise TypeError('max gap must be an int')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series_max_gap = df.groupby([series_id]).apply(lambda x: x[date_col].diff().max())\n    median_timestep = df.groupby([series_id])[date_col].diff().median()\n    series_to_keep = series_max_gap[(series_max_gap / median_timestep) <= max_gap].index.values\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n    dropped_df = df.loc[~df[series_id].isin(series_to_keep), :]\n\n    if output_dropped_series:\n        return sampled_df, dropped_df\n    else:\n        return sampled_df",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_preprocessing.py",
    "size": 5119,
    "description": "Authors: Justin Swansburg, Mark Philip",
    "tags": []
  },
  {
    "repo_name": "examples-for-data-scientists",
    "file_path": "Helper Functions/Time Series/Python/ts_projects.py",
    "file_name": "ts_projects.py",
    "file_type": "python",
    "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n#The functions below will help you evaluate a DataRobot TS project.\n\nimport datarobot as dr\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom ts_metrics import *\n\n\n######################\n# Project Evaluation\n######################\n\n\ndef get_top_models_from_project(\n    project, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    project: project object\n        DataRobot project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from a DataRobot project\n    \"\"\"\n    assert data_subset in [\n        'backtest_1',\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either backtest_1, allBacktests, or holdout'\n    if n_models is not None:\n        assert isinstance(n_models, int), 'n_models must be an int'\n    if n_models is not None:\n        assert n_models >= 1, 'n_models must be greater than or equal to 1'\n    assert isinstance(include_blenders, bool), 'include_blenders must be a boolean'\n\n    mapper = {\n        'backtest_1': 'backtestingScores',\n        'allBacktests': 'backtesting',\n        'holdout': 'holdout',\n    }\n\n    if metric is None:\n        metric = project.metric\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    models = [\n        m\n        for m in project.get_datetime_models()\n        if m.backtests[0]['status'] != 'BACKTEST_BOUNDARIES_EXCEEDED'\n    ]  # if m.holdout_status != 'HOLDOUT_BOUNDARIES_EXCEEDED']\n\n    if data_subset == 'backtest_1':\n        # models = sorted(models, key=lambda m: np.mean([i for i in m.metrics[metric][mapper[data_subset]][0] if i]), reverse=False)\n        models = sorted(\n            models, key=lambda m: m.metrics[metric][mapper[data_subset]][0], reverse=False\n        )\n    elif data_subset == 'allBacktests':\n        models = sorted(\n            models,\n            key=lambda m: m.metrics[metric][mapper[data_subset]]\n            if m.metrics[metric][mapper[data_subset]] is not None\n            else np.nan,\n            reverse=False,\n        )\n    else:\n        models = sorted(models, key=lambda m: m.metrics[metric][mapper[data_subset]], reverse=False)\n\n    if not include_blenders:\n        models = [m for m in models if m.model_category != 'blend']\n\n    if n_models is None:\n        n_models = len(models)\n\n    models = models[0:n_models]\n\n    assert len(models) > 0, 'You have not run any models for this project'\n\n    return models\n\n\ndef get_top_models_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Pull top models from leaderboard across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from DataRobot project(s)\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    models_all = []\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n        models_all.extend(models)\n    return models_all\n\n\ndef compute_backtests(\n    projects, n_models=5, data_subset='backtest_1', include_blenders=True, metric=None\n):\n    \"\"\"\n    Compute all backtests for top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        for m in models:\n            try:\n                m.score_backtests()  # request backtests for top models\n                print(f'Computing backtests for model {m.id} in Project {p.project_name}')\n            except dr.errors.ClientError:\n                pass\n        print(\n            f'All available backtests have been submitted for scoring for project {p.project_name}'\n        )\n\n\ndef get_or_request_backtest_scores(\n    projects, n_models=5, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get or request backtest and holdout scores from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    scores = pd.DataFrame()\n    for p in projects:\n\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        if metric is None:\n            metric = p.metric\n\n        backtest_scores = pd.DataFrame(\n            [\n                {\n                    'Project_Name': p.project_name,\n                    'Project_ID': p.id,\n                    'Model_ID': m.id,\n                    'Model_Type': m.model_type,\n                    'Featurelist': m.featurelist_name,\n                    f'Backtest_1_{metric}': m.metrics[metric]['backtestingScores'][0],\n                    'Backtest_1_MASE': m.metrics['MASE']['backtestingScores'][0],\n                    'Backtest_1_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'][0],\n                    'Backtest_1_SMAPE': m.metrics['SMAPE']['backtestingScores'][0],\n                    'Backtest_1_R_Squared': m.metrics['R Squared']['backtestingScores'][0],\n                    f'All_Backtests_{metric}': m.metrics[metric]['backtestingScores'],\n                    'All_Backtests_MASE': m.metrics['MASE']['backtestingScores'],\n                    'All_Backtests_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'],\n                    'All_Backtests_SMAPE': m.metrics['SMAPE']['backtestingScores'],\n                    'All_Backtests_R_Squared': m.metrics['R Squared']['backtestingScores'],\n                    f'Holdout_{metric}': m.metrics[metric]['holdout'],\n                    'Holdout_MASE': m.metrics['MASE']['holdout'],\n                    'Holdout_Theils_U': m.metrics[\"Theil's U\"]['holdout'],\n                    'Holdout_SMAPE': m.metrics['SMAPE']['holdout'],\n                    'Holdout_R_Squared': m.metrics['R Squared']['holdout'],\n                }\n                for m in models\n            ]\n        ).sort_values(by=[f'Backtest_1_{metric}'])\n\n        scores = scores.append(backtest_scores).reset_index(\n            drop=True\n        )  # append top model from each project\n\n    print(f'Scores for all {len(projects)} projects have been computed')\n\n    return scores\n\n\ndef get_or_request_training_predictions_from_model(model, data_subset='allBacktests'):\n    project = dr.Project.get(model.project_id)\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    try:\n        predict_job = model.request_training_predictions(data_subset)\n        training_predictions = predict_job.get_result_when_complete(max_wait=10000)\n\n    except dr.errors.ClientError:\n        prediction_id = [\n            p.prediction_id\n            for p in dr.TrainingPredictions.list(project.id)\n            if p.model_id == model.id and p.data_subset == data_subset\n        ][0]\n        training_predictions = dr.TrainingPredictions.get(project.id, prediction_id)\n\n    return training_predictions.get_all_as_dataframe(serializer='csv')\n\n\ndef get_or_request_training_predictions_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get row-level backtest or holdout predictions from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = pd.DataFrame()\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            tmp = get_or_request_training_predictions_from_model(m, data_subset)\n            tmp['Project_Name'] = p.project_name\n            tmp['Project_ID'] = p.id\n            tmp['Model_ID'] = m.id\n            tmp['Model_Type'] = m.model_type\n        preds = preds.append(tmp).reset_index(drop=True)\n\n    return preds\n\n\ndef get_preds_and_actuals(\n    df,\n    projects,\n    ts_settings,\n    n_models=1,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Get row-level predictions and merge onto actuals\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = get_or_request_training_predictions_from_projects(\n        projects,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    preds['timestamp'] = pd.to_datetime(preds['timestamp'].apply(lambda x: x[:-8]))\n    df = df.merge(\n        preds,\n        how='left',\n        left_on=[ts_settings['date_col'], ts_settings['series_id']],\n        right_on=['timestamp', 'series_id'],\n        validate='one_to_many',\n    )\n    df = df.loc[~np.isnan(df['prediction']), :].reset_index(drop=True)\n    return df\n\n\ndef get_cluster_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get cluster-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    print('Getting cluster accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby(['Cluster'])\n        .apply(lambda x: acc_calc(x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n    groups.columns = ['Cluster', f'Cluster_{acc_calc.__name__.upper()}']\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(\n        act=df[ts_settings['target']], pred=df['prediction']\n    )\n\n    return groups\n\n\ndef plot_cluster_acc(cluster_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse):\n    \"\"\"\n    Plots cluster-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_cluster_acc()\n    ts_settings: dict\n        Pparameters for time series project\n    data_subset: str\n        Choose either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    cluster_acc['Label'] = '=' + cluster_acc['Cluster']\n\n    fig = px.bar(cluster_acc, x='Label', y=f'Cluster_{acc_calc.__name__.upper()}').for_each_trace(\n        lambda t: t.update(name=t.name.replace('=', ''))\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=cluster_acc['Label'],\n            y=cluster_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Cluster Accuracy - {data_subset}')\n    fig.show()\n\n\ndef get_series_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get series-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    print('Getting series accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby([series_id]).apply(lambda x: acc_calc(x[target], x['prediction'])).reset_index()\n    )\n    groups.columns = [series_id, f'Series_{acc_calc.__name__.upper()}']\n    right = df[[series_id, 'Cluster']].drop_duplicates().reset_index(drop=True)\n    groups = groups.merge(right, how='left', on=series_id)\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(act=df[target], pred=df['prediction'])\n\n    return groups\n\n\ndef plot_series_acc(series_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse, n=50):\n    \"\"\"\n    Plots series-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_series_acc()\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Choose from either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    n_series = len(series_acc[ts_settings['series_id']].unique())\n    n = min(n_series, n)\n\n    series_acc.sort_values(by=f'Series_{acc_calc.__name__.upper()}', ascending=False, inplace=True)\n\n    series_acc = series_acc[0:n]\n\n    fig = px.bar(\n        series_acc,\n        x=ts_settings['series_id'],\n        y=f'Series_{acc_calc.__name__.upper()}',\n        color='Cluster',\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.add_trace(\n        go.Scatter(\n            x=series_acc[ts_settings['series_id']],\n            y=series_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Series Accuracy - {data_subset}')\n    fig.show()\n\n\ndef get_project_info(df):\n    \"\"\"\n    Parse project name to get FD, FDW, and Cluster information\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = df.copy()\n    try:\n        df['Cluster'] = df['Project_Name'].apply(lambda x: x.split('_Cluster-')[1])\n    except:\n        df['Cluster'] = 'all_series'\n\n    df['FD'] = df['Project_Name'].apply(lambda x: x.split('_FD:')[1].split('_FDW:')[0])\n    df['FDW'] = df['Project_Name'].apply(lambda x: x.split('_FDW:')[1].split('_Cluster-')[0])\n\n    return df\n\n\ndef filter_best_fdw_scores(scores, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset df to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = get_project_info(scores)\n    df['_tmp'] = df[col_error].apply(lambda x: np.nanmean(np.array(x, dtype=np.float32)))\n    idx = df.groupby(['Cluster', 'FD']).apply(lambda x: x['_tmp'].idxmin()).values\n    return scores.iloc[idx, :]\n\n\ndef filter_best_fdw_projects(scores, projects, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset list to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    projects: list\n        DataRobot projects object(s)\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    list\n    \"\"\"\n    df = filter_best_fdw_scores(scores, col_error)\n    return [p for p in projects if p.project_name in df['Project_Name'].unique()]\n\n\ndef plot_fd_accuracy(df, projects, ts_settings, data_subset='allBacktests', metric='RMSE'):\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    mapper = {\n        'MAE': mae,\n        'RMSE': rmse,\n        'Gamma Deviance': gamma_loss,\n        'Tweedie Deviance': tweedie_loss,\n        'Poisson Deviance': poisson_loss,\n    }\n\n    df = get_preds_and_actuals(\n        df, projects, ts_settings, n_models=1, data_subset=data_subset, metric=metric\n    )\n    df = (\n        df.groupby(['Project_Name', 'forecast_distance'])\n        .apply(lambda x: mapper[metric](x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n\n    df.columns = ['Project_Name', 'forecast_distance', mapper[metric].__name__.upper()]\n    fig = px.line(\n        df, x='forecast_distance', y=mapper[metric].__name__.upper(), color='Project_Name'\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.update_layout(title_text='Forecasting Accuracy per Forecast Distance')\n    fig.update_yaxes(title=mapper[metric].__name__.upper())\n    fig.update_xaxes(title='Forecast Distance')\n    fig.show()\n\n\ndef plot_fd_accuracy_by_cluster(\n    df, scores, projects, ts_settings, data_subset='holdout', metric='RMSE', split_col='Cluster'\n):\n    scores = get_project_info(scores)\n\n    for c in scores[split_col].unique():\n        project_names = list(\n            scores.loc[scores[split_col] == c, 'Project_Name'].reset_index(drop=True)\n        )\n        projects_by_cluster = [p for p in projects if p.project_name in project_names]\n        plot_fd_accuracy(df, projects_by_cluster, ts_settings, data_subset, metric)\n\n\n###########################\n# Performance Improvements\n###########################\n\n\ndef get_reduced_features_featurelist(project, model, threshold=0.99):\n    \"\"\"\n    Helper function for train_reduced_features_models()\n    project: DataRobot project object\n    model: DataRobot model object\n    threshold: np.float\n    Returns:\n    --------\n    DataRobot featurelist\n    \"\"\"\n    print(\n        f'Collecting Feature Impact for M{model.model_number} in project {project.project_name}...'\n    )\n\n    impact = pd.DataFrame.from_records(model.get_or_request_feature_impact())\n    impact['impactUnnormalized'] = np.where(\n        impact['impactUnnormalized'] < 0, 0, impact['impactUnnormalized']\n    )\n    impact['cumulative_impact'] = (\n        impact['impactUnnormalized'].cumsum() / impact['impactUnnormalized'].sum()\n    )\n\n    to_keep = np.where(impact['cumulative_impact'] <= threshold)[0]\n    if len(to_keep) < 1:\n        print('Applying this threshold would result in a featurelist with no features')\n        return None\n\n    idx = np.max(to_keep)\n\n    selected_features = impact.loc[0:idx, 'featureName'].to_list()\n    feature_list = project.create_modeling_featurelist(\n        f'Top {len(selected_features)} features M{model.model_number}', selected_features\n    )\n\n    return feature_list\n\n\ndef train_reduced_features_models(\n    projects,\n    n_models=1,\n    threshold=0.99,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Retrain top models with reduced feature featurelists\n    projects: list\n        DataRobot project objects(s)\n    n_models: int\n        Number of models to retrain with reduced feature featurelists\n    threshold: np.float\n        Controls the number of features to keep in the reduced feature list. Percentage of cumulative feature impact\n    data_subset: str\n        Choose from either holdout or allBacktests\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            try:\n                feature_list = get_reduced_features_featurelist(p, m, threshold)\n                if feature_list is None:\n                    continue\n                try:\n                    m.retrain(featurelist_id=feature_list.id)\n                    print(f'Training {m.model_type} on Featurelist {feature_list.name}')\n                except dr.errors.ClientError as e:\n                    print(e)\n            except dr.errors.ClientError as e:\n                print(e)",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_projects.py",
    "size": 25426,
    "description": "Author: Justin Swansburg, Mark Philip",
    "tags": [
      "datarobot-api",
      "project-creation",
      "time-series",
      "predictions"
    ]
  }
]