[
  {
    "name": "examples-for-data-scientists",
    "full_name": "datarobot-community/examples-for-data-scientists",
    "url": "https://github.com/datarobot-community/examples-for-data-scientists",
    "description": null,
    "files": [
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "AI Catalog/README.md",
        "file_name": "README.md",
        "file_type": "markdown",
        "content": "# AI Catalog API Demo\n\nScope: The scope of this notebook is to provide instructions on how to create and share datasets in AI Catalog and use them to create projects and run predictions.\n\n**Requirements:** Python 3.7 or higher; DataRobot API version 2.21 or higher",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/AI%20Catalog/README.md",
        "size": 264,
        "description": "Documentation: README",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "AI Catalog/Using-the-AI-Catalog.ipynb",
        "file_name": "Using-the-AI-Catalog.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Using the AI Catalog\\n\",\n    \"\\n\",\n    \"This code example provides instruction on how to create and share datasets in the AI Catalog and then use them to create projects and generate predictions.\\n\",\n    \"\\n\",\n    \"Download this notebook from the [code examples home page](index).\\n\",\n    \"\\n\",\n    \"## Requirements\\n\",\n    \"\\n\",\n    \"* Python 3.7+\\n\",\n    \"* DataRobot API version 2.21+\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Import libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import yaml\\n\",\n    \"import requests\\n\",\n    \"import pandas as pd\\n\",\n    \"import datarobot as dr\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Connect to DataRobot\\n\",\n    \"\\n\",\n    \"Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# To connect to a Zepl notebook:\\n\",\n    \"# dr.Client(token=z.getDatasource(\\\"datarobot_api\\\")['token'] , endpoint='https://app.datarobot.com/api/v2')\\n\",\n    \"\\n\",\n    \"# To connect to a Jupyter notebook:\\n\",\n    \"dr.Client(config_path = '/Users/nathan.goudreault/.config/datarobot/drconfig.yaml')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Create a dataset or a data source\\n\",\n    \"\\n\",\n    \"From the following commands, use the code that corresponds to your dataset or data source type to upload it to the AI Catalog. You can also use commands to [connect to a database](#connecting-to-a-database). Be sure to indicate the correct path to your dataset.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"path_to_data = 'data.csv' # Provide your dataset here\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# From a local file\\n\",\n    \"dataset = dr.Dataset.create_from_file(file_path=path_to_data)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# From a file object\\n\",\n    \"with open(path_to_data, 'rb') as f:\\n\",\n    \"    dataset = dr.Dataset.create_from_file(filelike=f)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"df = pd.read_csv(path_to_data)\\n\",\n    \"df_lst = df.to_dict(orient='records')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# From a pandas dataframe\\n\",\n    \"dataset = dr.Dataset.create_from_in_memory_data(data_frame=df)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# From a list of dictionaries representing rows of data\\n\",\n    \"dataset = dr.Dataset.create_from_in_memory_data(records=df_lst)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Based on CSV data from a URL\\n\",\n    \"dataset = dr.Dataset.create_from_url(url='https://data.csv')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Connect to a database\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Get a driver\\n\",\n    \"ms_sql_driver = [drv for drv in dr.DataDriver.list() if drv.class_name == 'com.microsoft.sqlserver.jdbc.SQLServerDriver'][-1]\\n\",\n    \"\\n\",\n    \"# Create a data store\\n\",\n    \"datastore = dr.DataStore.create(data_store_type='jdbc', \\n\",\n    \"                                canonical_name='Demo DB', \\n\",\n    \"                                driver_id=ms_sql_driver.id, \\n\",\n    \"                                jdbc_url=creds['jdbc_url'])\\n\",\n    \"\\n\",\n    \"# Create a data source based on a query\\n\",\n    \"query = \\\"select * from db.schema.table\\\"\\n\",\n    \"params = dr.DataSourceParameters(data_store_id=datastore.id, \\n\",\n    \"                                 query=query)\\n\",\n    \"\\n\",\n    \"datasource = dr.DataSource.create(data_source_type='jdbc', \\n\",\n    \"                                  canonical_name='datasource_query', \\n\",\n    \"                                  params=params)\\n\",\n    \"\\n\",\n    \"# Create a data source based on a table\\n\",\n    \"params = dr.DataSourceParameters(data_store_id=datastore.id, \\n\",\n    \"                                 schema='schema',\\n\",\n    \"                                 table='table')\\n\",\n    \"\\n\",\n    \"datasource = dr.DataSource.create(data_source_type='jdbc', \\n\",\n    \"                                  canonical_name='datasource_table', \\n\",\n    \"                                  params=params)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Share a dataset or data source\\n\",\n    \"\\n\",\n    \"Use the following commands to specify a list of users to share data with and assign them a role.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"users = ['user@domain.com']\\n\",\n    \"role = dr.enums.SHARING_ROLE.READ_ONLY\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# To share via an API call:\\n\",\n    \"data = {'data': [{'username': user, 'role': role} for user in users]}\\n\",\n    \"sharing_resp = dr_rest_call(f'/api/v2/datasets/{dataset.id}/accessControl', requests.patch, payload=data)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# To share a data source with Python:\\n\",\n    \"access_lst = [dr.SharingAccess(username=user, role=role) for user in users]\\n\",\n    \"datasource.share(access_lst)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Create a project\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Create a project from a dataset\\n\",\n    \"dr.Project.create_from_dataset(dataset_id=dataset.id, \\n\",\n    \"                               project_name=dataset.name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Create a project from a data source\\n\",\n    \"dr.Project.create_from_data_source(data_source_id=datasource.id, \\n\",\n    \"                                   username=creds['db_user'], \\n\",\n    \"                                   password=creds['db_pass'], \\n\",\n    \"                                   project_name=datasource.canonical_name\\n\",\n    \"                                  )\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Use a dataset to generate batch predictions\\n\",\n    \"\\n\",\n    \"You can use a dataset to generate batch predictions for a deployment. Before proceeding, select a deployment and obtain its [deployment ID](https://docs.datarobot.com/en/docs/predictions/predapi/dep-pred.html#predictions-for-deployments). Additionally, provide the dataset ID (obtained from the AI Catalog).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"deployment_id = 'deployment id'\\n\",\n    \"dataset_id = 'dataset id'\\n\",\n    \"\\n\",\n    \"# Prepare the parameters to run a batch prediction job\\n\",\n    \"data = {'deploymentId': deployment_id,\\n\",\n    \"        'passthroughColumnsSet': 'all',\\n\",\n    \"        'intakeSettings': \\n\",\n    \"            {'type': 'dataset',\\n\",\n    \"             'datasetId': dataset_id},\\n\",\n    \"        'outputSettings':\\n\",\n    \"            {'type': 'localFile', \\n\",\n    \"            }\\n\",\n    \"       }\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Initiate a batch prediction job\\n\",\n    \"batch_pred_resp = dr_rest_call('/api/v2/batchPredictions', requests.post, payload=data)\\n\",\n    \"\\n\",\n    \"# Retrieve the job ID and its object\\n\",\n    \"batch_pred_job_id = batch_pred_resp.json()['id']\\n\",\n    \"batch_pred_job = dr.BatchPredictionJob.get(batch_pred_job_id)\\n\",\n    \"\\n\",\n    \"# Once run, wait for the job to complete and for the results to write\\n\",\n    \"batch_pred_job.wait_for_completion()\\n\",\n    \"with open('data/predictions.csv', 'wb') as f:\\n\",\n    \"    batch_pred_job.download(f)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.12\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/AI%20Catalog/Using-the-AI-Catalog.ipynb",
        "size": 9206,
        "description": "Jupyter notebook example from AI Catalog/Using-the-AI-Catalog.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/Advanced Tuning.ipynb",
        "file_name": "Advanced Tuning.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Advanced Tuning\\n\",\n    \"**Author**: Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"### Scope\\n\",\n    \"The scope of this notebook is to provide instructions on how to do advanced tuning using the Python API.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"\\n\",\n    \"DataRobot is very good at choosing optimal hyperparameters for models to maximize speed and accuracy. However, sometimes we wish to change those hyperparameters ourselves.  This could be because we know something that DataRobot does not, we want to experiment with different approaches, or we have some other reason to use a particular parameter.\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.19.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\\n\",\n    \"\\n\",\n    \"It is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Advanced Tuning Interface\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"The easiest way to do advanced tuning is to set up a model and use the <code>start_advanced_tunning_session</code> method.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"tune = model.start_advanced_tuning_session()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"This function returns an object that you can use to see the default, current value, and possible values for each one of the parameters you can change.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Get Data on Parameters Available for Tuning\\n\",\n    \"If you wish to see the underlying data of which parameters are available for tuning for a model and what their default, current, and possible values are, then you can turn to <code>get_parameters</code>:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"{'tuning_description': None,\\n\",\n       \" 'tuning_parameters': [{'parameter_name': 'enet_alpha',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJlbmV0X2FscGhhIiwidmlkIjoiMTEifQ',\\n\",\n       \"   'default_value': 0.5,\\n\",\n       \"   'current_value': 0.5,\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'select': {'values': ['auto']},\\n\",\n       \"    'float': {'min': 0.0, 'max': 1.0, 'supports_grid_search': True}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'enet_lambda',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJlbmV0X2xhbWJkYSIsInZpZCI6IjExIn0',\\n\",\n       \"   'default_value': 0.00809400121608,\\n\",\n       \"   'current_value': 0.00809400121608,\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'select': {'values': ['auto']},\\n\",\n       \"    'float': {'min': 1e-10,\\n\",\n       \"     'max': 0.9999999999,\\n\",\n       \"     'supports_grid_search': True}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'fit_alpha_scaler',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJmaXRfYWxwaGFfc2NhbGVyIiwidmlkIjoiMTEifQ',\\n\",\n       \"   'default_value': 'True',\\n\",\n       \"   'current_value': 'True',\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'select': {'values': ['False', 'True']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'fit_intercept',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJmaXRfaW50ZXJjZXB0IiwidmlkIjoiMTEifQ',\\n\",\n       \"   'default_value': 'True',\\n\",\n       \"   'current_value': 'True',\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'select': {'values': ['False', 'True']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'max_iter',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtYXhfaXRlciIsInZpZCI6IjExIn0',\\n\",\n       \"   'default_value': 100,\\n\",\n       \"   'current_value': 100,\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'int': {'min': 1,\\n\",\n       \"     'max': 1000000,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'random_state',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJyYW5kb21fc3RhdGUiLCJ2aWQiOiIxMSJ9',\\n\",\n       \"   'default_value': 1234,\\n\",\n       \"   'current_value': 1234,\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'int': {'min': 0,\\n\",\n       \"     'max': 1000000000,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'sigma',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJzaWdtYSIsInZpZCI6IjExIn0',\\n\",\n       \"   'default_value': 1e-06,\\n\",\n       \"   'current_value': 1e-06,\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'float': {'min': 0.0,\\n\",\n       \"     'max': 1e-06,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'tol',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJ0b2wiLCJ2aWQiOiIxMSJ9',\\n\",\n       \"   'default_value': 0.0001,\\n\",\n       \"   'current_value': 0.0001,\\n\",\n       \"   'task_name': 'Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \"   'constraints': {'float': {'min': 1e-10,\\n\",\n       \"     'max': 10000000000.0,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'n_clusters',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJuX2NsdXN0ZXJzIiwidmlkIjoiOSJ9',\\n\",\n       \"   'default_value': 'auto',\\n\",\n       \"   'current_value': 'auto',\\n\",\n       \"   'task_name': 'K-Means Clustering',\\n\",\n       \"   'constraints': {'select': {'values': ['auto', 'pham']},\\n\",\n       \"    'int': {'min': 2, 'max': 10000, 'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'analyzer',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJhbmFseXplciIsInZpZCI6IjIifQ',\\n\",\n       \"   'default_value': 'word',\\n\",\n       \"   'current_value': 'word',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['word', 'char']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'binary',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJiaW5hcnkiLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 'True',\\n\",\n       \"   'current_value': 'True',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['False', 'True']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'language',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJsYW5ndWFnZSIsInZpZCI6IjIifQ',\\n\",\n       \"   'default_value': 'english',\\n\",\n       \"   'current_value': 'english',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['english',\\n\",\n       \"      'danish',\\n\",\n       \"      'dutch',\\n\",\n       \"      'finnish',\\n\",\n       \"      'french',\\n\",\n       \"      'german',\\n\",\n       \"      'hungarian',\\n\",\n       \"      'italian',\\n\",\n       \"      'norwegian',\\n\",\n       \"      'portuguese',\\n\",\n       \"      'romanian',\\n\",\n       \"      'russian',\\n\",\n       \"      'spanish',\\n\",\n       \"      'swedish',\\n\",\n       \"      'japanese',\\n\",\n       \"      'turkish']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'lowercase',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJsb3dlcmNhc2UiLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 'True',\\n\",\n       \"   'current_value': 'True',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['False', 'True']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'max_df',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtYXhfZGYiLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 0.5,\\n\",\n       \"   'current_value': 0.5,\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'int': {'min': 1,\\n\",\n       \"     'max': 99999,\\n\",\n       \"     'supports_grid_search': False},\\n\",\n       \"    'float': {'min': 0.0, 'max': 1.0, 'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'max_features',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 200000,\\n\",\n       \"   'current_value': 200000,\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['None']},\\n\",\n       \"    'int': {'min': 1,\\n\",\n       \"     'max': 1000000000000000019884624838656,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'max_ngram',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtYXhfbmdyYW0iLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 2,\\n\",\n       \"   'current_value': 2,\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'int': {'min': 0,\\n\",\n       \"     'max': 99,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'min_df',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtaW5fZGYiLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 2,\\n\",\n       \"   'current_value': 2,\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'int': {'min': 1,\\n\",\n       \"     'max': 99999,\\n\",\n       \"     'supports_grid_search': False},\\n\",\n       \"    'float': {'min': 0.0, 'max': 1.0, 'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'min_ngram',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtaW5fbmdyYW0iLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 1,\\n\",\n       \"   'current_value': 1,\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'int': {'min': 0,\\n\",\n       \"     'max': 99,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'norm',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJub3JtIiwidmlkIjoiMiJ9',\\n\",\n       \"   'default_value': 'l2',\\n\",\n       \"   'current_value': 'l2',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['None', 'l1', 'l2']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'smooth_idf',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJzbW9vdGhfaWRmIiwidmlkIjoiMiJ9',\\n\",\n       \"   'default_value': 'True',\\n\",\n       \"   'current_value': 'True',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['False', 'True']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'stemmer',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJzdGVtbWVyIiwidmlkIjoiMiJ9',\\n\",\n       \"   'default_value': 'None',\\n\",\n       \"   'current_value': 'None',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['None',\\n\",\n       \"      'snowball',\\n\",\n       \"      'lancaster',\\n\",\n       \"      'porter',\\n\",\n       \"      'wordnet']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'sublinear_tf',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJzdWJsaW5lYXJfdGYiLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 'False',\\n\",\n       \"   'current_value': 'False',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['False', 'True']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'tokenizer',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJ0b2tlbml6ZXIiLCJ2aWQiOiIyIn0',\\n\",\n       \"   'default_value': 'None',\\n\",\n       \"   'current_value': 'None',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['None',\\n\",\n       \"      'space',\\n\",\n       \"      'wordpunct',\\n\",\n       \"      'tweet',\\n\",\n       \"      'treebank',\\n\",\n       \"      'japtiny',\\n\",\n       \"      'mecab']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'use_idf',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJ1c2VfaWRmIiwidmlkIjoiMiJ9',\\n\",\n       \"   'default_value': 'False',\\n\",\n       \"   'current_value': 'False',\\n\",\n       \"   'task_name': 'Matrix of word-grams occurrences',\\n\",\n       \"   'constraints': {'select': {'values': ['False', 'True']}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'max_features',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiI3In0',\\n\",\n       \"   'default_value': 'None',\\n\",\n       \"   'current_value': 'None',\\n\",\n       \"   'task_name': 'One-Hot Encoding',\\n\",\n       \"   'constraints': {'select': {'values': ['None']},\\n\",\n       \"    'int': {'min': 1, 'max': 999999, 'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'max_features',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiIxIn0',\\n\",\n       \"   'default_value': 'None',\\n\",\n       \"   'current_value': 'None',\\n\",\n       \"   'task_name': 'One-Hot Encoding',\\n\",\n       \"   'constraints': {'select': {'values': ['None']},\\n\",\n       \"    'int': {'min': 1, 'max': 999999, 'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'max_features',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtYXhfZmVhdHVyZXMiLCJ2aWQiOiIxMCJ9',\\n\",\n       \"   'default_value': 'None',\\n\",\n       \"   'current_value': 'None',\\n\",\n       \"   'task_name': 'One-Hot Encoding',\\n\",\n       \"   'constraints': {'select': {'values': ['None']},\\n\",\n       \"    'int': {'min': 1, 'max': 999999, 'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'min_support',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtaW5fc3VwcG9ydCIsInZpZCI6IjEifQ',\\n\",\n       \"   'default_value': 10,\\n\",\n       \"   'current_value': 10,\\n\",\n       \"   'task_name': 'One-Hot Encoding',\\n\",\n       \"   'constraints': {'int': {'min': 1,\\n\",\n       \"     'max': 99999,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'min_support',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtaW5fc3VwcG9ydCIsInZpZCI6IjEwIn0',\\n\",\n       \"   'default_value': 1,\\n\",\n       \"   'current_value': 1,\\n\",\n       \"   'task_name': 'One-Hot Encoding',\\n\",\n       \"   'constraints': {'int': {'min': 1,\\n\",\n       \"     'max': 99999,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'min_support',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJtaW5fc3VwcG9ydCIsInZpZCI6IjcifQ',\\n\",\n       \"   'default_value': 10,\\n\",\n       \"   'current_value': 10,\\n\",\n       \"   'task_name': 'One-Hot Encoding',\\n\",\n       \"   'constraints': {'int': {'min': 1,\\n\",\n       \"     'max': 99999,\\n\",\n       \"     'supports_grid_search': False}},\\n\",\n       \"   'value': None},\\n\",\n       \"  {'parameter_name': 'k',\\n\",\n       \"   'parameter_id': 'eyJhcmciOiJrIiwidmlkIjoiOCJ9',\\n\",\n       \"   'default_value': 250,\\n\",\n       \"   'current_value': 250,\\n\",\n       \"   'task_name': 'Partial Principal Components Analysis',\\n\",\n       \"   'constraints': {'select': {'values': ['auto']},\\n\",\n       \"    'int': {'min': 1, 'max': 100000, 'supports_grid_search': False}},\\n\",\n       \"   'value': None}]}\"\n      ]\n     },\n     \"execution_count\": 5,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"tune.get_parameters()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Each model’s blueprint consists of a series of tasks including both preprocessing steps and the model itself. Each task contains tunable parameters. Let’s take a look at the available (tunable) tasks:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"['Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features',\\n\",\n       \" 'K-Means Clustering',\\n\",\n       \" 'Matrix of word-grams occurrences',\\n\",\n       \" 'One-Hot Encoding',\\n\",\n       \" 'Partial Principal Components Analysis']\"\n      ]\n     },\n     \"execution_count\": 6,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"tune.get_task_names()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"To see all of the available hyperparameter options for the particular task, use the get_parameter_names method on the tune object:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {\n    \"scrolled\": true\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"['analyzer',\\n\",\n       \" 'binary',\\n\",\n       \" 'language',\\n\",\n       \" 'lowercase',\\n\",\n       \" 'max_df',\\n\",\n       \" 'max_features',\\n\",\n       \" 'max_ngram',\\n\",\n       \" 'min_df',\\n\",\n       \" 'min_ngram',\\n\",\n       \" 'norm',\\n\",\n       \" 'smooth_idf',\\n\",\n       \" 'stemmer',\\n\",\n       \" 'sublinear_tf',\\n\",\n       \" 'tokenizer',\\n\",\n       \" 'use_idf']\"\n      ]\n     },\n     \"execution_count\": 7,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"task_name = tune.get_task_names()[2] #Save a task name by chance\\n\",\n    \"tune.get_parameter_names(task_name) # Get all of the hypermarameter options\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"set_parameter is a method used on the tune object that lets you fill in all the hyperparameters for your particular model. If you pass an XGB model, RunTune will have the hyperparameters to tune XGB (e.g., colsample_bytree), whereas if you pass an Elastic Net model, the function will instead have hyperparameters for Elastic Nets (e.g., lambda) instead.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"parameter_name = tune.get_parameter_names(task_name)[0] #Save a parameter name by chance\\n\",\n    \"\\n\",\n    \"tune.set_parameter(\\n\",\n    \"    task_name=task_name,\\n\",\n    \"    parameter_name=parameter_name,\\n\",\n    \"    value=1)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"When you are finished setting all the different parameters you want to chance, start tuning with the run method\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"ModelJob(Elastic-Net Classifier (mixing alpha=0.5 / Binomial Deviance) with Unsupervised Learning Features, status=inprogress)\"\n      ]\n     },\n     \"execution_count\": 9,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"tune.run()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Advanced%20Tuning%20and%20Partitioning/Python/Advanced%20Tuning.ipynb",
        "size": 20737,
        "description": "Jupyter notebook example from Advanced Tuning and Partitioning/Python/Advanced Tuning.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/AdvancedOptions object.ipynb",
        "file_name": "AdvancedOptions object.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Using the AdvancedOptions object\\n\",\n    \"**Author:** Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label:** Modeling Options\\n\",\n    \"\\n\",\n    \"Scope\\n\",\n    \"The scope of this notebook is to provide instructions on how to initiate a DataRobot project taking advantage of the `AdvancedOptions` object. The object has multiple options so refer to the official API documentation for a full overview.\\n\",\n    \"\\n\",\n    \"DataRobot API version >= 2.22.0. \\n\",\n    \"\\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"<datarobot.rest.RESTClientObject at 0x7f94d9075e10>\"\n      ]\n     },\n     \"execution_count\": 1,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"#Import datarobot library (make sure >=2.22.0)\\n\",\n    \"import datarobot as dr\\n\",\n    \"import pandas as pd\\n\",\n    \"\\n\",\n    \"#Connect to DataRobot\\n\",\n    \"dr.Client(token='YOUR_API_TOKEN',\\n\",\n    \"          endpoint = 'YOUR_API_ENDPOINT')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#Create an advanced options object\\n\",\n    \"advanced_options = dr.AdvancedOptions(accuracy_optimized_mb=True,\\n\",\n    \"                                      consider_blenders_in_recommendation=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"#Load data\\n\",\n    \"dataset = pd.read_excel('readmissions.xlsx')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"In progress: 2, queued: 11 (waited: 0s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 1s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 2s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 4s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 6s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 8s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 13s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 20s)\\n\",\n      \"In progress: 2, queued: 11 (waited: 34s)\\n\",\n      \"In progress: 1, queued: 11 (waited: 55s)\\n\",\n      \"In progress: 2, queued: 10 (waited: 76s)\\n\",\n      \"In progress: 2, queued: 9 (waited: 97s)\\n\",\n      \"In progress: 2, queued: 8 (waited: 118s)\\n\",\n      \"In progress: 2, queued: 8 (waited: 139s)\\n\",\n      \"In progress: 2, queued: 8 (waited: 160s)\\n\",\n      \"In progress: 2, queued: 6 (waited: 181s)\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"#Create project\\n\",\n    \"project = dr.Project.create(project_name='MyBinaryClassificationProject',\\n\",\n    \"                        sourcedata= dataset)\\n\",\n    \"\\n\",\n    \"#Set target and pass on the AdvancedOptions object\\n\",\n    \"project.set_target('readmitted', advanced_options=advanced_options)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"project.wait_for_autopilot() #Wait for autopilot to complete\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Advanced%20Tuning%20and%20Partitioning/Python/AdvancedOptions%20object.ipynb",
        "size": 3743,
        "description": "Jupyter notebook example from Advanced Tuning and Partitioning/Python/AdvancedOptions object.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/DataRobot_Optimizer/Optimizing Through API.ipynb",
        "file_name": "Optimizing Through API.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"pycharm\": {\n     \"is_executing\": false\n    }\n   },\n   \"source\": [\n    \"\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Controlling DataRobot Optimizer Through Python\\n\",\n    \"\\n\",\n    \"This code shows how to interact to DataRobot Optimizer App using the example Lending Club dataset **\\\"Lending Club Sample 30.csv\\\"**.\\n\",\n    \"In this example, we are trying to find the best combination of values for revol_util, inq_last_6mths, loan_amnt, and dti that minimizes the probability of a loan going bad.  \\n\",\n    \"\\n\",\n    \"### Result\\n\",\n    \"\\n\",\n    \"A dataframe with the original features, prediction, and the optimized features.  The optimized features have the prefix **opt_**\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"### Assumptions\\n\",\n    \"\\n\",\n    \"1. A model has been deployed \\n\",\n    \"2. An Optimization App has been created\\n\",\n    \"3. The dataset and Jupyter notebook are in the same folder\\n\",\n    \"4. The outcome of the optimization is added to the original dataframe\\n\",\n    \"\\n\",\n    \"### Steps\\n\",\n    \"\\n\",\n    \"1. Change key_dict \\n\",\n    \"    1. Get the URL from the application (see figure below)\\n\",\n    \"    2. Put the values in key_dict\\n\",\n    \"    3. Put the name of the dataset in ts_settings[\\\"filename\\\"]\\n\",\n    \"2. Read file into a dataframe\\n\",\n    \"3. result_df = perform_optimization(data_df)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"### Functions\\n\",\n    \"\\n\",\n    \"*get_optimization* : makes a post request to perform optimization.  It returns optimized values for the constraint features, and the predicted target\\n\",\n    \"\\n\",\n    \"*get_constraints*  : accesses the constraint features and their ranges from the Optimizer App\\n\",\n    \"\\n\",\n    \"*create_constrain_from_df* : if you want to decide which features to constrain, change the  \\\"cfeatures\\\" list in ts_settings and provide a file to estimate the min and the max for these features\\n\",\n    \"\\n\",\n    \"*set_optimizer* : prepares the elements required by the Optimizer App\\n\",\n    \"\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Get the URL with application id, and the token id from\\n\",\n    \"The URL is available from the share icon in the top menu of the Optimization App.\\n\",\n    \"\\n\",\n    \"<img src=\\\"Picture1.gif\\\">\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 62,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import requests  # you could also use aiohttp instead of requests to make calls asynchronously\\n\",\n    \"import json\\n\",\n    \"import pandas as pd\\n\",\n    \"\\n\",\n    \"'''\\n\",\n    \"You get the ID and Token from the Optimizer App by clicking the share triangle (in the top menu of the UI)\\n\",\n    \"'''\\n\",\n    \"#https://606c32b7462514a41c85c492.apps.datarobot.com/settings?token=xMdWLVsq2C2dNvMCTMlbDzbZ6IcF_GwA4E76iYtJTGs&lang=en&theme=light\\n\",\n    \"\\n\",\n    \"key_dict = {\\n\",\n    \"    'Complete_data': (\\n\",\n    \"                 '',  # ID\\n\",\n    \"                 '',  # Token\\n\",\n    \"                ),\\n\",\n    \"}\\n\",\n    \"#Set constraint features, and filename\\n\",\n    \"\\n\",\n    \"ts_settings = {\\\"cfeatures\\\" :['revol_util','inq_last_6mths','loan_amnt','dti'],\\n\",\n    \"              \\\"filename\\\":\\\"Lending Club Sample 30.csv\\\"}\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 63,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def get_optimization(app, constraints):\\n\",\n    \"    '''\\n\",\n    \"    Given the credentials and data, get the best target under the constrained features\\n\",\n    \"    :param app: 'Complete_data' in key_dict\\n\",\n    \"    :param constraints: a dict containing the constrained features, the data, and the optimization type\\n\",\n    \"    :return: optimization performance, and values tried for each constrained feature\\n\",\n    \"    '''\\n\",\n    \"    app_id, token = key_dict[app]\\n\",\n    \"    url = f'https://{app_id}.apps.datarobot.com/api/optimize'\\n\",\n    \"    headers = {\\\"Authorization\\\": f\\\"Bearer {token}\\\"}\\n\",\n    \"    return requests.post(url, json=constraints, headers=headers).json()\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 64,\n   \"metadata\": {\n    \"pycharm\": {\n     \"is_executing\": false\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def get_constraints(key_dict):\\n\",\n    \"    '''\\n\",\n    \"    Get a list of constraint features and their minimum and maximum values from the Optimization App  \\n\",\n    \"    :param key_dict: the dictionary with the required app id and token\\n\",\n    \"    :return: a list of constraint features and their min and max\\n\",\n    \"    '''\\n\",\n    \"    app_id, token = key_dict['Complete_data']\\n\",\n    \"    url = f'https://{app_id}.apps.datarobot.com/api/application'\\n\",\n    \"    headers = {\\\"Authorization\\\": f\\\"Bearer {token}\\\"}\\n\",\n    \"    constraints = requests.get(url,headers=headers).json()\\n\",\n    \"    constrain_list = constraints['constraints']\\n\",\n    \"    [x.pop('feature_type') for x in constrain_list]\\n\",\n    \"    return constrain_list\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 65,\n   \"metadata\": {\n    \"pycharm\": {\n     \"is_executing\": false\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"#You can build your constraint list from full dataset\\n\",\n    \"#The min and the max are calculated from the full dataset\\n\",\n    \"def create_constrain_from_df(df_tmp,cfeatures):\\n\",\n    \"    '''\\n\",\n    \"    Create a list of constraint features and their minimum and maximum values.  We assume we get the minimum and maximum \\n\",\n    \"    of the constraint features from a dataset\\n\",\n    \"    :param df_tmp: dataframe \\n\",\n    \"    :param cfeatures: the name of the constrained features\\n\",\n    \"    :return: a list of constraint features and their min and max\\n\",\n    \"    '''\\n\",\n    \"    constrain_list = []\\n\",\n    \"    for c_name in cfeatures:\\n\",\n    \"        c_dict = {'feature': c_name,'info':{'min': df_tmp[c_name].min(),'max':df_tmp[c_name].max(),'is_int':False}}\\n\",\n    \"        constrain_list.append(c_dict)\\n\",\n    \"    return constrain_list\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 66,\n   \"metadata\": {\n    \"pycharm\": {\n     \"is_executing\": false\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def set_optimizer(constrain_list,tmp1):\\n\",\n    \"    '''\\n\",\n    \"    Create the constraints dictionary:  It has to have the constraint list, and a row that need to be optimized\\n\",\n    \"    :param constrain_list: list of features and their constraints\\n\",\n    \"    :param tmp1: a row that has been changed to a JSON format\\n\",\n    \"    :return: a dict\\n\",\n    \"    '''\\n\",\n    \"    constraints = {\\n\",\n    \"        'settings': {\\n\",\n    \"        # Specify if you want to maximize or minimize\\n\",\n    \"            \\\"targetDirection\\\": 'max',\\n\",\n    \"\\n\",\n    \"            # Specify your flex feature ranges\\n\",\n    \"            'constraints': constrain_list,\\n\",\n    \"        },\\n\",\n    \"    #    'optimization': {'method': 'exhaust'},  # optional\\n\",\n    \"\\n\",\n    \"        # Specify your fixed values\\n\",\n    \"        'datapoint': tmp1,\\n\",\n    \"    }\\n\",\n    \"    return constraints\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 67,\n   \"metadata\": {\n    \"pycharm\": {\n     \"is_executing\": false\n    }\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"def perform_optimization(df):\\n\",\n    \"    '''\\n\",\n    \"    Perform a batch optimization.  Add the results to the original dataframe\\n\",\n    \"    :param df: dataframe with the dataset to be optimized \\n\",\n    \"    :return: a dataframe with the original dataframe with the predicted and optimized features.  The optimized features  \\n\",\n    \"             name are prefixed with opt_ \\n\",\n    \"    '''\\n\",\n    \"    #create constrain_list\\n\",\n    \"    constrain_list = get_constraints(key_dict)\\n\",\n    \"    #create new features and set them to 0.00\\n\",\n    \"    df[\\\"opt_prediction\\\"] = 0.00\\n\",\n    \"    for feature in ts_settings[\\\"cfeatures\\\"]:\\n\",\n    \"        df[\\\"opt_\\\"+feature] = 0.00\\n\",\n    \"\\n\",\n    \"    #For create a constraints for each row and request optimization from DataRobot Optimizer App\\n\",\n    \"    for index, row in df.iterrows():\\n\",\n    \"        tmp1 = json.loads(row.to_json())\\n\",\n    \"        constraints =set_optimizer(constrain_list,tmp1)\\n\",\n    \"        for app in key_dict:  # if using aiohttp, you could start multiple optimizations and have them run simultaneously\\n\",\n    \"            results = get_optimization(app, constraints)\\n\",\n    \"            #Get the optimized target: prediction, and the values of the constrained features that resulted in the optimized target\\n\",\n    \"            tp = results['optimized_simulation']\\n\",\n    \"            df.loc[index,\\\"opt_prediction\\\"] = tp['prediction']\\n\",\n    \"            for feature in tp['features']:\\n\",\n    \"                f_name = feature[\\\"name\\\"]\\n\",\n    \"                df.loc[index,\\\"opt_\\\"+f_name] = feature[\\\"value\\\"]\\n\",\n    \"    return df\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# RUN THIS \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"data_df = pd.read_csv(ts_settings[\\\"filename\\\"])\\n\",\n    \"result_df = perform_optimization(data_df)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 55,\n   \"metadata\": {\n    \"pycharm\": {\n     \"is_executing\": false\n    }\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>opt_prediction</th>\\n\",\n       \"      <th>opt_revol_util</th>\\n\",\n       \"      <th>opt_inq_last_6mths</th>\\n\",\n       \"      <th>opt_loan_amnt</th>\\n\",\n       \"      <th>opt_dti</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>0</td>\\n\",\n       \"      <td>0.048569</td>\\n\",\n       \"      <td>1.383494</td>\\n\",\n       \"      <td>8.192054</td>\\n\",\n       \"      <td>27291.322287</td>\\n\",\n       \"      <td>0.134797</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>1</td>\\n\",\n       \"      <td>0.291327</td>\\n\",\n       \"      <td>48.439644</td>\\n\",\n       \"      <td>3.468059</td>\\n\",\n       \"      <td>28272.498063</td>\\n\",\n       \"      <td>1.917662</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>2</td>\\n\",\n       \"      <td>0.213890</td>\\n\",\n       \"      <td>94.653879</td>\\n\",\n       \"      <td>2.516591</td>\\n\",\n       \"      <td>1048.202766</td>\\n\",\n       \"      <td>7.263397</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>3</td>\\n\",\n       \"      <td>0.276012</td>\\n\",\n       \"      <td>89.474905</td>\\n\",\n       \"      <td>1.336023</td>\\n\",\n       \"      <td>29563.737299</td>\\n\",\n       \"      <td>19.058788</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>4</td>\\n\",\n       \"      <td>0.053004</td>\\n\",\n       \"      <td>46.291825</td>\\n\",\n       \"      <td>1.916218</td>\\n\",\n       \"      <td>27783.824290</td>\\n\",\n       \"      <td>18.628171</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>5</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>6</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>7</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>8</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>9</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>10</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>11</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>12</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>13</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>14</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>15</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>16</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>17</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>18</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>19</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>20</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>21</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>22</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>23</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>24</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>25</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>26</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>27</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>28</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"    opt_prediction  opt_revol_util  opt_inq_last_6mths  opt_loan_amnt  \\\\\\n\",\n       \"0         0.048569        1.383494            8.192054   27291.322287   \\n\",\n       \"1         0.291327       48.439644            3.468059   28272.498063   \\n\",\n       \"2         0.213890       94.653879            2.516591    1048.202766   \\n\",\n       \"3         0.276012       89.474905            1.336023   29563.737299   \\n\",\n       \"4         0.053004       46.291825            1.916218   27783.824290   \\n\",\n       \"5         0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"6         0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"7         0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"8         0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"9         0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"10        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"11        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"12        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"13        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"14        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"15        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"16        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"17        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"18        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"19        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"20        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"21        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"22        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"23        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"24        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"25        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"26        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"27        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"28        0.379326       92.367910            2.879611   24905.041876   \\n\",\n       \"\\n\",\n       \"      opt_dti  \\n\",\n       \"0    0.134797  \\n\",\n       \"1    1.917662  \\n\",\n       \"2    7.263397  \\n\",\n       \"3   19.058788  \\n\",\n       \"4   18.628171  \\n\",\n       \"5   22.379689  \\n\",\n       \"6   22.379689  \\n\",\n       \"7   22.379689  \\n\",\n       \"8   22.379689  \\n\",\n       \"9   22.379689  \\n\",\n       \"10  22.379689  \\n\",\n       \"11  22.379689  \\n\",\n       \"12  22.379689  \\n\",\n       \"13  22.379689  \\n\",\n       \"14  22.379689  \\n\",\n       \"15  22.379689  \\n\",\n       \"16  22.379689  \\n\",\n       \"17  22.379689  \\n\",\n       \"18  22.379689  \\n\",\n       \"19  22.379689  \\n\",\n       \"20  22.379689  \\n\",\n       \"21  22.379689  \\n\",\n       \"22  22.379689  \\n\",\n       \"23  22.379689  \\n\",\n       \"24  22.379689  \\n\",\n       \"25  22.379689  \\n\",\n       \"26  22.379689  \\n\",\n       \"27  22.379689  \\n\",\n       \"28  22.379689  \"\n      ]\n     },\n     \"execution_count\": 55,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"result_df[['opt_prediction','opt_revol_util','opt_inq_last_6mths','opt_loan_amnt','opt_dti']]\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"{'non_flexible_features': {'Target (1)': None,\\n\",\n       \"  'Target (1) Prediction Value': None,\\n\",\n       \"  'addr_state': None,\\n\",\n       \"  'annual_inc': None,\\n\",\n       \"  'custom_id': None,\\n\",\n       \"  'delinq_2yrs': None,\\n\",\n       \"  'desc': None,\\n\",\n       \"  'earliest_cr_line': None,\\n\",\n       \"  'emp_length': None,\\n\",\n       \"  'emp_title': None,\\n\",\n       \"  'funded_amnt': None,\\n\",\n       \"  'grade': None,\\n\",\n       \"  'home_ownership': None,\\n\",\n       \"  'initial_list_status': None,\\n\",\n       \"  'installment': None,\\n\",\n       \"  'int_rate': None,\\n\",\n       \"  'is_bad': None,\\n\",\n       \"  'mths_since_last_delinq': None,\\n\",\n       \"  'mths_since_last_major_derog': None,\\n\",\n       \"  'mths_since_last_record': None,\\n\",\n       \"  'open_acc': None,\\n\",\n       \"  'opt_dti': 0.0,\\n\",\n       \"  'opt_inq_last_6mths': 0.0,\\n\",\n       \"  'opt_loan_amnt': 0.0,\\n\",\n       \"  'opt_prediction': 0.0,\\n\",\n       \"  'opt_revol_util': 0.0,\\n\",\n       \"  'policy_code': None,\\n\",\n       \"  'pub_rec': None,\\n\",\n       \"  'purpose': None,\\n\",\n       \"  'pymnt_plan': None,\\n\",\n       \"  'revol_bal': None,\\n\",\n       \"  'sub_grade': None,\\n\",\n       \"  'term': None,\\n\",\n       \"  'title': None,\\n\",\n       \"  'total_acc': None,\\n\",\n       \"  'url': None,\\n\",\n       \"  'verification_status': None,\\n\",\n       \"  'zip_code': None},\\n\",\n       \" 'target': 'max',\\n\",\n       \" 'constraints': [{'feature': 'revol_util',\\n\",\n       \"   'feature_type': 'Numeric',\\n\",\n       \"   'info': {'min': 11.1, 'max': 95.9, 'is_int': False}},\\n\",\n       \"  {'feature': 'inq_last_6mths',\\n\",\n       \"   'feature_type': 'Numeric',\\n\",\n       \"   'info': {'min': 0.0, 'max': 2.0, 'is_int': False}},\\n\",\n       \"  {'feature': 'loan_amnt',\\n\",\n       \"   'feature_type': 'Numeric',\\n\",\n       \"   'info': {'min': 3975.0, 'max': 25000.0, 'is_int': False}},\\n\",\n       \"  {'feature': 'dti',\\n\",\n       \"   'feature_type': 'Numeric',\\n\",\n       \"   'info': {'min': 6.48, 'max': 17.11, 'is_int': False}}],\\n\",\n       \" 'id': '606f24e1a2e6400001fe51ae',\\n\",\n       \" 'created_at': '2021-04-08 15:44:33.292224',\\n\",\n       \" 'updated_at': '2021-04-08 15:44:33.292224',\\n\",\n       \" 'dataset_id': '606f24dea2e6400001fe51ad',\\n\",\n       \" 'prediction': 0.2213493262,\\n\",\n       \" 'simulations': [{'features': [{'name': 'revol_util', 'value': 75.3089183286},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.0121556267},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 6351.9309600558},\\n\",\n       \"    {'name': 'dti', 'value': 15.7508365181}],\\n\",\n       \"   'prediction': 0.2213493262},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 45.9898803965},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.3715467761},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 22986.0407729629},\\n\",\n       \"    {'name': 'dti', 'value': 15.3598988959}],\\n\",\n       \"   'prediction': 0.2698968901},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 45.0049151077},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.5283001672},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 9895.0358223131},\\n\",\n       \"    {'name': 'dti', 'value': 16.9994890472}],\\n\",\n       \"   'prediction': 0.2325089264},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 36.8989496084},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.1108333666},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 20323.6183619076},\\n\",\n       \"    {'name': 'dti', 'value': 15.2680203057}],\\n\",\n       \"   'prediction': 0.2442418462},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 85.1210025953},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.3916550035},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 10287.7796167758},\\n\",\n       \"    {'name': 'dti', 'value': 16.3798095382}],\\n\",\n       \"   'prediction': 0.2599748296},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 13.7351761687},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.9472639234},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 24852.813391656},\\n\",\n       \"    {'name': 'dti', 'value': 6.9202648842}],\\n\",\n       \"   'prediction': 0.2267111463},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 21.6605689716},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.9671320113},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 19354.6466687579},\\n\",\n       \"    {'name': 'dti', 'value': 10.9410355775}],\\n\",\n       \"   'prediction': 0.2125952129},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 65.4168828772},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.7640812989},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 24879.2163814019},\\n\",\n       \"    {'name': 'dti', 'value': 12.1032434364}],\\n\",\n       \"   'prediction': 0.3100661141},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 63.494167749},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.0271427436},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 15272.5814180599},\\n\",\n       \"    {'name': 'dti', 'value': 10.6046142827}],\\n\",\n       \"   'prediction': 0.282925596},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 95.2447203999},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.9683800954},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 15141.2260745965},\\n\",\n       \"    {'name': 'dti', 'value': 7.3096336485}],\\n\",\n       \"   'prediction': 0.328156173},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 91.8879784118},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.4543316415},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 14666.3643678597},\\n\",\n       \"    {'name': 'dti', 'value': 6.537568976}],\\n\",\n       \"   'prediction': 0.3402189938},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 93.4833953214},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.719366406},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 5284.9371912345},\\n\",\n       \"    {'name': 'dti', 'value': 9.0186622784}],\\n\",\n       \"   'prediction': 0.2634209666},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 81.302248179},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.42669039},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 10855.2764557074},\\n\",\n       \"    {'name': 'dti', 'value': 13.3538904502}],\\n\",\n       \"   'prediction': 0.263408821},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 28.3946935177},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.998260119},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 18682.1257710116},\\n\",\n       \"    {'name': 'dti', 'value': 8.6154779801}],\\n\",\n       \"   'prediction': 0.2269842384},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 60.6111956391},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.3745211203},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 13419.2829805506},\\n\",\n       \"    {'name': 'dti', 'value': 8.5898111212}],\\n\",\n       \"   'prediction': 0.2780002195},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 72.8254576367},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.631396001},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 7783.3731939254},\\n\",\n       \"    {'name': 'dti', 'value': 13.7967193744}],\\n\",\n       \"   'prediction': 0.2625659994},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 89.9255202212},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.2450507362},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 4098.8064712948},\\n\",\n       \"    {'name': 'dti', 'value': 9.7129157475}],\\n\",\n       \"   'prediction': 0.2669822627},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 95.8907305848},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.1257277157},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 15905.7444754588},\\n\",\n       \"    {'name': 'dti', 'value': 7.0186165398}],\\n\",\n       \"   'prediction': 0.3309333302},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 95.3579001537},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.7136108728},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 17177.6830400163},\\n\",\n       \"    {'name': 'dti', 'value': 6.6291989201}],\\n\",\n       \"   'prediction': 0.3376584341},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 83.1111257531},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.7352386171},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 17303.6851475088},\\n\",\n       \"    {'name': 'dti', 'value': 6.5152202818}],\\n\",\n       \"   'prediction': 0.3039716097},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 76.3538211344},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.7419517757},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 12855.7429681084},\\n\",\n       \"    {'name': 'dti', 'value': 8.0394714525}],\\n\",\n       \"   'prediction': 0.2867518176},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 88.3424954986},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.5999148834},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 21410.0910474588},\\n\",\n       \"    {'name': 'dti', 'value': 9.7613512478}],\\n\",\n       \"   'prediction': 0.287835768},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 52.3656133662},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.9132520497},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 17216.0914850069},\\n\",\n       \"    {'name': 'dti', 'value': 7.6645625285}],\\n\",\n       \"   'prediction': 0.2742213185},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 71.5883788679},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.2808733752},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 12825.0088693607},\\n\",\n       \"    {'name': 'dti', 'value': 6.5025125975}],\\n\",\n       \"   'prediction': 0.2949951277},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 56.0550031723},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.8177252078},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 22110.8957829512},\\n\",\n       \"    {'name': 'dti', 'value': 12.0713823746}],\\n\",\n       \"   'prediction': 0.2863530612},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 80.4193213513},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.5606480175},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 17385.2476504224},\\n\",\n       \"    {'name': 'dti', 'value': 9.6313559924}],\\n\",\n       \"   'prediction': 0.2815199426},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 95.8681400578},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.1793427686},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 11792.2758850103},\\n\",\n       \"    {'name': 'dti', 'value': 10.6775069252}],\\n\",\n       \"   'prediction': 0.3243969569},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 68.2527480122},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.3989299088},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 16194.995032345},\\n\",\n       \"    {'name': 'dti', 'value': 8.0347125036}],\\n\",\n       \"   'prediction': 0.2864079621},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 76.6352413331},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.5854440744},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 7205.5260752878},\\n\",\n       \"    {'name': 'dti', 'value': 13.2010113217}],\\n\",\n       \"   'prediction': 0.2238153512},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 89.2444415711},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.8099329154},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 13953.9001852581},\\n\",\n       \"    {'name': 'dti', 'value': 6.6726280516}],\\n\",\n       \"   'prediction': 0.292055332},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 46.3403679924},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.8765845798},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 9100.961419054},\\n\",\n       \"    {'name': 'dti', 'value': 7.6825839762}],\\n\",\n       \"   'prediction': 0.2351267062},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 36.799456647},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.4622552189},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 18696.9157106096},\\n\",\n       \"    {'name': 'dti', 'value': 11.2982050211}],\\n\",\n       \"   'prediction': 0.2380610019},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 58.4759230445},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.6733721591},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 23236.067635923},\\n\",\n       \"    {'name': 'dti', 'value': 15.0312296444}],\\n\",\n       \"   'prediction': 0.2982348448},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 87.3967794833},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.0327203476},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 11536.5000507494},\\n\",\n       \"    {'name': 'dti', 'value': 10.214204467}],\\n\",\n       \"   'prediction': 0.2729464737},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 51.1122485101},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.4304121949},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 20826.0344615887},\\n\",\n       \"    {'name': 'dti', 'value': 8.986912674}],\\n\",\n       \"   'prediction': 0.2775705212},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 37.5025140363},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.3301253022},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 9075.2595962895},\\n\",\n       \"    {'name': 'dti', 'value': 14.1425715185}],\\n\",\n       \"   'prediction': 0.2135976787},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 77.7597681003},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.0682114702},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 19679.1629395437},\\n\",\n       \"    {'name': 'dti', 'value': 7.3256514118}],\\n\",\n       \"   'prediction': 0.2961062396},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 68.6355773141},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 0.8158646827},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 23547.2122722988},\\n\",\n       \"    {'name': 'dti', 'value': 12.5688089451}],\\n\",\n       \"   'prediction': 0.3007942532},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 92.1976329168},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.4925243546},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 18172.928352835},\\n\",\n       \"    {'name': 'dti', 'value': 11.2991337404}],\\n\",\n       \"   'prediction': 0.3375831517},\\n\",\n       \"  {'features': [{'name': 'revol_util', 'value': 13.2394185304},\\n\",\n       \"    {'name': 'inq_last_6mths', 'value': 1.8161104625},\\n\",\n       \"    {'name': 'loan_amnt', 'value': 14944.8526552772},\\n\",\n       \"    {'name': 'dti', 'value': 8.3542370952}],\\n\",\n       \"   'prediction': 0.1592060287}],\\n\",\n       \" 'row': {'revol_util': None,\\n\",\n       \"  'inq_last_6mths': None,\\n\",\n       \"  'loan_amnt': None,\\n\",\n       \"  'dti': None},\\n\",\n       \" 'optimized_simulation': {'features': [{'name': 'revol_util',\\n\",\n       \"    'value': 91.8879784118},\\n\",\n       \"   {'name': 'inq_last_6mths', 'value': 1.4543316415},\\n\",\n       \"   {'name': 'loan_amnt', 'value': 14666.3643678597},\\n\",\n       \"   {'name': 'dti', 'value': 6.537568976}],\\n\",\n       \"  'prediction': 0.3402189938,\\n\",\n       \"  'index': 10},\\n\",\n       \" 'feature_histograms': {},\\n\",\n       \" 'step_line': [0.2213493262,\\n\",\n       \"  0.2698968901,\\n\",\n       \"  0.2698968901,\\n\",\n       \"  0.2698968901,\\n\",\n       \"  0.2698968901,\\n\",\n       \"  0.2698968901,\\n\",\n       \"  0.2698968901,\\n\",\n       \"  0.3100661141,\\n\",\n       \"  0.3100661141,\\n\",\n       \"  0.328156173,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938,\\n\",\n       \"  0.3402189938],\\n\",\n       \" 'custom_expr': None}\"\n      ]\n     },\n     \"execution_count\": 8,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"# See example of output from the optimizer\\n\",\n    \"results\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"0.2709511209\"\n      ]\n     },\n     \"execution_count\": 9,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"df.loc[3,\\\"opt_prediction\\\"]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 61,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>custom_id</th>\\n\",\n       \"      <th>loan_amnt</th>\\n\",\n       \"      <th>funded_amnt</th>\\n\",\n       \"      <th>term</th>\\n\",\n       \"      <th>int_rate</th>\\n\",\n       \"      <th>installment</th>\\n\",\n       \"      <th>grade</th>\\n\",\n       \"      <th>sub_grade</th>\\n\",\n       \"      <th>emp_title</th>\\n\",\n       \"      <th>emp_length</th>\\n\",\n       \"      <th>...</th>\\n\",\n       \"      <th>mths_since_last_major_derog</th>\\n\",\n       \"      <th>policy_code</th>\\n\",\n       \"      <th>is_bad</th>\\n\",\n       \"      <th>Target (1)</th>\\n\",\n       \"      <th>Target (1) Prediction Value</th>\\n\",\n       \"      <th>opt_prediction</th>\\n\",\n       \"      <th>opt_revol_util</th>\\n\",\n       \"      <th>opt_inq_last_6mths</th>\\n\",\n       \"      <th>opt_loan_amnt</th>\\n\",\n       \"      <th>opt_dti</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>0</td>\\n\",\n       \"      <td>7.0</td>\\n\",\n       \"      <td>14000.0</td>\\n\",\n       \"      <td>8725.0</td>\\n\",\n       \"      <td>60 months</td>\\n\",\n       \"      <td>7.51%</td>\\n\",\n       \"      <td>174.88</td>\\n\",\n       \"      <td>A</td>\\n\",\n       \"      <td>A4</td>\\n\",\n       \"      <td>Peninsula Counseling Center</td>\\n\",\n       \"      <td>10+ years</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>1.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.998740</td>\\n\",\n       \"      <td>0.048569</td>\\n\",\n       \"      <td>1.383494</td>\\n\",\n       \"      <td>8.192054</td>\\n\",\n       \"      <td>27291.322287</td>\\n\",\n       \"      <td>0.134797</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>1</td>\\n\",\n       \"      <td>8.0</td>\\n\",\n       \"      <td>3975.0</td>\\n\",\n       \"      <td>3975.0</td>\\n\",\n       \"      <td>60 months</td>\\n\",\n       \"      <td>17.58%</td>\\n\",\n       \"      <td>100.04</td>\\n\",\n       \"      <td>D</td>\\n\",\n       \"      <td>D4</td>\\n\",\n       \"      <td>Health Plan of Nevada</td>\\n\",\n       \"      <td>6 years</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>1.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.910816</td>\\n\",\n       \"      <td>0.291327</td>\\n\",\n       \"      <td>48.439644</td>\\n\",\n       \"      <td>3.468059</td>\\n\",\n       \"      <td>28272.498063</td>\\n\",\n       \"      <td>1.917662</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>2</td>\\n\",\n       \"      <td>9.0</td>\\n\",\n       \"      <td>25000.0</td>\\n\",\n       \"      <td>25000.0</td>\\n\",\n       \"      <td>36 months</td>\\n\",\n       \"      <td>15.58%</td>\\n\",\n       \"      <td>873.76</td>\\n\",\n       \"      <td>D</td>\\n\",\n       \"      <td>D3</td>\\n\",\n       \"      <td>John Deere</td>\\n\",\n       \"      <td>2 years</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>1.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.722703</td>\\n\",\n       \"      <td>0.213890</td>\\n\",\n       \"      <td>94.653879</td>\\n\",\n       \"      <td>2.516591</td>\\n\",\n       \"      <td>1048.202766</td>\\n\",\n       \"      <td>7.263397</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>3</td>\\n\",\n       \"      <td>10.0</td>\\n\",\n       \"      <td>10000.0</td>\\n\",\n       \"      <td>10000.0</td>\\n\",\n       \"      <td>36 months</td>\\n\",\n       \"      <td>8.00%</td>\\n\",\n       \"      <td>313.37</td>\\n\",\n       \"      <td>A</td>\\n\",\n       \"      <td>A3</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>&lt; 1 year</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>1.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.953509</td>\\n\",\n       \"      <td>0.276012</td>\\n\",\n       \"      <td>89.474905</td>\\n\",\n       \"      <td>1.336023</td>\\n\",\n       \"      <td>29563.737299</td>\\n\",\n       \"      <td>19.058788</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>4</td>\\n\",\n       \"      <td>11.0</td>\\n\",\n       \"      <td>10000.0</td>\\n\",\n       \"      <td>10000.0</td>\\n\",\n       \"      <td>36 months</td>\\n\",\n       \"      <td>6.62%</td>\\n\",\n       \"      <td>307.04</td>\\n\",\n       \"      <td>A</td>\\n\",\n       \"      <td>A2</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>3 years</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>1.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.988155</td>\\n\",\n       \"      <td>0.053004</td>\\n\",\n       \"      <td>46.291825</td>\\n\",\n       \"      <td>1.916218</td>\\n\",\n       \"      <td>27783.824290</td>\\n\",\n       \"      <td>18.628171</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>5</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>6</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>7</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>8</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>9</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>10</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>11</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>12</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>13</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>14</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>15</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>16</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>17</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>18</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>19</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>20</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>21</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>22</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>23</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>24</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>25</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>26</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>27</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <td>28</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>0.379326</td>\\n\",\n       \"      <td>92.367910</td>\\n\",\n       \"      <td>2.879611</td>\\n\",\n       \"      <td>24905.041876</td>\\n\",\n       \"      <td>22.379689</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"<p>29 rows × 42 columns</p>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"    custom_id  loan_amnt  funded_amnt        term int_rate  installment grade  \\\\\\n\",\n       \"0         7.0    14000.0       8725.0   60 months    7.51%       174.88     A   \\n\",\n       \"1         8.0     3975.0       3975.0   60 months   17.58%       100.04     D   \\n\",\n       \"2         9.0    25000.0      25000.0   36 months   15.58%       873.76     D   \\n\",\n       \"3        10.0    10000.0      10000.0   36 months    8.00%       313.37     A   \\n\",\n       \"4        11.0    10000.0      10000.0   36 months    6.62%       307.04     A   \\n\",\n       \"5         NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"6         NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"7         NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"8         NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"9         NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"10        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"11        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"12        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"13        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"14        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"15        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"16        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"17        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"18        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"19        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"20        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"21        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"22        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"23        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"24        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"25        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"26        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"27        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"28        NaN        NaN          NaN         NaN      NaN          NaN   NaN   \\n\",\n       \"\\n\",\n       \"   sub_grade                    emp_title emp_length  ...  \\\\\\n\",\n       \"0         A4  Peninsula Counseling Center  10+ years  ...   \\n\",\n       \"1         D4        Health Plan of Nevada    6 years  ...   \\n\",\n       \"2         D3                   John Deere    2 years  ...   \\n\",\n       \"3         A3                          NaN   < 1 year  ...   \\n\",\n       \"4         A2                          NaN    3 years  ...   \\n\",\n       \"5        NaN                          NaN        NaN  ...   \\n\",\n       \"6        NaN                          NaN        NaN  ...   \\n\",\n       \"7        NaN                          NaN        NaN  ...   \\n\",\n       \"8        NaN                          NaN        NaN  ...   \\n\",\n       \"9        NaN                          NaN        NaN  ...   \\n\",\n       \"10       NaN                          NaN        NaN  ...   \\n\",\n       \"11       NaN                          NaN        NaN  ...   \\n\",\n       \"12       NaN                          NaN        NaN  ...   \\n\",\n       \"13       NaN                          NaN        NaN  ...   \\n\",\n       \"14       NaN                          NaN        NaN  ...   \\n\",\n       \"15       NaN                          NaN        NaN  ...   \\n\",\n       \"16       NaN                          NaN        NaN  ...   \\n\",\n       \"17       NaN                          NaN        NaN  ...   \\n\",\n       \"18       NaN                          NaN        NaN  ...   \\n\",\n       \"19       NaN                          NaN        NaN  ...   \\n\",\n       \"20       NaN                          NaN        NaN  ...   \\n\",\n       \"21       NaN                          NaN        NaN  ...   \\n\",\n       \"22       NaN                          NaN        NaN  ...   \\n\",\n       \"23       NaN                          NaN        NaN  ...   \\n\",\n       \"24       NaN                          NaN        NaN  ...   \\n\",\n       \"25       NaN                          NaN        NaN  ...   \\n\",\n       \"26       NaN                          NaN        NaN  ...   \\n\",\n       \"27       NaN                          NaN        NaN  ...   \\n\",\n       \"28       NaN                          NaN        NaN  ...   \\n\",\n       \"\\n\",\n       \"   mths_since_last_major_derog  policy_code is_bad Target (1)  \\\\\\n\",\n       \"0                          NaN          1.0    0.0        0.0   \\n\",\n       \"1                          NaN          1.0    0.0        0.0   \\n\",\n       \"2                          NaN          1.0    0.0        0.0   \\n\",\n       \"3                          NaN          1.0    0.0        0.0   \\n\",\n       \"4                          NaN          1.0    0.0        0.0   \\n\",\n       \"5                          NaN          NaN    NaN        NaN   \\n\",\n       \"6                          NaN          NaN    NaN        NaN   \\n\",\n       \"7                          NaN          NaN    NaN        NaN   \\n\",\n       \"8                          NaN          NaN    NaN        NaN   \\n\",\n       \"9                          NaN          NaN    NaN        NaN   \\n\",\n       \"10                         NaN          NaN    NaN        NaN   \\n\",\n       \"11                         NaN          NaN    NaN        NaN   \\n\",\n       \"12                         NaN          NaN    NaN        NaN   \\n\",\n       \"13                         NaN          NaN    NaN        NaN   \\n\",\n       \"14                         NaN          NaN    NaN        NaN   \\n\",\n       \"15                         NaN          NaN    NaN        NaN   \\n\",\n       \"16                         NaN          NaN    NaN        NaN   \\n\",\n       \"17                         NaN          NaN    NaN        NaN   \\n\",\n       \"18                         NaN          NaN    NaN        NaN   \\n\",\n       \"19                         NaN          NaN    NaN        NaN   \\n\",\n       \"20                         NaN          NaN    NaN        NaN   \\n\",\n       \"21                         NaN          NaN    NaN        NaN   \\n\",\n       \"22                         NaN          NaN    NaN        NaN   \\n\",\n       \"23                         NaN          NaN    NaN        NaN   \\n\",\n       \"24                         NaN          NaN    NaN        NaN   \\n\",\n       \"25                         NaN          NaN    NaN        NaN   \\n\",\n       \"26                         NaN          NaN    NaN        NaN   \\n\",\n       \"27                         NaN          NaN    NaN        NaN   \\n\",\n       \"28                         NaN          NaN    NaN        NaN   \\n\",\n       \"\\n\",\n       \"   Target (1) Prediction Value opt_prediction opt_revol_util  \\\\\\n\",\n       \"0                     0.998740       0.048569       1.383494   \\n\",\n       \"1                     0.910816       0.291327      48.439644   \\n\",\n       \"2                     0.722703       0.213890      94.653879   \\n\",\n       \"3                     0.953509       0.276012      89.474905   \\n\",\n       \"4                     0.988155       0.053004      46.291825   \\n\",\n       \"5                          NaN       0.379326      92.367910   \\n\",\n       \"6                          NaN       0.379326      92.367910   \\n\",\n       \"7                          NaN       0.379326      92.367910   \\n\",\n       \"8                          NaN       0.379326      92.367910   \\n\",\n       \"9                          NaN       0.379326      92.367910   \\n\",\n       \"10                         NaN       0.379326      92.367910   \\n\",\n       \"11                         NaN       0.379326      92.367910   \\n\",\n       \"12                         NaN       0.379326      92.367910   \\n\",\n       \"13                         NaN       0.379326      92.367910   \\n\",\n       \"14                         NaN       0.379326      92.367910   \\n\",\n       \"15                         NaN       0.379326      92.367910   \\n\",\n       \"16                         NaN       0.379326      92.367910   \\n\",\n       \"17                         NaN       0.379326      92.367910   \\n\",\n       \"18                         NaN       0.379326      92.367910   \\n\",\n       \"19                         NaN       0.379326      92.367910   \\n\",\n       \"20                         NaN       0.379326      92.367910   \\n\",\n       \"21                         NaN       0.379326      92.367910   \\n\",\n       \"22                         NaN       0.379326      92.367910   \\n\",\n       \"23                         NaN       0.379326      92.367910   \\n\",\n       \"24                         NaN       0.379326      92.367910   \\n\",\n       \"25                         NaN       0.379326      92.367910   \\n\",\n       \"26                         NaN       0.379326      92.367910   \\n\",\n       \"27                         NaN       0.379326      92.367910   \\n\",\n       \"28                         NaN       0.379326      92.367910   \\n\",\n       \"\\n\",\n       \"   opt_inq_last_6mths opt_loan_amnt    opt_dti  \\n\",\n       \"0            8.192054  27291.322287   0.134797  \\n\",\n       \"1            3.468059  28272.498063   1.917662  \\n\",\n       \"2            2.516591   1048.202766   7.263397  \\n\",\n       \"3            1.336023  29563.737299  19.058788  \\n\",\n       \"4            1.916218  27783.824290  18.628171  \\n\",\n       \"5            2.879611  24905.041876  22.379689  \\n\",\n       \"6            2.879611  24905.041876  22.379689  \\n\",\n       \"7            2.879611  24905.041876  22.379689  \\n\",\n       \"8            2.879611  24905.041876  22.379689  \\n\",\n       \"9            2.879611  24905.041876  22.379689  \\n\",\n       \"10           2.879611  24905.041876  22.379689  \\n\",\n       \"11           2.879611  24905.041876  22.379689  \\n\",\n       \"12           2.879611  24905.041876  22.379689  \\n\",\n       \"13           2.879611  24905.041876  22.379689  \\n\",\n       \"14           2.879611  24905.041876  22.379689  \\n\",\n       \"15           2.879611  24905.041876  22.379689  \\n\",\n       \"16           2.879611  24905.041876  22.379689  \\n\",\n       \"17           2.879611  24905.041876  22.379689  \\n\",\n       \"18           2.879611  24905.041876  22.379689  \\n\",\n       \"19           2.879611  24905.041876  22.379689  \\n\",\n       \"20           2.879611  24905.041876  22.379689  \\n\",\n       \"21           2.879611  24905.041876  22.379689  \\n\",\n       \"22           2.879611  24905.041876  22.379689  \\n\",\n       \"23           2.879611  24905.041876  22.379689  \\n\",\n       \"24           2.879611  24905.041876  22.379689  \\n\",\n       \"25           2.879611  24905.041876  22.379689  \\n\",\n       \"26           2.879611  24905.041876  22.379689  \\n\",\n       \"27           2.879611  24905.041876  22.379689  \\n\",\n       \"28           2.879611  24905.041876  22.379689  \\n\",\n       \"\\n\",\n       \"[29 rows x 42 columns]\"\n      ]\n     },\n     \"execution_count\": 61,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"resuld_df\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.5\"\n  },\n  \"pycharm\": {\n   \"stem_cell\": {\n    \"cell_type\": \"raw\",\n    \"metadata\": {\n     \"collapsed\": false\n    },\n    \"source\": []\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Advanced%20Tuning%20and%20Partitioning/Python/DataRobot_Optimizer/Optimizing%20Through%20API.ipynb",
        "size": 74676,
        "description": "Jupyter notebook example from Advanced Tuning and Partitioning/Python/DataRobot_Optimizer/Optimizing Through API.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/DataRobot_Optimizer/README.md",
        "file_name": "README.md",
        "file_type": "markdown",
        "content": "# Controlling DataRobot Optimizer through Python\n\nIn some cases, you want to find the combination of values for some features that give the best predicted value. For instance, consider a bank that wants to reduce loan risk by determining which loan size and other factors for given borrowers results in the lowest risk. For this purpose, the bank will build a model that predicts load default, and then use the Optimizer App to figure out the loan size and other factors.\nThis notebook shows how to interact with the DataRobot Optimizer App using an example Lending Club dataset, \"Lending Club Sample 30.csv\". In this example, we are trying to find the best combination of values for `revol_util`, `inq_last_6mths`, `loan_amnt`, and `dti` that minimizes the probability of a loan going bad.\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Advanced%20Tuning%20and%20Partitioning/Python/DataRobot_Optimizer/README.md",
        "size": 791,
        "description": "Documentation: README",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/DataRobot_Optimizer/tp.txt",
        "file_name": "tp.txt",
        "file_type": "text",
        "content": "\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Advanced%20Tuning%20and%20Partitioning/Python/DataRobot_Optimizer/tp.txt",
        "size": 1,
        "description": "Text file: tp",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/Datetime Partitioning.ipynb",
        "file_name": "Datetime Partitioning.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Datetime Partitioning\\n\",\n    \"\\n\",\n    \"**Author**: Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"\\n\",\n    \"### Scope\\n\",\n    \"\\n\",\n    \"The scope of this notebook is to provide instructions on how to do datetime partitioning using the Python API.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"\\n\",\n    \"When dividing your data for model training and validation, DataRobot will randomly choose a set of rows from your dataset to assign amongst different cross validation folds. This will verify that you have not overfit your model to the training set and that the model can perform well on new data. However when your data has an intrinsic time based component, then you have to be even more careful about target leakage.  DataRobot now posseses datetime partitioning which will be diligent within model training & validation to guard against this, but you should always use your domain exerptise to evaluate your features prior to modeling.\\n\",\n    \"\\n\",\n    \"Let’s look at how we would frame a problem with a time component within DataRobot. This project basically simulated what you would get if you tried \\\"Out of Time Validation\\\" within DataRobot interface which is **not the same as Time Series projects**, even though the way we define backtests is very similar.\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.20.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\\n\",\n    \"\\n\",\n    \"It is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model </code> object.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\\n\",\n    \"from datetime import datetime\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Running a DataRobot Project with a datetime partition\\n\",\n    \"The durations need to be specified in string using the format of the <code>dr.helpers.partitioning_methods.construct_duration_string()</code> method.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"spec = dr.DatetimePartitioningSpecification(datetime_partition_column = 'Date',\\n\",\n    \"                                            holdout_start_date=datetime(2017,1,2),\\n\",\n    \"                                            holdout_duration='P1Y0M0DT0H0M0S',\\n\",\n    \"                                            number_of_backtests = 2,\\n\",\n    \"                                            use_time_series = False)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"We took advantage of DataRobot’s automated partition date selection after we specified the number of backtests to use. DataRobot allows further control, where we can further specify the validation start date as well as duration. Let’s look at an example below.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Create Backtest Specifications\\n\",\n    \"The below method would work for both Time Series and Out of Time Validation projects. Currently, we have specified that <code>use_time_series = False</code> in the <code>dr.DatetimePartitioningSpecification()</code> method so this would be initiated as an OTV project.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Dates are not project specific but rather example dates\\n\",\n    \"spec.backtests=[dr.BacktestSpecification(0,gap_duration = 'P0Y0M0DT0H0M0S',\\n\",\n    \"                                         validation_start_date = datetime(2016,1,2), \\n\",\n    \"                                         validation_duration = 'P1Y0M0DT0H0M0S'),\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"                dr.BacktestSpecification(1,gap_duration = 'P0Y0M0DT0H0M0S',\\n\",\n    \"                                         validation_start_date = datetime(2015,1,2), \\n\",\n    \"                                         validation_duration = 'P1Y0M0DT0H0M0S')]\\n\",\n    \"\\n\",\n    \"#To start the project\\n\",\n    \"project = dr.Project.create(sourcedata = df, project_name = 'Project Name')\\n\",\n    \"project.set_target('target_column',partitioning_method = spec)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"The above methods will change the backtest specification for the first and second backtests.\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Advanced%20Tuning%20and%20Partitioning/Python/Datetime%20Partitioning.ipynb",
        "size": 5329,
        "description": "Jupyter notebook example from Advanced Tuning and Partitioning/Python/Datetime Partitioning.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "CODE_OF_CONDUCT.md",
        "file_name": "CODE_OF_CONDUCT.md",
        "file_type": "markdown",
        "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at community@datarobot.com. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/CODE_OF_CONDUCT.md",
        "size": 3355,
        "description": "Documentation: CODE_OF_CONDUCT",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "CONTRIBUTING.md",
        "file_name": "CONTRIBUTING.md",
        "file_type": "markdown",
        "content": "# Contributing Guidelines\n\nGuidelines for developing and contributing to this project.\n\n## List of project maintainers\n\n- [Emily Webber](https://github.com/emilyswebber)\n- [Thodoris Petropoulos](https://github.com/TheoPetropoulos)\n\n## Opening new issues\n\n- Before opening a new issue check if there are any existing FAQ entries (if one exists), issues or pull requests that match your case\n- Open an issue, and make sure to label the issue accordingly - bug, improvement, feature request, etc...\n- Be as specific and detailed as possible\n\n## Making a pull request \n\n- Have a branch with a descriptive name\n- Squash / rebase your commits before opening your pull request\n- Pull the latest changes from `master`\n- Provide sufficient description of the pull request. Include whether it relates to an existing issue, and specify what the pull request does - (bug fix, documentation correction, dependency update, new functionality, etc...). When in doubt, overcommunicate\n\n## Responding to issues and pull requests\n\nThis project's maintainers will make every effort to respond to any open issues as soon as possible.\n\nIf you don't get a response within 7 days of creating your issue or pull request, please send us an email at community@datarobot.com.\n\n\n\n\n\n\n\n\n\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/CONTRIBUTING.md",
        "size": 1257,
        "description": "Documentation: CONTRIBUTING",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Compliance Docs/Python/Generate-compliance-documentation.ipynb",
        "file_name": "Generate-compliance-documentation.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Generate Compliance documentation\\n\",\n    \"\\n\",\n    \"DataRobot automates many critical compliance tasks associated with developing a model and by doing so, decreases the time-to-deployment in highly regulated industries. For each model, you can generate individualized documentation to provide comprehensive guidance on what constitutes effective model risk management. Then, you can download the report as an editable Microsoft Word document (DOCX). The generated report includes the appropriate level of information and transparency necessitated by regulatory compliance demands.\\n\",\n    \"\\n\",\n    \"This notebooks explains how to generate [compliance documentation](https://docs.datarobot.com/en/docs/modeling/analyze-models/compliance/compliance.html) with the DataRobot Python Client.\\n\",\n    \"\\n\",\n    \"Download this notebook from the [code examples home page](index).\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"* Python version 3.7.3\\n\",\n    \"*  DataRobot API version 2.19.0\\n\",\n    \"* A DataRobot `Project` object and project ID\\n\",\n    \"* A DataRobot `Model` object and model ID\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Import libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Download compliance documentation\\n\",\n    \"\\n\",\n    \"To download compliance documentation for a particular model, you must call <code>ComplianceDocumentation</code> on a particular project and model. Next, generate the documentation (in DOCX format) and save it to your desired file path.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Use the default compliance documentation template\\n\",\n    \"doc = dr.ComplianceDocumentation(project.id, model.id)\\n\",\n    \"\\n\",\n    \"# Initiate the job to generate documentation\\n\",\n    \"job = doc.generate()\\n\",\n    \"\\n\",\n    \"# Once the job is complete, download the documentation as a DOCX file\\n\",\n    \"job.wait_for_completion()\\n\",\n    \"doc.download('/path/to/save')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Create a custom template\\n\",\n    \"\\n\",\n    \"You can also use your own custom templates to generate compliance documentation. To do so, download the default template as a JSON file using <code>ComplianceDocTemplate</code> and then customize it. Alternatively, [create a custom template from scratch](#create-a-blank-custom-template).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"GetComplianceDocTemplate(\\\"path/to/filename.json\\\")\\n\",\n    \"default_template = dr.ComplianceDocTemplate.get_default()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Update the default template\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Download the template and edit sections locally\\n\",\n    \"default_template.sections_to_json_file('path/to/save')\\n\",\n    \"\\n\",\n    \"# Create a new template from your local file\\n\",\n    \"my_template = dr.ComplianceDocTemplate.create_from_json_file(name='my_template', path='path/of/file')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Create a blank custom template\\n\",\n    \"\\n\",\n    \"Use the code below to create a custom template without using the default template as a starting point.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"sections = [{\\n\",\n    \"            'title': 'Missing Values Report',\\n\",\n    \"            'highlightedText': 'NOTICE',\\n\",\n    \"            'regularText': 'This dataset had a lot of Missing Values. See the chart below: {{missing_values}}',\\n\",\n    \"            'type': 'user'\\n\",\n    \"            },\\n\",\n    \"            {\\n\",\n    \"            'title': 'Blueprints',\\n\",\n    \"            'highlightedText': '',\\n\",\n    \"            'regularText': '{{blueprint_diagram}} /n Blueprint for this model'\\n\",\n    \"            'type': 'user'\\n\",\n    \"            }]\\n\",\n    \"template = dr.ComplianceDocTemplate.create(name='Example', sections=sections)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Download your custom template\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Specify the template_id to generate documentation using a custom template\\n\",\n    \"doc = dr.ComplianceDocumentation(project.id, model.id, template.id)\\n\",\n    \"job = doc.generate().wait_for_completion()\\n\",\n    \"doc.download('/path/to/save')\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3 (ipykernel)\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.12\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Compliance%20Docs/Python/Generate-compliance-documentation.ipynb",
        "size": 5441,
        "description": "Jupyter notebook example from Compliance Docs/Python/Generate-compliance-documentation.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "DataPrep/README.md",
        "file_name": "README.md",
        "file_type": "markdown",
        "content": "# Paxata Functions\n\nScope: The scope of this script is to provide you with ready python functions which you can use to interact with DataRobot Paxata.\n\n### Table of contents\n\n(1) Get Library Name and schema from LibraryID\n\n(1a) Get Library Name and schema from LibraryID (and Version)\n\n(2) Get all of the datasources from Paxata that are tagged with \"tag\"\n\n(3) POST Library data from Paxata and load it into a JSON structure\n\n(4) Get the Name of a Library from it's DatasetID\n\n(5) Get all the Projects that have been described with a specific \"description_tag\"\n\n(6) Post a file to the Paxata library (Paxata will guess how to parse it), return the new libraryId\n\n(7) Check if a Project Name exists and return it's ID\n\n(8) Delete a project based on ID (TEST THIS), not sure if i can access the content directly\n\n(9) Post (Run) a Project\n\n(10) Get the script of a projectId\n\n(11) Replace values in a json file (useful for updating Paxata project scripts)\n\n(12) Create a new (empty) Paxata project. Will return the projectId\n\n(13) Update an existing Project with a new script file (this is not recommended)\n\n(14) Get the script of a projectId\n\n(15) Delete a Library item from it's ID\n\n(16) Export/POST a libraryItem(Answerset) to a target\n\n(17) Get DatasourceId and ConnectorId from Name of the Datasource\n\n(18) Get the UserID From the REST API Token\n\n(19) Check that an Export/POST to an external source has been completed (version 2.22)\n\n(20) Check that an Export/POST to an external source has been completed (all versions)\n\n(21) Get the datasetId of a Library from it's Name (a potentially very expensive call)\n\n(22) Get all of the datasources (ordered_ for a tenant)\n\n(23) Get All columns names for a library item\n\n(25) Extract key values out of a json file def extract_values(obj, key)\n\n(26) Get the Library items of a Project\n\n### User Admin Functions\n\n(24) Get ALl users on a tenant\n\n(30) get_name_latest_version_and_schema_of_datasource\n\n**Requirements:** Python 3.7 or higher;",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/DataPrep/README.md",
        "size": 1983,
        "description": "Documentation: README",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "DataPrep/dataprep_functions.py",
        "file_name": "dataprep_functions.py",
        "file_type": "python",
        "content": "__author__ = 'Callum Finlayson'\n\nimport requests,json,re\nfrom collections import OrderedDict\n\n# (1) Get Library Name and schema from LibraryID\ndef get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_schema_dict\n\n# (1a) Get Library Name and schema from LibraryID (and Version)\ndef get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    library_name = \"\"\n    library_schema_dict = {}\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources.get('name')\n        library_schema_dict = jdata_datasources.get('schema')\n    return library_name,library_schema_dict\n\n\n# (2) Get all of the datasources from Paxata that are tagged with \"tag\"\ndef get_tagged_library_items(auth_token,paxata_url,tag):\n    tagged_datasets = []\n    get_tags_request = (paxata_url + \"/rest/library/tags\")\n    get_tags_response = requests.get(get_tags_request, auth=auth_token, verify=False)\n    if (get_tags_response.ok):\n        AllTagsDatasetsJson = json.loads(get_tags_response.content)\n        i=0\n        number_of_datasets = len(AllTagsDatasetsJson)\n        while i < number_of_datasets:\n            if (AllTagsDatasetsJson[i].get('name') == tag):\n                tagged_datasets.append(AllTagsDatasetsJson[i].get('dataFileId'), AllTagsDatasetsJson[i].get('name'))\n            i += 1\n    else:\n        print(\"bad request> \" + get_tags_response.status_code)\n    return tagged_datasets\n\n# (3) POST Library data from Paxata and load it into a JSON structure\ndef get_paxata_library_data(auth_token,paxata_url,library_dataset_id):\n    post_request = (paxata_url + \"/rest/datasource/exports/local/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData\n\n# (4) Get the Name of a Library from it's DatasetID\ndef get_name_of_datasource(auth_token_source,paxata_url_source,libraryId):\n    url_request = (paxata_url_source + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataDataSources = json.loads(my_response.content)\n        libraryName = jDataDataSources[0].get('name')\n    return libraryName\n\n# (5) Get all the Projects that have been described with a specific \"description_tag\"\ndef get_all_project_information(auth_token_source, paxata_url_source, description_tag):\n    Package_Tagged_Projects = []\n    package_counter = 0\n    max_num_of_projects = 0\n    ProjectNames = []\n    url_request = (paxata_url_source + \"/rest/projects\")\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataProjectIds = json.loads(my_response.content)\n        for item in jDataProjectIds:\n            if description_tag == jDataProjectIds[package_counter].get('description'):\n                ProjectNames.append(jDataProjectIds[package_counter].get('name'))\n                Package_Tagged_Projects.append(jDataProjectIds[package_counter].get('projectId'))\n                max_num_of_projects += 1\n            package_counter += 1\n    return Package_Tagged_Projects\n\n# (6) Post a file to the Paxata library (Paxata will guess how to parse it), return the new libraryId\ndef post_file_to_paxata_library(auth_token_target,paxata_url_target, new_file_name):\n    new_libraryId = \"\"\n    if new_file_name is None:\n        print(\"File doesn't exist??\")\n    else:\n        ds = str(new_file_name)\n        print(\"Uploading \\\"\" + str(new_file_name) + \"\\\" to Library\")\n        sourcetype = {'source': 'local'}\n        files = {'data': open(ds, 'rb')}\n        dataset_upload_response = \"\"\n        try:\n            dataset_upload_response = requests.post(paxata_url_target + \"/rest/datasource/imports/local\", data=sourcetype,\n                                                  files=files, auth=auth_token_target)\n        except:\n            print(\"Connection error. Please validate the URL provided: \" + paxata_url_target.url)\n        if not (dataset_upload_response.ok):\n            print(\"Couldn't upload the library data. Status Code = \" + str(dataset_upload_response.status_code))\n        else:\n            jDataDataSources = json.loads(dataset_upload_response.content)\n            new_libraryId = jDataDataSources.get('dataFileId')\n    return new_libraryId\n\n# (7) Check if a Project Name exists and return it's ID\ndef check_if_a_project_exists(auth_token,paxata_url,project_name):\n    projectId = \"\"\n    url_request = (paxata_url + \"/rest/projects?name=\" + project_name)\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        jdata_new_project_response = json.loads(my_response.content)\n        if (not jdata_new_project_response):\n            projectId = 0\n        else:\n            projectId = jdata_new_project_response[0]['projectId']\n    else:\n        my_response.raise_for_status()\n    return projectId\n\n# (8) Delete a project based on ID (TEST THIS), not sure if i can access the content directly\ndef delete_a_project_if_it_exists(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/projects/\" + str(projectId))\n    my_response = requests.delete(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", my_response.content.get('name'), \"\\\" deleted.\")\n    else:\n        my_response.raise_for_status()\n\n# (9) Run a Project and publish the answerset to the library\ndef run_a_project(auth_token,paxata_url,projectId):\n    post_request = (paxata_url + \"/rest/project/publish?projectId=\" + projectId + \"&all=true\")\n    postResponse = requests.post(post_request, auth=auth_token, verify=False)\n    if (postResponse.ok):\n        print(\"Project Run - \", projectId)\n    else:\n        print(\"Something went wrong with POST call \", str(postResponse))\n    # I need to investigate the below, sometimes postResponse.content is a dict, sometimes a list, hence the two below trys\n    try:\n        AnswersetId = json.loads(postResponse.content)[0].get('dataFileId')\n    except(AttributeError):\n        AnswersetId = json.loads(postResponse.content).get('dataFileId', 0)\n    return AnswersetId\n\n# (10) Get the script of a projectId\ndef get_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        json_of_empty_project = json.loads(my_response.content)\n    else:\n        json_of_empty_project = 0\n        my_response.raise_for_status()\n    #the below return has an index of 0 to only return the latest version\n    return json_of_empty_project[0]\n\n# (11) Replace values in a json file (useful for updating Paxata project scripts)\ndef replace_json_values(json_file,oldvalue,newvalue):\n    fileinput = json.dumps(json_file)\n    newfileoutput = []\n    newfile = \"\"\n    newfile = re.sub(oldvalue, newvalue, fileinput.rstrip())\n    newfileoutput = json.loads(newfile)\n    return newfileoutput\n\n# (12) Create a new (empty) Paxata project. Will return the projectId\ndef create_a_new_project(auth_token_target,paxata_url_target,Project_Name):\n    projectId = \"\"\n    url_request = (paxata_url_target + \"/rest/projects?name=\" + Project_Name)\n    my_response = requests.post(url_request,auth=auth_token_target , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", Project_Name ,\"\\\" created.\")\n        jdata_new_project_response = json.loads(my_response.content)\n        projectId = jdata_new_project_response['projectId']\n    else:\n        if my_response.status_code == 409:\n            print(\"Project Already Exists\")\n        else:\n            my_response.raise_for_status()\n    return projectId\n\n# (13) Update an existing Project with a new script file (this is not recommended)\ndef update_project_with_new_script(auth_token,paxata_url,final_updated_json_script,projectId,working_path):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(final_updated_json_script)}\n    my_response = requests.put(url_request, data=s, auth=auth_token, verify=False)\n    if (not my_response.ok):\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        with open(working_path + '/invalid_script_dump.json', 'w') as f:\n            json.dump(final_updated_json_script, f)\n        my_response.raise_for_status()\n\n# (14) Get an existing Project's script file\ndef get_new_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId + \"&version=\" + \"0\")\n    myResponse = requests.get(url_request, auth=auth_token, verify=False)\n    if (myResponse.ok):\n        # Loads (Load String) takes a Json file and converts into python data structure (dict or list, depending on JSON)\n        json_of_empty_project = json.loads(myResponse.content)\n    else:\n        json_of_empty_project = 0\n        myResponse.raise_for_status()\n    return(json_of_empty_project)\n\n# (15) Delete Library Data from LibraryID\ndef delete_library_item(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.delete(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        library_name = jdata_datasources.get('name')\n    return library_name\n\n# (16) Export/POST a libraryItem(Answerset) to a target\ndef post_paxata_library_data(auth_token,paxata_url,library_dataset_id,pax_datasourceId,pax_connectorId):\n    post_request = (paxata_url + \"/rest/datasource/exports/\" + pax_datasourceId +\"/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData\n\n# (17) Get DatasourceId and ConnectorId from Name of the Datasource\ndef get_datasource_id_from_name(auth_token,paxata_url,datasource_name):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == datasource_name:\n                pax_datasourceId = jdata_datasources[0].get('dataSourceId')\n                pax_connectorId = jdata_datasources[0].get('connectorId')\n            row_count +=1\n    return pax_datasourceId,pax_connectorId\n\n# (18) Get the UserID From the REST API Token\ndef get_user_from_token(auth_token, paxata_url, resttoken):\n    url_request = (paxata_url + \"/rest/users?authToken=true\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('authToken') == resttoken:\n                pax_name = jdata_datasources[row_count].get('name')\n                pax_userId = jdata_datasources[row_count].get('userId')\n            row_count += 1\n    return pax_userId, pax_name\n\n# (19) Check that an Export/POST to an external source has been completed (version 2.22)\ndef get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token)\n    if (get_response.ok):\n        jdata_datasources = json.loads(get_response.content)\n        print(\"Succesfully have the exportId status\")\n        exportIdStatus = jdata_datasources[0].get('exportId')\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return exportIdStatus\n\n# (20) Check that an Export/POST to an external source has been completed (all versions)\ndef get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token, verify=False)\n    if (get_response.ok):\n        # In version 2.22 postResponse.content is a dict, prior to that it is a list which i need to manually iterate through, hence the two below trys\n        print('.')\n        jdata_datasources = json.loads(get_response.content)\n        try:\n            exportIdState = jdata_datasources.get('state')\n            exporttimeStarted = jdata_datasources.get('timeStarted')\n            exporttimeFinished = jdata_datasources.get('timeFinished')\n        except(AttributeError):\n            row_count = 0\n            for row in jdata_datasources:\n                if jdata_datasources[row_count].get('exportId') == pax_exportId:\n                    exportIdState = jdata_datasources[row_count].get('state')\n                    exporttimeStarted = jdata_datasources[row_count].get('timeStarted')\n                    exporttimeFinished = jdata_datasources[row_count].get('timeFinished')\n                row_count += 1\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return(exportIdState,exporttimeStarted,exporttimeFinished)\n\n# (21) Get the datasetId of a Library from it's Name\ndef get_id_of_datasource(auth_token,paxata_url,dataset_name):\n    url_request = (paxata_url + \"/library/data/\")\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    dataFileId = 0\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == dataset_name:\n                dataFileId = jdata_datasources[row_count].get('dataFileId')\n            row_count +=1\n    return dataFileId\n\n\n# (22) Get all of the datasources (ordered_ for a tenant)\ndef get_datasource_configs(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    myResponse = requests.get(url_request, auth=authorization_token, verify=True)\n    if (myResponse.ok):\n        json_of_datasource_configs = json.loads(myResponse.content)\n    else:\n        json_of_datasource_configs = 0\n        myResponse.raise_for_status()\n    dict_of_datasources = {}\n    for item in json_of_datasource_configs:\n#        dict_of_datasources[item.get('connectorId')] = item.get('name')\n        dict_of_datasources[item.get('name')] = item.get('dataSourceId')\n\n    dict_of_datasources['0'] = ' - No Connector - Data already exists in Paxata - '\n    #returning a sorted dictionary\n    return(OrderedDict(sorted(dict_of_datasources.items(), key=lambda kv:(kv[0].lower(),kv[1]))))\n\n# (23) Get All columns names for a library item\ndef get_library_item_metadata(authorization_token,paxata_url,dataFileID):\n    url_request = (paxata_url + \"/rest/library/data/\"+ dataFileID)\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns\n\n# (24) Get ALl users on a tenant\ndef get_users_on_tenant(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/users\")\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns\n\n# (25) Extract key values out of a json file\ndef extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n\n# (26) Get the Library items of a Project\ndef get_libraryIds_of_lenses_exported_by_project(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/project/publish?projectId=\"+ projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items\n\n\n# (27) Get the information of where a =Library item was exported (ie which external system)\ndef get_info_of_exported_library_items(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/exports\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    json_of_library_items = []\n    if(my_response.ok):\n        library_export_info = json.loads(my_response.content)\n        for item in library_export_info:\n            if item.get('dataFileId') == libraryId:\n                i+=1\n                print(item.get('destination'))\n                print(\"Data Export - \" ,str(i) + \" for libraryId(\" ,libraryId + \")\")\n                print(\"Filename = \" , item.get('destination').get('name'))\n                print(\"Path = \" , item.get('destination').get('itemPath'))\n                print(\"ConnectorID = \" , item.get('destination').get('connectorId') + \"\\n\")\n                json_of_library_items.append(str(item.get('destination')))\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items\n\n\n# (28) Update an existing Project's script file\ndef update_project_with_new_script(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result\n\n# (29) Update an existing Project's Datasource file\ndef update_project_with_new_dataset(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=datasets&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result\n\n# (30) get_name_latest_version_and_schema_of_datasource\ndef get_name_latest_version_and_schema_of_datasource(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request, auth=auth_token, verify=True)\n    library_name = \"\"\n    library_schema_dict = \"\"\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_version = jdata_datasources[0].get('version')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_version,library_schema_dict\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/DataPrep/dataprep_functions.py",
        "size": 21427,
        "description": "(1) Get Library Name and schema from LibraryID",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Feature Discovery/Python/Soccer predictions/README.md",
        "file_name": "README.md",
        "file_type": "markdown",
        "content": "# Soccer prediction demo with Feature Discovery through API\n\n## Background\nThe goal of this project is to predict the number of goals that will be scored by the home and away teams, respectively, in future matches. We leverage DataRobot's Automated Feature Discovery tool (AFD) to engineer many features which we test in models built using Automated Machine Learning (AutoML). The features we lean on the most to make good predictions are \"expected goals\" and \"goals added\", explained (here)[https://www.americansocceranalysis.com/explanation] and (here)[https://www.americansocceranalysis.com/what-are-goals-added], respectively. These team-level metrics have been shown to be predictive of future success.  \n\nUsing the predicted number of goals scored for teams (the output of the AutoML models), and assuming each prediction is independently distributed as Poisson (a fairly reasonable simplifying assumption), we can also generate the probabilities of all possible score lines (e.g., a 3-2 win for the home team). This project was created such that the outputs could be used to simulate the goal outcomes of all remaining MLS games this season, to estimate probabilities of various playoff outcomes.\n\nThe data used in this project come from American Soccer Analysis's API, which feeds their publicly available data application (here)[https://app.americansocceranalysis.com]. Major League Soccer data were used in the examples here, but through the same API and application you can also get historical data from the National Women's Soccer League (NWSL), the United Soccer Leagues (USL), and the North American Soccer League (NASL, now defunct).\n\n## Contents\nThis repository contains:\n- An R notebook for gathering publicly available soccer data online and create primary and secondary datasets for a DataRobot AutoML project\n- A snapshot of the datasets produced by the notebook above, as of 2021-07-13\n- A Python notebook for setting up AutoML projects to predict goals scored for home and away teams in future matches\n\nThere is more information contained in each notebook.\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Discovery/Python/Soccer%20predictions/README.md",
        "size": 2079,
        "description": "Documentation: README",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Feature Lists Manipulation/Python/Advanced Feature Selection.ipynb",
        "file_name": "Advanced Feature Selection.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Advanced Feature Selection\\n\",\n    \"\\n\",\n    \"**Author**: Rajiv Shah\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"\\n\",\n    \"### Scope\\n\",\n    \"\\n\",\n    \"The scope of this notebook is to provide instructions on how to do advanced feature selection using all of the models created during a run of DataRobot autopilot.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"\\n\",\n    \"See https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.14.0/autodoc/api_reference.html#datarobot.models.Project.create_type_transform_feature for a discussion of which transforms are allowed\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.14.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\\n\",\n    \"\\n\",\n    \"It is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model </code> object.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\\n\",\n    \"import pandas as pd\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import seaborn as sns\\n\",\n    \"sns.set_style('ticks')\\n\",\n    \"sns.set_context('poster')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"I do not want to pick all of the models. I will ignore Blender, Auto-Tuned, and models trained with a small percentage of data. Lastly, I only care about models that were trained on 'Informative Features' but this can change depending on your needs.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"featurelist_name = \\\"Informative Features\\\"\\n\",\n    \"models = project.get_models()\\n\",\n    \"\\n\",\n    \"for model in models:\\n\",\n    \"    if (model.featurelist_name != featurelist_name \\n\",\n    \"        or \\\"Blender\\\" in model.model_type\\n\",\n    \"        or model.sample_pct < 60\\n\",\n    \"        or 'Auto-Tuned' in model.model_type):\\n\",\n    \"        \\n\",\n    \"        # Remove that model from the list\\n\",\n    \"        models = [x for x in models if x.id != model.id]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Then, we will create a DataFrame of feature relative rank for the top 5 models.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"all_impact = pd.DataFrame()\\n\",\n    \"for model in models[0:5]:\\n\",\n    \"    \\n\",\n    \"    # This can take a minute (for each)\\n\",\n    \"    feature_impact = model.get_or_request_feature_impact(max_wait=600)\\n\",\n    \"    \\n\",\n    \"    # Ready to be converted to DF\\n\",\n    \"    df = pd.DataFrame(feature_impact)\\n\",\n    \"    # Track model name and ID for bookkeeping purposes\\n\",\n    \"    df['model_type'] = model.model_type\\n\",\n    \"    df['model_id'] = model.id\\n\",\n    \"    # By sorting and re-indexing, the new index becomes our 'ranking'\\n\",\n    \"    df = df.sort_values(by='impactUnnormalized', ascending=False)\\n\",\n    \"    df = df.reset_index(drop=True)\\n\",\n    \"    df['rank'] = df.index.values\\n\",\n    \"    \\n\",\n    \"    # Add to our master list of all models' feature ranks\\n\",\n    \"    all_impact = pd.concat([all_impact, df], ignore_index=True)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>featureName</th>\\n\",\n       \"      <th>impactNormalized</th>\\n\",\n       \"      <th>impactUnnormalized</th>\\n\",\n       \"      <th>redundantWith</th>\\n\",\n       \"      <th>model_type</th>\\n\",\n       \"      <th>model_id</th>\\n\",\n       \"      <th>rank</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>number_inpatient</td>\\n\",\n       \"      <td>1.000000</td>\\n\",\n       \"      <td>0.031445</td>\\n\",\n       \"      <td>None</td>\\n\",\n       \"      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\\n\",\n       \"      <td>5e620be2d7c7a80c003d16a2</td>\\n\",\n       \"      <td>0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>discharge_disposition_id</td>\\n\",\n       \"      <td>0.950723</td>\\n\",\n       \"      <td>0.029896</td>\\n\",\n       \"      <td>None</td>\\n\",\n       \"      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\\n\",\n       \"      <td>5e620be2d7c7a80c003d16a2</td>\\n\",\n       \"      <td>1</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>medical_specialty</td>\\n\",\n       \"      <td>0.828289</td>\\n\",\n       \"      <td>0.026046</td>\\n\",\n       \"      <td>None</td>\\n\",\n       \"      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\\n\",\n       \"      <td>5e620be2d7c7a80c003d16a2</td>\\n\",\n       \"      <td>2</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>number_diagnoses</td>\\n\",\n       \"      <td>0.609419</td>\\n\",\n       \"      <td>0.019163</td>\\n\",\n       \"      <td>None</td>\\n\",\n       \"      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\\n\",\n       \"      <td>5e620be2d7c7a80c003d16a2</td>\\n\",\n       \"      <td>3</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>num_lab_procedures</td>\\n\",\n       \"      <td>0.543238</td>\\n\",\n       \"      <td>0.017082</td>\\n\",\n       \"      <td>None</td>\\n\",\n       \"      <td>eXtreme Gradient Boosted Trees Classifier with...</td>\\n\",\n       \"      <td>5e620be2d7c7a80c003d16a2</td>\\n\",\n       \"      <td>4</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"                featureName  impactNormalized  impactUnnormalized  \\\\\\n\",\n       \"0          number_inpatient          1.000000            0.031445   \\n\",\n       \"1  discharge_disposition_id          0.950723            0.029896   \\n\",\n       \"2         medical_specialty          0.828289            0.026046   \\n\",\n       \"3          number_diagnoses          0.609419            0.019163   \\n\",\n       \"4        num_lab_procedures          0.543238            0.017082   \\n\",\n       \"\\n\",\n       \"  redundantWith                                         model_type  \\\\\\n\",\n       \"0          None  eXtreme Gradient Boosted Trees Classifier with...   \\n\",\n       \"1          None  eXtreme Gradient Boosted Trees Classifier with...   \\n\",\n       \"2          None  eXtreme Gradient Boosted Trees Classifier with...   \\n\",\n       \"3          None  eXtreme Gradient Boosted Trees Classifier with...   \\n\",\n       \"4          None  eXtreme Gradient Boosted Trees Classifier with...   \\n\",\n       \"\\n\",\n       \"                   model_id  rank  \\n\",\n       \"0  5e620be2d7c7a80c003d16a2     0  \\n\",\n       \"1  5e620be2d7c7a80c003d16a2     1  \\n\",\n       \"2  5e620be2d7c7a80c003d16a2     2  \\n\",\n       \"3  5e620be2d7c7a80c003d16a2     3  \\n\",\n       \"4  5e620be2d7c7a80c003d16a2     4  \"\n      ]\n     },\n     \"execution_count\": 5,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"all_impact.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Now, we can find the N features with the highest median ranking and visualize the distributions:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {\n    \"scrolled\": false\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"image/png\": \"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\\n\",\n      \"text/plain\": [\n       \"<Figure size 1440x1800 with 1 Axes>\"\n      ]\n     },\n     \"metadata\": {\n      \"needs_background\": \"light\"\n     },\n     \"output_type\": \"display_data\"\n    }\n   ],\n   \"source\": [\n    \"from matplotlib.axes._axes import _log as matplotlib_axes_logger\\n\",\n    \"matplotlib_axes_logger.setLevel('ERROR')\\n\",\n    \"\\n\",\n    \"n_feats = 20\\n\",\n    \"top_feats = list(all_impact\\n\",\n    \"                 .groupby('featureName')\\n\",\n    \"                 .median()\\n\",\n    \"                 .sort_values('rank')\\n\",\n    \"                 .head(n_feats)\\n\",\n    \"                 .index\\n\",\n    \"                 .values)\\n\",\n    \"\\n\",\n    \"top_feat_impact = all_impact.query('featureName in @top_feats').copy()\\n\",\n    \"\\n\",\n    \"fig, ax = plt.subplots(figsize=(20, 25))\\n\",\n    \"sns.boxenplot(y='featureName', x='rank',\\n\",\n    \"            data=top_feat_impact, order=top_feats,\\n\",\n    \"            ax=ax, orient='h')\\n\",\n    \"_ = ax.set_ylabel('Feature Name')\\n\",\n    \"_ = ax.set_xlabel('Rank')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Finally, create a new feature list with the top features and re-run of DataRobot's Autopilot\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# ## Create new featurelist and run autopilot\\n\",\n    \"featurelist = project.create_featurelist(\\\"consensus-top-features\\\", list(top_feats))\\n\",\n    \"featurelist_id = featurelist.id\\n\",\n    \"\\n\",\n    \"project.start_autopilot(featurelist_id=featurelist_id)\\n\",\n    \"project.wait_for_autopilot()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Lists%20Manipulation/Python/Advanced%20Feature%20Selection.ipynb",
        "size": 114307,
        "description": "Jupyter notebook example from Feature Lists Manipulation/Python/Advanced Feature Selection.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Feature Lists Manipulation/Python/Feature Lists Manipulation.ipynb",
        "file_name": "Feature Lists Manipulation.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Feature Lists Manipulation\\n\",\n    \"\\n\",\n    \"**Author**: Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"\\n\",\n    \"### Scope\\n\",\n    \"The scope of this notebook is to provide instructions on how to create and manipulate custom feature lists using the Python API as well as how to train a model using a custom feature list.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"Feature lists control the subset of features that DataRobot uses to build models. \\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.19.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\\n\",\n    \"\\n\",\n    \"It is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model </code> object.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 10,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Listing Available Feature Lists\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"[Featurelist(Raw Features),\\n\",\n       \" Featurelist(Informative Features),\\n\",\n       \" Featurelist(Univariate Selections),\\n\",\n       \" Featurelist(DR Reduced Features M75)]\"\n      ]\n     },\n     \"execution_count\": 3,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"project.get_featurelists()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Creating a custom Feature List\\n\",\n    \"To create a custom feature list use the <code>create_featurelist</code> command.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"featurelist = project.create_featurelist('YOUR_CUSTOM_FEATURELIST_NAME', \\n\",\n    \"                                         features=project.get_featurelists()[0].features[0:15]) #I could also provide a list\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Training a model with a custom feature list\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"'170'\"\n      ]\n     },\n     \"execution_count\": 11,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"model.train(featurelist_id = featurelist.id)\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Lists%20Manipulation/Python/Feature%20Lists%20Manipulation.ipynb",
        "size": 3375,
        "description": "Jupyter notebook example from Feature Lists Manipulation/Python/Feature Lists Manipulation.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Feature Lists Manipulation/Python/FeatureSelection_using_Feature_Importance_Rank_Ensembling.ipynb",
        "file_name": "FeatureSelection_using_Feature_Importance_Rank_Ensembling.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Advanced Feature Selection using Feature Importance Rank Ensembling (FIRE)\\n\",\n    \"\\n\",\n    \"<b>Authors:</b> Vitalii Peretiatko, Rajiv Shah\\n\",\n    \"\\n\",\n    \"<b>Scope:</b><br>\\n\",\n    \"This notebook shows the benefits of advanced feature selection that uses median rank agggregation of feature impacts across several models created during a run of DataRobot autopilot. \\n\",\n    \"\\n\",\n    \"<b>Background:</b>\\n\",\n    \" - Original blogpost: https://www.datarobot.com/blog/using-feature-importance-rank-ensembling-fire-for-advanced-feature-selection/ \\n\",\n    \" - Blogpost on advanced feature selection in R: https://community.datarobot.com/t5/resources/advanced-feature-selection-with-r/ta-p/5307\\n\",\n    \" - Instructions and logic of how to do advanced feature selection in Python: https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Lists%20Manipulation/Python/Advanced%20Feature%20Selection.ipynb \\n\",\n    \" \\n\",\n    \"<b> Requirements </b><br>\\n\",\n    \"Python version >= 3.7.3<br>\\n\",\n    \"DataRobot API version >= 2.22.1.<br>\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<b>Dataset</b><br>\\n\",\n    \"We will be using Madelon dataset from this paper https://archive.ics.uci.edu/ml/datasets/Madelon <br>\\n\",\n    \"It can also be found here https://s3.amazonaws.com/datarobot_public_datasets/madelon_combined_80.csv\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<b>Import libraries and connect to DataRobot</b>\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"<datarobot.rest.RESTClientObject at 0x121fcdc90>\"\n      ]\n     },\n     \"execution_count\": 1,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"from collections import Counter\\n\",\n    \"from time import sleep\\n\",\n    \"\\n\",\n    \"import numpy as np\\n\",\n    \"import pandas as pd\\n\",\n    \"import datarobot as dr\\n\",\n    \"\\n\",\n    \"#connect to DataRobot using the config file\\n\",\n    \"dr.Client(config_path='../../drconfig.yaml')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<b>Define the function for FIRE feature selection</b>\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def feature_importance_rank_ensembling(project,\\n\",\n    \"                                       n_models=5,\\n\",\n    \"                                       metric=None,\\n\",\n    \"                                       by_partition='validation',\\n\",\n    \"                                       feature_list_name=None,\\n\",\n    \"                                       ratio=0.95,\\n\",\n    \"                                       model_search_params=None,\\n\",\n    \"                                       use_ranks=True,\\n\",\n    \"                                       ):\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    Function that implements the logic of Feature Selection using Feature Importance Rank Ensembling and restarts DR autopilot\\n\",\n    \"\\n\",\n    \"    Parameters:\\n\",\n    \"    -----------\\n\",\n    \"    project: DR project object,\\n\",\n    \"    n_models: int, get top N best models on the leaderboard to compute feature impact on. Default 5\\n\",\n    \"    metric: str, DR metric to check performance against. Default None. If Default, it will use DR project defined metric\\n\",\n    \"    by_partition: str, whether to use 'validation' or 'crossValidation' partition to get the best model on. Default 'validation'\\n\",\n    \"    feature_list_name: str, name of the feature list to start iterating from. Default None\\n\",\n    \"    ratio: float, ratio of total feature impact that new feature list will contain. Default 0.95\\n\",\n    \"    model_search_params: dict, dictionary of parameters to search the best model. See official DR python api docs. Default None\\n\",\n    \"    use_ranks: Boolean, True to use median rank aggregation or False to use total impact unnormalized. Default True\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"    models = get_best_models(project,\\n\",\n    \"                             metric=metric,\\n\",\n    \"                             by_partition=by_partition,\\n\",\n    \"                             start_featurelist_name=feature_list_name,\\n\",\n    \"                             model_search_params=model_search_params)\\n\",\n    \"\\n\",\n    \"    models = models.values[:n_models]\\n\",\n    \"\\n\",\n    \"    all_impact = pd.DataFrame()\\n\",\n    \"\\n\",\n    \"    print(\\\"Request Feature Impact calculations\\\")\\n\",\n    \"    # first kick off all FI requests, let DR deal with parallelizing\\n\",\n    \"    for model in models:\\n\",\n    \"        try:\\n\",\n    \"            model.request_feature_impact()\\n\",\n    \"        except:\\n\",\n    \"            pass\\n\",\n    \"\\n\",\n    \"    for model in models:\\n\",\n    \"        # This can take some time to compute feature impact\\n\",\n    \"        feature_impact = pd.DataFrame(model.get_or_request_feature_impact(max_wait=60 * 15))  # 15min\\n\",\n    \"\\n\",\n    \"        # Track model name and ID for bookkeeping purposes\\n\",\n    \"        feature_impact['model_type'] = model.model_type\\n\",\n    \"        feature_impact['model_id'] = model.id\\n\",\n    \"        # By sorting and re-indexing, the new index becomes our 'ranking'\\n\",\n    \"        feature_impact = feature_impact.sort_values(by='impactUnnormalized', ascending=False).reset_index(drop=True)\\n\",\n    \"        feature_impact['rank'] = feature_impact.index.values\\n\",\n    \"\\n\",\n    \"        # Add to our master list of all models' feature ranks\\n\",\n    \"        all_impact = pd.concat([all_impact, feature_impact], ignore_index=True)\\n\",\n    \"\\n\",\n    \"    # We need to get a threshold number of features to select.\\n\",\n    \"    # based on cumulative sum of impact\\n\",\n    \"    all_impact_agg = all_impact \\\\\\n\",\n    \"        .groupby('featureName')[['impactNormalized', 'impactUnnormalized']] \\\\\\n\",\n    \"        .sum() \\\\\\n\",\n    \"        .sort_values('impactUnnormalized', ascending=False) \\\\\\n\",\n    \"        .reset_index()\\n\",\n    \"\\n\",\n    \"    # calculate cumulative feature impact and take first features that possess <ratio> of total impact\\n\",\n    \"    all_impact_agg['impactCumulative'] = all_impact_agg['impactUnnormalized'].cumsum()\\n\",\n    \"    total_impact = all_impact_agg['impactCumulative'].max() * ratio\\n\",\n    \"    tmp_fl = list(set(all_impact_agg[all_impact_agg.impactCumulative <= total_impact]['featureName'].values.tolist()))\\n\",\n    \"\\n\",\n    \"    # that will be a number of feature to use\\n\",\n    \"    n_feats = len(tmp_fl)\\n\",\n    \"\\n\",\n    \"    if use_ranks:\\n\",\n    \"        # get top features based on median rank\\n\",\n    \"        top_ranked_feats = list(all_impact\\n\",\n    \"                                .groupby('featureName')\\n\",\n    \"                                .median()\\n\",\n    \"                                .sort_values('rank')\\n\",\n    \"                                .head(n_feats)\\n\",\n    \"                                .index\\n\",\n    \"                                .values)\\n\",\n    \"    else:\\n\",\n    \"        # otherwise get features based just on total unnormalized feature impact\\n\",\n    \"        top_ranked_feats = list(all_impact_agg.featureName.values[:n_feats])\\n\",\n    \"\\n\",\n    \"    # Create a new featurelist\\n\",\n    \"    featurelist = project.create_modeling_featurelist(f'Reduced FL by Median Rank, top{n_feats}', top_ranked_feats)\\n\",\n    \"    featurelist_id = featurelist.id\\n\",\n    \"\\n\",\n    \"    # Start autopilot\\n\",\n    \"    print('Starting AutoPilot on a reduced feature list')\\n\",\n    \"    project.start_autopilot(featurelist_id=featurelist_id,\\n\",\n    \"                            prepare_model_for_deployment=True,\\n\",\n    \"                            blend_best_models=False,\\n\",\n    \"                            )\\n\",\n    \"    project.wait_for_autopilot()\\n\",\n    \"    print('... AutoPilot is completed.')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<b>Define the function to get the best model on the DR leaderboard</b>\\n\",\n    \"\\n\",\n    \"We do not want to pick all of the models. <br>\\n\",\n    \"We want to avoid using models trained on a higher (80%,100%) than 3rd stage of autopilot sample size.<br>\\n\",\n    \"We ignore Blenders and Frozen models.\\n\",\n    \"That means we mainly pick models trained on 64% percent of the data.<br>\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def get_best_models(project, \\n\",\n    \"                    metric=None, \\n\",\n    \"                    by_partition='validation',\\n\",\n    \"                    start_featurelist_name=None,\\n\",\n    \"                    model_search_params=None\\n\",\n    \"                   ):\\n\",\n    \"    '''\\n\",\n    \"    Gets pd.Series of DR model objects sorted by performance. Excludes blenders, frozend and on DR Reduced FL\\n\",\n    \"    \\n\",\n    \"    Parameters:\\n\",\n    \"    -----------\\n\",\n    \"    project: DR project object\\n\",\n    \"    metric: str, metric to use for sorting models on lb, if None, default project metric will be used. Default None\\n\",\n    \"    by_partiton: boolean, whether to use 'validation' or 'crossValidation' partitioning. Default 'validation'\\n\",\n    \"    start_featurelist_name: str, initial featurelist name to get models on. Default None\\n\",\n    \"    model_search_params: dict to pass model search params. Default None\\n\",\n    \"    \\n\",\n    \"    Returns:\\n\",\n    \"    -----------\\n\",\n    \"    pd.Series of dr.Model objects, not blender, not frozen and not on DR Reduced Feature List\\n\",\n    \"    '''\\n\",\n    \"        \\n\",\n    \"    #list of metrics that get better as their value increases\\n\",\n    \"    desc_metric_list = ['AUC', 'Area Under PR Curve', 'Gini Norm', 'Kolmogorov-Smirnov', 'Max MCC', 'Rate@Top5%',\\n\",\n    \"                        'Rate@Top10%', 'Rate@TopTenth%', 'R Squared', 'FVE Gamma', 'FVE Poisson', 'FVE Tweedie',\\n\",\n    \"                        'Accuracy', 'Balanced Accuracy', 'FVE Multinomial', 'FVE Binomial'\\n\",\n    \"                       ]\\n\",\n    \"    \\n\",\n    \"    if not metric:\\n\",\n    \"        metric = project.metric\\n\",\n    \"        if 'Weighted' in metric:\\n\",\n    \"            desc_metric_list = ['Weighted ' + metric for metric in desc_metric_list]\\n\",\n    \"   \\n\",\n    \"    asc_flag = False if metric in desc_metric_list else True\\n\",\n    \"    \\n\",\n    \"    if project.is_datetime_partitioned:\\n\",\n    \"        assert by_partition in ['validation', 'backtesting', 'holdout'], \\\"Please specify correct partitioning, in datetime partitioned projects supported options are: 'validation', 'backtesting', 'holdout' \\\"\\n\",\n    \"        models_df =  pd.DataFrame(\\n\",\n    \"            [[model.metrics[metric]['validation'],\\n\",\n    \"              model.metrics[metric]['backtesting'],\\n\",\n    \"              model.model_category,\\n\",\n    \"              model.is_frozen,\\n\",\n    \"              model.featurelist_name,\\n\",\n    \"              model,\\n\",\n    \"             ] for model in project.get_datetime_models()],\\n\",\n    \"            columns=['validation', 'backtesting', 'category', 'is_frozen', 'featurelist_name', 'model']\\n\",\n    \"        ).sort_values([by_partition], ascending = asc_flag, na_position='last')\\n\",\n    \"    \\n\",\n    \"    else:\\n\",\n    \"        assert by_partition in ['validation', 'crossValidation', 'holdout'], \\\"Please specify correct partitioning, supported options are: 'validation', 'crossValidation', 'holdout' \\\"\\n\",\n    \"        models_df =  pd.DataFrame(\\n\",\n    \"            [[model.metrics[metric]['crossValidation'],\\n\",\n    \"              model.metrics[metric]['validation'],\\n\",\n    \"              model.model_category,\\n\",\n    \"              model.is_frozen,\\n\",\n    \"              model.featurelist_name,\\n\",\n    \"              model,\\n\",\n    \"         ] for model in project.get_models(with_metric = metric, search_params = model_search_params)],\\n\",\n    \"        columns=['crossValidation', 'validation', 'category', 'is_frozen', 'featurelist_name', 'model']\\n\",\n    \"    ).sort_values([by_partition], ascending = asc_flag, na_position='last')\\n\",\n    \"    \\n\",\n    \"    \\n\",\n    \"    if start_featurelist_name:\\n\",\n    \"        return models_df.loc[((models_df.category == 'model')&\\\\\\n\",\n    \"                              (models_df.is_frozen == False)&\\\\\\n\",\n    \"                              (models_df.featurelist_name == start_featurelist_name)\\n\",\n    \"                             ),'model']\\n\",\n    \"    else:\\n\",\n    \"        return models_df.loc[((models_df.category == 'model')&\\\\\\n\",\n    \"                              (models_df.is_frozen == False)&\\\\\\n\",\n    \"                              (models_df.featurelist_name.str.contains('DR Reduced Features M') == False)\\n\",\n    \"                             ),'model']\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<b>Let's define the function to perform FIRE several times</b>\\n\",\n    \"<br>This function automatically executes FIRE feature selection algorithm on top N models.\\n\",\n    \"<br>Once the reduced feature list is created, it re-runs the Autopilot and waits until it finishes.\\n\",\n    \"<br>It then automatically sorts the models based on the project metric and computes DR feature impact. And iterates over again.\\n\",\n    \"<br>If the new feature list produces a model worse based on a metric, it will consume one \\\"life\\\". The algorithm will stop performing feature selection when no lifes are available. We start with 3 lifes.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def main_feature_selection(project_id,\\n\",\n    \"                           start_featurelist_name=None,\\n\",\n    \"                           lives=3,\\n\",\n    \"                           top_n_models=5,\\n\",\n    \"                           partition='validation',\\n\",\n    \"                           main_scoring_metric=None,\\n\",\n    \"                           initial_impact_reduction_ratio=0.95,\\n\",\n    \"                           best_model_search_params=None,\\n\",\n    \"                           use_ranks=True,\\n\",\n    \"                           ):\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    Main function. Meant to get the optimal shortest feature list by repeating feature selection process until stop criteria is met.\\n\",\n    \"    Currently supports Binary, Regression, Multiclass, Datetime partitioned(OTV) and AutoTS DataRobot projects.\\n\",\n    \"\\n\",\n    \"    Example usage:\\n\",\n    \"    >> import datarobot as dr    \\n\",\n    \"    >> dr.Client(config_path='PATH_TO_DR_CONFIG/drconfig.yaml')\\n\",\n    \"    TIP: set best_model_search_params = {'sample_pct__lte': 65} to avoid using models trained on a higher sample size\\n\",\n    \"    than 3rd stage of autopilot, which is typically ~64% of the data\\n\",\n    \"\\n\",\n    \"    >> main_feature_reduction('INSERT_PROJECT_ID',\\n\",\n    \"                              start_featurelist_name=None,\\n\",\n    \"                              lives=3,\\n\",\n    \"                              top_n_models=5,\\n\",\n    \"                              partition='validation',\\n\",\n    \"                              main_scoring_metric=None,\\n\",\n    \"                              initial_impact_reduction_ratio=0.95,\\n\",\n    \"                              best_model_search_params=None,\\n\",\n    \"                              use_ranks=True)\\n\",\n    \"\\n\",\n    \"    Parameters:\\n\",\n    \"    -----------\\n\",\n    \"    project_id: str, id of DR project, \\n\",\n    \"    start_featurelist_name: str, name of feature list to start iterating from. Default None\\n\",\n    \"    lives: int, stopping criteria, if no best model produced after lives iterations, stop feature reduction. Default 3\\n\",\n    \"    top_n_models: int, only for 'Rank Aggregation method', get top N best models on the leaderboard. Default 5\\n\",\n    \"    partition: str, whether to use 'validation','crossValidation' or 'backtesting' partition to get the best model on. Default 'validation'\\n\",\n    \"    main_scoring_metric: str, DR metric to check performance against, If None DR project metric will be used\\n\",\n    \"    initial_impact_reduction_ratio: float, ratio of total feature impact that new feature list will contain. Default 0.95\\n\",\n    \"    best_model_search_params: dict, dictonary of parameters to search the best model. See official DR python api docs. Default None\\n\",\n    \"    use_ranks: Boolean, True to use median rank aggregation or False to use total impact unnormalized. Default True\\n\",\n    \"\\n\",\n    \"    Returns:\\n\",\n    \"    ----------\\n\",\n    \"    dr.Model object of the best model on the leaderboard\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    project = dr.Project.get(project_id)\\n\",\n    \"\\n\",\n    \"    ratio = initial_impact_reduction_ratio\\n\",\n    \"    assert ratio < 1, \\\"Please specify initial_impact_reduction_ratio < 1\\\"\\n\",\n    \"    assert lives > 0, \\\"Please provide at least one life\\\"\\n\",\n    \"\\n\",\n    \"    model_search_params = best_model_search_params\\n\",\n    \"\\n\",\n    \"    # find the current best model\\n\",\n    \"    best_model = get_best_models(project,\\n\",\n    \"                                 metric=main_scoring_metric,\\n\",\n    \"                                 by_partition=partition,\\n\",\n    \"                                 model_search_params=model_search_params).values[0]\\n\",\n    \"\\n\",\n    \"    runs = 0\\n\",\n    \"    # main function loop\\n\",\n    \"    while lives > 0:\\n\",\n    \"        if runs > 0:\\n\",\n    \"            start_featurelist_name = None\\n\",\n    \"        try:\\n\",\n    \"            # run FIRE\\n\",\n    \"            feature_importance_rank_ensembling(project,\\n\",\n    \"                                               n_models=top_n_models,\\n\",\n    \"                                               metric=main_scoring_metric,\\n\",\n    \"                                               by_partition=partition,\\n\",\n    \"                                               feature_list_name=start_featurelist_name,\\n\",\n    \"                                               ratio=ratio,\\n\",\n    \"                                               model_search_params=best_model_search_params,\\n\",\n    \"                                               use_ranks=use_ranks\\n\",\n    \"                                               )\\n\",\n    \"        except dr.errors.ClientError as e:\\n\",\n    \"            # decay the ratio\\n\",\n    \"            ratio *= ratio\\n\",\n    \"            print(e, f'\\\\nWill try again with a ratio decay ...  New ratio={ratio:.3f}')\\n\",\n    \"            continue\\n\",\n    \"\\n\",\n    \"        ##############################\\n\",\n    \"        ##### GET NEW BEST MODEL #####\\n\",\n    \"        ##############################\\n\",\n    \"        # find the best model now that we've reduced our feature list via FIRE\\n\",\n    \"        new_best_model = get_best_models(project,\\n\",\n    \"                                         metric=main_scoring_metric,\\n\",\n    \"                                         by_partition=partition,\\n\",\n    \"                                         model_search_params=model_search_params).values[0]\\n\",\n    \"\\n\",\n    \"        #################################\\n\",\n    \"        ##### PROCESS STOP CRITERIA #####\\n\",\n    \"        #################################\\n\",\n    \"        if best_model.id == new_best_model.id:\\n\",\n    \"            # if no better model is produced with a recent run, burn 1 life\\n\",\n    \"            lives -= 1\\n\",\n    \"\\n\",\n    \"            # if no lives left -> stop\\n\",\n    \"            if lives <= 0:\\n\",\n    \"                print('New model is worse. No lives left.\\\\nAUTOMATIC FEATURE SELECTION PROCESS HAS BEEN STOPPED')\\n\",\n    \"                return best_model\\n\",\n    \"\\n\",\n    \"            # decay the ratio\\n\",\n    \"            ratio *= ratio\\n\",\n    \"            print(\\n\",\n    \"                f'New model is worse. One life is burnt. '\\n\",\n    \"                f'Repeat again with decaying the cumulative impact ratio. New ratio={ratio:.3f}')\\n\",\n    \"\\n\",\n    \"        best_model = new_best_model\\n\",\n    \"        runs += 1\\n\",\n    \"        print('Run ', runs, ' completed')\\n\",\n    \"\\n\",\n    \"    return best_model\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Create DR project and kick-off AutoPilot\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 8})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 20, 'queue': 12})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1200 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1500 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1800 seconds... Counter({'inprogress': 4})\\n\",\n      \"Autopilot running for 2100 seconds... Counter({'inprogress': 2})\\n\",\n      \"Autopilot running for 2400 seconds... Counter()\\n\",\n      \"61a936fd1283f8039b8bbe7f\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"project = dr.Project.create('https://s3.amazonaws.com/datarobot_public_datasets/madelon_combined_80.csv')\\n\",\n    \"project.set_target(target='y',\\n\",\n    \"                   mode=dr.AUTOPILOT_MODE.FULL_AUTO,\\n\",\n    \"                   worker_count=-1,\\n\",\n    \"                  )\\n\",\n    \"\\n\",\n    \"def wait_for_autopilot(proj, check_interval=60*5):\\n\",\n    \"  total_wait = 0\\n\",\n    \"  while proj.get_status()['autopilot_done'] == False:\\n\",\n    \"    sleep(check_interval)\\n\",\n    \"    total_wait += check_interval\\n\",\n    \"    jobs = proj.get_model_jobs()\\n\",\n    \"    print(f'Autopilot running for {total_wait} seconds...', Counter(job.status for job in jobs))\\n\",\n    \"\\n\",\n    \"wait_for_autopilot(project)\\n\",\n    \"print(project.id)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Once Project's inital AutoPilot is finished, let's start performing feature selection.<br>\\n\",\n    \"\\n\",\n    \"We start new Autopilot on feature list based on median rank aggregation of feature impacts across top 5 models trained on \\\"Informative Features\\\" list.<br>\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 8})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 11})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1200 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1500 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  1  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 8})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 18, 'queue': 2})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 4})\\n\",\n      \"Autopilot running for 1200 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1500 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  2  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 8})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 18, 'queue': 1})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1200 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1500 seconds... Counter()\\n\",\n      \"Autopilot running for 1800 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  3  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter()\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 20, 'queue': 9})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 3})\\n\",\n      \"Autopilot running for 1200 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1500 seconds... Counter()\\n\",\n      \"Autopilot running for 1800 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  4  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'queue': 5})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 5})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  5  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 20, 'queue': 12})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 5})\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"Autopilot running for 1500 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  6  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 13})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  7  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 17, 'queue': 1})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  8  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter()\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 3})\\n\",\n      \"Autopilot running for 900 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  9  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 9})\\n\",\n      \"Autopilot running for 900 seconds... Counter()\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"Autopilot running for 1500 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  10  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter()\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 900 seconds... Counter()\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  11  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter()\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 900 seconds... Counter()\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  12  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 3})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 13})\\n\",\n      \"Autopilot running for 900 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"New model is worse. One life is burnt.\\n\",\n      \"Repeat again with decaying the cumulative impact ratio. New ratio=0.902\\n\",\n      \"Run  13  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter()\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 2})\\n\",\n      \"Autopilot running for 900 seconds... Counter()\\n\",\n      \"Autopilot running for 1200 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"Run  14  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 3})\\n\",\n      \"Autopilot running for 900 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"New model is worse. One life is burnt.\\n\",\n      \"Repeat again with decaying the cumulative impact ratio. New ratio=0.815\\n\",\n      \"Run  15  completed\\n\",\n      \"Request Feature Impact calculations\\n\",\n      \"Starting AutoPilot on a reduced feature list\\n\",\n      \"Autopilot running for 300 seconds... Counter({'inprogress': 1})\\n\",\n      \"Autopilot running for 600 seconds... Counter({'inprogress': 3})\\n\",\n      \"Autopilot running for 900 seconds... Counter()\\n\",\n      \"... AutoPilot is completed.\\n\",\n      \"New model is worse. No lifes left.\\n\",\n      \"AUTOMATIC FEATURE SELECTION PROCESS HAS BEEN STOPPED\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"#feel free adjust function's parameters for your purposes. \\n\",\n    \"best_model = main_feature_selection(project.id,\\n\",\n    \"                                    partition='crossValidation',\\n\",\n    \"                                    best_model_search_params={'sample_pct__lte': 65})\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The best model has LogLoss score = 0.26367 on the cross-validation partition on the list of 16 features\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"print(f\\\"The best model has {project.metric} score = {best_model.metrics[project.metric]['crossValidation']} on the cross-validation partition \\\\\\n\",\n    \"on the list of {len(best_model.get_features_used())} features\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.8\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Lists%20Manipulation/Python/FeatureSelection_using_Feature_Importance_Rank_Ensembling.ipynb",
        "size": 32149,
        "description": "Jupyter notebook example from Feature Lists Manipulation/Python/FeatureSelection_using_Feature_Importance_Rank_Ensembling.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Feature Lists Manipulation/Python/Transforming Feature Types.ipynb",
        "file_name": "Transforming Feature Types.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Transforming Feature Types\\n\",\n    \"\\n\",\n    \"**Author**: Peter Simon\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"\\n\",\n    \"### Scope\\n\",\n    \"\\n\",\n    \"The scope of this notebook is to provide instructions on how to transform feature types within DataRobot.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"\\n\",\n    \"This is the procedure we are going to follow:\\n\",\n    \"\\n\",\n    \"* Calculate the feature importance for each trained model\\n\",\n    \"* Get the feature ranking for each trained model\\n\",\n    \"* Get the ranking distribution for each feature across models\\n\",\n    \"* Sort by mean rank and visualize\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.19.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\\n\",\n    \"\\n\",\n    \"It is assumed you already have a DataRobot <code>Project</code> object.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Set dictionary\\n\",\n    \"Let's set up a dict with the variables we want to change. NOTE: if you want to do more than one type transformation on a single variable, you will need to do this via a dataframe instead. Variable types must be one of the following: {'categorical', 'categoricalInt', 'numeric', 'text'}\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\\n\",\n    \"VAR_TYPES = {'region'            : 'text',\\n\",\n    \"             'reviews_seasonal'  : 'categoricalInt',\\n\",\n    \"             'min_score'         : 'categorical',\\n\",\n    \"             'max_score'         : 'an example of an illegal type',\\n\",\n    \"             'reviews_department': 'text',\\n\",\n    \"             'locality'          : 'text',\\n\",\n    \"             }\\n\",\n    \"\\n\",\n    \"# build a dict of the new feature names - we'll need it later\\n\",\n    \"new_features = {}\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Initiate transformation\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print('Starting variable type transformations.')\\n\",\n    \"\\n\",\n    \"# now we'll run through the dict and do the transforms one-by-one\\n\",\n    \"for feat, transf in VAR_TYPES.items():\\n\",\n    \"    new_name = feat + ' (' + transf + ')'\\n\",\n    \"    try:\\n\",\n    \"        new_features[feat] = project.create_type_transform_feature(name=new_name,\\n\",\n    \"                                                                   parent_name=feat,\\n\",\n    \"                                                                   variable_type=transf).name\\n\",\n    \"        print('Transformed', feat, 'to', new_name, 'successfully.')\\n\",\n    \"    except dr.errors.ClientError as e:\\n\",\n    \"        assert e.status_code == 422\\n\",\n    \"        print(feat, 'transformation to', transf, 'failed.')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Create new feature list\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# let's make some feature lists which substitute the re-typed features for their parents\\n\",\n    \"featurelists = project.get_featurelists()\\n\",\n    \"new_featurelists = {}\\n\",\n    \"\\n\",\n    \"print('Adding feature lists.')\\n\",\n    \"for fl in featurelists:\\n\",\n    \"    new_featlist_name = fl.name + ' (retyped)'\\n\",\n    \"    new_featurelists[new_featlist_name] = project.create_featurelist(\\n\",\n    \"        name=new_featlist_name,\\n\",\n    \"        features=[new_features[f] if f in new_features.keys() else f for f in fl.features]\\n\",\n    \"    ).id\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Run autopilot\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# now run autopilot on the retyped Informative Features list\\n\",\n    \"print('Starting autopilot.')\\n\",\n    \"project.set_target(target=TARGET,\\n\",\n    \"                   worker_count=-1,  # set to run with max. workers permitted\\n\",\n    \"                   featurelist_id=new_featurelists['Informative Features (retyped)'],\\n\",\n    \"                   )\\n\",\n    \"\\n\",\n    \"# and invoke the browser, to carry on there\\n\",\n    \"project.open_leaderboard_browser()\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Lists%20Manipulation/Python/Transforming%20Feature%20Types.ipynb",
        "size": 5121,
        "description": "Jupyter notebook example from Feature Lists Manipulation/Python/Transforming Feature Types.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Modeling/Python/README.md",
        "file_name": "README.md",
        "file_type": "markdown",
        "content": "# Short Description of files\n\n**run_specific_blueprints.py**\n\nA function that helps you search for specific blueprints within a DataRobot's project's repository and then initiates all of these models.\n\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Modeling/Python/README.md",
        "size": 202,
        "description": "Documentation: README",
        "tags": [
          "modeling"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Modeling/Python/run_specifc_blueprints.py",
        "file_name": "run_specifc_blueprints.py",
        "file_type": "python",
        "content": "#Author: Thodoris Petropoulos\n\ndef run_specific_blueprints(project_object, search_term, featurelist_id = None):\n    \"\"\"Runs all of the blueprints that match the search term use provides\n        Input:\n        - project_object <DataRobot Project> (Your DataRobot project)\n        - search_term <string> (What to search for in the name of the Blueprint. e.g: \"Gradient\") \n        - featurelist_id <DataRobot Featurelist id> (Optional parameter to specify featurelist to use)\n    \"\"\"\n\n    blueprints = project_object.get_blueprints()\n    models_to_run = [blueprint for blueprint in blueprints if blueprint.model_type == search_term]\n    for model in models_to_run:\n        project_object.train(model, sample_pct = 80, featurelist_id=featurelist_id)\n\n    while len(project_object.get_all_jobs()) > 0:\n    time.sleep(1)\n    pass\n        \n    ",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Modeling/Python/run_specifc_blueprints.py",
        "size": 837,
        "description": "Author: Thodoris Petropoulos",
        "tags": [
          "modeling"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/README.md",
        "file_name": "README.md",
        "file_type": "markdown",
        "content": "# Short Description of files\n\n**ts_clustering.py**\n\nFunction to cluster time series data in different ways. This will allow you to split a multi time series project into multiple time series projects and increase accuracy.\n\n**ts_create_project_advanced.py**\n\nFunctions with advanced settings to run a time series project without having to research all of the documentation available.\n\n**ts_data_quality_check.py**\n\nClass with multiple methods that check various metrics of data quality for a time series dataset.\n\n**ts_fill_dates_per_series.py**\n\nFunction together with example on how to fill missing dates in sparse time series data.\n\n**ts_preprocessing.py**\n\nMultiple functions to help preprocess time series datasets.\n\n**ts_ion_cannon.py**\nUse this python package and aim at your time series project with completed Autopilot to unleash the brute force of AutoTS.\n\n**ts_clone_project.py**\nUse this function to clone a TS DataRobot project. It will make a complete copy and start autopilot. Manipulate the `dtp_spec_for_clone` variable as you see fit if you need to make changes.\n\n**ts_metrics.py**\nMultiple functions for extracting common accuracy metrics for TS project.\n\n**ts_modeling.py**\nFunctions to create one or multiple DataRobot TS projects.\n\n**ts_projects.py**\nMultiple functions to manipulate lists of DataRobot TS projects, find out the best models and rerun them on reduced feature lists.",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/README.md",
        "size": 1403,
        "description": "Documentation: README",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_clone_project.py",
        "file_name": "ts_clone_project.py",
        "file_type": "python",
        "content": "#Author: Katy Chow Haynie\n\n#Make sure you are connected to DataRobot Client.\n\n\n#This function will help you create a copy of a TS project using the same exact settings. Manipulate as you see fit.\n\nimport datarobot as dr\n\ndef clone_ts_project(pid):\n    \"\"\"This function will copy a DataRobot TS project with the same settings\n        Input:\n         - pid <str> the id of the project you want to copy\n         \n        Manipulate this function as you see fit in case you dont want a complete 1:1 copy.\n    \"\"\"\n    p = dr.Project.get(pid)\n    c_p = p.clone_project('Clone of {}'.format(p.project_name))\n    c_pid = c_p.id\n    \n    #Get datetimePartitioning data\n    #This will include calendar, backtesting and known-in-advance features.\n    dtp = dr.DatetimePartitioning.get(pid)\n    dtp_spec_for_clone = dtp.to_specification()\n    \n    #Fix the datetime_partition_column which will have an ' (actual)' string appended to it.\n    dtp_spec_for_clone.datetime_partition_column = dtp_spec_for_clone.datetime_partition_column.replace(' (actual)','')\n    \n    ##Place changes below##\n    #Manipulate dtp_spec_for_clone as you see fit (you can directly change its attribites)\n    \n    ##\n    \n    c_p.set_target(target = 'Sales',\n                       partitioning_method = dtp_spec_for_clone,\n                       mode = 'auto',\n                       worker_count = -1\n                  )\n    \n##Usage##\n#clone_ts_project('YOUR_PROJECT_ID')",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_clone_project.py",
        "size": 1438,
        "description": "Author: Katy Chow Haynie",
        "tags": [
          "datarobot-api"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_clustering.py",
        "file_name": "ts_clustering.py",
        "file_type": "python",
        "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport datetime as dt\nimport operator\n\nimport datarobot as dr\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy.spatial.distance import cdist, pdist, squareform\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom statsmodels.tsa.stattools import pacf\n\nfrom ts_metrics import *\nfrom ts_modeling import create_dr_project\nfrom ts_projects import get_preds_and_actuals\n\n\n####################\n# Series Clustering\n####################\n\n\ndef _split_series(df, series_id, target, by='quantiles', cuts=5, split_col='Cluster'):\n    \"\"\"\n    Split series into clusters by rank or quantile  of average target value\n\n    by: str\n        Rank or quantiles\n    cuts: int\n        Number of clusters\n    split_col: str\n        Name of new column\n\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    group = df.groupby([series_id]).mean()\n\n    if by == 'quantiles':\n        group[split_col] = pd.qcut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    elif by == 'rank':\n        group[split_col] = pd.cut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    else:\n        raise ValueError(f'{by} is not a supported value. Must be set to either quantiles or rank')\n\n    df = df.merge(\n        group[split_col], how='left', left_on=series_id, right_index=True, validate='many_to_one'\n    )\n\n    df[split_col] = df[split_col].astype('str')\n    n_clusters = len(df[split_col].unique())\n    mapper_clusters = {k: v for (k, v) in zip(df[split_col].unique(), range(1, n_clusters + 1))}\n    df[split_col] = df[split_col].map(mapper_clusters)\n\n    return df.reset_index(drop=True)\n\n\ndef _get_pacf_coefs(df, col, nlags, alpha, scale, scale_method):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    df: pandas df\n    col: str\n        Series name\n    nlags: int\n        Number of AR coefficients to include in pacf\n    alpha: float\n        Cutoff value for p-values to determine statistical significance\n    scale: boolean\n        Whether to standardize input data\n    scale_method: str\n        Choose from 'min_max' or 'normalize'\n\n    Returns:\n    --------\n    List of AR(n) coefficients\n\n    \"\"\"\n    if scale:\n        if scale_method == 'min_max':\n            df = df.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)), axis=0)\n        elif scale_method == 'normalize':\n            df = df.apply(lambda x: (x - np.mean(x)) / np.std(x), axis=0)\n        else:\n            raise ValueError(\n                f'{scale_method} is not a supported value. scale_method must be set to either min_max or normalize'\n            )\n\n    # if df[col].dropna().shape[0] == 0:\n    #     print(col, df[col].dropna())\n    # print('Running PAC...')\n    clf = pacf(df[col].dropna(), method='ols', nlags=nlags, alpha=alpha)\n    if alpha:\n        coefs = clf[0][1:]\n        zero_in_interval = [not i[0] < 0 < i[1] for i in clf[1][1:]]\n        adj_coefs = [c if z else 0.0 for c, z in zip(coefs, zero_in_interval)]\n        return adj_coefs\n    else:\n        coefs = clf[1:]\n        return coefs\n\ndef _get_optimal_n_clusters(df, n_series, max_clusters, plot=True):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    Get the number of clusters that results in the max silhouette score\n\n    Returns:\n    --------\n    int\n\n    \"\"\"\n    clusters = list(np.arange(min(max_clusters, n_series)) + 2)[:-1]\n    print(f'Testing {clusters[0]} to {clusters[-1]} clusters')\n    scores = {}\n    d = []\n    for c in clusters:\n        kmean = KMeans(n_clusters=c).fit(df)\n        d.append(sum(np.min(cdist(df, kmean.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n        preds = kmean.predict(df)\n        score = silhouette_score(df, preds, metric='euclidean')\n        scores[c] = score\n        print(f'For n_clusters = {c}, silhouette score is {score}')\n\n    n_clusters = max(scores.items(), key=operator.itemgetter(1))[0]\n    best_score = scores[n_clusters]\n    print(f'optimal n_clusters = {n_clusters}, max silhouette score is {best_score}')\n\n    if max_clusters > 2:\n        if plot:\n            fig = px.line(x=clusters, y=d)\n            fig.update_layout(height=500, width=750, title_text='Kmeans Optimal Number of Clusters')\n            fig.update_xaxes(title='Number of Clusters', range=[clusters[0], clusters[-1]])\n            fig.update_yaxes(title='Distortion')\n            fig.show()\n\n    return n_clusters\n\n\ndef add_cluster_labels(\n    df,\n    ts_settings,\n    method,\n    nlags=None,\n    scale=True,\n    scale_method='min_max',\n    alpha=0.05,\n    split_method=None,\n    n_clusters=None,\n    max_clusters=None,\n    plot=True,\n):\n    \"\"\"\n    Calculates series clusters and appends a column of cluster labels to the input df. This will only work on regularly spaced time series datasets.\n\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    method: type of clustering technique: must choose from either pacf, correlation, or target\n    nlags: int (Optional)\n        Number of AR(n) lags. Only applies to PACF method\n    scale: boolean (Optional)\n        Only applies to PACF method\n    scale_method: str (Optiona)\n        Choose between normalize (subtract the mean and divide by the std) or min_max (subtract the min and divide by the range)\n    split_method: str (Optional)\n        Choose between rank and quanitles. Only applies to target method\n    n_clusters: int\n        Number of clusters to create. If None, defaults to maximum silhouette score\n    max_clusters: int\n        Maximum number of clusters to create. If None, default to the number of series - 1\n\n    Returns:\n    --------\n    Updated pandas df with a new column 'Cluster' of clusters labels\n            -silhouette score per cluster:\n            (The best value is 1 and the worst value is -1. Values near 0 indicate overlapping\n            clusters. Negative values generally indicate that a sample has been assigned to the\n            wrong cluster.)\n            -plot of distortion per cluster\n    \"\"\"\n    target = ts_settings['target']\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    series = df[series_id].unique()\n    n_series = len(series)\n\n    if max_clusters is None:\n        max_clusters = n_series - 1\n\n    assert (\n        1 < max_clusters < n_series\n    ), 'max_clusters must be greater than 1 and less than or equal to the number of unique series -1'\n\n    if n_clusters:\n        assert (\n            1 < n_clusters <= max_clusters\n        ), f'n_clusters must be greater than 1 and less than {max_clusters}'\n\n    c = df.pivot(index=date_col, columns=series_id, values=target)\n\n    if method == 'pacf':\n        d = pd.DataFrame(\n            [_get_pacf_coefs(c, x, nlags, alpha, scale, scale_method) for x in c.columns]\n        )  # ignore missing values\n        d.index = c.columns\n        distances = pdist(d, 'minkowski', p=2)  # 1 for manhattan distance and 2 for euclidean\n        dist_matrix = squareform(distances)\n        dist_df = pd.DataFrame(dist_matrix)\n        dist_df.columns = series\n        dist_df.index = dist_df.columns\n\n    elif method == 'correlation':\n        dist_df = c.corr(method='pearson')\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=1)\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    elif method == 'target':\n        if split_method is not None:\n            if n_clusters:\n                cuts = n_clusters\n            else:\n                cuts = max_clusters\n\n            new_df = _split_series(df, series_id, target, by=split_method, cuts=cuts)\n            return new_df  # exit function\n        else:\n            dist_df = df.groupby(series_id).agg({target: 'mean'})\n\n    else:\n        raise ValueError(\n            f'{method} is not a supported value. Must be set to either pacf, correlation, or target'\n        )\n\n    # Find optimal number of clulsters is n_clusters is not specified\n    if n_clusters is None:\n        n_clusters = _get_optimal_n_clusters(\n            df=dist_df, n_series=n_series, max_clusters=max_clusters, plot=plot\n        )\n\n    kmeans = KMeans(n_clusters).fit(dist_df)\n    labels = kmeans.predict(dist_df)\n\n    df_clusters = (\n        pd.concat([pd.Series(series), pd.Series(labels)], axis=1)\n        .sort_values(by=1)\n        .reset_index(drop=True)\n    )\n    df_clusters.columns = [series_id, 'Cluster']\n\n    df_w_cluster_labels = df.merge(df_clusters, how='left', on=series_id)\n\n    return df_w_cluster_labels.reset_index(drop=True)\n\n\ndef plot_clusters(df, ts_settings, split_col='Cluster', max_sample_size=50000):\n    \"\"\"\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    col: cluster_id columns\n\n    Returns:\n    --------\n    Plotly bar plot\n\n    \"\"\"\n    assert split_col in df.columns, f'{split_col} must be a column in the df'\n\n    date_col = ts_settings['date_col']\n    target = ts_settings['target']\n    series_id = ts_settings['series_id']\n\n    n_clusters = len(df[split_col].unique())\n\n    if df.shape[0] > max_sample_size:  # limit the data points displayed in the charts to reduce lag\n        df = df.sample(n=max_sample_size).reset_index(drop=True)\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[split_col, date_col], inplace=True)\n    df_agg = df.groupby([split_col, date_col]).agg({target: 'mean'}).reset_index()\n    groups = df_agg.groupby([split_col])\n    fig = make_subplots(rows=n_clusters, cols=1)\n\n    a = 1\n    for name, group in groups:\n        n_series = len(df.loc[df[split_col] == name, series_id].unique())\n        fig.append_trace(\n            go.Line(\n                x=group[date_col], y=group[target], name=f'{split_col}={name} - {n_series} Series'\n            ),\n            row=a,\n            col=1,\n        )\n        a += 1\n\n    fig.update_layout(height=1000, width=1000, title_text=\"Cluster Subplots\")\n    fig.show()\n\n\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_clustering.py",
        "size": 10020,
        "description": "Authors: Justin Swansburg, Mark Philip",
        "tags": [
          "datarobot-api",
          "predictions"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_create_project_advanced.py",
        "file_name": "ts_create_project_advanced.py",
        "file_type": "python",
        "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport datetime as dt\nimport time\n\nimport datarobot as dr\nimport numpy as np\nimport pandas as pd\n\nfrom ts_data_quality import get_timestep #ts_data_quality is a helper function that exists within this Repo\n\n\n###################\n# Project Creation\n###################\n\n\ndef create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project\n\n\ndef create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects\n\n\ndef wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_create_project_advanced.py",
        "size": 10104,
        "description": "Authors: Justin Swansburg, Mark Philip",
        "tags": [
          "datarobot-api",
          "project-creation",
          "time-series"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_data_quality_check.py",
        "file_name": "ts_data_quality_check.py",
        "file_type": "python",
        "content": "#Authors:  Mark Philip, Justin Swansburg\n\nimport datetime as dt\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport statsmodels.api as sm\n\n\n###################################\n# Time Series Data Quality Checks\n###################################\n\n\nclass DataQualityCheck:\n    \"\"\"\n    A class used to capture summary stats and data quality checks prior to uploading time series data to DataRobot\n    Attributes:\n    -----------\n    df : DataFrame\n        time series data, including a date column and target variable at a minimum\n    settings : dict\n        definitions of date_col, target_col, series_id and time series parameters\n    stats : dict\n        summary statistics generated from `calc_summary_stats`\n    duplicate_dates : int\n        duplicate dates in the time series date_col\n    series_timesteps : series\n        steps between time units for each series_id\n    series_max_gap : series\n        maximum time gap per series\n    series_lenth : series\n        length of each series_id\n    series_pct : series\n        percent of series with complete time steps\n    irregular : boolean\n        True if df contains irregular time series data\n    series_negative_target_pct : float\n        Percent of target values that are negative\n    Methods:\n    --------\n    calc_summary_stats(settings, df)\n        generates a dictionary of summary statistics\n    calc_time_steps(settings, df)\n        calculate time steps per series_id\n    hierarchical_check(settings, df)\n        check if time series data passes heirarchical check\n    zero_inflated_check(settings, df)\n        check if target value contains zeros\n    negative_values_check(settings, df)\n        check if target value contains negative values\n    time_steps_gap_check(settings, df)\n        check if any series has missing time steps\n    irregular_check(settings, df)\n        check is time series data irregular\n    \"\"\"\n\n    def __init__(self, df, ts_settings):\n        self.df = df\n        self.settings = ts_settings\n        self.stats = None\n        self.duplicate_dates = None\n        self.series_time_steps = None\n        self.series_length = None\n        self.series_pct = None\n        self.irregular = None\n        self.series_negative_target_pct = None\n        self.project_time_unit = None\n        self.project_time_step = None\n        self.calc_summary_stats()\n        self.calc_time_steps()\n        self.run_all_checks()\n\n    def calc_summary_stats(self):\n        \"\"\"\n        Analyze time series data to perform checks and gather summary statistics prior to modeling.\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df[date_col] = pd.to_datetime(df[date_col])\n        df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n        # Create dictionary of helpful statistics\n        stats = dict()\n\n        stats['rows'] = df.shape[0]\n        stats['columns'] = df.shape[1]\n        stats['min_' + str(target)] = df[target].min()\n        stats['max_' + str(target)] = df[target].max()\n        stats['series'] = len(df[series_id].unique())\n        stats['start_date'] = df[date_col].min()\n        stats['end_date'] = df[date_col].max()\n        stats['timespan'] = stats['end_date'] - stats['start_date']\n        stats['median_timestep'] = df.groupby([series_id])[date_col].diff().median()\n        stats['min_timestep'] = df.groupby([series_id])[date_col].diff().min()\n        stats['max_timestep'] = df.groupby([series_id])[date_col].diff().max()\n\n        # create data for histogram of series lengths\n        stats['series_length'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.max() - x.min())\n            / stats['median_timestep']\n        )\n\n        # calculate max gap per series\n        stats['series_max_gap'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max())\n            / stats['median_timestep']\n        )\n\n        self.stats = stats\n\n    def calc_percent_missing(self, missing_value=np.nan):\n        \"\"\"\n        Calculate percentage of rows where target is np.nan\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if np.isnan(missing_value):\n            percent_missing = sum(np.isnan(df[target])) / len(df)\n        else:\n            percent_missing = sum(df[target] == missing_value) / len(df)\n\n        self.stats['percent_missing'] = percent_missing\n        print('{:0.2f}% of the rows are missing a target value'.format(percent_missing * 100))\n\n    def get_zero_inflated_series(self, cutoff=0.99):\n        \"\"\"\n        Identify series where the target is 0.0 in more than x% of the rows\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        assert 0 < cutoff <= 1.0, 'cutoff must be between 0 and 1'\n\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n        series = df[df >= cutoff].index.values\n\n        pct = len(series) / self.stats['series']\n\n        print(\n            '{:0.2f}% series have zeros in more than {:0.2f}% or more of the rows'.format(\n                pct * 100, cutoff * 100\n            )\n        )\n\n    def calc_time_steps(self):\n        \"\"\"\n        Calculate timesteps per series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # create data for histogram of timestep\n        series_timesteps = df.groupby([series_id])[date_col].diff() / self.stats['median_timestep']\n        self.series_time_steps = series_timesteps\n\n    def hierarchical_check(self):\n        \"\"\"\n        Calculate percentage of series that appear on each timestep\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # Test if series passes the hierarchical check\n        series_pct = df.groupby([date_col])[series_id].apply(\n            lambda x: x.count() / self.stats['series']\n        )\n        if np.where(series_pct > 0.95, 1, 0).mean() > 0.95:\n            self.stats['passes_hierarchical_check'] = True\n            print(\n                'Data passes hierarchical check! DataRobot hierarchical blueprints will run if you enable cross series features.'\n            )\n        else:\n            print('Data fails hierarchical check! No hierarchical blueprints will run.')\n            self.stats['passes_hierarchical_check'] = False\n\n        self.series_pct = series_pct\n\n    def zero_inflated_check(self):\n        \"\"\"\n        Check if minimum target value is 0.0\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if min(df[target]) == 0:\n            self.stats['passes_zero_inflated_check'] = False\n            print('The minimum target value is zero. Zero-Inflated blueprints will run.')\n        else:\n            self.stats['passes_zero_inflated_check'] = True\n            print('Minimum target value is <> 0. Zero-inflated blueprints will not run.')\n\n    def negative_values_check(self):\n        \"\"\"\n        Check if any series contain negative values. If yes, identify and call out which series by id.\n        \"\"\"\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df['target_sign'] = np.sign(df[target])\n\n        try:\n            # Get percent of series that have at least one negative value\n            any_series_negative = (\n                df.groupby([series_id])['target_sign'].value_counts().unstack()[-1]\n            )\n            series_negative_target_pct = np.sign(any_series_negative).sum() / len(\n                df[series_id].unique()\n            )\n            df.drop('target_sign', axis=1, inplace=True)\n            self.stats['passes_negative_values_check'] = False\n\n            print(\n                '{0:.2f}% of series have at least one negative {1} value.'.format(\n                    (round(series_negative_target_pct * 100), 2), target\n                )\n            )\n\n            # Identify which series have negative values\n            # print('{} contain negative values. Consider creating a seperate project for these series.'.format(any_series_negative[any_series_negative == 1].index.values))\n        except:\n            series_negative_target_pct = 0\n            self.stats['passes_negative_values_check'] = True\n            print('No negative values are contained in {}.'.format(target))\n\n        self.series_negative_target_pct = series_negative_target_pct\n\n    def new_series_check(self):\n        \"\"\"\n        Check if any series start after the the minimum datetime\n        \"\"\"\n        min_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].min()\n        new_series = min_dates > self.stats['start_date'] + dt.timedelta(days=30)\n\n        if new_series.sum() == 0:\n            self.stats['series_introduced_over_time'] = False\n            print('No new series were introduced after the start of the training data')\n        else:\n            self.stats['series_introduced_over_time'] = True\n            print(\n                'Warning: You may encounter new series at prediction time. \\n {0:.2f}% of the series appeared after the start of the training data'.format(\n                    round(new_series.mean() * 100, 0)\n                )\n            )\n\n    def old_series_check(self):\n        \"\"\"\n        Check if any series end before the maximum datetime\n        \"\"\"\n        max_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].max()\n        old_series = max_dates < self.stats['end_date'] - dt.timedelta(days=30)\n\n        if old_series.sum() == 0:\n            self.stats['series_removed_over_time'] = False\n            print('No series were removed before the end of the training data')\n        else:\n            self.stats['series_removed_over_time'] = True\n            print(\n                'Warning: You may encounter fewer series at prediction time. \\n {0:.2f}% of the series were removed before the end of the training data'.format(\n                    round(old_series.mean() * 100, 0)\n                )\n            )\n\n    def leading_or_trailing_zeros_check(self, threshold=5, drop=True):\n        \"\"\"\n        Check for contain consecutive zeros at the beginning or end of each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        new_df = remove_leading_and_trailing_zeros(\n            df,\n            series_id,\n            date_col,\n            target,\n            leading_threshold=threshold,\n            trailing_threshold=threshold,\n            drop=drop,\n        )\n\n        if new_df.shape[0] < df.shape[0]:\n            print(f'Warning: Leading and trailing zeros detected within series')\n        else:\n            print(f'No leading or trailing zeros detected within series')\n\n    def duplicate_dates_check(self):\n        \"\"\"\n        Check for duplicate datetimes within each series\n        \"\"\"\n\n        duplicate_dates = self.df.groupby([self.settings['series_id'], self.settings['date_col']])[\n            self.settings['date_col']\n        ].count()\n        duplicate_dates = duplicate_dates[duplicate_dates > 1]\n        if len(duplicate_dates) == 0:\n            print(f'No duplicate timestamps detected within any series')\n            self.stats['passes_duplicate_timestamp_check'] = True\n        else:\n            print('Warning: Data contains duplicate timestamps within series!')\n            self.stats['passes_duplicate_timestamp_check'] = False\n\n    def time_steps_gap_check(self):\n        \"\"\"\n        Check for missing timesteps within each series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n        gap_size = self.stats['median_timestep']\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # check is series has any missing time steps\n        self.stats['pct_series_w_gaps'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max()) > gap_size\n        ).mean()\n\n        print(\n            '{0:.2f}% of series have at least one missing time step.'.format(\n                round(self.stats['pct_series_w_gaps'] * 100), 2\n            )\n        )\n\n    def _get_spacing(self, df, project_time_unit):\n        \"\"\"\n        Helper function for self.irregular_check()\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        project_time_unit = self.project_time_unit\n        ts_settings = self.settings\n        date_col = ts_settings['date_col']\n        series_id = ts_settings['series_id']\n\n        df['indicator'] = 1\n        df = fill_missing_dates(df=df, ts_settings=ts_settings)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        sums = df.groupby([series_id, project_time_unit])['indicator'].sum()\n        counts = df.groupby([series_id, project_time_unit])['indicator'].agg(\n            lambda x: x.fillna(0).count()\n        )\n\n        pcts = sums / counts\n\n        irregular = pcts.reset_index(drop=True) < 0.8\n        irregular = irregular[irregular]\n\n        return irregular\n\n    def irregular_check(self, plot=False):\n        \"\"\"\n        Check for irregular spacing within each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        df = self.df.copy()\n\n        # first cast date column to a pandas datetime type\n        df[date_col] = pd.to_datetime(df[date_col])\n\n        project_time_unit, project_time_step = get_timestep(self.df, self.settings)\n\n        self.project_time_unit = project_time_unit\n        self.project_time_step = project_time_step\n\n        print('Project Timestep: ', project_time_step, ' ', project_time_unit)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        # Plot histogram of timesteps\n        time_unit_counts = df[project_time_unit].value_counts()\n\n        if plot:\n            time_unit_percent = time_unit_counts / sum(time_unit_counts.values)\n\n            fig = px.bar(\n                time_unit_percent,\n                x=time_unit_percent.index,\n                y=time_unit_percent.values,\n                title=f'Percentage of records per {project_time_unit}',\n            )\n            fig.update_xaxes(title=project_time_unit)\n            fig.update_yaxes(title='Percentage')\n            fig.show()\n\n        # Detect uncommon time steps\n        # If time bin has less than 30% of most common bin then it is an uncommon time bin\n        uncommon_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) < 0.3].index\n        )\n        common_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) >= 0.3].index\n        )\n\n        if len(uncommon_time_bins) > 0:\n            print(f'Uncommon {project_time_unit}s:', uncommon_time_bins)\n        else:\n            print('There are no uncommon time steps')\n\n        # Detect irregular series\n        df = df.loc[df[project_time_unit].isin(common_time_bins), :]\n        irregular_series = self._get_spacing(df, project_time_unit)\n\n        if len(irregular_series) > 0:\n            print(\n                'Series are irregularly spaced. Projects will only be able to run in row-based mode!'\n            )\n            self.stats['passes_irregular_check'] = False\n        else:\n            self.stats['passes_irregular_check'] = True\n            print(\n                'Timesteps are regularly spaced. You will be able to run projects in either time-based or row-based mode'\n            )\n\n    def detect_periodicity(self, alpha=0.05):\n        \"\"\"\n        Calculate project-level periodicity\n        \"\"\"\n\n        timestep = self.project_time_unit\n        df = self.df\n        target = self.settings['target']\n        date_col = self.settings['date_col']\n        metric = self.settings['metric']\n\n        metrics = {\n            'LogLoss': sm.families.Binomial(),\n            'RMSE': sm.families.Gaussian(),\n            'Poisson Deviance': sm.families.Poisson(),\n            'Gamma Deviance': sm.families.Gamma(),\n        }\n\n        periodicity = {\n            'moh': 'hourly',\n            'hod': 'daily',\n            'dow': 'weekly',\n            'dom': 'monthly',\n            'month': 'yearly',\n        }\n\n        try:\n            loss = metrics[metric]\n        except KeyError:\n            loss = metrics['RMSE']\n\n        # Instantiate a glm with the default link function.\n        df[date_col] = pd.to_datetime(df[date_col])\n        df = df.loc[np.isfinite(df[target]), :].copy()\n\n        df['moh'] = df[date_col].dt.minute\n        df['hod'] = df[date_col].dt.hour\n        df['dow'] = df[date_col].dt.dayofweek\n        df['dom'] = df[date_col].dt.day\n        df['month'] = df[date_col].dt.month\n\n        if timestep == 'minute':\n            inputs = ['moh', 'hod', 'dow', 'dom', 'month']\n        elif timestep == 'hour':\n            inputs = ['hod', 'dow', 'dom', 'month']\n        elif timestep == 'day':\n            inputs = ['dow', 'dom', 'month']\n        elif timestep == 'week':\n            inputs = ['month']\n        else:\n            raise ValueError('timestep has to be either minute, hour, day, week, or month')\n\n        output = []\n        for i in inputs:\n            x = pd.DataFrame(df[i])\n            y = df[target]\n\n            x = pd.get_dummies(x.astype('str'), drop_first=True)\n            x['const'] = 1\n\n            clf = sm.GLM(endog=y, exog=x, family=loss)\n            model = clf.fit()\n\n            if any(model.pvalues[:-1] <= alpha):\n                output.append(periodicity[i])\n                # print(f'Detected periodicity: {periodicity[i]}')\n                # return periodicity[i]\n\n        if len(output) > 0:\n            print(f'Detected periodicity: {output}')\n        else:\n            print('No periodicity detected')\n\n    def run_all_checks(self):\n        \"\"\"\n        Runner function to run all data checks in one call\n        \"\"\"\n        print('Running all data quality checks...\\n')\n\n        series = self.stats['series']\n        start_date = self.stats['start_date']\n        end_date = self.stats['end_date']\n        rows = self.stats['rows']\n        cols = self.stats['columns']\n\n        print(f'There are {rows} rows and {cols} columns')\n        print(f'There are {series} series')\n        print(f'The data spans from  {start_date} to {end_date}')\n\n        self.hierarchical_check()\n        self.zero_inflated_check()\n        self.new_series_check()\n        self.old_series_check()\n        self.duplicate_dates_check()\n        self.leading_or_trailing_zeros_check()\n        self.time_steps_gap_check()\n        self.calc_percent_missing()\n        self.get_zero_inflated_series()\n        self.irregular_check()\n        self.detect_periodicity()\n\n\ndef get_timestep(df, ts_settings):\n    \"\"\"\n    Calculate the project-level timestep\n    Returns:\n    --------\n    project_time_unit: minute, hour, day, week, or month\n    project_time_step: int\n    Examples:\n    --------\n    '1 days'\n    '4 days'\n    '1 week'\n    '2 months'\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    # Cast date column to a pandas datetime type and sort df\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n    # Calculate median timestep\n    deltas = df.groupby([series_id])[date_col].diff().reset_index(drop=True)\n    median_timestep = deltas.apply(lambda x: x.total_seconds()).median()\n\n    # Logic to detect project time step and time unit\n    if (60 <= median_timestep < 3600) & (median_timestep % 60 == 0):\n        project_time_unit = 'minute'\n        project_time_step = int(median_timestep / 60)\n        df['minute'] = df[date_col].dt.minute\n    elif (3600 <= median_timestep < 86400) & (median_timestep % 3600 == 0):\n        project_time_unit = 'hour'\n        project_time_step = int(median_timestep / 3600)\n        df['hour'] = df[date_col].dt.hour\n    elif (86400 <= median_timestep < 604800) & (median_timestep % 86400 == 0):\n        project_time_unit = 'day'\n        project_time_step = int(median_timestep / 86400)\n        df['day'] = df[date_col].dt.strftime('%A')\n    elif (604800 <= median_timestep < 2.628e6) & (median_timestep % 604800 == 0):\n        project_time_unit = 'week'\n        project_time_step = int(median_timestep / 604800)\n        df['week'] = df[date_col].dt.week\n    elif (median_timestep >= 2.628e6) & (median_timestep % 2.628e6 == 0):\n        project_time_unit = 'month'\n        project_time_step = int(median_timestep / 2.628e6)\n        df['month'] = df[date_col].dt.month\n    else:\n        raise ValueError(f'{median_timestep} seconds is not a supported timestep')\n\n    # print('Project Timestep: 1', project_time_unit)\n\n    return project_time_unit, project_time_step\n\n\ndef _reindex_dates(group, freq):\n    \"\"\"\n    Helper function for fill_missing_dates()\n    \"\"\"\n    date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    group = group.reindex(date_range)\n    return group\n\n\ndef fill_missing_dates(df, ts_settings, freq=None):\n    \"\"\"\n    Insert rows with np.nan targets for series with missing timesteps between the series start and end dates\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    freq: project time unit and timestep\n    Returns:\n    --------\n    pandas df with inserted rows\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    if freq is None:\n        mapper = {'minute': 'min', 'hour': 'H', 'day': 'D', 'week': 'W', 'month': 'M'}\n        project_time_unit, project_time_step = get_timestep(df, ts_settings)\n        freq = str(project_time_step) + mapper[project_time_unit]\n\n    df = (\n        df.set_index(date_col)\n        .groupby(series_id)\n        .apply(_reindex_dates, freq)\n        .rename_axis((series_id, date_col))\n        .drop(series_id, axis=1)\n        .reset_index()\n    )\n\n    return df.reset_index(drop=True)\n\n\ndef _remove_leading_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    min_date = df_non_zero[date_col].min()\n    df_begin = df[df[date_col] < min_date]\n    if df_begin[target].dropna().shape[0] >= threshold or pd.isnull(min_date):\n        if drop:\n            if pd.isnull(min_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] >= min_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(min_date) or row[date_col] < min_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df\n\n\ndef _remove_trailing_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    max_date = df_non_zero[date_col].max()\n    df_end = df[df[date_col] > max_date]\n    if df_end[target].dropna().shape[0] >= threshold or pd.isnull(max_date):\n        if drop:\n            if pd.isnull(max_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] <= max_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(max_date) or row[date_col] > max_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df\n\n\ndef remove_leading_and_trailing_zeros(\n    df, series_id, date_col, target, leading_threshold=5, trailing_threshold=5, drop=False\n):\n    \"\"\"\n    Remove excess zeros at the beginning or end of series\n    df: pandas df\n    leading_threshold: minimum number of consecutive zeros at the beginning of a series before rows are dropped\n    trailing_threshold: minimum number of consecutive zeros at the end of series before rows are dropped\n    drop: specifies whether to drop the zeros or set them to np.nan\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_leading_zeros, date_col, target, leading_threshold, drop)\n        .reset_index(drop=True)\n    )\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_trailing_zeros, date_col, target, trailing_threshold, drop)\n        .reset_index(drop=True)\n    )\n\n    return df.reset_index(drop=True)\n\n\n#####################\n# Data Visualization\n#####################\n\n\ndef _cut_series_by_rank(df, ts_settings, n=1, top=True):\n    df_agg = df.groupby(ts_settings['series_id']).mean()\n    selected_series_names = (\n        df_agg.sort_values(by=ts_settings['target'], ascending=top).tail(n).index.values\n    )\n\n    return selected_series_names\n\n\ndef _cut_series_by_quantile(df, ts_settings, quantile=0.95, top=True):\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df_agg = df.groupby(series_id).mean()\n\n    if top:\n        selected_series_names = df_agg[\n            df_agg[target] >= df_agg[target].quantile(quantile)\n        ].index.values\n    else:\n        selected_series_names = df_agg[\n            df_agg[target] <= df_agg[target].quantile(quantile)\n        ].index.values\n\n    return selected_series_names\n\n\ndef plot_series_average(df, settings):\n    date_col = settings['date_col']\n    target = settings['target']\n\n    # Average of all series over time\n    df_agg = df.groupby(date_col).mean()\n    df_agg['Date'] = pd.to_datetime(df_agg.index.values)\n\n    fig = px.line(df_agg, x='Date', y=target)\n    fig.update_layout(title_text='Average of all Series')\n    fig.show()\n\n\ndef plot_individual_series(df, ts_settings, n=None, top=True):\n    \"\"\"\n    Plot individual series on the same chart\n    n: (int) number of series to plot\n    top: (boolean) whether to select the top n largest or smallest series ranked by average target value\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    if n is None:\n        n = len(df[series_id].unique())\n\n    series = _cut_series_by_rank(df, ts_settings, n=n, top=top)\n    df_subset = df[df[series_id].isin(series)]\n\n    fig = px.line(df_subset, x=date_col, y=target, color=df_subset[series_id])\n    fig.update_layout(title_text='Top Series By Target Over Time')\n    fig.show()\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_data_quality_check.py",
        "size": 27776,
        "description": "Authors:  Mark Philip, Justin Swansburg",
        "tags": [
          "predictions"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_fill_dates_per_series.py",
        "file_name": "ts_fill_dates_per_series.py",
        "file_type": "python",
        "content": "#Author: Thodoris Petropoulos\n\nimport pandas as pd\n\ndef fill_missing_dates(group, date_col, freq = 'D', per_group_imputation = False):\n    \"\"\"This function can be used together with apply to fill in missing dates per group will fill missing dates per time series group. values per time series group follow the full script to see how you\n    Input:\n        - group <grouped pandas DataFrame> (No need to specify anything. It will work with .apply method)\n        - date_col <string> (Column that represents time)\n        - freq <string> Frequency of data imputation (\"D\" for daily, \"W\" for weekly, \"M\" for monthly)\n        - per_group_imputation <BOOLEAN> (If True, then the min and max value of dates will be specified by the individual series.\n                                          If False, then the min and max value of dates will be specified by the whole dataset).\n    \"\"\"\n    if per_group_imputation == True:\n        date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    else: \n        date_range = pd.date_range(df[date_col].min(), df[date_col].max(), freq=freq)\n    group = group.reindex(date_range)\n    return group\n\n##########\n#Usage\n##########s\n\n#To use the function above: \ndataframe = dataframe.set_index(date_col).groupby('series_id').apply(fill_missing_dates).rename_axis(('series_id',date_col)).drop('series_id', 1).reset_index()\n\n#Impute missing values of target feature with 0\ndataframe['target'].fillna(0,inplace=True)\n\n#Forward fill on the categorical features if needed (depending on dataset)\ndm_imputed.update(dm_imputed.groupby('series_id')[categorical_features].ffill())\n\n#Backward fill on the categorical features if needed (depending on dataset)\ndm_imputed.update(dm_imputed.groupby('series_id')[categorical_features].bfill())\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_fill_dates_per_series.py",
        "size": 1783,
        "description": "Author: Thodoris Petropoulos",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_ion_cannon.py",
        "file_name": "ts_ion_cannon.py",
        "file_type": "python",
        "content": "#Author: Lukas Innig\n\n#Make sure you are connected to DataRobot and have a completed TS project.\n\nimport datarobot as dr\nfrom datarobot.errors import ClientError\n\nclass TimeSeriesIonCannon(dr.Project):\n    \"\"\" This class takes as input a DataRobot Object and initiates a brute force search to increase accuracy.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        all_models = self.get_models()\n        self.supported_metrics = all_models[0].metrics.keys()\n        self.training_duration = [m for m in all_models if m.training_duration][0].training_duration\n    sort_order = {'MASE': False,\n     'FVE Poisson': True,\n     \"Theil's U\": False,\n     'RMSE': False,\n     'FVE Gamma': True,\n     'R Squared': True,\n     'Gamma Deviance': False,\n     'FVE Tweedie': True,\n     'MAE': False,\n     'SMAPE': True,\n     'MAPE': True,\n     'Gini Norm': True,\n     'Tweedie Deviance': False,\n     'Poisson Deviance': False,\n     'RMSLE': False}\n    \n    @classmethod\n    def aim(cls, *args, **kwargs):\n        return super().get(*args, **kwargs)\n    \n    def get_models_sorted(self, partition='validation', metric='RMSE', model_type_filter = ['']):\n        if partition not in ['backtesting', 'holdout', 'validation']:\n            raise ValueError(f\"Partition {partition} not in ['backtesting', 'holdout', 'validation']\")\n        if partition == 'holdout' and not self.holdout_unlocked:\n            print(\"Holdout not unlocked!\")\n            return []\n        if metric not in self.supported_metrics:\n            raise ValueError(f'Metric {metric} not supported')\n        reverse = self.sort_order.get(metric)\n        return sorted([m for m in self.get_datetime_models() \n                       if metric in m.metrics \n                       and m.metrics[metric][partition] \n                       and any([f in m.model_type for f in model_type_filter])], \n                      key=lambda m: m.metrics[metric][partition], reverse=reverse)\n    \n    def calculate_backtests(self, models):\n        def score_backtests(m):\n            try: \n                return m.score_backtests()\n            except ClientError as e:\n                return None\n        jobs = [score_backtests(m) for m in models]\n        [job.wait_for_completion() for job in jobs if job]\n    \n    def identify_best_featurelist(self):\n        best_models = self.get_models_sorted('backtesting')\n        if not best_models:\n            print('calculate some backtests')\n        featurelists = [m.featurelist_id for m in best_models[:20] if 'Blender' not in m.model_type]\n        reduced_fl = [fl for fl in featurelists if 'Reduced' in fl.name]\n        other_fl = [fl for fl in featurelists if 'Reduced' not in fl.name]\n        return reduced_fl + other_fl[:1]\n    \n    def run_all_blueprints(self, featurelist, training_duration=None, \n                           model_type_filter=['Mean', 'Eureqa', 'Keras', 'VARMAX']):\n        if not training_duration:\n            training_duration = self.training_duration\n        def train_blueprint(bp, fl):\n            try:\n                return self.train_datetime(bp.id, fl.id, training_duration=training_duration)\n            except ClientError as e:\n                print(e)\n                return None\n        bps = [bp for bp in self.get_blueprints() if all([f not in bp.model_type for f in model_type_filter])]\n        jobs = [train_blueprint(bp, featurelist) for bp in bps]\n        [job.wait_for_completion() for job in jobs if job]\n    def run_blenders(self):\n        def blend(model_ids, blender_method):\n            try:\n                return self.blend(model_ids, blender_method)\n            except ClientError as e:\n                print(e)\n                return None\n        best_models = self.get_models_sorted('backtesting')\n        best_models = [m for m in best_models if 'Blender' not in m.model_type]\n        jobs = []\n        for n in [3, 5, 7]:\n            for blender_method in [dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_AVG, \n                                   dr.enums.BLENDER_METHOD.AVERAGE,\n                                   dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_ENET]:\n                jobs.append(blend([m.id for m in best_models[:n]], blender_method=blender_method))\n        blender_models = [j.get_result_when_complete() for j in jobs if j]\n        blender_models = [dr.DatetimeModel.get(self.id, bm.id) for bm in blender_models]\n        return blender_models\n    def shoot(self):\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        fls = self.identify_best_featurelist()\n        for fl in fls:\n            self.run_all_blueprints(fl)\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        self.run_blenders()\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n\n\n##USAGE##\n#cannon = TimeSeriesIonCannon.aim('YOUR_PROJECT_ID')\n#cannon.shoot()",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_ion_cannon.py",
        "size": 4904,
        "description": "Author: Lukas Innig",
        "tags": [
          "datarobot-api",
          "time-series"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_metrics.py",
        "file_name": "ts_metrics.py",
        "file_type": "python",
        "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n\n#These functions can be used to calculate common evaluation metrics\n\nimport numpy as np\n\n\n#####################\n# Evaluation Metrics\n#####################\n\ndef mae(act, pred, weight=None):\n    \"\"\"\n    MAE = Mean Absolute Error = mean( abs(act - pred) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    ad = np.abs(d)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        ad = ad * weight / weight.mean()\n    mae = ad.mean()\n\n    if np.isnan(mae):\n        return np.finfo(np.float64).max\n    else:\n        return mae\n\n\ndef mape(act, pred, nan='ignore'):\n\n    # ignore NAN (drop rows), do nothing, replace Nan with 0\n    if nan not in ['ignore', 'set_to_zero', 'error']:\n        raise ValueError(f'{nan} must be either ignore, set_to_zero, or error')\n\n    act, pred = np.array(act), np.array(pred)\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(act - pred)\n    d = act\n    ape = n / d\n\n    if nan == 'set_to_zero':\n        ape[~np.isfinite(ape)] = 0\n    elif nan == 'ignore':\n        ape = ape[np.isfinite(ape)]\n\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape\n\n\ndef smape(act, pred):\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(pred - act)\n    d = (np.abs(pred) + np.abs(act)) / 2\n    ape = n / d\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape\n\n\ndef rmse(act, pred, weight=None):\n    \"\"\"\n    RMSE = Root Mean Squared Error = sqrt( mean( (act - pred)**2 ) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    sd = np.power(d, 2)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        sd = sd * weight / weight.mean()\n    mse = sd.mean()\n    rmse = np.sqrt(mse)\n\n    if np.isnan(rmse):\n        return np.finfo(np.float64).max\n    else:\n        return rmse\n\n\ndef gamma_loss(act, pred, weight=None):\n    \"\"\"Gamma deviance\"\"\"\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, eps)  # ensure actuals are strictly positive\n    d = 2 * (-np.log(act / pred) + (act - pred) / pred)\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)\n\n\ndef tweedie_loss(act, pred, weight=None, p=1.5):\n    \"\"\"tweedie deviance for p = 1.5 only\"\"\"\n\n    if p <= 1 or p >= 2:\n        raise ValueError('p equal to %s is not supported' % p)\n\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are not negative\n    d = (\n        (act ** (2.0 - p)) / ((1 - p) * (2 - p))\n        - (act * (pred ** (1 - p))) / (1 - p)\n        + (pred ** (2 - p)) / (2 - p)\n    )\n    d = 2 * d\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)\n\n\ndef poisson_loss(act, pred, weight=None):\n    \"\"\"\n        Poisson Deviance = 2*(act*log(act/pred)-(act-pred))\n        ONLY WORKS FOR POSITIVE RESPONSES\n    \"\"\"\n    if len(pred.shape) > 1:\n        pred = pred.ravel()\n    pred = np.maximum(pred, 1e-8)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are non-negative\n    d = np.zeros(len(act))\n    d[act == 0] = pred[act == 0]\n    cond = act > 0\n    d[cond] = act[cond] * np.log(act[cond] / pred[cond]) - (act[cond] - pred[cond])\n    d = d * 2\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        d = d * weight / weight.mean()\n    return d.mean()",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_metrics.py",
        "size": 3935,
        "description": "Author: Justin Swansburg, Mark Philip",
        "tags": [
          "predictions"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_modeling.py",
        "file_name": "ts_modeling.py",
        "file_type": "python",
        "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n\n#This function will help you create DataRobot Time series projects.\n\nimport datetime as dt\nimport time\n\nimport datarobot as dr\nimport numpy as np\nimport pandas as pd\n\nfrom ts_data_quality_check import get_timestep\n\n\n###################\n# Project Creation\n###################\n\n\ndef create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project\n\n\ndef create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects\n\n\ndef wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_modeling.py",
        "size": 10163,
        "description": "Author: Justin Swansburg, Mark Philip",
        "tags": [
          "datarobot-api",
          "project-creation",
          "time-series"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_preprocessing.py",
        "file_name": "ts_preprocessing.py",
        "file_type": "python",
        "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport numpy as np\n\n\n#####################\n# Preprocessing Funcs\n#####################\n\n\ndef dataset_reduce_memory(df):\n    \"\"\"\n    Recast numerics to lower precision\n    \"\"\"\n    for c in df.select_dtypes(include=['float64']).columns:\n        df[c] = df[c].astype(np.float32)\n    for c in df.select_dtypes(include=['int64']).columns:\n        df[c] = df[c].astype(np.int32)\n    return df\n\n\ndef create_series_id(df, cols_to_concat, convert=True):\n    \"\"\"\n    Concatenate columns\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    df = df[cols_to_concat].copy()\n    non_strings = [c for c in df[cols_to_concat] if df[c].dtype != 'object']\n\n    if len(non_strings) > 0:\n        if convert:\n            df[non_strings] = df[non_strings].applymap(str)\n        else:\n            raise TypeError(\"columns must all be type str\")\n\n    df['series_id'] = df[cols_to_concat].apply(lambda x: '_'.join(x), axis=1)\n    return df['series_id']\n\n\ndef _create_cross_series_feature(df, group, col, func):\n    col_name = col + '_' + func\n    df.loc[:, col_name] = df.groupby(group)[col].transform(func)\n    return df\n\n\ndef create_cross_series_features(df, group, cols, funcs):\n    \"\"\"\n    Create custom aggregations across groups\n    Returns:\n    --------\n    pandas df with new cross series features\n    Example:\n    --------\n    df_agg = create_cross_series_features(df,\n                                          group=[date_col,'Cluster'],\n                                          cols=[target,'feat_1'],\n                                          funcs=['mean','std'])\n    \"\"\"\n    for c in cols:\n        for f in funcs:\n            df = _create_cross_series_feature(df, group, c, f)\n    return df.reset_index(drop=True)\n\n\ndef get_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    \"\"\"\n    Identify series where the target is 0.0 in more than x% of the rows\n    Returns:\n    --------\n    List of series\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n    series = df[df >= cutoff].index.values\n\n    return series\n\n\ndef drop_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    series_id = ts_settings['series_id']\n\n    series_to_drop = get_zero_inflated_series(df, ts_settings, cutoff=cutoff)\n\n    if len(series_to_drop) > 0:\n        print('Dropping ', len(series_to_drop), ' zero-inflated series')\n        df = df.loc[~df[series_id].isin(series_to_drop), :].reset_index(drop=True)\n        print('Remaining series: ', len(df[series_id].unique()))\n    else:\n        print('There are no zero-inflated series to drop')\n\n    return df\n\n\ndef sample_series(df, series_id, date_col, target, x=1, method='random', **kwargs):\n    \"\"\"\n    Sample series\n    x: percent of series to sample\n    random: sample x% of the series at random\n    target: sample the largest x% of series\n    timespan: sample the top x% of series with the longest histories\n    \"\"\"\n    if (x > 1) | (x < 0):\n        raise ValueError('x must be between 0 and 1')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series = round(x * len(df[series_id].unique()))\n\n    if method == 'random':\n        series_to_keep = np.random.choice(df[series_id].values, size=series)\n\n    elif method == 'target':\n        series_to_keep = (\n            df.groupby([series_id])[target]\n            .mean()\n            .sort_values(ascending=False)\n            .reset_index()\n            .loc[0:series, series_id]\n        )\n\n    elif method == 'timespan':\n        max_timespan = df[date_col].max() - df[date_col].min()\n        series_timespans = (\n            df.groupby([series_id])[date_col]\n            .apply(lambda x: x.max() - x.min())\n            .sort_values(ascending=False)\n            .reset_index()\n        )\n        series_to_keep = series_timespans.loc[0:series, series_id]\n        if kwargs.get('full_timespan'):\n            series_to_keep = series_timespans.loc[series_timespans == max_timespan, series_id]\n\n    else:\n        raise ValueError('Method not supported. Must be either random, target, or timespan')\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n\n    return sampled_df.reset_index(drop=True)\n\n\ndef drop_series_w_gaps(df, series_id, date_col, target, max_gap=1, output_dropped_series=False):\n    \"\"\"\n    Sample series\n    max_gap: number of timesteps\n    \"\"\"\n    if not isinstance(max_gap, int):\n        raise TypeError('max gap must be an int')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series_max_gap = df.groupby([series_id]).apply(lambda x: x[date_col].diff().max())\n    median_timestep = df.groupby([series_id])[date_col].diff().median()\n    series_to_keep = series_max_gap[(series_max_gap / median_timestep) <= max_gap].index.values\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n    dropped_df = df.loc[~df[series_id].isin(series_to_keep), :]\n\n    if output_dropped_series:\n        return sampled_df, dropped_df\n    else:\n        return sampled_df",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_preprocessing.py",
        "size": 5119,
        "description": "Authors: Justin Swansburg, Mark Philip",
        "tags": []
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_projects.py",
        "file_name": "ts_projects.py",
        "file_type": "python",
        "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n#The functions below will help you evaluate a DataRobot TS project.\n\nimport datarobot as dr\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom ts_metrics import *\n\n\n######################\n# Project Evaluation\n######################\n\n\ndef get_top_models_from_project(\n    project, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    project: project object\n        DataRobot project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from a DataRobot project\n    \"\"\"\n    assert data_subset in [\n        'backtest_1',\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either backtest_1, allBacktests, or holdout'\n    if n_models is not None:\n        assert isinstance(n_models, int), 'n_models must be an int'\n    if n_models is not None:\n        assert n_models >= 1, 'n_models must be greater than or equal to 1'\n    assert isinstance(include_blenders, bool), 'include_blenders must be a boolean'\n\n    mapper = {\n        'backtest_1': 'backtestingScores',\n        'allBacktests': 'backtesting',\n        'holdout': 'holdout',\n    }\n\n    if metric is None:\n        metric = project.metric\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    models = [\n        m\n        for m in project.get_datetime_models()\n        if m.backtests[0]['status'] != 'BACKTEST_BOUNDARIES_EXCEEDED'\n    ]  # if m.holdout_status != 'HOLDOUT_BOUNDARIES_EXCEEDED']\n\n    if data_subset == 'backtest_1':\n        # models = sorted(models, key=lambda m: np.mean([i for i in m.metrics[metric][mapper[data_subset]][0] if i]), reverse=False)\n        models = sorted(\n            models, key=lambda m: m.metrics[metric][mapper[data_subset]][0], reverse=False\n        )\n    elif data_subset == 'allBacktests':\n        models = sorted(\n            models,\n            key=lambda m: m.metrics[metric][mapper[data_subset]]\n            if m.metrics[metric][mapper[data_subset]] is not None\n            else np.nan,\n            reverse=False,\n        )\n    else:\n        models = sorted(models, key=lambda m: m.metrics[metric][mapper[data_subset]], reverse=False)\n\n    if not include_blenders:\n        models = [m for m in models if m.model_category != 'blend']\n\n    if n_models is None:\n        n_models = len(models)\n\n    models = models[0:n_models]\n\n    assert len(models) > 0, 'You have not run any models for this project'\n\n    return models\n\n\ndef get_top_models_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Pull top models from leaderboard across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from DataRobot project(s)\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    models_all = []\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n        models_all.extend(models)\n    return models_all\n\n\ndef compute_backtests(\n    projects, n_models=5, data_subset='backtest_1', include_blenders=True, metric=None\n):\n    \"\"\"\n    Compute all backtests for top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        for m in models:\n            try:\n                m.score_backtests()  # request backtests for top models\n                print(f'Computing backtests for model {m.id} in Project {p.project_name}')\n            except dr.errors.ClientError:\n                pass\n        print(\n            f'All available backtests have been submitted for scoring for project {p.project_name}'\n        )\n\n\ndef get_or_request_backtest_scores(\n    projects, n_models=5, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get or request backtest and holdout scores from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    scores = pd.DataFrame()\n    for p in projects:\n\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        if metric is None:\n            metric = p.metric\n\n        backtest_scores = pd.DataFrame(\n            [\n                {\n                    'Project_Name': p.project_name,\n                    'Project_ID': p.id,\n                    'Model_ID': m.id,\n                    'Model_Type': m.model_type,\n                    'Featurelist': m.featurelist_name,\n                    f'Backtest_1_{metric}': m.metrics[metric]['backtestingScores'][0],\n                    'Backtest_1_MASE': m.metrics['MASE']['backtestingScores'][0],\n                    'Backtest_1_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'][0],\n                    'Backtest_1_SMAPE': m.metrics['SMAPE']['backtestingScores'][0],\n                    'Backtest_1_R_Squared': m.metrics['R Squared']['backtestingScores'][0],\n                    f'All_Backtests_{metric}': m.metrics[metric]['backtestingScores'],\n                    'All_Backtests_MASE': m.metrics['MASE']['backtestingScores'],\n                    'All_Backtests_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'],\n                    'All_Backtests_SMAPE': m.metrics['SMAPE']['backtestingScores'],\n                    'All_Backtests_R_Squared': m.metrics['R Squared']['backtestingScores'],\n                    f'Holdout_{metric}': m.metrics[metric]['holdout'],\n                    'Holdout_MASE': m.metrics['MASE']['holdout'],\n                    'Holdout_Theils_U': m.metrics[\"Theil's U\"]['holdout'],\n                    'Holdout_SMAPE': m.metrics['SMAPE']['holdout'],\n                    'Holdout_R_Squared': m.metrics['R Squared']['holdout'],\n                }\n                for m in models\n            ]\n        ).sort_values(by=[f'Backtest_1_{metric}'])\n\n        scores = scores.append(backtest_scores).reset_index(\n            drop=True\n        )  # append top model from each project\n\n    print(f'Scores for all {len(projects)} projects have been computed')\n\n    return scores\n\n\ndef get_or_request_training_predictions_from_model(model, data_subset='allBacktests'):\n    project = dr.Project.get(model.project_id)\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    try:\n        predict_job = model.request_training_predictions(data_subset)\n        training_predictions = predict_job.get_result_when_complete(max_wait=10000)\n\n    except dr.errors.ClientError:\n        prediction_id = [\n            p.prediction_id\n            for p in dr.TrainingPredictions.list(project.id)\n            if p.model_id == model.id and p.data_subset == data_subset\n        ][0]\n        training_predictions = dr.TrainingPredictions.get(project.id, prediction_id)\n\n    return training_predictions.get_all_as_dataframe(serializer='csv')\n\n\ndef get_or_request_training_predictions_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get row-level backtest or holdout predictions from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = pd.DataFrame()\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            tmp = get_or_request_training_predictions_from_model(m, data_subset)\n            tmp['Project_Name'] = p.project_name\n            tmp['Project_ID'] = p.id\n            tmp['Model_ID'] = m.id\n            tmp['Model_Type'] = m.model_type\n        preds = preds.append(tmp).reset_index(drop=True)\n\n    return preds\n\n\ndef get_preds_and_actuals(\n    df,\n    projects,\n    ts_settings,\n    n_models=1,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Get row-level predictions and merge onto actuals\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = get_or_request_training_predictions_from_projects(\n        projects,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    preds['timestamp'] = pd.to_datetime(preds['timestamp'].apply(lambda x: x[:-8]))\n    df = df.merge(\n        preds,\n        how='left',\n        left_on=[ts_settings['date_col'], ts_settings['series_id']],\n        right_on=['timestamp', 'series_id'],\n        validate='one_to_many',\n    )\n    df = df.loc[~np.isnan(df['prediction']), :].reset_index(drop=True)\n    return df\n\n\ndef get_cluster_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get cluster-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    print('Getting cluster accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby(['Cluster'])\n        .apply(lambda x: acc_calc(x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n    groups.columns = ['Cluster', f'Cluster_{acc_calc.__name__.upper()}']\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(\n        act=df[ts_settings['target']], pred=df['prediction']\n    )\n\n    return groups\n\n\ndef plot_cluster_acc(cluster_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse):\n    \"\"\"\n    Plots cluster-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_cluster_acc()\n    ts_settings: dict\n        Pparameters for time series project\n    data_subset: str\n        Choose either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    cluster_acc['Label'] = '=' + cluster_acc['Cluster']\n\n    fig = px.bar(cluster_acc, x='Label', y=f'Cluster_{acc_calc.__name__.upper()}').for_each_trace(\n        lambda t: t.update(name=t.name.replace('=', ''))\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=cluster_acc['Label'],\n            y=cluster_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Cluster Accuracy - {data_subset}')\n    fig.show()\n\n\ndef get_series_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get series-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    print('Getting series accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby([series_id]).apply(lambda x: acc_calc(x[target], x['prediction'])).reset_index()\n    )\n    groups.columns = [series_id, f'Series_{acc_calc.__name__.upper()}']\n    right = df[[series_id, 'Cluster']].drop_duplicates().reset_index(drop=True)\n    groups = groups.merge(right, how='left', on=series_id)\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(act=df[target], pred=df['prediction'])\n\n    return groups\n\n\ndef plot_series_acc(series_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse, n=50):\n    \"\"\"\n    Plots series-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_series_acc()\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Choose from either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    n_series = len(series_acc[ts_settings['series_id']].unique())\n    n = min(n_series, n)\n\n    series_acc.sort_values(by=f'Series_{acc_calc.__name__.upper()}', ascending=False, inplace=True)\n\n    series_acc = series_acc[0:n]\n\n    fig = px.bar(\n        series_acc,\n        x=ts_settings['series_id'],\n        y=f'Series_{acc_calc.__name__.upper()}',\n        color='Cluster',\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.add_trace(\n        go.Scatter(\n            x=series_acc[ts_settings['series_id']],\n            y=series_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Series Accuracy - {data_subset}')\n    fig.show()\n\n\ndef get_project_info(df):\n    \"\"\"\n    Parse project name to get FD, FDW, and Cluster information\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = df.copy()\n    try:\n        df['Cluster'] = df['Project_Name'].apply(lambda x: x.split('_Cluster-')[1])\n    except:\n        df['Cluster'] = 'all_series'\n\n    df['FD'] = df['Project_Name'].apply(lambda x: x.split('_FD:')[1].split('_FDW:')[0])\n    df['FDW'] = df['Project_Name'].apply(lambda x: x.split('_FDW:')[1].split('_Cluster-')[0])\n\n    return df\n\n\ndef filter_best_fdw_scores(scores, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset df to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = get_project_info(scores)\n    df['_tmp'] = df[col_error].apply(lambda x: np.nanmean(np.array(x, dtype=np.float32)))\n    idx = df.groupby(['Cluster', 'FD']).apply(lambda x: x['_tmp'].idxmin()).values\n    return scores.iloc[idx, :]\n\n\ndef filter_best_fdw_projects(scores, projects, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset list to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    projects: list\n        DataRobot projects object(s)\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    list\n    \"\"\"\n    df = filter_best_fdw_scores(scores, col_error)\n    return [p for p in projects if p.project_name in df['Project_Name'].unique()]\n\n\ndef plot_fd_accuracy(df, projects, ts_settings, data_subset='allBacktests', metric='RMSE'):\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    mapper = {\n        'MAE': mae,\n        'RMSE': rmse,\n        'Gamma Deviance': gamma_loss,\n        'Tweedie Deviance': tweedie_loss,\n        'Poisson Deviance': poisson_loss,\n    }\n\n    df = get_preds_and_actuals(\n        df, projects, ts_settings, n_models=1, data_subset=data_subset, metric=metric\n    )\n    df = (\n        df.groupby(['Project_Name', 'forecast_distance'])\n        .apply(lambda x: mapper[metric](x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n\n    df.columns = ['Project_Name', 'forecast_distance', mapper[metric].__name__.upper()]\n    fig = px.line(\n        df, x='forecast_distance', y=mapper[metric].__name__.upper(), color='Project_Name'\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.update_layout(title_text='Forecasting Accuracy per Forecast Distance')\n    fig.update_yaxes(title=mapper[metric].__name__.upper())\n    fig.update_xaxes(title='Forecast Distance')\n    fig.show()\n\n\ndef plot_fd_accuracy_by_cluster(\n    df, scores, projects, ts_settings, data_subset='holdout', metric='RMSE', split_col='Cluster'\n):\n    scores = get_project_info(scores)\n\n    for c in scores[split_col].unique():\n        project_names = list(\n            scores.loc[scores[split_col] == c, 'Project_Name'].reset_index(drop=True)\n        )\n        projects_by_cluster = [p for p in projects if p.project_name in project_names]\n        plot_fd_accuracy(df, projects_by_cluster, ts_settings, data_subset, metric)\n\n\n###########################\n# Performance Improvements\n###########################\n\n\ndef get_reduced_features_featurelist(project, model, threshold=0.99):\n    \"\"\"\n    Helper function for train_reduced_features_models()\n    project: DataRobot project object\n    model: DataRobot model object\n    threshold: np.float\n    Returns:\n    --------\n    DataRobot featurelist\n    \"\"\"\n    print(\n        f'Collecting Feature Impact for M{model.model_number} in project {project.project_name}...'\n    )\n\n    impact = pd.DataFrame.from_records(model.get_or_request_feature_impact())\n    impact['impactUnnormalized'] = np.where(\n        impact['impactUnnormalized'] < 0, 0, impact['impactUnnormalized']\n    )\n    impact['cumulative_impact'] = (\n        impact['impactUnnormalized'].cumsum() / impact['impactUnnormalized'].sum()\n    )\n\n    to_keep = np.where(impact['cumulative_impact'] <= threshold)[0]\n    if len(to_keep) < 1:\n        print('Applying this threshold would result in a featurelist with no features')\n        return None\n\n    idx = np.max(to_keep)\n\n    selected_features = impact.loc[0:idx, 'featureName'].to_list()\n    feature_list = project.create_modeling_featurelist(\n        f'Top {len(selected_features)} features M{model.model_number}', selected_features\n    )\n\n    return feature_list\n\n\ndef train_reduced_features_models(\n    projects,\n    n_models=1,\n    threshold=0.99,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Retrain top models with reduced feature featurelists\n    projects: list\n        DataRobot project objects(s)\n    n_models: int\n        Number of models to retrain with reduced feature featurelists\n    threshold: np.float\n        Controls the number of features to keep in the reduced feature list. Percentage of cumulative feature impact\n    data_subset: str\n        Choose from either holdout or allBacktests\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            try:\n                feature_list = get_reduced_features_featurelist(p, m, threshold)\n                if feature_list is None:\n                    continue\n                try:\n                    m.retrain(featurelist_id=feature_list.id)\n                    print(f'Training {m.model_type} on Featurelist {feature_list.name}')\n                except dr.errors.ClientError as e:\n                    print(e)\n            except dr.errors.ClientError as e:\n                print(e)",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Helper%20Functions/Time%20Series/Python/ts_projects.py",
        "size": 25426,
        "description": "Author: Justin Swansburg, Mark Philip",
        "tags": [
          "datarobot-api",
          "project-creation",
          "time-series",
          "predictions"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Binary Classification Project.ipynb",
        "file_name": "Starting a Binary Classification Project.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Starting a Binary Classification Project\\n\",\n    \"\\n\",\n    \"**Author**: Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"### Scope\\n\",\n    \"The scope of this notebook is to provide instructions on how to initiate a DataRobot project for a Binary Classification target using the Python API.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"Binary classification is the task of classifying the elements of a given set into two groups.\\n\",\n    \"\\n\",\n    \"Examples:\\n\",\n    \"\\n\",\n    \"- A customer is a churner or not.\\n\",\n    \"- A loan is going to default or not.\\n\",\n    \"- A patient has a disease or not.\\n\",\n    \"\\n\",\n    \"Most commonly, the target column will have values:\\n\",\n    \"\\n\",\n    \"- 0/1\\n\",\n    \"- Yes/No\\n\",\n    \"- True/False\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.19.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\\n\",\n    \"import pandas as pd\\n\",\n    \"import numpy as np\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Dataset\\n\",\n    \"We will be loading the breast cancer dataset. A very simple binary classification dataset that is available through sk-learn.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>mean radius</th>\\n\",\n       \"      <th>mean texture</th>\\n\",\n       \"      <th>mean perimeter</th>\\n\",\n       \"      <th>mean area</th>\\n\",\n       \"      <th>mean smoothness</th>\\n\",\n       \"      <th>mean compactness</th>\\n\",\n       \"      <th>mean concavity</th>\\n\",\n       \"      <th>mean concave points</th>\\n\",\n       \"      <th>mean symmetry</th>\\n\",\n       \"      <th>mean fractal dimension</th>\\n\",\n       \"      <th>...</th>\\n\",\n       \"      <th>worst texture</th>\\n\",\n       \"      <th>worst perimeter</th>\\n\",\n       \"      <th>worst area</th>\\n\",\n       \"      <th>worst smoothness</th>\\n\",\n       \"      <th>worst compactness</th>\\n\",\n       \"      <th>worst concavity</th>\\n\",\n       \"      <th>worst concave points</th>\\n\",\n       \"      <th>worst symmetry</th>\\n\",\n       \"      <th>worst fractal dimension</th>\\n\",\n       \"      <th>target</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>17.99</td>\\n\",\n       \"      <td>10.38</td>\\n\",\n       \"      <td>122.80</td>\\n\",\n       \"      <td>1001.0</td>\\n\",\n       \"      <td>0.11840</td>\\n\",\n       \"      <td>0.27760</td>\\n\",\n       \"      <td>0.3001</td>\\n\",\n       \"      <td>0.14710</td>\\n\",\n       \"      <td>0.2419</td>\\n\",\n       \"      <td>0.07871</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>17.33</td>\\n\",\n       \"      <td>184.60</td>\\n\",\n       \"      <td>2019.0</td>\\n\",\n       \"      <td>0.1622</td>\\n\",\n       \"      <td>0.6656</td>\\n\",\n       \"      <td>0.7119</td>\\n\",\n       \"      <td>0.2654</td>\\n\",\n       \"      <td>0.4601</td>\\n\",\n       \"      <td>0.11890</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>20.57</td>\\n\",\n       \"      <td>17.77</td>\\n\",\n       \"      <td>132.90</td>\\n\",\n       \"      <td>1326.0</td>\\n\",\n       \"      <td>0.08474</td>\\n\",\n       \"      <td>0.07864</td>\\n\",\n       \"      <td>0.0869</td>\\n\",\n       \"      <td>0.07017</td>\\n\",\n       \"      <td>0.1812</td>\\n\",\n       \"      <td>0.05667</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>23.41</td>\\n\",\n       \"      <td>158.80</td>\\n\",\n       \"      <td>1956.0</td>\\n\",\n       \"      <td>0.1238</td>\\n\",\n       \"      <td>0.1866</td>\\n\",\n       \"      <td>0.2416</td>\\n\",\n       \"      <td>0.1860</td>\\n\",\n       \"      <td>0.2750</td>\\n\",\n       \"      <td>0.08902</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>19.69</td>\\n\",\n       \"      <td>21.25</td>\\n\",\n       \"      <td>130.00</td>\\n\",\n       \"      <td>1203.0</td>\\n\",\n       \"      <td>0.10960</td>\\n\",\n       \"      <td>0.15990</td>\\n\",\n       \"      <td>0.1974</td>\\n\",\n       \"      <td>0.12790</td>\\n\",\n       \"      <td>0.2069</td>\\n\",\n       \"      <td>0.05999</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>25.53</td>\\n\",\n       \"      <td>152.50</td>\\n\",\n       \"      <td>1709.0</td>\\n\",\n       \"      <td>0.1444</td>\\n\",\n       \"      <td>0.4245</td>\\n\",\n       \"      <td>0.4504</td>\\n\",\n       \"      <td>0.2430</td>\\n\",\n       \"      <td>0.3613</td>\\n\",\n       \"      <td>0.08758</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>11.42</td>\\n\",\n       \"      <td>20.38</td>\\n\",\n       \"      <td>77.58</td>\\n\",\n       \"      <td>386.1</td>\\n\",\n       \"      <td>0.14250</td>\\n\",\n       \"      <td>0.28390</td>\\n\",\n       \"      <td>0.2414</td>\\n\",\n       \"      <td>0.10520</td>\\n\",\n       \"      <td>0.2597</td>\\n\",\n       \"      <td>0.09744</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>26.50</td>\\n\",\n       \"      <td>98.87</td>\\n\",\n       \"      <td>567.7</td>\\n\",\n       \"      <td>0.2098</td>\\n\",\n       \"      <td>0.8663</td>\\n\",\n       \"      <td>0.6869</td>\\n\",\n       \"      <td>0.2575</td>\\n\",\n       \"      <td>0.6638</td>\\n\",\n       \"      <td>0.17300</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>20.29</td>\\n\",\n       \"      <td>14.34</td>\\n\",\n       \"      <td>135.10</td>\\n\",\n       \"      <td>1297.0</td>\\n\",\n       \"      <td>0.10030</td>\\n\",\n       \"      <td>0.13280</td>\\n\",\n       \"      <td>0.1980</td>\\n\",\n       \"      <td>0.10430</td>\\n\",\n       \"      <td>0.1809</td>\\n\",\n       \"      <td>0.05883</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>16.67</td>\\n\",\n       \"      <td>152.20</td>\\n\",\n       \"      <td>1575.0</td>\\n\",\n       \"      <td>0.1374</td>\\n\",\n       \"      <td>0.2050</td>\\n\",\n       \"      <td>0.4000</td>\\n\",\n       \"      <td>0.1625</td>\\n\",\n       \"      <td>0.2364</td>\\n\",\n       \"      <td>0.07678</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"<p>5 rows × 31 columns</p>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\\\\n\",\n       \"0        17.99         10.38          122.80     1001.0          0.11840   \\n\",\n       \"1        20.57         17.77          132.90     1326.0          0.08474   \\n\",\n       \"2        19.69         21.25          130.00     1203.0          0.10960   \\n\",\n       \"3        11.42         20.38           77.58      386.1          0.14250   \\n\",\n       \"4        20.29         14.34          135.10     1297.0          0.10030   \\n\",\n       \"\\n\",\n       \"   mean compactness  mean concavity  mean concave points  mean symmetry  \\\\\\n\",\n       \"0           0.27760          0.3001              0.14710         0.2419   \\n\",\n       \"1           0.07864          0.0869              0.07017         0.1812   \\n\",\n       \"2           0.15990          0.1974              0.12790         0.2069   \\n\",\n       \"3           0.28390          0.2414              0.10520         0.2597   \\n\",\n       \"4           0.13280          0.1980              0.10430         0.1809   \\n\",\n       \"\\n\",\n       \"   mean fractal dimension   ...    worst texture  worst perimeter  worst area  \\\\\\n\",\n       \"0                 0.07871   ...            17.33           184.60      2019.0   \\n\",\n       \"1                 0.05667   ...            23.41           158.80      1956.0   \\n\",\n       \"2                 0.05999   ...            25.53           152.50      1709.0   \\n\",\n       \"3                 0.09744   ...            26.50            98.87       567.7   \\n\",\n       \"4                 0.05883   ...            16.67           152.20      1575.0   \\n\",\n       \"\\n\",\n       \"   worst smoothness  worst compactness  worst concavity  worst concave points  \\\\\\n\",\n       \"0            0.1622             0.6656           0.7119                0.2654   \\n\",\n       \"1            0.1238             0.1866           0.2416                0.1860   \\n\",\n       \"2            0.1444             0.4245           0.4504                0.2430   \\n\",\n       \"3            0.2098             0.8663           0.6869                0.2575   \\n\",\n       \"4            0.1374             0.2050           0.4000                0.1625   \\n\",\n       \"\\n\",\n       \"   worst symmetry  worst fractal dimension  target  \\n\",\n       \"0          0.4601                  0.11890     0.0  \\n\",\n       \"1          0.2750                  0.08902     0.0  \\n\",\n       \"2          0.3613                  0.08758     0.0  \\n\",\n       \"3          0.6638                  0.17300     0.0  \\n\",\n       \"4          0.2364                  0.07678     0.0  \\n\",\n       \"\\n\",\n       \"[5 rows x 31 columns]\"\n      ]\n     },\n     \"execution_count\": 2,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"from sklearn.datasets import load_breast_cancer\\n\",\n    \"data = load_breast_cancer()\\n\",\n    \"\\n\",\n    \"df = pd.DataFrame(np.c_[data['data'], data['target']],\\n\",\n    \"                  columns= np.append(data['feature_names'], ['target']))\\n\",\n    \"df.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Connect to DataRobot\\n\",\n    \"Connect to DataRobot using your credentials and your endpoint. Change input below accordingly.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"dr.Client(token='YOUR_API_KEY', \\n\",\n    \"          endpoint='YOUR_DATAROBOT_HOSTNAME')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Initiate Project\\n\",\n    \"I will be initiating a project calling the method <code>dr.Project.start</code>:\\n\",\n    \"* project_name: Name of project\\n\",\n    \"* source_data: Data source (Path to file or pandas dataframe)\\n\",\n    \"* target: String with target variable name\\n\",\n    \"* worker_count: Amount of workers to use\\n\",\n    \"* metric: Optimisation metric to use\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"project = dr.Project.start(project_name='MyBinaryClassificationProject',\\n\",\n    \"                        sourcedata= df,\\n\",\n    \"                        target='target')\\n\",\n    \"\\n\",\n    \"project.wait_for_autopilot() #Wait for autopilot to complete\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Initiating%20Projects/Python/Starting%20a%20Binary%20Classification%20Project.ipynb",
        "size": 12194,
        "description": "Jupyter notebook example from Initiating Projects/Python/Starting a Binary Classification Project.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Multiclass Classification Project.ipynb",
        "file_name": "Starting a Multiclass Classification Project.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Starting a Multiclass Classification Project\\n\",\n    \"\\n\",\n    \"**Author**: Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"### Scope\\n\",\n    \"The scope of this notebook is to provide instructions on how to initiate a DataRobot project for a Multiclass Classification target using the Python API.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"Multiclass classification is the task of classifying the elements of a given set into more than two groups.\\n\",\n    \"\\n\",\n    \"Examples:\\n\",\n    \"\\n\",\n    \"- A customer would be more interested in one of A,B,C,D... products.\\n\",\n    \"- A patient has one of A,B,C,D... diseases.\\n\",\n    \"- A customer would have a higher propensity to respond to one of A,B,C,D... campaigns.\\n\",\n    \"\\n\",\n    \"Most commonly, the target column will have values:\\n\",\n    \"\\n\",\n    \"- AAA/BBB/CCC/...(example text)\\n\",\n    \"- 0/1/2/3/4/...\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.19.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\\n\",\n    \"import pandas as pd\\n\",\n    \"import numpy as np\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Dataset\\n\",\n    \"We will be loading the iris dataset. A very simple Multiclass classification dataset that is available through sk-learn.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>sepal length (cm)</th>\\n\",\n       \"      <th>sepal width (cm)</th>\\n\",\n       \"      <th>petal length (cm)</th>\\n\",\n       \"      <th>petal width (cm)</th>\\n\",\n       \"      <th>target</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>5.1</td>\\n\",\n       \"      <td>3.5</td>\\n\",\n       \"      <td>1.4</td>\\n\",\n       \"      <td>0.2</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>4.9</td>\\n\",\n       \"      <td>3.0</td>\\n\",\n       \"      <td>1.4</td>\\n\",\n       \"      <td>0.2</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>4.7</td>\\n\",\n       \"      <td>3.2</td>\\n\",\n       \"      <td>1.3</td>\\n\",\n       \"      <td>0.2</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>4.6</td>\\n\",\n       \"      <td>3.1</td>\\n\",\n       \"      <td>1.5</td>\\n\",\n       \"      <td>0.2</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>5.0</td>\\n\",\n       \"      <td>3.6</td>\\n\",\n       \"      <td>1.4</td>\\n\",\n       \"      <td>0.2</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\\\\n\",\n       \"0                5.1               3.5                1.4               0.2   \\n\",\n       \"1                4.9               3.0                1.4               0.2   \\n\",\n       \"2                4.7               3.2                1.3               0.2   \\n\",\n       \"3                4.6               3.1                1.5               0.2   \\n\",\n       \"4                5.0               3.6                1.4               0.2   \\n\",\n       \"\\n\",\n       \"   target  \\n\",\n       \"0     0.0  \\n\",\n       \"1     0.0  \\n\",\n       \"2     0.0  \\n\",\n       \"3     0.0  \\n\",\n       \"4     0.0  \"\n      ]\n     },\n     \"execution_count\": 2,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"from sklearn.datasets import load_iris\\n\",\n    \"data = load_iris()\\n\",\n    \"\\n\",\n    \"df = pd.DataFrame(np.c_[data['data'], data['target']],\\n\",\n    \"                  columns= np.append(data['feature_names'], ['target']))\\n\",\n    \"df.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Connect to DataRobot\\n\",\n    \"Connect to DataRobot using your credentials and your endpoint. Change input below accordingly.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"dr.Client(token='YOUR_API_KEY}', \\n\",\n    \"          endpoint='YOUR_DATAROBOT_HOSTNAME')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Initiate Project\\n\",\n    \"I will be initiating a project calling the method <code>dr.Project.start</code>:\\n\",\n    \"* project_name: Name of project\\n\",\n    \"* source_data: Data source (Path to file or pandas dataframe)\\n\",\n    \"* target: String with target variable name\\n\",\n    \"* worker_count: Amount of workers to use\\n\",\n    \"* metric: Optimization metric to use\\n\",\n    \"\\n\",\n    \"If your target is categorical and has a cardinality of up to 10, we will automatically select a Multiclass target_type and that argument is not needed when calling Project.start. However, if the target is numerical and you would like to force it to be seen as a Multiclass project in DataRobot, you can specify the target_type as seen below:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"project = dr.Project.start(project_name='MyMulticlassClassificationProject',\\n\",\n    \"                        sourcedata= df,\\n\",\n    \"                        target='target',\\n\",\n    \"                        target_type = dr.enums.TARGET_TYPE.MULTICLASS)\\n\",\n    \"\\n\",\n    \"project.wait_for_autopilot() #Wait for autopilot to complete\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Initiating%20Projects/Python/Starting%20a%20Multiclass%20Classification%20Project.ipynb",
        "size": 7306,
        "description": "Jupyter notebook example from Initiating Projects/Python/Starting a Multiclass Classification Project.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Project with Selected Blueprints.ipynb",
        "file_name": "Starting a Project with Selected Blueprints.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Starting a Project with Selected Blueprints\\n\",\n    \"\\n\",\n    \"**Author**: Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"\\n\",\n    \"### Scope\\n\",\n    \"The scope of this notebook is to provide instructions on how to initiate a DataRobot project manually where the user has the option to choose which models/blueprints he wants to initiate. The procedure below should work for any type of problem you are trying to solve (regression, classification, time series, etc).\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.19.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 27,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\\n\",\n    \"import pandas as pd\\n\",\n    \"import numpy as np\\n\",\n    \"import time\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Dataset\\n\",\n    \"We will be loading the breast cancer dataset. A very simple binary classification dataset that is available through sk-learn.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>mean radius</th>\\n\",\n       \"      <th>mean texture</th>\\n\",\n       \"      <th>mean perimeter</th>\\n\",\n       \"      <th>mean area</th>\\n\",\n       \"      <th>mean smoothness</th>\\n\",\n       \"      <th>mean compactness</th>\\n\",\n       \"      <th>mean concavity</th>\\n\",\n       \"      <th>mean concave points</th>\\n\",\n       \"      <th>mean symmetry</th>\\n\",\n       \"      <th>mean fractal dimension</th>\\n\",\n       \"      <th>...</th>\\n\",\n       \"      <th>worst texture</th>\\n\",\n       \"      <th>worst perimeter</th>\\n\",\n       \"      <th>worst area</th>\\n\",\n       \"      <th>worst smoothness</th>\\n\",\n       \"      <th>worst compactness</th>\\n\",\n       \"      <th>worst concavity</th>\\n\",\n       \"      <th>worst concave points</th>\\n\",\n       \"      <th>worst symmetry</th>\\n\",\n       \"      <th>worst fractal dimension</th>\\n\",\n       \"      <th>target</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>17.99</td>\\n\",\n       \"      <td>10.38</td>\\n\",\n       \"      <td>122.80</td>\\n\",\n       \"      <td>1001.0</td>\\n\",\n       \"      <td>0.11840</td>\\n\",\n       \"      <td>0.27760</td>\\n\",\n       \"      <td>0.3001</td>\\n\",\n       \"      <td>0.14710</td>\\n\",\n       \"      <td>0.2419</td>\\n\",\n       \"      <td>0.07871</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>17.33</td>\\n\",\n       \"      <td>184.60</td>\\n\",\n       \"      <td>2019.0</td>\\n\",\n       \"      <td>0.1622</td>\\n\",\n       \"      <td>0.6656</td>\\n\",\n       \"      <td>0.7119</td>\\n\",\n       \"      <td>0.2654</td>\\n\",\n       \"      <td>0.4601</td>\\n\",\n       \"      <td>0.11890</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>20.57</td>\\n\",\n       \"      <td>17.77</td>\\n\",\n       \"      <td>132.90</td>\\n\",\n       \"      <td>1326.0</td>\\n\",\n       \"      <td>0.08474</td>\\n\",\n       \"      <td>0.07864</td>\\n\",\n       \"      <td>0.0869</td>\\n\",\n       \"      <td>0.07017</td>\\n\",\n       \"      <td>0.1812</td>\\n\",\n       \"      <td>0.05667</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>23.41</td>\\n\",\n       \"      <td>158.80</td>\\n\",\n       \"      <td>1956.0</td>\\n\",\n       \"      <td>0.1238</td>\\n\",\n       \"      <td>0.1866</td>\\n\",\n       \"      <td>0.2416</td>\\n\",\n       \"      <td>0.1860</td>\\n\",\n       \"      <td>0.2750</td>\\n\",\n       \"      <td>0.08902</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>19.69</td>\\n\",\n       \"      <td>21.25</td>\\n\",\n       \"      <td>130.00</td>\\n\",\n       \"      <td>1203.0</td>\\n\",\n       \"      <td>0.10960</td>\\n\",\n       \"      <td>0.15990</td>\\n\",\n       \"      <td>0.1974</td>\\n\",\n       \"      <td>0.12790</td>\\n\",\n       \"      <td>0.2069</td>\\n\",\n       \"      <td>0.05999</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>25.53</td>\\n\",\n       \"      <td>152.50</td>\\n\",\n       \"      <td>1709.0</td>\\n\",\n       \"      <td>0.1444</td>\\n\",\n       \"      <td>0.4245</td>\\n\",\n       \"      <td>0.4504</td>\\n\",\n       \"      <td>0.2430</td>\\n\",\n       \"      <td>0.3613</td>\\n\",\n       \"      <td>0.08758</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>11.42</td>\\n\",\n       \"      <td>20.38</td>\\n\",\n       \"      <td>77.58</td>\\n\",\n       \"      <td>386.1</td>\\n\",\n       \"      <td>0.14250</td>\\n\",\n       \"      <td>0.28390</td>\\n\",\n       \"      <td>0.2414</td>\\n\",\n       \"      <td>0.10520</td>\\n\",\n       \"      <td>0.2597</td>\\n\",\n       \"      <td>0.09744</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>26.50</td>\\n\",\n       \"      <td>98.87</td>\\n\",\n       \"      <td>567.7</td>\\n\",\n       \"      <td>0.2098</td>\\n\",\n       \"      <td>0.8663</td>\\n\",\n       \"      <td>0.6869</td>\\n\",\n       \"      <td>0.2575</td>\\n\",\n       \"      <td>0.6638</td>\\n\",\n       \"      <td>0.17300</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>20.29</td>\\n\",\n       \"      <td>14.34</td>\\n\",\n       \"      <td>135.10</td>\\n\",\n       \"      <td>1297.0</td>\\n\",\n       \"      <td>0.10030</td>\\n\",\n       \"      <td>0.13280</td>\\n\",\n       \"      <td>0.1980</td>\\n\",\n       \"      <td>0.10430</td>\\n\",\n       \"      <td>0.1809</td>\\n\",\n       \"      <td>0.05883</td>\\n\",\n       \"      <td>...</td>\\n\",\n       \"      <td>16.67</td>\\n\",\n       \"      <td>152.20</td>\\n\",\n       \"      <td>1575.0</td>\\n\",\n       \"      <td>0.1374</td>\\n\",\n       \"      <td>0.2050</td>\\n\",\n       \"      <td>0.4000</td>\\n\",\n       \"      <td>0.1625</td>\\n\",\n       \"      <td>0.2364</td>\\n\",\n       \"      <td>0.07678</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"<p>5 rows × 31 columns</p>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\\\\n\",\n       \"0        17.99         10.38          122.80     1001.0          0.11840   \\n\",\n       \"1        20.57         17.77          132.90     1326.0          0.08474   \\n\",\n       \"2        19.69         21.25          130.00     1203.0          0.10960   \\n\",\n       \"3        11.42         20.38           77.58      386.1          0.14250   \\n\",\n       \"4        20.29         14.34          135.10     1297.0          0.10030   \\n\",\n       \"\\n\",\n       \"   mean compactness  mean concavity  mean concave points  mean symmetry  \\\\\\n\",\n       \"0           0.27760          0.3001              0.14710         0.2419   \\n\",\n       \"1           0.07864          0.0869              0.07017         0.1812   \\n\",\n       \"2           0.15990          0.1974              0.12790         0.2069   \\n\",\n       \"3           0.28390          0.2414              0.10520         0.2597   \\n\",\n       \"4           0.13280          0.1980              0.10430         0.1809   \\n\",\n       \"\\n\",\n       \"   mean fractal dimension   ...    worst texture  worst perimeter  worst area  \\\\\\n\",\n       \"0                 0.07871   ...            17.33           184.60      2019.0   \\n\",\n       \"1                 0.05667   ...            23.41           158.80      1956.0   \\n\",\n       \"2                 0.05999   ...            25.53           152.50      1709.0   \\n\",\n       \"3                 0.09744   ...            26.50            98.87       567.7   \\n\",\n       \"4                 0.05883   ...            16.67           152.20      1575.0   \\n\",\n       \"\\n\",\n       \"   worst smoothness  worst compactness  worst concavity  worst concave points  \\\\\\n\",\n       \"0            0.1622             0.6656           0.7119                0.2654   \\n\",\n       \"1            0.1238             0.1866           0.2416                0.1860   \\n\",\n       \"2            0.1444             0.4245           0.4504                0.2430   \\n\",\n       \"3            0.2098             0.8663           0.6869                0.2575   \\n\",\n       \"4            0.1374             0.2050           0.4000                0.1625   \\n\",\n       \"\\n\",\n       \"   worst symmetry  worst fractal dimension  target  \\n\",\n       \"0          0.4601                  0.11890     0.0  \\n\",\n       \"1          0.2750                  0.08902     0.0  \\n\",\n       \"2          0.3613                  0.08758     0.0  \\n\",\n       \"3          0.6638                  0.17300     0.0  \\n\",\n       \"4          0.2364                  0.07678     0.0  \\n\",\n       \"\\n\",\n       \"[5 rows x 31 columns]\"\n      ]\n     },\n     \"execution_count\": 2,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"from sklearn.datasets import load_breast_cancer\\n\",\n    \"data = load_breast_cancer()\\n\",\n    \"\\n\",\n    \"df = pd.DataFrame(np.c_[data['data'], data['target']],\\n\",\n    \"                  columns= np.append(data['feature_names'], ['target']))\\n\",\n    \"df.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Connect to DataRobot\\n\",\n    \"Connect to DataRobot using your credentials and your endpoint. Change input below accordingly.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"<datarobot.rest.RESTClientObject at 0x11fc81cf8>\"\n      ]\n     },\n     \"execution_count\": 3,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"dr.Client(token='YOUR_API_KEY', \\n\",\n    \"          endpoint='YOUR_DATAROBOT_HOSTNAME')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Initiate project\\n\",\n    \"We will be initiating the project using <code>autopilot_on = False</code>. This way DataRobot will not start building models until we specify which ones we want to build\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"project = dr.Project.start(project_name='MyBinaryClassificationProject',\\n\",\n    \"                        sourcedata= df,\\n\",\n    \"                        autopilot_on = False,\\n\",\n    \"                        target='target')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Find all of the blueprints\\n\",\n    \"We can use the <code>get_blueprints</code> method to see all of the blueprints DataRobot generated.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 22,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"blueprints = project.get_blueprints()\\n\",\n    \"\\n\",\n    \"#Now that we have the Blueprints, we can search for a specific blueprint. \\n\",\n    \"#For example all models that have \\\"Gradient\\\" in their name\\n\",\n    \"\\n\",\n    \"models_to_run = []\\n\",\n    \"for blueprint in blueprints:\\n\",\n    \"    if 'Gradient' in blueprint.model_type:\\n\",\n    \"        models_to_run.append(blueprint)\\n\",\n    \"        \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 23,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"[Blueprint(Gradient Boosted Trees Classifier),\\n\",\n       \" Blueprint(Stochastic Gradient Descent Classifier),\\n\",\n       \" Blueprint(Light Gradient Boosted Trees Classifier with Early Stopping),\\n\",\n       \" Blueprint(Gradient Boosted Greedy Trees Classifier),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier with Early Stopping),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier (learning rate =0.01)),\\n\",\n       \" Blueprint(Light Gradient Boosting on ElasticNet Predictions ),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier (learning rate =0.01)),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier (learning rate =0.01)),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier with Unsupervised Learning Features),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier with Early Stopping),\\n\",\n       \" Blueprint(Gradient Boosted Trees Classifier),\\n\",\n       \" Blueprint(eXtreme Gradient Boosted Trees Classifier with Early Stopping - Forest (10x))]\"\n      ]\n     },\n     \"execution_count\": 23,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"models_to_run\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Lets now initiate these models\\n\",\n    \"We can use the <code>train</code> method to initiate modeling for a specific blueprint. By default, the feature list used will be the <code>informative features </code> list produced by DataRobot but you can define your own feature list and pass it on the <code>featurelist_id</code> variable.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 24,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"for model in models_to_run:\\n\",\n    \"    project.train(model, sample_pct = 80, featurelist_id=None)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Waiting for job completion\\n\",\n    \"We can use the <code>get_all_jobs</code> method to wait for the models to finish running\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 28,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"while len(project.get_all_jobs()) > 0:\\n\",\n    \"    time.sleep(1)\\n\",\n    \"    pass\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Initiating%20Projects/Python/Starting%20a%20Project%20with%20Selected%20Blueprints.ipynb",
        "size": 15309,
        "description": "Jupyter notebook example from Initiating Projects/Python/Starting a Project with Selected Blueprints.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      },
      {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Regression Project.ipynb",
        "file_name": "Starting a Regression Project.ipynb",
        "file_type": "notebook",
        "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Starting a Regression Project\\n\",\n    \"\\n\",\n    \"**Author**: Thodoris Petropoulos\\n\",\n    \"\\n\",\n    \"**Label**: Modeling Options\\n\",\n    \"### Scope\\n\",\n    \"The scope of this notebook is to provide instructions on how to initiate a DataRobot project for a numerical target using the R API.\\n\",\n    \"\\n\",\n    \"### Background\\n\",\n    \"Regression Analysis is the task of predicting the value of a continuous target column.\\n\",\n    \"\\n\",\n    \"Examples:\\n\",\n    \"\\n\",\n    \"- Predict Life Time Value (LTV) of customer.\\n\",\n    \"- Predicting player performance.\\n\",\n    \"- Predicting house price.\\n\",\n    \"\\n\",\n    \"The target column will always be a continuous numeric variable even though regression could also be applicable a discreet high cardinality variable.\\n\",\n    \"\\n\",\n    \"### Requirements\\n\",\n    \"\\n\",\n    \"- Python version 3.7.3\\n\",\n    \"-  DataRobot API version 2.19.0. \\n\",\n    \"Small adjustments might be needed depending on the Python version and DataRobot API version you are using.\\n\",\n    \"\\n\",\n    \"Full documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Libraries\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import datarobot as dr\\n\",\n    \"import pandas as pd\\n\",\n    \"import numpy as np\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Import Dataset\\n\",\n    \"We will be loading the Boston Housing dataset. A very simple dataset for regression that is available through sk-learn.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>CRIM</th>\\n\",\n       \"      <th>ZN</th>\\n\",\n       \"      <th>INDUS</th>\\n\",\n       \"      <th>CHAS</th>\\n\",\n       \"      <th>NOX</th>\\n\",\n       \"      <th>RM</th>\\n\",\n       \"      <th>AGE</th>\\n\",\n       \"      <th>DIS</th>\\n\",\n       \"      <th>RAD</th>\\n\",\n       \"      <th>TAX</th>\\n\",\n       \"      <th>PTRATIO</th>\\n\",\n       \"      <th>B</th>\\n\",\n       \"      <th>LSTAT</th>\\n\",\n       \"      <th>target</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>0.00632</td>\\n\",\n       \"      <td>18.0</td>\\n\",\n       \"      <td>2.31</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.538</td>\\n\",\n       \"      <td>6.575</td>\\n\",\n       \"      <td>65.2</td>\\n\",\n       \"      <td>4.0900</td>\\n\",\n       \"      <td>1.0</td>\\n\",\n       \"      <td>296.0</td>\\n\",\n       \"      <td>15.3</td>\\n\",\n       \"      <td>396.90</td>\\n\",\n       \"      <td>4.98</td>\\n\",\n       \"      <td>24.0</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>0.02731</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>7.07</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.469</td>\\n\",\n       \"      <td>6.421</td>\\n\",\n       \"      <td>78.9</td>\\n\",\n       \"      <td>4.9671</td>\\n\",\n       \"      <td>2.0</td>\\n\",\n       \"      <td>242.0</td>\\n\",\n       \"      <td>17.8</td>\\n\",\n       \"      <td>396.90</td>\\n\",\n       \"      <td>9.14</td>\\n\",\n       \"      <td>21.6</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>0.02729</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>7.07</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.469</td>\\n\",\n       \"      <td>7.185</td>\\n\",\n       \"      <td>61.1</td>\\n\",\n       \"      <td>4.9671</td>\\n\",\n       \"      <td>2.0</td>\\n\",\n       \"      <td>242.0</td>\\n\",\n       \"      <td>17.8</td>\\n\",\n       \"      <td>392.83</td>\\n\",\n       \"      <td>4.03</td>\\n\",\n       \"      <td>34.7</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>0.03237</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>2.18</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.458</td>\\n\",\n       \"      <td>6.998</td>\\n\",\n       \"      <td>45.8</td>\\n\",\n       \"      <td>6.0622</td>\\n\",\n       \"      <td>3.0</td>\\n\",\n       \"      <td>222.0</td>\\n\",\n       \"      <td>18.7</td>\\n\",\n       \"      <td>394.63</td>\\n\",\n       \"      <td>2.94</td>\\n\",\n       \"      <td>33.4</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>0.06905</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>2.18</td>\\n\",\n       \"      <td>0.0</td>\\n\",\n       \"      <td>0.458</td>\\n\",\n       \"      <td>7.147</td>\\n\",\n       \"      <td>54.2</td>\\n\",\n       \"      <td>6.0622</td>\\n\",\n       \"      <td>3.0</td>\\n\",\n       \"      <td>222.0</td>\\n\",\n       \"      <td>18.7</td>\\n\",\n       \"      <td>396.90</td>\\n\",\n       \"      <td>5.33</td>\\n\",\n       \"      <td>36.2</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\\\\n\",\n       \"0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \\n\",\n       \"1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \\n\",\n       \"2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \\n\",\n       \"3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \\n\",\n       \"4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \\n\",\n       \"\\n\",\n       \"   PTRATIO       B  LSTAT  target  \\n\",\n       \"0     15.3  396.90   4.98    24.0  \\n\",\n       \"1     17.8  396.90   9.14    21.6  \\n\",\n       \"2     17.8  392.83   4.03    34.7  \\n\",\n       \"3     18.7  394.63   2.94    33.4  \\n\",\n       \"4     18.7  396.90   5.33    36.2  \"\n      ]\n     },\n     \"execution_count\": 2,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"from sklearn.datasets import load_boston\\n\",\n    \"data = load_boston()\\n\",\n    \"\\n\",\n    \"df = pd.DataFrame(np.c_[data['data'], data['target']],\\n\",\n    \"                  columns= np.append(data['feature_names'], ['target']))\\n\",\n    \"df.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Connect to DataRobot\\n\",\n    \"Connect to DataRobot using your credentials and your endpoint. Change input below accordingly.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"dr.Client(token='YOUR_API_KEY', \\n\",\n    \"          endpoint='YOUR_DATAROBOT_HOSTNAME')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Initiate Project\\n\",\n    \"I will be initiating a project calling the method <code>dr.Project.start</code>:\\n\",\n    \"* project_name: Name of project\\n\",\n    \"* source_data: Data source (Path to file or pandas dataframe)\\n\",\n    \"* target: String with target variable name\\n\",\n    \"* worker_count: Amount of workers to use\\n\",\n    \"* metric: Optimisation metric to use\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"project = dr.Project.start(project_name='MyRegressionProject',\\n\",\n    \"                        sourcedata= df,\\n\",\n    \"                        target='target')\\n\",\n    \"project.wait_for_autopilot() #Wait for autopilot to complete\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n",
        "url": "https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Initiating%20Projects/Python/Starting%20a%20Regression%20Project.ipynb",
        "size": 8552,
        "description": "Jupyter notebook example from Initiating Projects/Python/Starting a Regression Project.ipynb",
        "tags": [
          "datarobot-example",
          "jupyter-notebook"
        ]
      }
    ],
    "readme_content": "",
    "topics": []
  }
]