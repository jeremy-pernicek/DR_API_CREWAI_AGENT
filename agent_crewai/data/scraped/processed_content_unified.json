{
  "processed_at": "2025-08-23T12:46:49.285044",
  "total_items": 360,
  "content_items": [
    {
      "id": "readthedocs_-6670105666492849321",
      "title": "Python API client",
      "content": "Skip to content\nDataRobot API resources\n>\nAPI reference documentation\n>\nPython API client\nPython API client¶\nThe DataRobot Python client is a library for working with the DataRobot API. To access other clients and additional information about DataRobot’s APIs, visit the API documentation home.\nThe reference documentation outlines the functionality supported by the Python client. For information about specific endpoints, select a topic from the table of contents on the left.\nTo get started with the Python client, reference DataRobot’s API Quickstart guide. This guide outlines how to configure your environment to use the API.\nYou can learn about use cases and experiment with code examples using the Python client in the API user guide.\nIn addition to code examples and use cases, you can browse AI accelerators. AI accelerators are designed to help speed up model experimentation, development, and production readiness using the DataRobot API. They codify and package data science expertise in building and delivering successful machine learning projects into repeatable, code-first workflows and modular building blocks.\nBack to top",
      "content_type": "template",
      "source_type": "readthedocs",
      "source_file": "https://docs.datarobot.com/en/docs/api/reference/sdk/index.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://docs.datarobot.com/en/docs/api/reference/sdk/index.html",
        "content_length": 1140
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.25,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2112999024024695479",
      "title": "Python API client user guide",
      "content": "Skip to content\nDataRobot API resources\n>\nAPI user guide\n>\nPython API client user guide\nPython API client user guide¶\nTopic\nDescription\nAdministration\nHow to manage DataRobot Self-Managed AI Platform deployments.\nData\nHow to manage data for machine learning, including importing and transforming data, and connecting to data sources.\nMLOps\nHow to deploy, monitor, manage, and govern your models in production.\nModeling\nHow to set modeling parameters before building, use the modeling workflow, and manage models and projects.\nPredictions\nHow to get predictions (“scoring”) on new data from a model.\nUse Cases\nHow to use Use Cases to group everything related to solving a specific business problem.\nBack to top",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://docs.datarobot.com/en/docs/api/guide/python/index.html",
      "tags": [
        "api-reference",
        "guide"
      ],
      "metadata": {
        "page_type": "guide",
        "url": "https://docs.datarobot.com/en/docs/api/guide/python/index.html",
        "content_length": 709
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.75,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-194105260010035865",
      "title": "API quickstart",
      "content": "Skip to content\nDataRobot API resources\n>\nAPI quickstart\nAPI quickstart¶\nThe DataRobot API provides a programmatic alternative to the web interface for creating and managing DataRobot projects. The API can be used via REST, or DataRobot's Python or R clients in Windows, UNIX, and OS X environments. This guide walks you through setting up your environment and then you can follow a sample problem that outlines an end-to-end workflow for the API.\nNote\nThe API quickstart guide uses methods for 3.x versions of DataRobot's Python client. If you are a Self-Managed AI Platform user, consult the API resources page to verify which versions of DataRobot's clients are supported for your version of the DataRobot application.\nPrerequisites¶\nBefore proceeding, access and install the DataRobot client package for Python or R (instructions are provided below). Review the API Reference documentation to familiarize yourself with the code-first resources available to you.\nPythonR\nThe following prerequisites are for 3.x versions of DataRobot's Python client:\nPython >=3.7\nA registered DataRobot account\npip\nR >= 3.2\nhttr (≥ 1.2.0)\njsonlite (≥ 1.0)\nyaml (≥ 2.1.19)\nA registered DataRobot account\nInstall the client¶\nNote\nSelf-Managed AI Platform users may want to install a previous version of the client in order to match their installed version of the DataRobot application. Reference the available versions to map your installation to the correct version of the API client.\nPythonR\npip install datarobot datarobot-predict\n(Optional) If you would like to build custom blueprints programmatically, install two additional packages: graphviz and blueprint-workshop.\nFor Windows users:\nDownload the graphviz installer\nFor Ubuntu users:\nsudo apt-get install graphviz\nFor Mac users:\nbrew install graphviz\nOnce graphviz is installed, install the workshop:\npip install datarobot-bp-workshop\ninstall.packages(“datarobot”)\nConfigure your environment¶\nThis section walks through how to execute a complete modeling workflow using the DataRobot API, from uploading a dataset to making predictions on a model deployed in a production environment.\nCreate a DataRobot API key¶\nFrom the DataRobot UI, click the user icon in the top right corner and select API keys and tools.\nClick Create new key.\nName the new key, and click Create. The key is activated and ready for use.\nOnce created, each individual key has three pieces of information:\nLabel\nElement\nDescription\n1\nName\nThe name of the key, which you can edit.\n2\nKey\nThe key value.\n3\nDate created\nThe date the key was created. Newly created and not yet used keys display “—”.\n4\nLast used\nThe date the key was last used.\nRetrieve the API endpoint¶\nDataRobot provides several deployment options to meet your business requirements. Each deployment type has its own set of endpoints. Choose from the tabs below:\nAI Platform (US)AI Platform (EU)AI Platform (JP)Self-Managed AI Platform\nThe AI Platform (US) offering is primarily accessed by US users. It can be accessed at https://app.datarobot.com.\nAPI endpoint root: https://app.datarobot.com/api/v2\nThe AI Platform (EU) offering is primarily accessed by EMEA users. It can be accessed at https://app.eu.datarobot.com.\nAPI endpoint root: https://app.eu.datarobot.com/api/v2\nThe AI Platform (JP) offering is primarily accessed by users in Japan. It can be accessed at https://app.jp.datarobot.com.\nAPI endpoint root: https://app.jp.datarobot.com/api/v2\nFor Self-Managed AI Platform users, the API root will be the same as your DataRobot UI root. In the URL below, replace {datarobot.example.com} with your deployment endpoint.\nAPI endpoint root: https://{datarobot.example.com}/api/v2\nConfigure API authentication¶\nTo authenticate with DataRobot's API, your code needs to have access to an endpoint and token from the previous steps. This can be done in three ways:\ndrconfig.yamlEnvironment variablesEmbed in your code\nDataRobot's recommended authentication method is to use a drconfig.yaml file. This is a file that the DataRobot Python and R clients automatically look for. You can instruct the API clients to look for the file in a specific location, ~/.config/datarobot/drconfig.yaml by default, or under a unique name. Therefore, you can leverage this to have multiple config files. The example below demonstrates the format of the .yaml:\nendpoint: 'https://app.datarobot.com/api/v2'\ntoken: 'NjE3ZjA3Mzk0MmY0MDFmZGFiYjQ0MztergsgsQwOk9G'\nOnce created, you can test your access to the API.\nFor Python:\nIf the config file is located at ~/.config/datarobot/drconfig.yaml, then all you need to do is import the library:\nimport datarobot as dr\nOtherwise, use the following command:\nimport datarobot as dr\ndr.Client(config_path = \"<file-path-to-drconfig.yaml>\")\nFor R:\nIf the config file is located at ~/.config/datarobot/drconfig.yaml, then all you need to do is load the library:\nlibrary(datarobot)\nOtherwise, use the following command:\nConnectToDataRobot(configPath = \"<file-path-to-drconfig.yaml>\"))\nFor Windows:\nFor Windows users, open the Command Prompt or PowerShell as an administrator and set the following environment variables:\nsetx DATAROBOT_ENDPOINT \"https://app.datarobot.com/api/v2\"\nsetx DATAROBOT_API_TOKEN \"your_api_token\"\nOnce set, close and reopen the Command Prompt or PowerShell for the changes to take effect.\nTo configure persisting environment variables on Windows, search for \"Environment Variables\" in the Start menu and select Edit the system environment variables.\nThen, click Environment Variables and, under System variables, click New to add the variables shown above.\nFor macOS and Linux:\nFor macOS and Linux users, open a terminal window and set the following environment variables:\nexport DATAROBOT_ENDPOINT=\"https://app.datarobot.com/api/v2\"\nexport DATAROBOT_API_TOKEN=\"your_api_token\"\nTo configure persisting environment variables on macOS or Linux, edit the shell configuration file (~/.bash_profile, ~/.bashrc, or ~/.zshrc) and add the environment variables shown above. Then, save the file and restart your terminal or run source ~/.bash_profile (or use any relevant file).\nOnce the environment variables are set, authenticate to connect to DataRobot.\nFor Python:\nimport datarobot as dr\ndr.Project.list()\nFor R:\nlibrary(datarobot)\nFor cURL:\ncurl --location -X GET \"${DATAROBOT_API_ENDPOINT}/projects\" --header \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\"\n(Optional) Be cautious to never commit your credentials to Git.\nFor Python:\nimport datarobot as dr\ndr.Client(endpoint='https://app.datarobot.com/api/v2', token='NjE3ZjA3Mzk0MmY0MDFmZGFiYjQ0MztergsgsQwOk9G')\nFor R:\nConnectToDataRobot(endpoint =\n\"https://app.datarobot.com/api/v2\",\ntoken =\n'NjE3ZjA3Mzk0MmY0MDFmZGFiYjQ0MztergsgsQwOk9G')\nFor cURL:\nGET https://app.datarobot.com/api/v2/ HTTP/1.1\nAuthorization: Bearer DnwzBUNTOtKBO6Sp1hoUByG4YgZwCCw4\nUse the API: Predicting fuel economy¶\nOnce the API credentials, endpoints, and environment are configured, use the DataRobot API to follow this example.\nThe example uses the Python client and the REST API (using cURL), so a basic understanding of Python3 or cURL is required.\nIt progresses through a simple problem: predicting the miles-per-gallon fuel economy from known automobile data (e.g., vehicle weight, number of cylinders, etc.).\nFor additional code examples, reference DataRobot's AI accelerators.\nNote\nThe following workflow uses methods introduced in version 3.0 of the Python client. Ensure that the client is up-to-date before executing the code included in this example.\nThe following sections provide sample code for Python and cURL that will:\nUpload a dataset.\nTrain a model to learn from the dataset.\nTest prediction outcomes on the model with new data.\nDeploy the model.\nPredict outcomes on the deployed model using new data.\nUpload a dataset¶\nThe first step to create a project is uploading a dataset. This example uses the dataset auto-mpg.csv and its supporting test dataset, auto-mpg-test.csv, both of which you can download here.\nPythoncURLR\nimport datarobot as dr\ndr.Client(config_path = \"./drconfig.yaml\")\n# Set to the location of your auto-mpg.csv and auto-mpg-test.csv data files\n# Example: dataset_file_path = '/Users/myuser/Downloads/auto-mpg.csv'\ntraining_dataset_file_path = './auto-mpg.csv'\ntest_dataset_file_path = './auto-mpg-test.csv'\nprint(\"--- Starting DataRobot Model Training Script ---\")\n# Load dataset\ntraining_dataset = dr.Dataset.create_from_file(training_dataset_file_path)\n# Create a new project based on dataset\nproject = dr.Project.create_from_dataset(training_dataset.id, project_name='Auto MPG DR-Client')\nDATAROBOT_API_TOKEN=${DATAROBOT_API_TOKEN}\nDATAROBOT_ENDPOINT=${DATAROBOT_ENDPOINT}\nlocation=$(curl -Lsi \\\n-X POST \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n-F 'projectName=\"Auto MPG\"' \\\n-F \"file=@${DATASET_FILE_PATH}\" \\\n\"${DATAROBOT_ENDPOINT}\"/projects/ | grep -i 'Location: .*$' | \\\ncut -d \" \" -f2 | tr -d '\\r')\necho \"Uploaded dataset. Checking status of project at: ${location}\"\nwhile true; do\nproject_id=$(curl -Ls \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \"${location}\" \\\n| grep -Eo 'id\":\\s\"\\w+' | cut -d '\"' -f3 | tr -d '\\r')\nif [ \"${project_id}\" = \"\" ]\nthen\necho \"Setting up project...\"\nsleep 10\nelse\necho \"Project setup complete.\"\necho \"Project ID: ${project_id}\"\nbreak\nfi\ndone\n# Set to the location of your auto-mpg.csv and auto-mpg-test.csv data files\n# Example: dataset_file_path = '/Users/myuser/Downloads/auto-mpg.csv'\ntraining_dataset_file_path <- \"\"\ntest_dataset_file_path <- \"\"\ntraining_dataset <- utils::read.csv(training_dataset_file_path)\ntest_dataset <- utils::read.csv(test_dataset_file_path)\nhead(training_dataset)\nproject <- SetupProject(dataSource = training_dataset, projectName = \"Auto MPG DR-Client\", maxWait = 60 * 60)\nTrain models¶\nNow that DataRobot has data, it can use the data to train and build models with Autopilot.\nAutopilot is DataRobot's \"survival of the fittest\" modeling mode that automatically selects the best predictive models for the specified target feature and runs them at increasing sample sizes.\nThe outcome of Autopilot is not only a selection of best-suited models, but also identification of a recommended model—the model that best understands how to predict the target feature \"mpg\".\nChoosing the best model is a balance of accuracy, metric performance, and model simplicity.\nYou can read more about the model recommendation process in the UI documentation.\nNote\nThis code opens a browser window to display progress in the DataRobot classic UI. Once the window has opened, click the NextGen UI drop-down and select Console to view the deployment once it is complete.\nPythoncURLR\n# Use training data to build models\nfrom datarobot import AUTOPILOT_MODE\n# Set the project's target and initiate Autopilot (runs in Quick mode unless a different mode is specified)\nproject.analyze_and_model(target='mpg', worker_count=-1, mode=AUTOPILOT_MODE.QUICK)\nprint(\"\\nAutopilot is running. This may take some time...\")\nproject.wait_for_autopilot()\nprint(\"Autopilot has completed!\")\n# Open the project in a web browser to view progress\nprint(\"Opening the project in your default web browser to view real-time events...\")\nproject.open_in_browser()\n# Get the recommended model (the best model for deployment)\nprint(\"\\nRetrieving the best model from the Leaderboard...\")\nbest_model = project.recommended_model()\nprint(f\"Best Model Found:\")\nprint(f\"  - Model Type: {best_model.model_type}\")\nprint(f\"  - Blueprint ID: {best_model.blueprint_id}\")\nresponse=$(curl -Lsi \\\n-X PATCH \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n-H \"Content-Type: application/json\" \\\n--data '{\"target\": \"mpg\", \"mode\": \"quick\"}' \\\n\"${DATAROBOT_ENDPOINT}/projects/${project_id}/aim\" | grep 'location: .*$' \\\n| cut -d \" \" | tr -d '\\r')\necho \"AI training initiated. Checking status of training at: ${response}\"\nwhile true; do\ninitial_project_status=$(curl -Ls \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \"${response}\" \\\n| grep -Eo 'stage\":\\s\"\\w+' | cut -d '\"' -f3 | tr -d '\\r')\nif [ \"${initial_project_status}\" = \"\" ]\nthen\necho \"Setting up AI training...\"\nsleep 10\nelse\necho \"Training AI.\"\necho \"Grab a coffee or catch up on email.\"\nbreak\nfi\ndone\nproject_status=$(curl -Lsi \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n\"${DATAROBOT_ENDPOINT}/projects/${project_id}/status\" \\\n| grep -Eo 'autopilotDone\":\\strue'\n)\nif [ \"${project_status}\" = \"\" ]\nthen\necho \"Autopilot training in progress...\"\nsleep 60\nelse\necho \"Autopilot training complete. Model ready to deploy.\"\nbreak\nfi\ndone\n# Set the project target and initiate Autopilot\nSetTarget(project,\ntarget = \"mpg\")\n# Block execution until Autopilot is complete\nWaitForAutopilot(project)\nmodel <- GetRecommendedModel(project, type = RecommendedModelType$RecommendedForDeployment)\nDeploy the model¶\nDeployment is the method by which you integrate a machine learning model into an existing production environment to make predictions with live data and generate insights. See the deployment overview for more information.\nPythoncURLR\n# Deploy the model to a serverless prediction environment\nprint(\"\\nDeploying the model to a serverless prediction environment...\")\n# Find or create a serverless prediction environment\nserverless_env = None\nfor env in dr.PredictionEnvironment.list():\nif env.platform == 'datarobotServerless':\nserverless_env = env\nbreak\nif serverless_env is None:\nprint(\"Creating a new serverless prediction environment...\")\nserverless_env = dr.PredictionEnvironment.create(\nname=\"Auto MPG Serverless Environment\",\nplatform='datarobotServerless'\n)\n# First, register the model to create a registered model version\nprint(\"Registering the model...\")\n# Check if the registered model already exists\nregistered_model_name = \"Auto MPG Registered Model\"\nexisting_models = [m for m in dr.RegisteredModel.list() if m.name == registered_model_name]\nif existing_models:\nprint(f\"Using existing registered model: {registered_model_name}\")\nregistered_model = existing_models[0]\n# Create a new version of the existing model\nregistered_model_version = dr.RegisteredModelVersion.create_for_leaderboard_item(\nbest_model.id,\nname=\"Auto MPG Model\",\nregistered_model_id=registered_model.id\n)\nelse:\nprint(f\"Creating new registered model: {registered_model_name}\")\n# Create a new registered model\nregistered_model_version = dr.RegisteredModelVersion.create_for_leaderboard_item(\nbest_model.id,\nname=\"Auto MPG Model\",\nregistered_model_name=registered_model_name\n)\n# Retrieve the newly created registered model object by ID\nregistered_model = dr.RegisteredModel.get(registered_model_version.registered_model_id)\n# Wait for the model build to complete\nprint(\"Waiting for model build to complete...\")\nwhile True:\ncurrent_version = registered_model.get_version(registered_model_version.id)\nif current_version.build_status in ('READY', 'complete'):\nprint(\"Model build completed successfully!\")\nregistered_model_version = current_version  # Update our reference\nbreak\nelif current_version.build_status == 'FAILED':\nraise Exception(\"Model build failed. Please check the model registration.\")\nelse:\nprint(f\"Build status: {current_version.build_status}. Waiting...\")\nimport time\ntime.sleep(30)  # Wait 30 seconds before checking again\n# Deploy the model to the serverless environment using the registered model version\ndeployment = dr.Deployment.create_from_registered_model_version(\nregistered_model_version.id,\nlabel=\"Auto MPG Predictions\",\ndescription=\"Deployed with DataRobot client for Auto MPG predictions\",\nprediction_environment_id=serverless_env.id\n)\nprint(f\"Model deployed successfully! Deployment ID: {deployment.id}\")\nrecommended_model_id=$(curl -s \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n\"${DATAROBOT_ENDPOINT}/projects/${project_id}/recommendedModels\"\\\n\"/recommendedModel/\" \\\n| grep -Eo 'modelId\":\\s\"\\w+' | cut -d '\"' -f3 | tr -d '\\r')\nserver_data=$(curl -s -X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n\"${DATAROBOT_ENDPOINT}/predictionServers/\")\ndefault_server_id=$(echo $server_data \\\n| grep -Eo 'id\":\\s\"\\w+' | cut -d '\"' -f3 | tr -d '\\r')\nserver_url=$(echo $server_data | grep -Eo 'url\":\\s\".*?\"' \\\n| cut -d '\"' -f3 | tr -d '\\r')\nserver_key=$(echo $server_data | grep -Eo 'datarobot-key\":\\s\".*?\"' \\\n| cut -d '\"' -f3 | tr -d '\\r')\nrequest_data=\"{\\\n\\\"defaultPredictionServerId\\\":\\\"${default_server_id}\\\",\\\n\\\"modelId\\\":\\\"${recommended_model_id}\\\",\\\n\\\"description\\\":\\\"Deployed with cURL\\\",\\\n\\\"label\\\":\\\"MPG Prediction Server\\\"\\\n}\"\ndeployment_response=$(curl -Lsi -X POST \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n-H \"Content-Type: application/json\" \\\n--data \"${request_data}\" \\\n\"${DATAROBOT_ENDPOINT}/deployments/fromLearningModel/\")\ndeploy_response_code_202=$(echo $deployment_response | grep -Eo 'HTTP/2 202')\nif [ \"${deploy_response_code_202}\" = \"\" ]\nthen\ndeployment_id=$(echo \"$deployment_response\" | grep -Eo 'id\":\\s\"\\w+' \\\n| cut -d '\"' -f3 | tr -d '\\r')\necho \"Prediction server ready.\"\nelse\ndeployment_status=$(echo \"$deployment_response\" | grep -Eo 'location: .*$' \\\n| cut -d \" \" | tr -d '\\r')\nwhile true; do\ndeployment_ready=$(curl -Ls \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \"${deployment_status}\" \\\n| grep -Eo 'id\":\\s\"\\w+' | cut -d '\"' -f3 | tr -d '\\r')\nif [ \"${deployment_ready}\" = \"\" ]\nthen\necho \"Waiting for deployment...\"\nsleep 10\nelse\ndeployment_id=$deployment_ready\necho \"Prediction server ready.\"\nbreak\nfi\ndone\nfi\npredictionServer <- ListPredictionServers()[[1]]\ndeployment <- CreateDeployment(model,\nlabel = \"MPG Prediction Server\",\ndescription = \"Deployed with DataRobot client\",\ndefaultPredictionServerId = predictionServer)\nMake predictions against the deployed model¶\nWhen you have successfully deployed a model, you can use the DataRobot Prediction API to further test the model by making predictions on new data. This allows you to access advanced model management features such as data drift, accuracy, and service health statistics.\nDataRobot offers several methods for making predictions on new data. You can read more about prediction methods in the UI documentation. You can also reference a Python prediction snippet from the UI. Navigate to the Deployments page, select your deployment, and go to Predictions > Prediction API to reference the snippet for making predictions.\nPythoncURLR\nThis code makes predictions on the model using the test set you identified in the first step (test_dataset_file_path), when you uploaded data.\n# Make predictions on test data\nprint(\"\\nMaking predictions on test data...\")\n# Read the test data directly\nimport pandas as pd\nfrom datarobot_predict.deployment import predict\ntest_data = pd.read_csv(test_dataset_file_path)\n# Use datarobot-predict for deployment predictions\npredictions, response_headers = predict(deployment, test_data)\n# Display the results\nprint(\"\\nPrediction Results:\")\nprint(predictions.head())\nprint(f\"\\nTotal predictions made: {len(predictions)}\")\nThis code makes predictions on the recommended model using the test set identified in the first step (test_dataset_file_path), when you uploaded data.\n# Test predictions on new data\n# shellcheck disable=SC2089\nprediction_location=$(curl -Lsi\\\n-X POST \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n-F \"file=@${TEST_DATASET_FILE_PATH}\" \\\n\"${DATAROBOT_ENDPOINT}/projects/${project_id}/predictionDatasets/fileUploads/\"\\\n| grep -i 'location: .*$' | cut -d \" \" -f2 | tr -d '\\r')\necho \"Uploaded prediction dataset. Checking status of upload at: ${prediction_location}\"\nwhile true; do\nprediction_dataset_id=$(curl -Ls \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \"${prediction_location}\" \\\n| grep -Eo 'id\":\\s\"\\w+' | cut -d '\"' -f3 | tr -d '\\r')\nif [ \"${prediction_dataset_id}\" = \"\" ]\nthen\necho \"Uploading predictions...\"\nsleep 10\nelse\necho \"Predictions upload complete.\"\necho \"Predictions dataset ID: ${prediction_dataset_id}\"\nbreak\nfi\ndone\nprediction_request_data=\"{\\\n\\\"modelId\\\":\\\"${recommended_model_id}\\\",\\\n\\\"datasetId\\\":\\\"${prediction_dataset_id}\\\"\\\n}\"\npredict_job=$(curl -Lsi \\\n-X POST \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \\\n--data \"${prediction_request_data}\" \\\n\"${DATAROBOT_ENDPOINT}/projects/${project_id}/predictions/\"\\\n| grep -i 'location: .*$' | cut -d \" \" -f2 | tr -d '\\r')\nwhile true; do\ninitial_job_response=$(curl -Ls \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \"${predict_job}\" \\\n| grep -Eo 'status\":\\s\"\\w+' | cut -d '\"' -f3 | tr -d '\\r')\nif [ \"${initial_job_status}\" = \"inprogress\" ]\nthen\necho \"Generating predictions...\"\nsleep 10\nelse\necho \"Predictions complete.\"\nbreak\nfi\ndone\ncurl -Ls \\\n-X GET \\\n-H \"Authorization: Bearer ${DATAROBOT_API_TOKEN}\" \"${predict_job}\"\nThis code makes predictions on the recommended model using the test set you identified in the first step (test_dataset_file_path), when you uploaded data.\n# Uploading the testing dataset\nscoring <- UploadPredictionDataset(project, dataSource = test_dataset)\n# Requesting prediction\npredict_job_id <- RequestPredictions(project, modelId = model$modelId, datasetId = scoring$id)\n# Grabbing predictions\npredictions_prob <- GetPredictions(project, predictId = predict_job_id, type = \"probability\")\nhead(predictions_prob)\nLearn more¶\nAfter getting started with DataRobot's APIs, navigate to the user guide for overviews, Jupyter notebooks, and task-based tutorials that help you find complete examples of common data science and machine learning workflows. Browse AI accelerators to try out repeatable, code-first workflows and modular building blocks. You can also read the reference documentation available for the REST API and Python API client.\nBack to top",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://docs.datarobot.com/en/docs/api/api-quickstart/index.html",
      "tags": [
        "api-reference",
        "advanced",
        "beginner",
        "example",
        "tutorial"
      ],
      "metadata": {
        "page_type": "tutorial",
        "url": "https://docs.datarobot.com/en/docs/api/api-quickstart/index.html",
        "content_length": 21622
      },
      "code_examples": [],
      "api_methods": [
        "project.wait_for_autopilot",
        "dr.registeredmodel.list",
        "deployment.create_from_registered_model_version",
        "datarobot.example.com",
        "dr.project.create_from_dataset",
        "model.get_version",
        "dr.dataset.create_from_file",
        "model.id",
        "dr.deployment.create_from_registered_model_version",
        "project.list",
        "deployment.id",
        "dr.project.list",
        "dr.predictionenvironment.create",
        "model.blueprint_id",
        "project.open_in_browser",
        "model.get",
        "project.recommended_model",
        "model.model_type",
        "dr.registeredmodel.get",
        "project.analyze_and_model",
        "model.list",
        "project.create_from_dataset",
        "dr.predictionenvironment.list",
        "dr.registeredmodelversion.create_for_leaderboard_item"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_8850226802486059049",
      "title": "API reference documentation",
      "content": "Skip to content\nDataRobot API resources\n>\nAPI reference documentation\nAPI reference documentation¶\nThe table below outlines the reference documentation available for DataRobot's API, SDKs, and code-first tools.\nResource\nDescription\nREST API\nThe DataRobot REST API provides a programmatic alternative to the UI for creating and managing DataRobot assets. It allows you to automate processes and iterate more quickly, and lets you use DataRobot with scripted control. The API provides an intuitive modeling and prediction interface.\nOpenAPI specification\nReference the OpenAPI specification for the DataRobot REST API, which helps automate the generation of a client for languages that DataRobot doesn't directly support. It also assists with the design, implementation, and testing integration with DataRobot's REST API using a variety of automated OpenAPI-compatible tools. Note that accessing the OpenAPI spec requires you to be logged into the DataRobot application.\nPython API client\nInstallation, configuration, and usage guidelines for working with the Python client library.\nR client\nInstallation, configuration, and reference documentation for working with the R client library.\nBlueprint workshop\nConstruct and modify DataRobot blueprints and their tasks using a programmatic interface.\nDataRobot Model Metrics\nThe DataRobot Model Metrics library provides the tools necessary to compute model metrics over time and produce aggregated metrics.\nPrediction API\nGenerate predictions with a deployment by submitting JSON or CSV input data via a POST request.\nBatch Prediction API\nScore large datasets with flexible options for intake and output using the prediction servers you have deployed via the Batch Prediction API.\nCode-first tools\nReview the various programmatic tools DataRobot has to offer in addition to the APIs.\nAPI changelogs\nChangelogs contain curated, ordered lists of notable changes for each versioned release for DataRobot's SDKs and REST API.\nCovalent\nA code-first solution that simplifies building and scaling complex AI and high-performance computing applications.\nDeclarative API\nA Terraform-native declarative API used to programmatically provision DataRobot entities such as models, deployments, applications, and more.\nTroubleshoot the Python client\nOutlines cases that can cause issues with using the Python client and provides known fixes.\nSelf-managed resources\nDetails the resources available for self-managed DataRobot deployments.\nBack to top",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://docs.datarobot.com/en/docs/api/reference/index.html",
      "tags": [
        "api_reference",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://docs.datarobot.com/en/docs/api/reference/index.html",
        "content_length": 2477
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.65,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_6995773586466055975",
      "title": "Python client changelog",
      "content": "Skip to content\nDataRobot API resources\n>\nAPI reference documentation\n>\nPython client changelog\nPython client changelog¶\n3.8.2¶\nFixes¶\nRemove a broken link in the package description that pointed to the DataRobot documentation.\nFix error in URL generation for the AsyncOAuth class.\nFixed validator errors in EvaluationDatasetMetricAggregation.\n3.8.1¶\nFixes¶\nFixed a bug in client retries that caused the client to not retry requests when the server returned 502 and 503 errors.\n3.8.0¶\nThis release adds support for unstructured data in the Data Registry, notebooks and Codespaces, and chunking service v2. There are improvements related to playgrounds, vector databases, agentic workflows, incremental learning, and datasets. This release focuses heavily on file management capabilities.\nThere are two new package extras: auth and auth-authlib. The auth extra provides OAuth2 support, while the auth-authlib extra provides OAuth2 support using the Authlib library.\nNew features¶\nGenAI¶\nAdded AGENTIC_WORKFLOW target type.\nAdded VectorDatabase.send_to_custom_model_workshop to create a new custom model from a vector database.\nAdded VectorDatabase.deploy to create a new deployment from a vector database.\nAdded optional parameters vector_database_default_prediction_server_id, vector_database_prediction_environment_id, vector_database_maximum_memory, vector_database_resource_bundle_id, vector_database_replicas, and vector_database_network_egress_policy to LLMBlueprint.register_custom_model to allow specifying resources in cases where we automatically deploy a vector database when this function is called.\nAdded ReferenceToolCall for creating tool calls in the evaluation dataset.\nAdded ReferenceToolCalls to represent a list of tool calls in the evaluation dataset.\nAdded VectorDatabase.update_connected to add a dataset and optional additional metadata to a connected vector database.\nNotebooks and Codespaces¶\nThe notebook and Codespace APIs are now GA and the related classes have been promoted to the stable client.\nChanged name of the Notebook run() method to be Notebook.run_as_job.\nAdded support for Codespaces to the Notebook.is_finished_executing method.\nAdded the NotebookKernel.get method.\nAdded the NotebookScheduledJob.cancel method.\nAdded the NotebookScheduledJob.list method.\nAdded the Notebook.list_schedules method.\nUnstructured data¶\nThe client now supports unstructured data in the Data Registry.\nAdded the Files class to manage files on the DataRobot platform. The class supports file metadata including the description, creation date, and creator information.\nUse Files.get to retrieve file information.\nUse Files.upload as a convenient facade method to upload files from URLs, file paths, or file objects (does not support DataFrames).\nUse Files.create_from_url to upload a new file from a URL.\nUse Files.create_from_file to upload a new file from a local file or file-like object.\nUse Files.create_from_data_source to create a new file from a DataSource.\nUse Files.list_files to retrieve all individual files contained within a Files object. This is useful for Files objects ingested from archives that contain multiple files.\nUse Files.download to download a file’s contents.\nUse Files.modify to update a file’s name, description, and/or tags.\nUse Files.update to refresh a file object with the latest information from the server.\nUse Files.delete to soft-delete a file.\nUse Files.un_delete to restore a previously deleted file.\nUse Files.search_catalog to search for files in the catalog based on name, tags, or other criteria.\nAdded the FilesCatalogSearch class to represent file catalog search results with metadata such as catalog name, creator, and tags.\nAdded the File class to represent individual files within a Files archive. The class provides information about individual files such as name, size, and path within the archive.\nOAuth¶\nThe client provides better support for OAuth2 authorization workflows in applications using the DataRobot platform. These features are available in the datarobot.auth module.\nAdded the methods set_authorization_context and get_authorization_context to handle context needed for OAuth access token management.\nAdded the decorator datarobot_tool_auth to inject OAuth access tokens into the agent tool functions.\nOther features¶\nIntroduced support for Chunking Service V2. The chunking_service_v2 classes have been moved out of the experimental directory and are now available to all users.\nAdded Model.continue_incremental_learning_from_incremental_model to continue training of the incremental learning model.\nAdded optional parameter chunk_definition_id in Model.start_incremental_learning_from_sample to begin training using new chunking service.\nAdded a new attribute snapshot_policy to datarobot.models.RecipeDatasetInput to specify the snapshot policy to use.\nAdded a new attribute dataset_id to datarobot.models.JDBCTableDataSourceInput to specify the exact dataset ID to use.\nAdded Dataset.create_version_from_recipe to create a new dataset version based on the Recipe.\nEnhancements¶\nAdded the use_tcp_keepalive parameter to Client to enable TCP keep-alive packets when connections are timing out, enabled by default.\nEnabled Playground to create agentic playgrounds via input param playground_type=PlaygroundType.AGENTIC.\nExtended PlaygroundOOTBMetricConfiguration.create with additional reference column names for agentic metrics.\nUpdated CustomTemplate.list to return all custom templates when no offset is specified.\nExtended MetricInsights.list with the option to pass llm_blueprint_ids.\nExtended OOTBMetricConfigurationRequest and OOTBMetricConfigurationResponse with support for extra_metric_settings, which provides an additional configuration option for the Tool Call Accuracy metric.\nExtended VectorDatabase.create to support creation of connected vector databases via input param external_vector_database_connection.\nExtended VectorDatabase.create to support an additional metadata dataset via input params metadata_dataset_id and metadata_combination_strategy.\nExtended VectorDatabase.update to support updating the credential used to access a connected vector database via input param credential_id.\nExtended VectorDatabase.download_text_and_embeddings_asset to support downloading additional files via input param part.\nAdded docker_image_uri to datarobot.ExecutionEnvironmentVersion.\nAdded optional parameter docker_image_uri to ExecutionEnvironmentVersion.create.\nChanged parameter docker_context_path in ExecutionEnvironmentVersion.create to be optional.\nAdded a new attribute image_id to datarobot.ExecutionEnvironmentVersion.\nBugfixes¶\nFixed PlaygroundOOTBMetricConfiguration.create by using the right payload for customModelLLMValidationId instead of customModelLlmValidationId.\nFixed datarobot.models.RecipeDatasetInput to use correct fields for to_api.\nFixed EvaluationDatasetConfiguration.create to use the correct payload for is_synthetic_dataset.\nDeprecation summary¶\nRemove unreleased Insight configuration routes.  These were replaced with the new MetricInsights class, and insight specific configurations.\nDeployment.create_from_learning_model <datarobot.models.deployment.Deployment.create_from_learning_model> method is deprecated. Please first register the leaderboard model with RegisteredModelVersion.create_for_leaderboard_item <datarobot.models.model_registry.registered_model_version.RegisteredModelVersion.create_for_leaderboard_item>, then create a deployment with Deployment.create_from_registered_model_version <datarobot.models.deployment.Deployment.create_from_registered_model_version>.\nDocumentation changes¶\nUpdated the example for GenAI to show creation of a metric aggregation job.\nExperimental changes¶\nAdded attribute version to DatasetInfo to identify the analysis version.\nAdded attribute dataset_definition_info_version to ChunkDefinition to identify the analysis information version.\nAdded a version query parameter to the DatasetDefinition class, allowing users to specify the analysis version in the get method.\nAdded DatasetDefinitionInfoHistory with the DatasetDefinitionInfoHistory.list method to retrieve a list of dataset information history records.\nAdded the DatasetDefinitionInfoHistory.list_versions method to retrieve a list of dataset information records.\n3.7.0¶\nNew features¶\nThe DataRobot Python Client now supports Python 3.12 and Python 3.13.\nAdded Deployment.get_retraining_settings to retrieve retraining settings.\nAdded Deployment.update_retraining_settings to update retraining settings.\nUpdated RESTClientObject to retry requests when the server returns a 104 connection reset error.\nAdded support for datasphere as an intake and output type in batch predictions.\nAdded Deployment.get_accuracy_metrics_settings to retrieve accuracy metrics settings.\nAdded Deployment.update_accuracy_metrics_settings to update accuracy metrics settings.\nAdded CustomMetricValuesOverSpace to retrieve custom metric values over space.\nAdded CustomMetric.get_values_over_space to retrieve custom metric values over space.\nCreated ComplianceDocTemplateProjectType, an enum to define project type supported by the compliance documentation custom template.\nAdded attribute project_type to ComplianceDocTemplate to identify the template supported project type.\nAdded optional parameter project_type in ComplianceDocTemplate.get_default to retrieve the project type’s default template.\nAdded optional parameter project_type in ComplianceDocTemplate.create to specify the project type supported by the template to create.\nAdded optional parameter project_type in ComplianceDocTemplate.create_from_json_file to specify the project type supported by the template to create.\nAdded optional parameter project_type in ComplianceDocTemplate.update to allow updating an existing template’s project type.\nAdded optional parameter project_type in ComplianceDocTemplate.list to allow to filtering/searching by template’s project type.\nAdded ShapMatrix.get_as_csv to retrieve SHAP matrix results as a CSV file.\nAdded ShapMatrix.get_as_dataframe to retrieve SHAP matrix results as a dataframe.\nAdded a new class LiftChart to interact with lift chart insights.\nAdded a new class RocCurve to interact with ROC curve insights.\nAdded a new class Residuals to interact with residuals insights.\nAdded Project.create_from_recipe to create Feature Discovery projects using recipes.\nAdded an optional parameter recipe_type to datarobot.models.Recipe.from_dataset() to create Wrangling recipes.\nAdded an optional parameter recipe_type to datarobot.models.Recipe.from_data_store() to create Wrangling recipes.\nAdded Recipe.set_recipe_metadata to update recipe metadata.\nAdded an optional parameter snapshot_policy to datarobot.models.Recipe.from_dataset() to specify the snapshot policy to use.\nAdded new attributes prediction_point, relationships_configuration_id and feature_discovery_supervised_feature_reduction to RecipeSettings.\nAdded several optional parameters to ExecutionEnvironment for list, create and update methods.\nAdded optional parameter metadata_filter to ComparisonPrompt.create.\nAdded CustomInferenceModel.share to update access control settings for a custom model.\nAdded CustomInferenceModel.get_access_list to retrieve access control settings for a custom model.\nAdded new attribute latest_successful_version to ExecutionEnvironment.\nAdded Dataset.create_from_project to create datasets from project data.\nAdded ExecutionEnvironment.share to update access control settings for an execution environment.\nAdded ExecutionEnvironment.get_access_list to retrieve access control settings an execution environment.\nCreated ModerationTemplate to interact with LLM moderation templates.\nCreated ModerationConfiguration to interact with LLM moderation configuration.\nCreated CustomTemplate to interact with custom-templates elements.\nExtended the advanced options available when setting a target to include parameter: ‘feature_engineering_prediction_point’(part of the AdvancedOptions object).\nAdded optional parameter substitute_url_parameters to DataStore <datarobot.models.DataStore for list and get methods.\nAdded Model.start_incremental_learning_from_sample to initialize the incremental learning model and begin training using the chunking service. Requires the “Project Creation from a Dataset Sample” feature flag.\nAdded NonChatAwareCustomModelValidation as the base class for CustomModelVectorDatabaseValidation and CustomModelEmbeddingValidation.\nIn contrast, CustomModelLLMValidation now implements the create and update methods differently to interact with the deployments that support the chat completion API.\nAdded optional parameter chat_model_id to CustomModelLLMValidation.create and CustomModelLLMValidation.update to allow adding deployed LLMs that support the chat completion API.\nFixed ComparisonPrompt not being able to load errored comparison prompt results.\nAdded optional parameters retirement_date, is_deprecated, and is_active to LLMDefinition and added an optional parameter llm_is_deprecated to the MetricMetadata to expose LLM deprecation and retirement-related information.\nEnhancements¶\nAdded Deployment.share as an alias for Deployment.update_shared_roles.\nInternally use the existing input argument max_wait in CustomModelVersion.clean_create <datarobot.CustomModelVersion.create_clean>, to set the READ request timeout.\nBugfixes¶\nMade user_id and username fields in management_meta optional for PredictionEnvironment to support API responses without these fields.\nFixed the enum values of ComplianceDocTemplateType.\nFixed the enum values of WranglingOperations.\nFixed the enum values of DataWranglingDialect.\nPlayground id parameter is no longer optional in EvaluationDatasetConfiguration.list\nCopy insights path fixed in MetricInsights.copy_to_playground\nMissing fields for prompt_type and warning were added to PromptTrace.\nFixed a query parameter name in SidecarModelMetricValidation.list.\nFix typo in attribute VectorDatabase: metadata_columns which was metada_columns\nDo not camelCase metadata_filter dict in ChatPrompt.create\nFixed a Use Case query parameter name in CustomModelLLMValidation.list, CustomModelEmbeddingValidation.list, and CustomModelVectorDatabaseValidation.list.\nFixed featureDiscoverySettings parameter name in RelationshipsConfiguration.create and RelationshipsConfiguration.replace.\nAPI changes¶\nMethod CustomModelLLMValidation.create no longer requires the prompt_column_name and target_column_name parameters, and can accept an optional chat_model_id parameter. The parameter order has changed. If the custom model LLM deployment supports the chat completion API, it is recommended to use chat_model_id now instead of (or in addition to) specifying the column names.\nDeprecation summary¶\nRemoved the deprecated capabilities attribute of Deployment.\nMethod Model.request_lift_chart is deprecated and will be removed in favor of LiftChart.compute.\nMethod Model.get_lift_chart is deprecated and will be removed in favor of LiftChart.get.\nMethod Model.get_all_lift_charts is deprecated and will be removed in favor of LiftChart.list.\nMethod Model.request_roc_curve is deprecated and will be removed in favor of RocCurve.compute.\nMethod Model.get_roc_curve is deprecated and will be removed in favor of RocCurve.get.\nMethod Model.get_all_roc_curves is deprecated and will be removed in favor of RocCurve.list.\nMethod Model.request_residuals_chart is deprecated and will be removed in favor of Residuals.compute.\nMethod Model.get_residuals_chart is deprecated and will be removed in favor of Residuals.get.\nMethod Model.get_all_residuals_charts is deprecated and will be removed in favor of Residuals.list.\nDocumentation changes¶\nStarting with this release, Python client documentation will be available at https://docs.datarobot.com/ as well as on ReadTheDocs. Content has been reorganized to support this change.\nRemoved numpydoc as a dependency. Docstring parsing has been handled by sphinx.ext.napoleon since 3.6.0.\nFix issues with how the Table of Contents is rendered on ReadTheDocs. sphinx-external-toc is now a dev dependency.\nFix minor issues with formatting across the ReadTheDocs site.\nUpdated docs on Anomaly Assessment objects to remove duplicate information.\nExperimental changes¶\nAdded use_case and deployment_id properties to RetrainingPolicy class.\nAdded create and update_use_case methods to RetrainingPolicy class.\nRenamed the method ‘train_first_incremental_from_sample’ to ‘start_incremental_learning_from_sample’.\nAdded new parameters : ‘early_stopping_rounds’ and ‘first_iteration_only’.\nAdded the credentials_id parameter to the create method in ChunkDefinition.\nBugfix the next_run_time property of the NotebookScheduledJob class to be nullable.\nAdded the highlight_whitespace property to the NotebookSettings.\nCreate new directory specifically for notebooks in the experimental portion of the client.\nAdded methods to the Notebook class to work with session: start_session(), stop_session(), get_session_status(), is_running().\nAdded methods to the Notebook in order to execute and check related execution status: execute(), get_execution_status(), is_finished_executing().\nAdded Notebook.create_revision to the Notebook class in order to create revisions.\nMoved ModerationTemplate class to ModerationTemplate.\nMoved ModerationConfiguration class to ModerationConfiguration to interact with LLM moderation configuration.\nUpdates to Notebook.run method in the Notebook class in order to encourage proper usage as well as add more descriptive TypedDict as annotation.\nAdded NotebookScheduledJob.get_most_recent_run to the NotebookScheduledJob class to aid in more idiomatic code when dealing with manual runs.\nUpdates to Notebook.run method in the Notebook class in order to support Codespace Notebook execution as well as multiple related new classes and methods to expand API coverage which is needed for the underlying execution.\nAdded ExecutionEnvironment.assign_environment to the ExecutionEnvironment class, which gives the ability to assign or update a notebook’s environment.\nRemoved deprecated experimental method Model.get_incremental_learning_metadata.\nRemoved deprecated experimental method Model.start_incremental_learning.\n3.6.0¶\nNew features¶\nAdded OCRJobResource for running OCR jobs.\nAdded new Jina V2 embedding model in VectorDatabaseEmbeddingModel.\nAdded new Small MultiLingual Embedding Model in VectorDatabaseEmbeddingModel.\nAdded Deployment.get_segment_attributes to retrieve segment attributes.\nAdded Deployment.get_segment_values to retrieve segment values.\nAdded AutomatedDocument.list_all_available_document_types to return a list of document types.\nAdded Model.request_per_class_fairness_insights to return per-class bias & fairness insights.\nAdded MLOpsEvent to report MLOps Events.  Currently supporting moderation MLOps events only\nAdded Deployment.get_moderation_events to retrieve moderation events for that deployment.\nExtended the advanced options available when setting a target to include new\nparameter: ‘number_of_incremental_learning_iterations_before_best_model_selection’(part of the AdvancedOptions object).\nThis parameter allows you to specify how long top 5 models will run for prior to best model selection.\nAdd support for ‘connector_type’ in :meth:` Connector.create `.\nDeprecate file_path for :meth:` Connector.create ` and :meth:` Connector.update `.\nAdded DataQualityExport and Deployment.list_data_quality_exports to retrieve a list of data quality records.\nAdded secure config support for Azure Service Principal credentials.\nAdded support for categorical custom metrics in CustomMetric.\nAdded NemoConfiguration to manage Nemo configurations.\nAdded NemoConfiguration.create to create or update a Nemo configuration.\nAdded NemoConfiguration.get to retrieve a Nemo configuration.\nAdded a new class ShapDistributions to interact with SHAP distribution insights.\nAdded the MODEL_COMPLIANCE_GEN_AI value to the attribute document_type from DocumentOption to generate compliance documentation for LLMs in the Registry.\nAdded new attribute prompts_count to Chat.\nAdded Recipe modules for Data Wrangling.\nAdded :class: RecipeOperation <datarobot.models.recipe_operation.RecipeOperation> and a set of subclasses to represent a single Recipe.operations operation.\nAdded new attribute similarity_score to Citation.\nAdded new attributes retriever and add_neighbor_chunks to VectorDatabaseSettings.\nAdded new attribute metadata to Citation.\nAdded new attribute metadata_filter to ChatPrompt.\nAdded new attribute metadata_filter to ComparisonPrompt.\nAdded new attribute custom_chunking to ChunkingParameters.\nAdded new attribute custom_chunking to VectorDatabase.\nAdded a new class LLMTestConfiguration for LLM test configurations.\nLLMTestConfiguration.get to retrieve a hosted LLM test configuration.\nLLMTestConfiguration.list to list hosted LLM test configurations.\nLLMTestConfiguration.create to create an LLM test configuration.\nLLMTestConfiguration.update to update an LLM test configuration.\nLLMTestConfiguration.delete to delete an LLM test configuration.\nAdded a new class LLMTestConfigurationSupportedInsights for LLM test configuration supported insights.\nLLMTestConfigurationSupportedInsights.list to list hosted LLM test configuration supported insights.\nAdded a new class LLMTestResult for LLM test results.\nLLMTestResult.get to retrieve a hosted LLM test result.\nLLMTestResult.list to list hosted LLM test results.\nLLMTestResult.create to create an LLM test result.\nLLMTestResult.delete to delete an LLM test result.\nAdded new attribute dataset_name to OOTBDatasetDict.\nAdded new attribute rows_count to OOTBDatasetDict.\nAdded new attribute max_num_prompts to DatasetEvaluationDict.\nAdded new attribute prompt_sampling_strategy to DatasetEvaluationDict.\nAdded a new class DatasetEvaluationRequestDict for Dataset Evaluations in create/edit requests.\nAdded new attribute evaluation_dataset_name to InsightEvaluationResult.\nAdded new attribute chat_name to InsightEvaluationResult.\nAdded new attribute llm_test_configuration_name to LLMTestResult.\nAdded new attribute creation_user_name to LLMTestResult.\nAdded new attribute pass_percentage to LLMTestResult.\nAdded new attribute evaluation_dataset_name to DatasetEvaluation.\nAdded new attribute datasets_compatibility to LLMTestConfigurationSupportedInsights.\nAdded a new class NonOOTBDataset for non out-of-the-box (OOTB) dataset entities.\nNonOOTBDataset.list to retrieve non OOTB datasets for compliance testing.\nAdded a new class OOTBDataset for OOTB dataset entities.\nOOTBDataset.list to retrieve OOTB datasets for compliance testing.\nAdded a new class TraceMetadata to retrieve trace metadata.\nAdd new attributes to VectorDatabase: parent_id, family_id, metadata_columns, added_dataset_ids, added_dataset_names,  and`version`.\nVectorDatabase.get_supported_retrieval_settings to retrieve supported retrieval settings.\nVectorDatabase.submit_export_dataset_job to submit the vector database as dataset to the AI catalog.\nUpdated the method VectorDatabase.create to create a new vector database version.\nAdded a new class SupportedRetrievalSettings for supported vector database retrieval settings.\nAdded a new class SupportedRetrievalSetting for supported vector database retrieval setting.\nAdded a new class VectorDatabaseDatasetExportJob for vector database dataset export jobs.\nAdded new attribute playground_id to CostMetricConfiguration.\nAdded new attribute name to CostMetricConfiguration.\nAdded a new class SupportedInsights to support lists.\n: SupportedInsights.list to list supported insights.\nAdded a new class MetricInsights for the new metric insights routes.\n: MetricInsights.list to list metric insights.\nMetricInsights.copy_to_playground to copy metrics to another playground.\nAdded a new class PlaygroundOOTBMetricConfiguration for OOTB metric configurations.\nUpdated the schema for EvaluationDatasetMetricAggregation to include the new attributes ootb_dataset_name, dataset_id and dataset_name.\nUpdated the method EvaluationDatasetMetricAggregation.list with additional optional filter parameters.\nAdded new attribute warning to OOTBDataset.\nAdded new attribute warning to OOTBDatasetDict.\nAdded new attribute warnings to LLMTestConfiguration.\nAdded a new parameter playground_id to SidecarModelMetricValidation.create to support sidecar model metrics transition to playground.\nUpdated the schema for NemoConfiguration to include the new attributes prompt_pipeline_template_id and response_pipeline_template_id.\nAdded new attributes to EvaluationDatasetConfiguration: rows_count, playground_id.\nFix retrieving shap_remaining_total when requesting predictions with SHAP insights. This should return the remaining shap values when present.\nAPI changes¶\nUpdated ServerError’s exc_message to be constructed with a request ID to help with debugging.\nAdded method Deployment.get_capabilities to retrieve a list of :class: Capability <datarobot.models.deployment.Capability> objects containing capability details.\nAdvanced options parameters: modelGroupId, modelRegimeId, and modelBaselines were renamed into seriesId, forecastDistance, and forecastOffsets.\nAdded the parameter use_sample_from_dataset from Project.create_from_dataset. This parameter, when set, uses the EDA sample of the dataset to start the project.\nAdded the parameter quick_compute to functions in the classes ShapMatrix, ShapImpact, and ShapPreview.\nAdded the parameter copy_insights to Playground.create to copy the insights from existing Playground to the new one.\nAdded the parameter llm_test_configuration_ids, LLMBlueprint.register_custom_model, to run LLM compliance tests when a blueprint is sent to the custom model workshop.\nEnhancements¶\nAdded standard pagination parameters (e.g. limit, offset) to Deployment.list, allowing you to get deployment data in smaller chunks.\nAdded the parameter base_path to get_encoded_file_contents_from_paths and get_encoded_image_contents_from_paths, allowing you to better control script behavior when using relative file paths.\nBugfixes¶\nFixed field in CustomTaskVersion\nfor controlling network policies. This is changed from outgoing_network_policy to outbound_network_policy.\nWhen performing a GET action, this field was incorrect and always resolved to None. When attempting\na POST or PATCH action, the incorrect field would result in a 422.\nAlso changed the name of datarobot.enums.CustomTaskOutgoingNetworkPolicy to\ndatarobot.enums.CustomTaskOutboundNetworkPolicy to reflect the proper field name.\nFixed schema for DataSliceSizeInfo, so it\nnow allows an empty list for the messages field.\nDeprecation summary¶\nRemoved the parameter in_use from ImageAugmentationList.create. This parameter was deprecated in v3.1.0.\nDeprecated AutomatedDocument.list_available_document_types. Please use AutomatedDocument.list_all_available_document_types instead.\nDeprecated Model.request_fairness_insights. Please use Model.request_per_class_fairness_insights instead, to return StatusCheckJob instead of status_id.\nDeprecated Model.get_prime_eligibility. Prime models are no longer supported.\neligibleForPrime field will no longer be returned from Model.get_supported_capabilities and will be removed after version 3.8 is released.\nDeprecated the property ShapImpact.row_count <datarobot.insights.ShapImpact.row_count> and it will be removed after version 3.7 is released.\nAdvanced options parameters: modelGroupId, modelRegimeId, and modelBaselines were renamed into seriesId, forecastDistance, and forecastOffsets and are deprecated and they will be removed after version 3.6 is released.\nRenamed datarobot.enums.CustomTaskOutgoingNetworkPolicy to datarobot.enums.CustomTaskOutboundNetworkPolicy to reflect bug fix changes. The original enum was unusable.\nRemoved parameter user_agent_suffix in datarobot.Client. Please use trace_context instead.\nRemoved deprecated method DataStore.get_access_list. Please use DataStore.get_shared_roles instead.\nRemoved support for SharingAccess instances in DataStore.update_access_list. Please use SharingRole instances instead.\nConfiguration changes¶\nRemoved upper bound pin on urllib3 package to allow versions 2.0.2 and above.\nUpgraded the Pillow library to version 10.3.0. Users installing DataRobot with the “images” extra (pip install datarobot[images]) should note that this is a required library.\nDocumentation changes¶\nThe API Reference page has been split into multiple sections for better usability.\nFixed docs for Project.refresh to clarify that it does not return a value.\nFixed code example for ExternalScores.\nAdded copy button to code examples in ReadTheDocs documentation, for convenience.\nRemoved the outdated ‘examples’ section from the documentation. Please refer to DataRobot’s API Documentation Home for more examples.\nRemoved the duplicate ‘getting started’ section from the documentation.\nUpdated to Sphinx RTD Theme v3.\nUpdated the description for the parameter: ‘number_of_incremental_learning_iterations_before_best_model_selection’ (part of the AdvancedOptions object).\nExperimental changes¶\nAdded the force_update parameter to the update method in ChunkDefinition.\nRemoved attribute select_columns from ChunkDefinition\nAdded initial experimental support for Chunking Service V2\n-DatasetDefinition\n-DatasetProps\n-DatasetInfo\n-DynamicDatasetProps\n-RowsChunkDefinition\n-FeaturesChunkDefinition\n-ChunkDefinitionStats\n-ChunkDefinition\nAdded new method update to {class}`ChunkDefinition\nAdded experimental support for time series wrangling, including usage template:\ndatarobot._experimental.models.time_series_wrangling_template.user_flow_template\nExperimental changes offer automated time series feature engineering for the data in Snowflake or Postgres.\nAdded the ability to use the Spark dialect when creating a recipe, allowing data wrangling support for files.\nAdded new attribute warning to Chat.\nMoved all modules from datarobot._experimental.models.genai to datarobot.models.genai.\nAdded a new method ‘Model.train_first_incremental_from_sample’ that will train first incremental learning iteration from existing sample model. Requires “Project Creation from a Dataset Sample” feature flag.\n3.5.0¶\nNew features¶\nAdded support for BYO LLMs using serverless predictions in CustomModelLLMValidation.\nAdded attribute creation_user_name to LLMBlueprint.\nAdded a new class HostedCustomMetricTemplate for hosted custom metrics templates.\nHostedCustomMetricTemplate.get to retrieve a hosted custom metric template.\nHostedCustomMetricTemplate.list to list hosted custom metric templates.\nAdded Job.create_from_custom_metric_gallery_template to create a job from a custom metric gallery template.\nAdded a new class HostedCustomMetricTemplate for hosted custom metrics.\n: HostedCustomMetric.list to list hosted custom metrics.\nHostedCustomMetric.update to update a hosted custom metrics.\nHostedCustomMetric.delete to delete a hosted custom metric.\nHostedCustomMetric.create_from_custom_job to create a hosted custom metric from existing custom job.\nHostedCustomMetric.create_from_template to create hosted custom metric from template.\nAdded a new class datarobot.models.deployment.custom_metrics.HostedCustomMetricBlueprint for hosted custom metric blueprints.\n: HostedCustomMetricBlueprint.get to get a hosted custom metric blueprint.\nHostedCustomMetricBlueprint.create to create a hosted custom metric blueprint.\nHostedCustomMetricBlueprint.update to update a hosted custom metric blueprint.\nAdded Job.list_schedules to list job schedules.\nAdded a new class JobSchedule for the registry job schedule.\n: JobSchedule.create to create a job schedule.\nJobSchedule.update to update a job schedule.\nJobSchedule.delete to delete a job schedule.\nAdded attribute credential_type to RuntimeParameter.\nAdded a new class EvaluationDatasetConfiguration <datarobot._experimental.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration> for configuration of evaluation datasets.\nEvaluationDatasetConfiguration.get <datarobot._experimental.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration.get> to get an evaluation dataset configuration.\nEvaluationDatasetConfiguration.list <datarobot._experimental.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration.list> to list the evaluation dataset configurations for a Use Case.\nEvaluationDatasetConfiguration.create <datarobot._experimental.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration.create> to create an evaluation dataset configuration.\nEvaluationDatasetConfiguration.update <datarobot._experimental.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration.update> to update an evaluation dataset configuration.\nEvaluationDatasetConfiguration.delete <datarobot._experimental.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration.delete> to delete an evaluation dataset configuration.\nAdded a new class EvaluationDatasetMetricAggregation <datarobot._experimental.models.genai.evaluation_dataset_metric_aggregation.EvaluationDatasetMetricAggregation> for metric aggregation results.\nEvaluationDatasetMetricAggregation.list <datarobot._experimental.models.genai.evaluation_dataset_metric_aggregation.EvaluationDatasetMetricAggregation.list> to get the metric aggregation results.\nEvaluationDatasetMetricAggregation.create <datarobot._experimental.models.genai.evaluation_dataset_metric_aggregation.EvaluationDatasetMetricAggregation.create> to create the metric aggregation job.\nEvaluationDatasetMetricAggregation.delete <datarobot._experimental.models.genai.evaluation_dataset_metric_aggregation.EvaluationDatasetMetricAggregation.delete> to delete metric aggregation results.\nAdded a new class SyntheticEvaluationDataset <datarobot._experimental.models.genai.synthetic_evaluation_dataset_generation.SyntheticEvaluationDataset> for synthetic dataset generation.\nUse SyntheticEvaluationDataset.create <datarobot._experimental.models.genai.synthetic_evaluation_dataset_generation.SyntheticEvaluationDataset.create> to create a synthetic evaluation dataset.\nAdded a new class SidecarModelMetricValidation <datarobot._experimental.models.genai.sidecar_model_metric.SidecarModelMetricValidation> for sidecar model metric validations.\nSidecarModelMetricValidation.create <datarobot._experimental.models.genai.sidecar_model_metric.SidecarModelMetricValidation.create> to create a sidecar model metric validation.\nSidecarModelMetricValidation.list <datarobot._experimental.models.genai.sidecar_model_metric.SidecarModelMetricValidation.list> to list sidecar model metric validations.\nSidecarModelMetricValidation.get <datarobot._experimental.models.genai.sidecar_model_metric.SidecarModelMetricValidation.get> to get a sidecar model metric validation.\nSidecarModelMetricValidation.revalidate <datarobot._experimental.models.genai.sidecar_model_metric.SidecarModelMetricValidation.revalidate> to rerun a sidecar model metric validation.\nSidecarModelMetricValidation.update <datarobot._experimental.models.genai.sidecar_model_metric.SidecarModelMetricValidation.update> to update a sidecar model metric validation.\nSidecarModelMetricValidation.delete <datarobot._experimental.models.genai.sidecar_model_metric.SidecarModelMetricValidation.delete> to delete a sidecar model metric validation.\nAdded experimental support for Chunking Service:\nAdded a new attribute, is_descending_order to:\nDatasourceDefinition\nDatasourceAICatalogInfo\nDatasourceDataWarehouseInfo\nBugfixes¶\nUpdated the trafaret column prediction from TrainingPredictionsIterator for\nsupporting extra list of strings.\nConfiguration changes¶\nUpdated black version to 23.1.0.\nRemoves dependency on package mock, since it is part of the standard library.\nDocumentation changes¶\nRemoved incorrect can_share parameters in Use Case sharing example\nAdded usage of external_llm_context_size in llm_settings in genai_example.rst.\nUpdated doc string for llm_settings to include attribute external_llm_context_size for external LLMs.\nUpdated genai_example.rst to link to DataRobot doc pages for external vector database and external LLM deployment creation.\nAPI changes¶\nRemove ImportedModel object since it was API for SSE (standalone scoring engine) which is not part of DataRobot anymore.\nAdded number_of_clusters parameter to Project.get_model_records\nto filter models by number of clusters in unsupervised clustering projects.\nRemove an unsupported NETWORK_EGRESS_POLICY.DR_API_ACCESS value for custom models. This value\nwas used by a feature that was never released as a GA and is not supported in the current API.\nImplemented support for dr-connector-v1 to DataStore <datarobot.models.DataStore> and DataSource <datarobot.models.DataStore>.\nAdded a new parameter name to DataStore.list for searching data stores by name.\nAdded a new parameter entity_type to the compute and create methods of the classes ShapMatrix, ShapImpact, ShapPreview. Insights can be computed for custom models if the parameter entity_type=\"customModel\" is passed. See also the User Guide: :ref:SHAP insights overview<shap_insights_overview>.\nExperimental changes¶\nAdded experimental api support for Data Wrangling. See Recipe.\nRecipe.from_data_store to create a Recipe from data store.\nRecipe.retrieve_preview to get a sample of the data after recipe is applied.\nRecipe.set_inputs to set inputs to the recipe.\nRecipe.set_operations to set operations to the recipe.\nAdded new experimental DataStore that adds\nget_spark_session for Databricks databricks-v1 data stores to get a Spark session.\nAdded attribute chunking_type to DatasetChunkDefinition.\nAdded OTV attributes to DatasourceDefinition.\nAdded DatasetChunkDefinition.patch_validation_dates to patch validation dates of OTV datasource definitions after sampling job.\n3.4.1¶\nNew features¶\nEnhancements¶\nBugfixes¶\nUpdated the validation logic of RelationshipsConfiguration to work with native database connections\nAPI changes¶\nDeprecation summary¶\nConfiguration changes¶\nDocumentation changes¶\nExperimental changes¶\n3.4.0¶\nNew features¶\nAdded the following classes for generative AI. Importing these from datarobot._experimental.models.genai is deprecated and will be removed by the release of DataRobot 10.1 and API Client 3.5.\nPlayground to manage generative AI playgrounds.\nLLMDefinition to get information about supported LLMs.\nLLMBlueprint to manage LLM blueprints.\nChat to manage chats for LLM blueprints.\nChatPrompt to submit prompts within a chat.\nComparisonChat to manage comparison chats across multiple LLM blueprints within a playground.\nComparisonPrompt to submit a prompt to multiple LLM blueprints within a comparison chat.\nVectorDatabase to create vector databases from datasets in the AI Catalog for retrieval augmented generation with an LLM blueprint.\nCustomModelVectorDatabaseValidation to validate a deployment for use as a vector database.\nCustomModelLLMValidation to validate a deployment for use as an LLM.\nUserLimits to get counts of vector databases and LLM requests for a user.\nExtended the advanced options available when setting a target to include new\nparameter: incrementalLearningEarlyStoppingRounds (part of the AdvancedOptions object).\nThis parameter allows you to specify when to stop for incremental learning automation.\nAdded experimental support for Chunking Service:\nDatasetChunkDefinition for defining how chunks are created from a data source.\nDatasetChunkDefinition.create to create a new dataset chunk definition.\nDatasetChunkDefinition.get to get a specific dataset chunk definition.\nDatasetChunkDefinition.list to list all dataset chunk definitions.\nDatasetChunkDefinition.get_datasource_definition to retrieve the data source definition.\nDatasetChunkDefinition.get_chunk to get specific chunk metadata belonging to a dataset chunk definition.\nDatasetChunkDefinition.list_chunks to list all chunk metadata belonging to a dataset chunk definition.\nDatasetChunkDefinition.create_chunk to submit a job to retrieve the data from the origin data source.\nDatasetChunkDefinition.create_chunk_by_index to submit a job to retrieve data from the origin data source by index.\nOriginStorageType\nChunk\nChunkStorageType\nChunkStorage\nDatasourceDefinition\nDatasourceAICatalogInfo to define the datasource AI catalog information to create a new dataset chunk definition.\nDatasourceDataWarehouseInfo to define the datasource data warehouse (snowflake, big query, etc) information to create a new dataset chunk definition.\nRuntimeParameter for retrieving runtime parameters assigned to CustomModelVersion.\nRuntimeParameterValue to define runtime parameter override value, to be assigned to CustomModelVersion.\nAdded Snowflake Key Pair authentication for uploading datasets from Snowflake or creating a project from Snowflake data\nAdded Project.get_model_records to retrieve models.\nMethod Project.get_models is deprecated and will be removed soon in favour of Project.get_model_records.\nExtended the advanced options available when setting a target to include new\nparameter: chunkDefinitionId (part of the AdvancedOptions object). This parameter allows you to specify the chunking definition needed for incremental learning automation.\nExtended the advanced options available when setting a target to include new Autopilot\nparameters: incrementalLearningOnlyMode and incrementalLearningOnBestModel (part of the AdvancedOptions object). These parameters allow you to specify how Autopilot is performed with the chunking service.\nAdded a new method DatetimeModel.request_lift_chart to support Lift Chart calculations for datetime partitioned projects with support of Sliced Insights.\nAdded a new method DatetimeModel.get_lift_chart to support Lift chart retrieval for datetime partitioned projects with support of Sliced Insights.\nAdded a new method DatetimeModel.request_roc_curve to support ROC curve calculation for datetime partitioned projects with support of Sliced Insights.\nAdded a new method DatetimeModel.get_roc_curve to support ROC curve retrieval for datetime partitioned projects with support of Sliced Insights.\nUpdate method DatetimeModel.request_feature_impact to support use of Sliced Insights.\nUpdate method DatetimeModel.get_feature_impact to support use of Sliced Insights.\nUpdate method DatetimeModel.get_or_request_feature_impact to support use of Sliced Insights.\nUpdate method DatetimeModel.request_feature_effect to support use of Sliced Insights.\nUpdate method DatetimeModel.get_feature_effect to support use of Sliced Insights.\nUpdate method DatetimeModel.get_or_request_feature_effect to support use of Sliced Insights.\nAdded a new method FeatureAssociationMatrix.create to support the creation of FeatureAssociationMatricies for Featurelists.\nIntroduced a new method Deployment.perform_model_replace as a replacement for Deployment.replace_model.\nIntroduced a new property, model_package, which provides an overview of the currently used model package in datarobot.models.Deployment.\nAdded new parameter prediction_threshold to BatchPredictionJob.score_with_leaderboard_model\nand BatchPredictionJob.score that automatically assigns the positive class label to any prediction exceeding the threshold.\nAdded two new enum values to  :class: datarobot.models.data_slice.DataSlicesOperators, “BETWEEN” and “NOT_BETWEEN”, which are used to allow slicing.\nAdded a new class Challenger for interacting with DataRobot challengers to support the following methods:\nChallenger.get to retrieve challenger objects by ID.\nChallenger.list to list all challengers.\nChallenger.create to create a new challenger.\nChallenger.update to update a challenger.\nChallenger.delete to delete a challenger.\nAdded a new method Deployment.get_challenger_replay_settings to retrieve the challenger replay settings of a deployment.\nAdded a new method Deployment.list_challengers to retrieve the challengers of a deployment.\nAdded a new method Deployment.get_champion_model_package to retrieve the champion model package from a deployment.\nAdded a new method Deployment.list_prediction_data_exports to retrieve deployment prediction data exports.\nAdded a new method Deployment.list_actuals_data_exports to retrieve deployment actuals data exports.\nAdded a new method Deployment.list_training_data_exports to retrieve deployment training data exports.\nManage deployment health settings with the following methods:\nGet health settings Deployment.get_health_settings\nUpdate health settings Deployment.update_health_settings\nGet default health settings Deployment.get_default_health_settings\nAdded new enum value to datarobot.enums._SHARED_TARGET_TYPE to support Text Generation use case.\nAdded new enum value datarobotServerless to datarobot.enums.PredictionEnvironmentPlatform to support DataRobot Serverless prediction environments.\nAdded new enum value notApplicable to datarobot.enums.PredictionEnvironmentHealthType to support new health status from DataRobot API.\nAdded new enum value to datarobot.enums.TARGET_TYPE and datarobot.enums.CUSTOM_MODEL_TARGET_TYPE to support text generation custom inference models.\nUpdated datarobot.CustomModel to support the creation of text generation custom models.\nAdded a new class CustomMetric for interacting with DataRobot custom metrics to support the following methods:\nCustomMetric.get to retrieve a custom metric object by ID from a given deployment.\nCustomMetric.list to list all custom metrics from a given deployment.\nCustomMetric.create to create a new custom metric for a given deployment.\nCustomMetric.update to update a custom metric for a given deployment.\nCustomMetric.delete to delete a custom metric for a given deployment.\nCustomMetric.unset_baseline to remove baseline for a given custom metric.\nCustomMetric.submit_values to submit aggregated custom metrics values from code. The provided data should be in the form of a dict or a Pandas DataFrame.\nCustomMetric.submit_single_value to submit a single custom metric value.\nCustomMetric.submit_values_from_catalog to submit aggregated custom metrics values from a dataset via the AI Catalog.\nCustomMetric.get_values_over_time to retrieve values of a custom metric over a time period.\nCustomMetric.get_summary to retrieve the summary of a custom metric over a time period.\nCustomMetric.get_values_over_batch to retrieve values of a custom metric over batches.\nCustomMetric.get_batch_summary to retrieve the summary of a custom metric over batches.\nAdded CustomMetricValuesOverTime to retrieve custom metric over time information.\nAdded CustomMetricSummary to retrieve custom metric over time summary.\nAdded CustomMetricValuesOverBatch to retrieve custom metric over batch information.\nAdded CustomMetricBatchSummary to retrieve custom metric batch summary.\nAdded Job and  JobRun  to create, read, update, run, and delete jobs in the Registry.\nAdded KeyValue to create, read, update, and delete key values.\nAdded a new class PredictionDataExport for interacting with DataRobot deployment data export to support the following methods:\nPredictionDataExport.get to retrieve a prediction data export object by ID from a given deployment.\nPredictionDataExport.list to list all prediction data exports from a given deployment.\nPredictionDataExport.create to create a new prediction data export for a given deployment.\nPredictionDataExport.fetch_data to retrieve a prediction export data as a DataRobot dataset.\nAdded a new class ActualsDataExport for interacting with DataRobot deployment data export to support the following methods:\nActualsDataExport.get to retrieve an actuals data export object by ID from a given deployment.\nActualsDataExport.list to list all actuals data exports from a given deployment.\nActualsDataExport.create to create a new actuals data export for a given deployment.\nActualsDataExport.fetch_data to retrieve an actuals export data  as a DataRobot dataset.\nAdded a new class TrainingDataExport for interacting with DataRobot deployment data export to support the following methods:\nTrainingDataExport.get to retrieve a training data export object by ID from a given deployment.\nTrainingDataExport.list to list all training data exports from a given deployment.\nTrainingDataExport.create to create a new training data export for a given deployment.\nTrainingDataExport.fetch_data to retrieve a training export data as a DataRobot dataset.\nAdded a new parameter base_environment_version_id to CustomModelVersion.create_clean for overriding the default environment version selection behavior.\nAdded a new parameter base_environment_version_id to CustomModelVersion.create_from_previous for overriding the default environment version selection behavior.\nAdded a new class PromptTrace <datarobot._experimental.models.genai.prompt_trace.PromptTrace> for interacting with DataRobot prompt trace to support the following methods:\nPromptTrace.list <datarobot._experimental.models.genai.prompt_trace.PromptTrace.list> to list all prompt traces from a given playground.\nPromptTrace.export_to_ai_catalog <datarobot._experimental.models.genai.prompt_trace.PromptTrace.export_to_ai_catalog> to export prompt traces for the playground to AI catalog.\nAdded a new class InsightsConfiguration <datarobot._experimental.models.genai.insights_configuration.InsightsConfiguration> for describing available insights and configured insights for a playground.\nInsightsConfiguration.list <datarobot._experimental.models.genai.insights_configuration.InsightsConfiguration.list> to list the insights that are available to be configured.\nAdded a new class Insights <datarobot._experimental.models.genai.insights_configuration.Insights> for configuring insights for a playground.\nInsights.get <datarobot._experimental.models.genai.insights_configuration.Insights.get> to get the current insights configuration for a playground.\nInsights.create <datarobot._experimental.models.genai.insights_configuration.Insights.create> to create or update the insights configuration for a playground.\nAdded a new class :class: CostMetricConfiguration <datarobot._experimental.models.genai.cost_metric_configurations.CostMetricConfiguration> for describing available cost metrics and configured cost metrics for a Use Case.\nCostMetricConfiguration.get <datarobot._experimental.models.genai.cost_metric_configurations.CostMetricConfiguration.get> to get the cost metric configuration.\nCostMetricConfiguration.create <datarobot._experimental.models.genai.cost_metric_configurations.CostMetricConfiguration.create> to create a cost metric configuration.\nCostMetricConfiguration.update <datarobot._experimental.models.genai.cost_metric_configurations.CostMetricConfiguration.update> to update the cost metric configuration.\nCostMetricConfiguration.delete <datarobot._experimental.models.genai.cost_metric_configurations.CostMetricConfiguration.delete> to delete the cost metric configuration.Key\nAdded a new class LLMCostConfiguration <datarobot._experimental.models.genai.cost_metric_configurations.LLMCostConfiguration> for the cost configuration of a specific llm within a Use Case.\nAdded new classes ShapMatrix, ShapImpact, ShapPreview to interact with SHAP-based insights. See also the User Guide: :ref:SHAP insights overview<shap_insights_overview>\nAPI changes¶\nParameter Overrides: Users can now override most of the previously set configuration values directly through parameters when initializing the Client. Exceptions: The endpoint and token values must be initialized from one source (client params, environment, or config file) and cannot be overridden individually, for security and consistency reasons. The new configuration priority is as follows:\nClient Params\nClient config_path param\nEnvironment Variables\nDefault to reading YAML config file from ~/.config/datarobot/drconfig.yaml\nDATAROBOT_API_CONSUMER_TRACKING_ENABLED now always defaults to True.\nAdded Databricks personal access token and service principal (also shared credentials via secure config) authentication for uploading datasets from Databricks or creating a project from Databricks data.\nAdded secure config support for AWS long term credentials.\nImplemented support for dr-database-v1 to DataStore <datarobot.models.DataStore>, DataSource <datarobot.models.DataStore>, and  DataDriver <datarobot.models.DataDriver>. Added enum classes to support the changes.\nYou can retrieve the canonical URI for a Use Case using UseCase.get_uri.\nYou can open a Use Case in a browser using UseCase.open_in_browser.\nEnhancements¶\nAdded a new parameter to Dataset.create_from_url to support fast dataset registration:\nsample_size\nAdded a new parameter to Dataset.create_from_data_source to support fast dataset registration:\nsample_size\nJob.get_result_when_complete\nreturns datarobot.models.DatetimeModel instead of the datarobot.models.Model\nif a datetime model was trained.\nDataset.get_as_dataframe can handle\ndownloading parquet files as well as csv files.\nImplement support for dr-database-v1 in DataStore <datarobot.models.DataStore>\nAdded two new parameters to BatchPredictionJobDefinition.list for paginating long job definitions lists:\noffset\nlimit\nAdded two new parameters to BatchPredictionJobDefinition.list for filtering the job definitions:\ndeployment_id\nsearch_name\nAdded new parameter to Deployment.validate_replacement_model to support replacement validation based on model package ID:\nnew_registered_model_version_id\nAdded support for Native Connectors to Connector <datarobot.models.Connector> for everything other than :meth:` Connector.create ` and :meth:` Connector.update `\nDeprecation summary¶\nRemoved Model.get_leaderboard_ui_permalink and Model.open_model_browser\nDeprecated Project.get_models in favour of Project.get_model_records.\nBatchPredictionJobDefinition.list will no longer return all job definitions after version 3.6 is released.\nTo preserve current behavior please pass limit=0.\nnew_model_id parameter in Deployment.validate_replacement_model will be removed after version 3.6 is released.\nDeployment.replace_model will be removed after version 3.6 is released.\nMethod Deployment.perform_model_replace should be used instead.\nCustomInferenceModel.assign_training_data was marked as deprecated in v3.2. The deprecation period has been extended, and the feature will now be removed in v3.5.\nUse CustomModelVersion.create_clean and CustomModelVersion.create_from_previous instead.\nDocumentation changes¶\nUpdated genai_example.rst to utilize latest genAI features and methods introduced most recently in the API client.\nExperimental changes¶\nAdded new attribute, prediction_timeout to CustomModelValidation.\nAdded new attributes, feedback_result, metrics, and final_prompt to ResultMetadata <datarobot._experimental.models.genai.chat_prompt.ResultMetadata>.\nAdded use_case_id to CustomModelValidation.\nAdded llm_blueprints_count and user_name to Playground.\nAdded custom_model_embedding_validations to SupportedEmbeddings <datarobot._experimental.models.genai.vector_database.SupportedEmbeddings>.\nAdded embedding_validation_id and is_separator_regex to VectorDatabase <datarobot._experimental.models.genai.vector_database.VectorDatabase>.\nAdded optional parameters, use_case, name, and model to CustomModelValidation.create.\nAdded a method CustomModelValidation.list, to list custom model validations available to a user with several optional parameters to filter the results.\nAdded a method CustomModelValidation.update, to update a custom model validation.\nAdded an optional parameter, use_case, to LLMDefinition.list,\nto include in the returned LLMs the external LLMs available for the specified use_case as well.\nAdded optional parameter, playground to VectorDatabase.list\nto list vector databases by playground.\nAdded optional parameter, comparison_chat, to ComparisonPrompt.list, to list comparison prompts by comparison chat.\nAdded optional parameter, comparison_chat, to ComparisonPrompt.create, to specify the comparison chat to create the comparison prompt in.\nAdded optional parameter, feedback_result, to ComparisonPrompt.update <datarobot._experimental.models.genai.comparison_prompt.ComparisonPrompt.update>, to update a comparison prompt with feedback.\nAdded optional parameters, is_starred to LLMBlueprint.update\nto update the LLM blueprint’s starred status.\nAdded optional parameters, is_starred to LLMBlueprint.list\nto filter the returned LLM blueprints to those matching is_starred.\nAdded a new enum PromptType, PromptType to identify the LLMBlueprint’s prompting type.\nAdded optional parameters, prompt_type to LLMBlueprint.create,\nto specify the LLM blueprint’s prompting type. This can be set with PromptType.\nAdded optional parameters, prompt_type to LLMBlueprint.update,\nto specify the updated LLM blueprint’s prompting type. This can be set with PromptType.\nAdded a new class, ComparisonChat, for interacting with DataRobot generative AI comparison chats.\nComparisonChat.get retrieves a comparison chat object by ID.\nComparisonChat.list lists all comparison chats available to the user.\nComparisonChat.create creates a new comparison chat.\nComparisonChat.update updates the name of a comparison chat.\nComparisonChat.delete deletes a single comparison chat.\nAdded optional parameters, playground and chat to ChatPrompt.list, to list chat prompts by playground and chat.\nAdded optional parameter, chat to ChatPrompt.create, to specify the chat to create the chat prompt in.\nAdded a new method, ChatPrompt.update <datarobot._experimental.models.genai.chat_prompt.ChatPrompt.update>, to update a chat prompt with custom metrics and feedback.\nAdded a new class, Chat, for interacting with DataRobot generative AI chats.\nChat.get retrieves a chat object by ID.\nChat.list lists all chats available to the user.\nChat.create creates a new chat.\nChat.update updates the name of a chat.\nChat.delete deletes a single chat.\nRemoved the model_package module. Use RegisteredModelVersion instead.\nAdded new class UserLimits\nAdded support to get the count of users’ LLM API requests. UserLimits.get_llm_requests_count\nAdded support to get the count of users’ vector databases. UserLimits.get_vector_database_count\nAdded new methods to the class Notebook which includes Notebook.run and Notebook.download_revision. See the documentation for example usage.\nAdded new class NotebookScheduledJob.\nAdded new class NotebookScheduledRun.\nAdded a new method Model.get_incremental_learning_metadata that retrieves incremental learning metadata for a model.\nAdded a new method Model.start_incremental_learning that starts incremental learning for a model.\nUpdated the API endpoint prefix for all GenerativeAI routes to align with the publicly documented routes.\nBugfixes¶\nFixed how async url is build in Model.get_or_request_feature_impact\nFixed setting ssl_verify by env variables.\nResolved a problem related to tilde-based paths in the Client’s ‘config_path’ attribute.\nChanged the force_size default of ImageOptions to apply the same transformations by default, which are applied when image archive datasets are uploaded to DataRobot.\n3.3.0¶\nNew features¶\nAdded support for Python 3.11.\nAdded new library strenum to add StrEnum support while maintaining backwards compatibility with Python 3.7-3.10. DataRobot does not use the native StrEnum class in Python 3.11.\nAdded a new class PredictionEnvironment for interacting with DataRobot Prediction environments.\nExtended the advanced options available when setting a target to include new\nparameters: modelGroupId, modelRegimeId, and modelBaselines (part of the AdvancedOptions object). These parameters allow you to specify the user columns required to run time series models without feature derivation in OTV projects.\nAdded a new method PredictionExplanations.create_on_training_data, for computing prediction explanation on training data.\nAdded a new class RegisteredModel for interacting with DataRobot registered models to support the following methods:\nRegisteredModel.get to retrieve RegisteredModel object by ID.\nRegisteredModel.list to list all registered models.\nRegisteredModel.archive to permanently archive registered model.\nRegisteredModel.update to update registered model.\nRegisteredModel.get_shared_roles to retrieve access control information for registered model.\nRegisteredModel.share to share a registered model.\nRegisteredModel.get_version to retrieve RegisteredModelVersion object by ID.\nRegisteredModel.list_versions to list registered model versions.\nRegisteredModel.list_associated_deployments to list deployments associated with a registered model.\nAdded a new class RegisteredModelVersion for interacting with DataRobot registered model versions (also known as model packages) to support the following methods:\nRegisteredModelVersion.create_for_external to create a new registered model version from an external model.\nRegisteredModelVersion.list_associated_deployments to list deployments associated with a registered model version.\nRegisteredModelVersion.create_for_leaderboard_item to create a new registered model version from a Leaderboard model.\nRegisteredModelVersion.create_for_custom_model_version to create a new registered model version from a custom model version.\nAdded a new method Deployment.create_from_registered_model_version to support creating deployments from registered model version.\nAdded a new method Deployment.download_model_package_file to support downloading model package files (.mlpkg) of the currently deployed model.\nAdded support for retrieving document thumbnails:\nDocumentThumbnail\nDocumentPageFile\nAdded support to retrieve document text extraction samples using:\nDocumentTextExtractionSample\nDocumentTextExtractionSamplePage\nDocumentTextExtractionSampleDocument\nAdded new fields to CustomTaskVersion\nfor controlling network policies. The new fields were also added to the response. This can be set with\ndatarobot.enums.CustomTaskOutgoingNetworkPolicy.\nAdded a new method BatchPredictionJob.score_with_leaderboard_model to run batch predictions using a Leaderboard model instead of a deployment.\nSet IntakeSettings and OutputSettings to use IntakeAdapters and OutputAdapters enum values respectively for the property type.\nAdded method meth:Deployment.get_predictions_vs_actuals_over_time <datarobot.models.Deployment.get_predictions_vs_actuals_over_time> to retrieve a deployment’s predictions vs actuals over time data.\nBugfixes¶\nPayload property subset renamed to source in Model.request_feature_effect\nFixed an issue where Context.trace_context was not being set from environment variables or DR config files.\nProject.refresh no longer sets Project.advanced_options to a dictionary.\nFixed Dataset.modify to clarify behavior of when to preserve or clear categories.\nFixed an issue with enums in f-strings resulting in the enum class and property being printed instead of the enum property’s value in Python 3.11 environments.\nDeprecation summary¶\nProject.refresh will no longer set Project.advanced_options to a dictionary after version 3.5 is released.\n: All interactions with Project.advanced_options should be expected to be through the AdvancedOptions class.\nExperimental changes¶\nAdded a new class, VectorDatabase, for interacting with DataRobot vector databases.\nVectorDatabase.get retrieves a VectorDatabase object by ID.\nVectorDatabase.list lists all VectorDatabases available to the user.\nVectorDatabase.create creates a new VectorDatabase.\nVectorDatabase.create allows you to use a validated deployment of a custom model as your own Vector Database.\nVectorDatabase.update updates the name of a VectorDatabase.\nVectorDatabase.delete deletes a single VectorDatabase.\nVectorDatabase.get_supported_embeddings retrieves all supported embedding models.\nVectorDatabase.get_supported_text_chunkings retrieves all supported text chunking configurations.\nVectorDatabase.download_text_and_embeddings_asset download a parquet file with internal vector database data.\nAdded a new class, CustomModelVectorDatabaseValidation, for validating custom model deployments for use as a vector database.\nCustomModelVectorDatabaseValidation.get retrieves a CustomModelVectorDatabaseValidation object by ID.\nCustomModelVectorDatabaseValidation.get_by_values retrieves a CustomModelVectorDatabaseValidation object by field values.\nCustomModelVectorDatabaseValidation.create starts validation of the deployment.\nCustomModelVectorDatabaseValidation.revalidate repairs an unlinked external vector database.\nAdded a new class, Playground, for interacting with DataRobot generative AI playgrounds.\nPlayground.get retrieves a playground object by ID.\nPlayground.list lists all playgrounds available to the user.\nPlayground.create creates a new playground.\nPlayground.update updates the name and description of a playground.\nPlayground.delete deletes a single playground.\nAdded a new class, LLMDefinition, for interacting with DataRobot generative AI LLMs.\nLLMDefinition.list lists all LLMs available to the user.\nAdded a new class, LLMBlueprint, for interacting with DataRobot generative AI LLM blueprints.\nLLMBlueprint.get retrieves an LLM blueprint object by ID.\nLLMBlueprint.list lists all LLM blueprints available to the user.\nLLMBlueprint.create creates a new LLM blueprint.\nLLMBlueprint.create_from_llm_blueprint creates a new LLM blueprint from an existing one.\nLLMBlueprint.update updates an LLM blueprint.\nLLMBlueprint.delete deletes a single LLM blueprint.\nAdded a new class, ChatPrompt, for interacting with DataRobot generative AI chat prompts.\nChatPrompt.get retrieves a chat prompt object by ID.\nChatPrompt.list lists all chat prompts available to the user.\nChatPrompt.create creates a new chat prompt.\nChatPrompt.delete deletes a single chat prompt.\nAdded a new class, CustomModelLLMValidation, for validating custom model deployments for use as a custom model LLM.\nCustomModelLLMValidation.get retrieves a CustomModelLLMValidation object by ID.\nCustomModelLLMValidation.get_by_values retrieves a CustomModelLLMValidation object by field values.\nCustomModelLLMValidation.create starts validation of the deployment.\nCustomModelLLMValidation.revalidate repairs an unlinked external custom model LLM.\nAdded a new class, ComparisonPrompt, for interacting with DataRobot generative AI comparison prompts.\nComparisonPrompt.get retrieves a comparison prompt object by ID.\nComparisonPrompt.list lists all comparison prompts available to the user.\nComparisonPrompt.create creates a new comparison prompt.\nComparisonPrompt.update updates a comparison prompt.\nComparisonPrompt.delete deletes a single comparison prompt.\nExtended UseCase, adding two new fields to represent the count of vector databases and playgrounds.\nAdded a new method, ChatPrompt.create_llm_blueprint, to create an LLM blueprint from a chat prompt.\nAdded a new method, CustomModelLLMValidation.delete, to delete a custom model LLM validation record.\nAdded a new method, LLMBlueprint.register_custom_model, for registering a custom model from a generative AI LLM blueprint.\n3.2.0¶\nNew features¶\nAdded new methods to trigger batch monitoring jobs without providing a job definition.\nBatchMonitoringJob.run\nBatchMonitoringJob.get_status\nBatchMonitoringJob.cancel\nBatchMonitoringJob.download\nAdded Deployment.submit_actuals_from_catalog_async to submit actuals from the AI Catalog.\nAdded a new class StatusCheckJob which represents a job for a status check of submitted async jobs.\nAdded a new class JobStatusResult represents the result for a status check job of a submitted async task.\nAdded DatetimePartitioning.datetime_partitioning_log_retrieve to download the datetime partitioning log.\nAdded method DatetimePartitioning.datetime_partitioning_log_list to list the datetime partitioning log.\nAdded DatetimePartitioning.get_input_data to retrieve the input data used to create an optimized datetime partitioning.\nAdded DatetimePartitioningId, which can be passed as a partitioning_method to Project.analyze_and_model.\nAdded the ability to share deployments. See :ref:deployment sharing <deployment_sharing> for more information on sharing deployments.\nAdded new methods get_bias_and_fairness_settings and update_bias_and_fairness_settings to retrieve or update bias and fairness settings.\nDeployment.get_bias_and_fairness_settings\nDeployment.update_bias_and_fairness_settings\nAdded a new class UseCase for interacting with the DataRobot Use Cases API.\nAdded a new class Application for retrieving DataRobot Applications available to the user.\nAdded a new class SharingRole to hold user or organization access rights.\nAdded a new class BatchMonitoringJob for interacting with batch monitoring jobs.\nAdded a new class BatchMonitoringJobDefinition for interacting with batch monitoring jobs definitions.\nAdded a new methods for handling monitoring job definitions: list, get, create, update, delete, run_on_schedule and run_once\nBatchMonitoringJobDefinition.list\nBatchMonitoringJobDefinition.get\nBatchMonitoringJobDefinition.create\nBatchMonitoringJobDefinition.update\nBatchMonitoringJobDefinition.delete\nBatchMonitoringJobDefinition.run_on_schedule\nBatchMonitoringJobDefinition.run_once\nAdded a new method to retrieve a monitoring job\nBatchMonitoringJob.get\nAdded the ability to filter return objects by a Use Case ID passed to the following methods:\nDataset.list\nProject.list\nAdded the ability to automatically add a newly created dataset or project to a Use Case by passing a UseCase, list of UseCase objects, UseCase ID or list of UseCase IDs using the keyword argument use_cases to the following methods:\nDataset.create_from_file\nDataset.create_from_in_memory_data\nDataset.create_from_url\nDataset.create_from_data_source\nDataset.create_from_query_generator\nDataset.create_project\nProject.create\nProject.create_from_data_source\nProject.create_from_dataset\nProject.create_segmented_project_from_clustering_model\nProject.start\nAdded the ability to set a default UseCase for requests. It can be set in several ways.\nIf the user configures the client via Client(...), then invoke Client(..., default_use_case = <id>).\nIf the user configures the client via dr.config.yaml, then add the property default_use_case: <id>.\nIf the user configures the client via env vars, then set the env var DATAROBOT_DEFAULT_USE_CASE.\nThe default use case can also be set programmatically as a context manager via with UseCase.get(<id>):.\nAdded the ability to configure the collection of client usage metrics to send to DataRobot. Note that this feature only tracks which DataRobot package methods are called and does not collect any user data. You can configure collection with the following settings:\nIf the user configures the client via Client(...), then invoke Client(..., enable_api_consumer_tracking = <True/False>).\nIf the user configures the client via dr.config.yaml, then add the property enable_api_consumer_tracking: <True/False>.\nIf the user configures the client via env vars, then set the env var DATAROBOT_API_CONSUMER_TRACKING_ENABLED.\nCurrently the default value for enable_api_consumer_tracking is True.\n- Added method meth:Deployment.get_predictions_over_time <datarobot.models.Deployment.get_predictions_over_time> to retrieve deployment predictions over time data.\n- Added a new class FairnessScoresOverTime to retrieve fairness over time information.\n- Added a new method Deployment.get_fairness_scores_over_time to retrieve fairness scores over time of a deployment.\n- Added a new use_gpu parameter to the method Project.analyze_and_model to set whether the project should allow usage of GPU\n- Added a new use_gpu parameter to the class Project with information whether project allows usage of GPU\n- Added a new class TrainingData for retrieving TrainingData assigned to CustomModelVersion.\n- Added a new class HoldoutData for retrieving HoldoutData assigned to CustomModelVersion.\n- Added the ability to retrieve the model and blueprint json using the following methods:\nModel.get_model_blueprint_json\nBlueprint.get_json\n- Added Credential.update which allows you to update existing credential resources.\n- Added a new optional parameter trace_context to datarobot.Client to provide additional information on the DataRobot code being run. This parameter defaults to None.\n- Updated methods in Model to support use of Sliced Insights:\nModel.get_feature_effect\nModel.request_feature_effect\nModel.get_or_request_feature_effect\nModel.get_lift_chart\nModel.get_all_lift_charts\nModel.get_residuals_chart\nModel.get_all_residuals_charts\nModel.request_lift_chart\nModel.request_residuals_chart\nModel.get_roc_curve\nModel.get_feature_impact\nModel.request_feature_impact\nModel.get_or_request_feature_impact\n- Added support for SharingRole to the following methods:\n- DataStore.share\n- Added new methods for retrieving SharingRole information for the following classes:\n- DataStore.get_shared_roles\n- Added new method for calculating sliced roc curve Model.request_roc_curve\n- Added new DataSlice to support the following slices methods:\nDataSlice.list to retrieve all data slices in a project.\nDataSlice.create to create a new data slice.\nDataSlice.delete to delete the data slice calling this method.\nDataSlice.request_size to submit a request to calculate a data slice size on a source.\nDataSlice.get_size_info to get the data slice’s info when applied to a source.\nDataSlice.get to retrieve a specific data slice.\n- Added new DataSliceSizeInfo to define the result of a data slice applied to a source.\n- Added new method for retrieving all available feature impacts for the model Model.get_all_feature_impacts.\n- Added new method for StatusCheckJob to wait and return the completed object once it is generated datarobot.models.StatusCheckJob.get_result_when_complete()\nEnhancements¶\nImprove error message of SampleImage.list\nto clarify that a selected parameter cannot be used when a project has not proceeded to the\ncorrect stage prior to calling this method.\nExtended SampleImage.list by two parameters\nto filter for a target value range in regression projects.\nAdded text explanations data to PredictionExplanations and made sure it is returned in both datarobot.PredictionExplanations.get_all_as_dataframe()  and datarobot.PredictionExplanations.get_rows() method.\nAdded two new parameters to Project.upload_dataset_from_catalog:\n: - credential_id\n- credential_data\nImplemented training and holdout data assignment for Custom Model Version creation APIs:\n: - CustomModelVersion.create_clean\n- CustomModelVersion.create_from_previous\nThe parameters added to both APIs are:\n: - training_dataset_id\n- partition_column\n- holdout_dataset_id\n- keep_training_holdout_data\n- max_wait\nExtended CustomInferenceModel.create and CustomInferenceModel.update\nwith the parameter is_training_data_for_versions_permanently_enabled.\nAdded value DR_API_ACCESS to the NETWORK_EGRESS_POLICY enum.\nAdded new parameter low_memory to Dataset.get_as_dataframe to allow a low memory mode for larger datasets\nAdded two new parameters to Project.list for paginating long project lists:\n: - offset\n- limit\nBugfixes¶\nFixed incompatibilities with Pandas 2.0 in DatetimePartitioning.to_dataframe.\nFixed a crash when using non-“latin-1” characters in Panda’s DataFrame used as prediction data in BatchPredictionJob.score.\nFixed an issue where failed authentication when invoking datarobot.client.Client() raises a misleading error about client-server compatibility.\nFixed incompatibilities with Pandas 2.0 in AccuracyOverTime.get_as_dataframe. The method will now throw a ValueError if an empty list is passed to the parameter metrics.\nAPI changes¶\nAdded parameter unsupervised_type to the class DatetimePartitioning.\nThe sliced insight API endpoint GET: api/v2/insights/<insight_name>/ returns a paginated response. This means that it returns an empty response if no insights data is found, unlike GET: api/v2/projects/<pid>/models/<lid>/<insight_name>/, which returns 404 NOT FOUND in this case. To maintain backwards-compatibility, all methods that retrieve insights data raise 404 NOT FOUND if the insights API returns an empty response.\nDeprecation summary¶\nModel.get_feature_fit_metadata has been removed.\nUse Model.get_feature_effect_metadata instead.\nDatetimeModel.get_feature_fit_metadata has been removed.\nUse DatetimeModel.get_feature_effect_metadata instead.\nModel.request_feature_fit has been removed.\nUse Model.request_feature_effect instead.\nDatetimeModel.request_feature_fit has been removed.\nUse DatetimeModel.request_feature_effect instead.\nModel.get_feature_fit has been removed.\nUse Model.get_feature_effect instead.\nDatetimeModel.get_feature_fit has been removed.\nUse DatetimeModel.get_feature_effect instead.\nModel.get_or_request_feature_fit has been removed.\nUse Model.get_or_request_feature_effect instead.\nDatetimeModel.get_or_request_feature_fit has been removed.\nUse DatetimeModel.get_or_request_feature_effect instead.\nDeprecated the use of SharingAccess in favor of SharingRole for sharing in the following classes:\nDataStore.share\nDeprecated the following methods for retrieving SharingAccess information.\nDataStore.get_access_list. Please use DataStore.get_shared_roles instead.\nCustomInferenceModel.assign_training_data was marked as deprecated and will be removed in v3.4.\nUse CustomModelVersion.create_clean and CustomModelVersion.create_from_previous instead.\nConfiguration changes¶\nPins dependency on package urllib3  to be less than version 2.0.0.\nDeprecation summary¶\nDeprecated parameter user_agent_suffix in datarobot.Client. user_agent_suffix will be removed in v3.4. Please use trace_context instead.\nDocumentation changes¶\nFixed in-line documentation of DataRobotClientConfig.\nFixed documentation around client configuration from environment variables or config file.\nExperimental changes¶\nAdded experimental support for data matching:\nDataMatching\nDataMatchingQuery\nAdded new method DataMatchingQuery.get_result for returning data matching query results as pandas dataframes to DataMatchingQuery .\nChanged behavior for returning results in the DataMatching. Instead of saving the results as a file, a pandas dataframe will be returned in the following methods:\n: - DataMatching.get_closest_data\n- DataMatching.get_closest_data_for_model\n- DataMatching.get_closest_data_for_featurelist\nAdded experimental support for model lineage: ModelLineage\nChanged behavior for methods that search for the closest data points in DataMatching. If the index is missing, instead of throwing the error, methods try to create the index and then query it. This is enabled by default, but if this is not the intended behavior it can be changed by passing False to the new build_index parameter added to the methods:\n: - DataMatching.get_closest_data\n- DataMatching.get_closest_data_for_model\n- DataMatching.get_closest_data_for_featurelist\nAdded a new class Notebook for retrieving DataRobot Notebooks available to the user.\nAdded experimental support for data wrangling:\nRecipe\n3.1.1¶\nConfiguration changes¶\nRemoves dependency on package contextlib2  since the package is Python 3.7+.\nUpdate typing-extensions to be inclusive of versions from 4.3.0 to < 5.0.0.\n3.1.0¶\nEnhancements¶\nAdded new methods BatchPredictionJob.apply_time_series_data_prep_and_score\nand BatchPredictionJob.apply_time_series_data_prep_and_score_to_file\nthat apply time series data prep to a file or dataset and make batch predictions with a deployment.\nAdded new methods DataEngineQueryGenerator.prepare_prediction_dataset\nand DataEngineQueryGenerator.prepare_prediction_dataset_from_catalog\nthat apply time series data prep to a file or catalog dataset and upload the prediction dataset to a\nproject.\nAdded new max_wait parameter to method Project.create_from_dataset.\nValues larger than the default can be specified to avoid timeouts when creating a project from Dataset.\nAdded new method for creating a segmented modeling project from an existing clustering project and model\nProject.create_segmented_project_from_clustering_model.\nPlease switch to this function if you are previously using ModelPackage for segmented modeling purposes.\nAdded new method is_unsupervised_clustering_or_multiclass for checking whether the clustering or multiclass parameters are used, quick and efficient without extra API calls.\nPredictionExplanations.is_unsupervised_clustering_or_multiclass\nRetry idempotent requests which result in HTTP 502 and HTTP 504 (in addition to the previous HTTP 413, HTTP 429 and HTTP 503)\nAdded value PREPARED_FOR_DEPLOYMENT to the RECOMMENDED_MODEL_TYPE enum\nAdded two new methods to the ImageAugmentationList class:\nImageAugmentationList.list,\nImageAugmentationList.update\nBugfixes¶\nAdded format key to Batch Prediction intake and output settings for S3, GCP and Azure\nAPI changes¶\nThe method PredictionExplanations.is_multiclass now adds an additional API call to check for multiclass target validity, which adds a small delay.\nAdvancedOptions parameter blend_best_models defaults to false\nAdvancedOptions parameter consider_blenders_in_recommendation defaults to false\nDatetimePartitioning has parameter unsupervised_mode\nDeprecation summary¶\nDeprecated method Project.create_from_hdfs.\nDeprecated method DatetimePartitioning.generate.\nDeprecated parameter in_use from ImageAugmentationList.create as DataRobot will take care of it automatically.\nDeprecated property Deployment.capabilities from Deployment.\nImageAugmentationSample.compute was removed in v3.1. You\ncan get the same information with the method ImageAugmentationList.compute_samples.\nsample_id parameter removed from ImageAugmentationSample.list. Please use auglist_id instead.\nDocumentation changes¶\nUpdate the documentation to suggest that setting use_backtest_start_end_format of DatetimePartitioning.to_specification to True will mirror the same behavior as the Web UI.\nUpdate the documentation to suggest setting use_start_end_format of Backtest.to_specification to True will mirror the same behavior as the Web UI.\n3.0.3¶\nBugfixes¶\nFixed an issue affecting backwards compatibility in datarobot.models.DatetimeModel, where an unexpected keyword from the DataRobot API would break class deserialization.\n3.0.2¶\nBugfixes¶\nRestored Model.get_leaderboard_ui_permalink, Model.open_model_browser,\nThese methods were accidentally removed instead of deprecated.\nFix for ipykernel < 6.0.0 which does not persist contextvars across cells\nDeprecation summary¶\nDeprecated method Model.get_leaderboard_ui_permalink. Please use Model.get_uri instead.\nDeprecated method Model.open_model_browser. Please use Model.open_in_browser instead.\n3.0.1¶\nBugfixes¶\nAdded typing-extensions as a required dependency for the DataRobot Python API client.\n3.0.0¶\nNew features¶\nVersion 3.0 of the Python client does not support Python 3.6 and earlier versions. Version 3.0 currently supports Python 3.7+.\nThe default Autopilot mode for project.start_autopilot has changed to Quick mode.\nFor datetime-aware models, you can now calculate and retrieve feature impact for backtests other than zero and holdout:\nDatetimeModel.get_feature_impact\nDatetimeModel.request_feature_impact\nDatetimeModel.get_or_request_feature_impact\nAdded a backtest field to feature impact metadata: Model.get_or_request_feature_impact. This field is null for non-datetime-aware models and greater than or equal to zero for holdout in datetime-aware models.\nYou can use a new method to retrieve the canonical URI for a project, model, deployment, or dataset:\nProject.get_uri\nModel.get_uri\nDeployment.get_uri\nDataset.get_uri\nYou can use a new method to open a class in a browser based on their URI (project, model, deployment, or dataset):\nProject.open_in_browser\nModel.open_in_browser\nDeployment.open_in_browser\nDataset.open_in_browser\nAdded a new method for opening DataRobot in a browser: datarobot.rest.RESTClientObject.open_in_browser(). Invoke the method via dr.Client().open_in_browser().\nAltered method Project.create_featurelist to accept five new parameters (please see documentation for information about usage):\nstarting_featurelist\nstarting_featurelist_id\nstarting_featurelist_name\nfeatures_to_include\nfeatures_to_exclude\nAdded a new method to retrieve a feature list by name: Project.get_featurelist_by_name.\nAdded a new convenience method to create datasets: Dataset.upload.\nAltered the method Model.request_predictions to accept four new parameters:\ndataset\nfile\nfile_path\ndataframe\nNote that the method already supports the parameter dataset_id and all data source parameters are mutually exclusive.\nAdded a new method to datarobot.models.Dataset, Dataset.get_as_dataframe, which retrieves all the originally uploaded data in a pandas DataFrame.\nAdded a new method to datarobot.models.Dataset, Dataset.share, which allows the sharing of a dataset with another user.\nAdded new convenience methods to datarobot.models.Project for dealing with partition classes. Both methods should be called before Project.analyze_and_model.\nProject.set_partitioning_method intelligently creates the correct partition class for a regular project, based on input arguments.\nProject.set_datetime_partitioning creates the correct partition class for a time series project.\nAdded a new method to datarobot.models.Project Project.get_top_model which returns the highest scoring model for a metric of your choice.\nUse the new method Deployment.predict_batch to pass a file, file path, or DataFrame to datarobot.models.Deployment to easily make batch predictions and return the results as a DataFrame.\nAdded support for passing in a credentials ID or credentials data to Project.create_from_data_source as an alternative to providing a username and password.\nYou can now pass in a max_wait value to AutomatedDocument.generate.\nAdded a new method to datarobot.models.Project Project.get_dataset which retrieves the dataset used during creation of a project.\nAdded two new properties to datarobot.models.Project:\ncatalog_id\ncatalog_version_id\nAdded a new Autopilot method to datarobot.models.Project Project.analyze_and_model which allows you to initiate Autopilot or data analysis against data uploaded to DataRobot.\nAdded a new convenience method to datarobot.models.Project Project.set_options which allows you to save AdvancedOptions values for use in modeling.\nAdded a new convenience method to datarobot.models.Project Project.get_options which allows you to retrieve saved modeling options.\nEnhancements¶\nRefactored the global singleton client connection (datarobot.client.Client()) to use ContextVar instead of a global variable for better concurrency support.\nAdded support for creating monotonic feature lists for time series projects. Set skip_datetime_partition_column to\nTrue to create monotonic feature list. For more information see datarobot.models.Project.create_modeling_featurelist().\nAdded information about vertex to advanced tuning parameters datarobot.models.Model.get_advanced_tuning_parameters().\nAdded the ability to automatically use saved AdvancedOptions set using Project.set_options in Project.analyze_and_model.\nBugfixes¶\nDataset.list no longer throws errors when listing datasets with no owner.\nFixed an issue with the creation of BatchPredictionJobDefinitions containing a schedule.\nFixed error handling in datarobot.helpers.partitioning_methods.get_class.\nFixed issue with portions of the payload not using camelCasing in Project.upload_dataset_from_catalog.\nAPI changes¶\nThe Python client now outputs a DataRobotProjectDeprecationWarning when you attempt to access certain resources (projects, models, deployments, etc.) that are deprecated or disabled as a result of the DataRobot platform’s migration to Python 3.\nThe Python client now raises a TypeError when you try to retrieve a labelwise ROC on a binary model or a binary ROC on a multilabel model.\nThe method Dataset.create_from_data_source now raises InvalidUsageError if username and password are not passed as a pair together.\nDeprecation summary¶\nModel.get_leaderboard_ui_permalink has been removed.\nUse Model.get_uri instead.\nModel.open_model_browser has been removed.\nUse Model.open_in_browser instead.\nProject.get_leaderboard_ui_permalink has been removed.\nUse Project.get_uri instead.\nProject.open_leaderboard_browser has been removed.\nUse Project.open_in_browser instead.\nEnum VARIABLE_TYPE_TRANSFORM.CATEGORICAL has been removed\nInstantiation of Blueprint using a dict has been removed. Use Blueprint.from_data instead.\nSpecifying an environment to use for testing with CustomModelTest has been removed.\nCustomModelVersion’s required_metadata parameter has been removed. Use required_metadata_values instead.\nCustomTaskVersion’s required_metadata parameter has been removed. Use required_metadata_values instead.\nInstantiation of Feature using a dict has been removed. Use Feature.from_data instead.\nInstantiation of Featurelist using a dict has been removed. Use Featurelist.from_data instead.\nInstantiation of Model using a dict, tuple, or the data parameter has been removed. Use Model.from_data instead.\nInstantiation of Project using a dict has been removed. Use Project.from_data instead.\nProject’s quickrun parameter has been removed. Pass AUTOPILOT_MODE.QUICK as the mode instead.\nProject’s scaleout_max_train_pct and scaleout_max_train_rows parameters have been removed.\nComplianceDocumentation has been removed. Use AutomatedDocument instead.\nThe Deployment method create_from_custom_model_image was removed. Use Deployment.create_from_custom_model_version instead.\nPredictJob.create has been removed. Use Model.request_predictions instead.\nModel.fetch_resource_data has been removed. Use Model.get instead.\nThe class CustomInferenceImage was removed. Use CustomModelVersion with base_environment_id instead.\nProject.set_target has been deprecated. Use Project.analyze_and_model instead.\nConfiguration changes¶\nAdded a context manager client_configuration that can be used to change the connection configuration temporarily, for use in asynchronous or multithreaded code.\nUpgraded the Pillow library to version 9.2.0. Users installing DataRobot with the “images” extra (pip install datarobot[images]) should note that this is a required library.\nExperimental changes¶\nAdded experimental support for retrieving document thumbnails:\nDocumentThumbnail\nDocumentPageFile\nAdded experimental support to retrieve document text extraction samples using:\nDocumentTextExtractionSample\nDocumentTextExtractionSamplePage\nDocumentTextExtractionSampleDocument\nAdded experimental deployment improvements:\nRetrainingPolicy can be used to manage retraining policies associated with a deployment.\nAdded an experimental deployment improvement:\nUse RetrainingPolicyRun to manage retraining policies run for a retraining policy associated with a deployment.\nAdded new methods to RetrainingPolicy:\nUse RetrainingPolicy.get to get a retraining policy associated with a deployment.\nUse RetrainingPolicy.delete to delete a retraining policy associated with a deployment.\n2.29.0¶\nNew features¶\nAdded support to pass max_ngram_explanations parameter in batch predictions that will trigger the\ncompute of text prediction explanations.\nBatchPredictionJob.score\nAdded support to pass calculation mode to prediction explanations\n(mode parameter in PredictionExplanations.create)\nas well as batch scoring\n(explanations_mode in BatchPredictionJob.score)\nfor multiclass models. Supported modes:\nTopPredictionsMode\nClassListMode\nAdded method datarobot.CalendarFile.create_calendar_from_dataset() to the calendar file that allows us\nto create a calendar from a dataset.\nAdded experimental support for n_clusters parameter in\nModel.train_datetime and\nDatetimeModel.retrain\nthat allows to specify number of clusters when creating models in Time Series Clustering project.\nAdded new parameter clone to datarobot.CombinedModel.set_segment_champion() that allows to\nset a new champion model in a cloned model instead of the original one, leaving latter unmodified.\nAdded new property is_active_combined_model to datarobot.CombinedModel that indicates\nif the selected combined model is currently the active one in the segmented project.\nAdded new datarobot.models.Project.get_active_combined_model() that allows users to get\nthe currently active combined model in the segmented project.\nAdded new parameters read_timeout to method ShapMatrix.get_as_dataframe.\nValues larger than the default can be specified to avoid timeouts when requesting large files.\nShapMatrix.get_as_dataframe\nAdded support for bias mitigation with the following methods\nProject.get_bias_mitigated_models\nProject.apply_bias_mitigation\nProject.request_bias_mitigation_feature_info\nProject.get_bias_mitigation_feature_info\nand by adding new bias mitigation params\nbias_mitigation_feature_name\nbias_mitigation_technique\ninclude_bias_mitigation_feature_as_predictor_variable\nto the existing method\nProject.start\nand by adding this enum to supply params to some of the above functionality datarobot.enums.BiasMitigationTechnique\nAdded new property status to datarobot.models.Deployment that represents model deployment status.\nAdded new Deployment.activate\nand Deployment.deactivate\nthat allows deployment activation and deactivation\nAdded new Deployment.delete_monitoring_data to delete deployment monitoring data.\nEnhancements¶\nAdded support for specifying custom endpoint URLs for S3 access in batch predictions:\nBatchPredictionJob.score\nBatchPredictionJob.score\nSee: endpoint_url parameter.\n- Added guide on :ref:working with binary data <binary_data>\n- Added multithreading support to binary data helper functions.\n- Binary data helpers image defaults aligned with application’s image preprocessing.\n- Added the following accuracy metrics to be retrieved for a deployment - TPR, PPV, F1 and MCC :ref:Deployment monitoring <deployment_monitoring>\nBugfixes¶\nDon’t include holdout start date, end date, or duration in datetime partitioning payload when\nholdout is disabled.\nRemoved ICE Plot capabilities from Feature Fit.\nHandle undefined calendar_name in CalendarFile.create_calendar_from_dataset\nRaise ValueError for submitted calendar names that are not strings\nAPI changes¶\nversion field is removed from ImportedModel object\nDeprecation summary¶\nReason Codes objects deprecated in 2.13 version were removed.\nPlease use Prediction Explanations instead.\nConfiguration changes¶\nThe upper version constraint on pandas has been removed.\nDocumentation changes¶\nFixed a minor typo in the example for Dataset.create_from_data_source.\nUpdate the documentation to suggest that feature_derivation_window_end of datarobot.DatetimePartitioningSpecification class should be a negative or zero.\n2.28.0¶\nNew features¶\nAdded new parameter upload_read_timeout to BatchPredictionJob.score\nand BatchPredictionJob.score_to_file to indicate how many seconds to wait\nuntil intake dataset uploads to server. Default value 600s.\nAdded the ability to turn off supervised feature reduction for Time Series projects. Option\nuse_supervised_feature_reduction can be set in AdvancedOptions.\nAllow maximum_memory to be input for custom tasks versions. This will be used for setting the limit\nto which a custom task prediction container memory can grow.\nAdded method datarobot.models.Project.get_multiseries_names() to the project service which will\nreturn all the distinct entries in the multiseries column\nAdded new segmentation_task_id attribute to datarobot.models.Project.set_target() that allows to\nstart project as Segmented Modeling project.\nAdded new property is_segmented to datarobot.models.Project that indicates if project is a\nregular one or Segmented Modeling project.\nAdded method datarobot.models.Project.restart_segment() to the project service that allows to\nrestart single segment that hasn’t reached modeling phase.\nAdded the ability to interact with Combined Models in Segmented Modeling projects.\nAvailable with new class: datarobot.CombinedModel.\nFunctionality:\n: - datarobot.CombinedModel.get()\n- datarobot.CombinedModel.get_segments_info()\n- datarobot.CombinedModel.get_segments_as_dataframe()\n- datarobot.CombinedModel.get_segments_as_csv()\n- datarobot.CombinedModel.set_segment_champion()\n- Added the ability to create and retrieve segmentation tasks used in Segmented Modeling projects.\nAvailable with new class: datarobot.SegmentationTask.\nFunctionality:\n: - datarobot.SegmentationTask.create()\n- datarobot.SegmentationTask.list()\n- datarobot.SegmentationTask.get()\n- Added new class: datarobot.SegmentInfo that allows to get information on all segments of\nSegmented modeling projects, i.e. segment project ID, model counts, autopilot status.\nFunctionality:\n: - datarobot.SegmentInfo.list()\n- Added new methods to base APIObject to assist with dictionary and json serialization of child objects.\nFunctionality:\n: - APIObject.to_dict\n- APIObject.to_json\n- Added new methods to ImageAugmentationList for interacting with image augmentation samples.\nFunctionality:\n: - ImageAugmentationList.compute_samples\n- ImageAugmentationList.retrieve_samples\n- Added the ability to set a prediction threshold when creating a deployment from a learning model.\n- Added support for governance, owners, predictionEnvironment, and fairnessHealth fields when querying for a Deployment object.\n- Added helper methods for working with files, images and documents. Methods support conversion of\nfile contents into base64 string representations. Methods for images provide also image resize and\ntransformation support.\nFunctionality:\n: - get_encoded_file_contents_from_urls\n- get_encoded_file_contents_from_paths\n- get_encoded_image_contents_from_paths\n- get_encoded_image_contents_from_urls\nEnhancements¶\nRequesting metadata instead of actual data of datarobot.PredictionExplanations to reduce the amount of data transfer\nBugfixes¶\nFix a bug in Job.get_result_when_complete for Prediction Explanations job type to\npopulate all attribute of of datarobot.PredictionExplanations instead of just one\nFix a bug in datarobot.models.ShapImpact where row_count was not optional\nAllow blank value for schema and catalog in RelationshipsConfiguration response data\nFix a bug where credentials were incorrectly formatted in\nProject.upload_dataset_from_catalog\nand\nProject.upload_dataset_from_data_source\nRejecting downloads of Batch Prediction data that was not written to the localfile output adapter\nFix a bug in datarobot.models.BatchPredictionJobDefinition.create() where schedule was not optional for all cases\nAPI changes¶\nUser can include ICE plots data in the response when requesting Feature Effects/Feature Fit. Extended methods are\n: - Model.get_feature_effect,\n- Model.get_feature_fit <datarobot.models.Model.get_feature_fit>,\n- DatetimeModel.get_feature_effect and\n- DatetimeModel.get_feature_fit <datarobot.models.DatetimeModel.get_feature_fit>.\nDeprecation summary¶\nattrs library is removed from library dependencies\nImageAugmentationSample.compute was marked as deprecated and will be removed in v2.30. You\ncan get the same information with newly introduced method ImageAugmentationList.compute_samples\nImageAugmentationSample.list using sample_id\nDeprecating scaleout parameters for projects / models. Includes scaleout_modeling_mode,\nscaleout_max_train_pct, and scaleout_max_train_rows\nConfiguration changes¶\npandas upper version constraint is updated to include version 1.3.5.\nDocumentation changes¶\nFixed “from datarobot.enums” import in Unsupervised Clustering example provided in docs.\n2.27.0¶\nNew features¶\ndatarobot.UserBlueprint is now mature with full support of functionality. Users\nare encouraged to use the Blueprint Workshop instead of\nthis class directly.\nAdded the arguments attribute in datarobot.CustomTaskVersion.\nAdded the ability to retrieve detected errors in the potentially multicategorical feature types that prevented the\nfeature to be identified as multicategorical.\nProject.download_multicategorical_data_format_errors\nAdded the support of listing/updating user roles on one custom task.\n: - datarobot.CustomTask.get_access_list()\n- datarobot.CustomTask.share()\nAdded a method datarobot.models.Dataset.create_from_query_generator(). This creates a dataset\nin the AI catalog from a datarobot.DataEngineQueryGenerator.\nAdded the new functionality of creating a user blueprint with a custom task version id.\ndatarobot.UserBlueprint.create_from_custom_task_version_id().\nThe DataRobot Python Client is no longer published under the Apache-2.0 software license, but rather under the terms\nof the DataRobot Tool and Utility Agreement.\nAdded a new class: datarobot.DataEngineQueryGenerator. This class generates a Spark\nSQL query to apply time series data prep to a dataset in the AI catalog.\nFunctionality:\n: - datarobot.DataEngineQueryGenerator.create()\n- datarobot.DataEngineQueryGenerator.get()\n- datarobot.DataEngineQueryGenerator.create_dataset()\nSee the :ref:time series data prep documentation <time_series_data_prep> for more information.\n- Added the ability to upload a prediction dataset into a project from the AI catalog\nProject.upload_dataset_from_catalog.\n- Added the ability to specify the number of training rows to use in SHAP based Feature Impact computation. Extended\nmethod:\nShapImpact.create\nAdded the ability to retrieve and restore features that have been reduced using the time series feature generation and\nreduction functionality. The functionality comes with a new\nclass: datarobot.models.restore_discarded_features.DiscardedFeaturesInfo.\nFunctionality:\n: - datarobot.models.restore_discarded_features.DiscardedFeaturesInfo.retrieve()\n- datarobot.models.restore_discarded_features.DiscardedFeaturesInfo.restore()\n- Added the ability to control class mapping aggregation in multiclass projects via\nClassMappingAggregationSettings passed as a parameter to\nProject.set_target\n- Added support for :ref:unsupervised clustering projects<unsupervised_clustering>\n- Added the ability to compute and retrieve Feature Effects for a Multiclass model using\ndatarobot.models.Model.request_feature_effects_multiclass(),\ndatarobot.models.Model.get_feature_effects_multiclass() or\ndatarobot.models.Model.get_or_request_feature_effects_multiclass() methods. For datetime models use following\nmethods datarobot.models.DatetimeModel.request_feature_effects_multiclass(),\ndatarobot.models.DatetimeModel.get_feature_effects_multiclass() or\ndatarobot.models.DatetimeModel.get_or_request_feature_effects_multiclass() with backtest_index specified\n- Added the ability to get and update challenger model settings for deployment\nclass: datarobot.models.Deployment\nFunctionality:\n: - datarobot.models.Deployment.get_challenger_models_settings()\n- datarobot.models.Deployment.update_challenger_models_settings()\n- Added the ability to get and update segment analysis settings for deployment\nclass: datarobot.models.Deployment\nFunctionality:\n: - datarobot.models.Deployment.get_segment_analysis_settings()\n- datarobot.models.Deployment.update_segment_analysis_settings()\n- Added the ability to get and update predictions by forecast date settings for deployment\nclass: datarobot.models.Deployment\nFunctionality:\n: - datarobot.models.Deployment.get_predictions_by_forecast_date_settings()\n- datarobot.models.Deployment.update_predictions_by_forecast_date_settings()\n- Added the ability to specify multiple feature derivation windows when creating a Relationships Configuration using\nRelationshipsConfiguration.create\n- Added the ability to manipulate a legacy conversion for a custom inference model, using the\nclass: CustomModelVersionConversion\nFunctionality:\n: - CustomModelVersionConversion.run_conversion\n- CustomModelVersionConversion.stop_conversion\n- CustomModelVersionConversion.get\n- CustomModelVersionConversion.get_latest\n- CustomModelVersionConversion.list\nEnhancements¶\nProject.get returns the query_generator_id used for time series data prep when applicable.\nFeature Fit & Feature Effects can return datetime instead of numeric for feature_type field for\nnumeric features that are derived from dates.\nThese methods now provide additional field rowCount in SHAP based Feature Impact results.\nShapImpact.create\nShapImpact.get\nImproved performance when downloading prediction dataframes for Multilabel projects using:\n: - Predictions.get_all_as_dataframe\n- PredictJob.get_predictions\n- Job.get_result\nBugfixes¶\nfix datarobot.CustomTaskVersion and datarobot.CustomModelVersion to correctly format required_metadata_values\nbefore sending them via API\nFixed response validation that could cause DataError when using datarobot.models.Dataset for a dataset with a description that is an empty string.\nAPI changes¶\nRelationshipsConfiguration.create will include a\nnew key data_source_id in data_source field when applicable\nDeprecation summary¶\nModel.get_all_labelwise_roc_curves has been removed.\nYou can get the same information with multiple calls of\nModel.get_labelwise_roc_curves, one per data source.\nModel.get_all_multilabel_lift_charts has been removed.\nYou can get the same information with multiple calls of\nModel.get_multilabel_lift_charts, one per data source.\nDocumentation changes¶\nThis release introduces a new documentation organization. The organization has been modified to better reflect the end-to-end modeling workflow. The new “Tutorials” section has 5 major topics that outline the major components of modeling: Data, Modeling, Predictions, MLOps, and Administration.\nThe Getting Started workflow is now hosted at DataRobot’s API Documentation Home.\nAdded an example of how to set up optimized datetime partitioning for time series projects.\n2.26.0¶\nNew features¶\nAdded the ability to use external baseline predictions for time series project. External\ndataset can be validated using datarobot.models.Project.validate_external_time_series_baseline().\nOption can be set in AdvancedOptions to scale\ndatarobot models’ accuracy performance using external dataset’s accuracy performance.\nSee the :ref:external baseline predictions documentation <external_baseline_predictions>\nfor more information.\nAdded the ability to generate exponentially weighted moving average features for time series\nproject. Option can be set in AdvancedOptions\nand controls the alpha parameter used in exponentially weighted moving average operation.\nAdded the ability to request a specific model be prepared for deployment using\nProject.start_prepare_model_for_deployment.\nAdded a new class: datarobot.CustomTask. This class is a custom task that you can use\nas part (or all) of your blue print for training models. It needs\ndatarobot.CustomTaskVersion before it can properly be used.\nFunctionality:\n: - Create, copy, update or delete:\n: - datarobot.CustomTask.create()\n- datarobot.CustomTask.copy()\n- datarobot.CustomTask.update()\n- datarobot.CustomTask.delete()\n- list, get and refresh current tasks:\n: - datarobot.CustomTask.get()\n- datarobot.CustomTask.list()\n- datarobot.CustomTask.refresh()\n- Download the latest datarobot.CustomTaskVersion of the datarobot.CustomTask\n: - datarobot.CustomTask.download_latest_version()\n- Added a new class: datarobot.CustomTaskVersion. This class\nis for management of specific versions of a custom task.\nFunctionality:\n: - Create new custom task versions:\n: - datarobot.CustomTaskVersion.create_clean()\n- datarobot.CustomTaskVersion.create_from_previous()\n- list, get and refresh current available versions:\n: - datarobot.CustomTaskVersion.list()\n- datarobot.CustomTaskVersion.get()\n- datarobot.CustomTaskVersion.refresh()\n- datarobot.CustomTaskVersion.download()\nwill download a tarball of the files used to create the custom task\n- datarobot.CustomTaskVersion.update()\nupdates the metadata for a custom task.\n- Added the ability compute batch predictions for an in-memory DataFrame using\nBatchPredictionJob.score\n- Added the ability to specify feature discovery settings when creating a Relationships Configuration using\nRelationshipsConfiguration.create\nEnhancements¶\nImproved performance when downloading prediction dataframes using:\n: - Predictions.get_all_as_dataframe\n- PredictJob.get_predictions\n- Job.get_result\nAdded new max_wait parameter to methods:\n: - Dataset.create_from_url\n- Dataset.create_from_in_memory_data\n- Dataset.create_from_data_source\n- Dataset.create_version_from_in_memory_data\n- Dataset.create_version_from_url\n- Dataset.create_version_from_data_source\nBugfixes¶\nModel.get will return a DatetimeModel instead of Model\nwhenever the project is datetime partitioned. This enables the\nModelRecommendation.get_model to return\na DatetimeModel instead of Model whenever the project is datetime partitioned.\nTry to read Feature Impact result if existing jobId is None in\nModel.get_or_request_feature_impact.\nSet upper version constraints for pandas.\nRelationshipsConfiguration.create will return a catalog\nin data_source field\nArgument required_metadata_keys was not properly being sent in the update and create requests for\ndatarobot.ExecutionEnvironment.\nFix issue with datarobot.ExecutionEnvironment create method failing when used against older versions of the application\ndatarobot.CustomTaskVersion was not properly handling required_metadata_values from the API response\nAPI changes¶\nUpdated Project.start to use AUTOPILOT_MODE.QUICK when the\nautopilot_on param is set to True. This brings it in line with Project.set_target.\nUpdated project.start_autopilot to accept\nthe following new GA parameters that are already in the public API: consider_blenders_in_recommendation,\nrun_leakage_removed_feature_list\nDeprecation summary¶\nThe required_metadata property of datarobot.CustomModelVersion has been deprecated.\nrequired_metadata_values should be used instead.\nThe required_metadata property of datarobot.CustomTaskVersion has been deprecated.\nrequired_metadata_values should be used instead.\nConfiguration changes¶\nNow requires dependency on package scikit-learn  rather than\nsklearn. Note: This dependency is only used in example code. See\nthis scikit-learn issue for more information.\nNow permits dependency on package attrs  to be less than version 21. This\nfixes compatibility with apache-airflow.\nAllow to setup Authorization: <type> <token> type header for OAuth2 Bearer tokens.\nDocumentation changes¶\nUpdate the documentation with respect to the permission that controls AI Catalog dataset snapshot behavior.\n2.25.0¶\nNew features¶\nThere is a new AnomalyAssessmentRecord object that\nimplements public API routes to work with anomaly assessment insight. This also adds explanations\nand predictions preview classes. The insight is available for anomaly detection models in time\nseries unsupervised projects which also support calculation of Shapley values.\nAnomalyAssessmentPredictionsPreview\nAnomalyAssessmentExplanations\nFunctionality:\nInitialize an anomaly assessment insight for the specified subset.\nDatetimeModel.initialize_anomaly_assessment\nGet anomaly assessment records, shap explanations, predictions preview:\nDatetimeModel.get_anomaly_assessment_records list available records\nAnomalyAssessmentRecord.get_predictions_preview get predictions preview for the record\nAnomalyAssessmentRecord.get_latest_explanations get latest predictions along with shap explanations for the most anomalous records.\nAnomalyAssessmentRecord.get_explanations get predictions along with shap explanations for the most anomalous records for the specified range.\nDelete anomaly assessment record:\nAnomalyAssessmentRecord.delete delete record\nAdded an ability to calculate and retrieve Datetime trend plots for DatetimeModel.\nThis includes Accuracy over Time, Forecast vs Actual, and Anomaly over Time.\nPlots can be calculated using a common method:\nDatetimeModel.compute_datetime_trend_plots\nMetadata for plots can be retrieved using the following methods:\nDatetimeModel.get_accuracy_over_time_plots_metadata\nDatetimeModel.get_forecast_vs_actual_plots_metadata\nDatetimeModel.get_anomaly_over_time_plots_metadata\nPlots can be retrieved using the following methods:\nDatetimeModel.get_accuracy_over_time_plot\nDatetimeModel.get_forecast_vs_actual_plot\nDatetimeModel.get_anomaly_over_time_plot\nPreview plots can be retrieved using the following methods:\nDatetimeModel.get_accuracy_over_time_plot_preview\nDatetimeModel.get_forecast_vs_actual_plot_preview\nDatetimeModel.get_anomaly_over_time_plot_preview\nSupport for Batch Prediction Job Definitions has now been added through the following class:\nBatchPredictionJobDefinition.\nYou can create, update, list and delete definitions using the following methods:\nBatchPredictionJobDefinition.list\nBatchPredictionJobDefinition.create\nBatchPredictionJobDefinition.update\nBatchPredictionJobDefinition.delete\nEnhancements¶\nAdded a new helper function to create Dataset Definition, Relationship and Secondary Dataset used by\nFeature Discovery Project. They are accessible via\nDatasetDefinition\nRelationship\nSecondaryDataset\nAdded new helper function to projects to retrieve the recommended model.\nProject.recommended_model\nAdded method to download feature discovery recipe SQLs (limited beta feature).\nProject.download_feature_discovery_recipe_sqls.\nAdded docker_context_size and docker_image_size to datarobot.ExecutionEnvironmentVersion\nBugfixes¶\nRemove the deprecation warnings when using with latest versions of urllib3.\nFeatureAssociationMatrix.get is now using correct query param\nname when featurelist_id is specified.\nHandle scalar values in shapBaseValue while converting a predictions response to a data frame.\nEnsure that if a configured endpoint ends in a trailing slash, the resulting full URL does\nnot end up with double slashes in the path.\nModel.request_frozen_datetime_model is now implementing correct\nvalidation of input parameter training_start_date.\nAPI changes¶\nArguments secondary_datasets now accept SecondaryDataset\nto create secondary dataset configurations\nSecondaryDatasetConfigurations.create\nArguments dataset_definitions and relationships now accept DatasetDefinition Relationship\nto create and replace relationships configuration\nRelationshipsConfiguration.create creates a new relationships configuration between datasets\nRelationshipsConfiguration.retrieve retrieve the requested relationships\nconfiguration\nArgument required_metadata_keys has been added to datarobot.ExecutionEnvironment.  This should be used to\ndefine a list of RequiredMetadataKey.\ndatarobot.CustomModelVersion that use a base environment with required_metadata_keys must define\nvalues for these fields in their respective required_metadata\nArgument required_metadata has been added to datarobot.CustomModelVersion.  This should be set with\nrelevant values defined by the base environment’s required_metadata_keys\n2.24.0¶\nNew features¶\nPartial history predictions can be made with time series time series multiseries models using the\nallow_partial_history_time_series_predictions attribute of the\ndatarobot.DatetimePartitioningSpecification.\nSee the :ref:Time Series <time_series> documentation for more info.\nMulticategorical Histograms are now retrievable. They are accessible via\nMulticategoricalHistogram or\nFeature.get_multicategorical_histogram.\nAdd methods to retrieve per-class lift chart data for multilabel models:\nModel.get_multilabel_lift_charts and\nModel.get_all_multilabel_lift_charts.\nAdd methods to retrieve labelwise ROC curves for multilabel models:\nModel.get_labelwise_roc_curves and\nModel.get_all_labelwise_roc_curves.\nMulticategorical Pairwise Statistics are now retrievable. They are accessible via\nPairwiseCorrelations,\nPairwiseJointProbabilities and\nPairwiseConditionalProbabilities or\nFeature.get_pairwise_correlations,\nFeature.get_pairwise_joint_probabilities and\nFeature.get_pairwise_conditional_probabilities.\nAdd methods to retrieve prediction results of a deployment:\n: - Deployment.get_prediction_results\n- Deployment.download_prediction_results\nAdd method to download scoring code of a deployment using Deployment.download_scoring_code.\nAdded Automated Documentation: now you can automatically generate documentation about various\nentities within the platform, such as specific models or projects. Check out the\nref:\nAutomated Documentation overview<automated_documentation_overview> and also refer to\nthe :ref:API Reference<automated_documentation_api> for more details.\nCreate a new Dataset version for a given dataset by uploading from a file, URL or in-memory datasource.\n: - Dataset.create_version_from_file\n- Dataset.create_version_from_in_memory_data\n- Dataset.create_version_from_url\n- Dataset.create_version_from_data_source\nEnhancements¶\nAdded a new status called FAILED to from BatchPredictionJob as\nthis is a new status coming to Batch Predictions in an upcoming version of DataRobot.\nAdded base_environment_version_id to datarobot.CustomModelVersion.\nSupport for downloading feature discovery training or prediction dataset using\nProject.download_feature_discovery_dataset.\nAdded datarobot.models.FeatureAssociationMatrix, datarobot.models.FeatureAssociationMatrixDetails\nand datarobot.models.FeatureAssociationFeaturelists that can be used to retrieve feature associations\ndata as an alternative to Project.get_associations,\nProject.get_association_matrix_details and\nProject.get_association_featurelists methods.\nBugfixes¶\nFixed response validation that could cause DataError when using\nTrainingPredictions.list and\nTrainingPredictions.get_all_as_dataframe\nmethods if there are training predictions computed with explanation_algorithm.\nAPI changes¶\nRemove desired_memory param from the following classes: datarobot.CustomInferenceModel,\ndatarobot.CustomModelVersion, datarobot.CustomModelTest\nRemove desired_memory param from the following methods:\nCustomInferenceModel.create,\nCustomModelVersion.create_clean,\nCustomModelVersion.create_from_previous,\nCustomModelTest.create and\nCustomModelTest.create\nDeprecation summary¶\nclass ComplianceDocumentation\nwill be deprecated in v2.24 and will be removed entirely in v2.27. Use\nAutomatedDocument\ninstead. To start off, see the\nref:\nAutomated Documentation overview<automated_documentation_overview> for details.\nDocumentation changes¶\nRemove reference to S3 for Project.upload_dataset since it is not supported by the server\n2.23.0¶\nNew features¶\nCalendars for time series projects can now be automatically generated by providing a country code to the method\nCalendarFile.create_calendar_from_country_code.\nA list of allowed country codes can be retrieved using CalendarFile.get_allowed_country_codes\nFor more information, see the :ref:calendar documentation <preloaded_calendar_files>.\nAdded calculate_all_series`` param to\n[DatetimeModel.compute_series_accuracy`](datarobot-models.md#datarobot.models.DatetimeModel.compute_series_accuracy).\nThis option allows users to compute series accuracy for all available series at once,\nwhile by default it is computed for first 1000 series only.\nAdded ability to specify sampling method when setting target of OTV project. Option can be set\nin AdvancedOptions and changes a way training data\nis defined in autopilot steps.\nAdd support for custom inference model k8s resources management. This new feature enables\nusers to control k8s resources allocation for their executed model in the k8s cluster.\nIt involves in adding the following new parameters: network_egress_policy, desired_memory,\nmaximum_memory, replicas to the following classes: datarobot.CustomInferenceModel,\ndatarobot.CustomModelVersion, datarobot.CustomModelTest\nAdd support for multiclass custom inference and training models. This enables users to create\nclassification custom models with more than two class labels. The datarobot.CustomInferenceModel\nclass can now use datarobot.TARGET_TYPE.MULTICLASS for their target_type parameter. Class labels for inference models\ncan be set/updated using either a file or as a list of labels.\nSupport for Listing all the secondary dataset configuration for a given project:\n: - SecondaryDatasetConfigurations.list\nAdd support for unstructured custom inference models. The datarobot.CustomInferenceModel\nclass can now use datarobot.TARGET_TYPE.UNSTRUCTURED for its target_type parameter.\ntarget_name parameter is optional for UNSTRUCTURED target type.\nAll per-class lift chart data is now available for multiclass models using\nModel.get_multiclass_lift_chart.\nAUTOPILOT_MODE.COMPREHENSIVE, a new mode, has been added to\nProject.set_target.\nAdd support for anomaly detection custom inference models. The datarobot.CustomInferenceModel\nclass can now use datarobot.TARGET_TYPE.ANOMALY for its target_type parameter.\ntarget_name parameter is optional for ANOMALY target type.\nSupport for Updating and retrieving the secondary dataset configuration for a Feature discovery deployment:\n: - Deployment.update_secondary_dataset_config\n- Deployment.get_secondary_dataset_config\nAdd support for starting and retrieving Feature Impact information for datarobot.CustomModelVersion\nSearch for interaction features and Supervised Feature reduction for feature discovery project can now be specified\n: in AdvancedOptions.\nFeature discovery projects can now be created using the Project.start\nmethod by providing relationships_configuration_id.\nActions applied to input data during automated feature discovery can now be retrieved using FeatureLineage.get\nCorresponding feature lineage id is available as a new datarobot.models.Feature field feature_lineage_id.\nLift charts and ROC curves are now calculated for backtests 2+ in time series and OTV models.\nThe data can be retrieved for individual backtests using Model.get_lift_chart\nand Model.get_roc_curve.\nThe following methods now accept a new argument called credential_data, the credentials to authenticate with the database, to use instead of user/password or credential ID:\n: - Dataset.create_from_data_source\n- Dataset.create_project\n- Project.create_from_dataset\nAdd support for DataRobot Connectors, datarobot.Connector provides a simple implementation to interface with connectors.\nEnhancements¶\nRunning Autopilot on Leakage Removed feature list can now be specified in AdvancedOptions.\nBy default, Autopilot will always run on Informative Features - Leakage Removed feature list if it exists. If the parameter\nrun_leakage_removed_feature_list is set to False, then Autopilot will run on Informative Features or available custom feature list.\nMethod Project.upload_dataset\nand Project.upload_dataset_from_data_source\nsupport new optional parameter secondary_datasets_config_id for Feature discovery project.\nBugfixes¶\nadded disable_holdout param in datarobot.DatetimePartitioning\nUsing Credential.create_gcp produced an incompatible credential\nSampleImage.list now supports Regression & Multilabel projects\nUsing BatchPredictionJob.score could in some circumstances\nresult in a crash from trying to abort the job if it fails to start\nUsing BatchPredictionJob.score or\nBatchPredictionJob.score would produce incomplete\nresults in case a job was aborted while downloading. This will now raise an exception.\nAPI changes¶\nNew sampling_method param in Model.train_datetime,\nProject.train_datetime,\nModel.train_datetime and\nModel.train_datetime.\nNew target_type param in datarobot.CustomInferenceModel\nNew arguments secondary_datasets, name, creator_full_name, creator_user_id, created,\n: featurelist_id, credentials_ids, project_version and is_default in datarobot.models.SecondaryDatasetConfigurations\nNew arguments secondary_datasets, name, featurelist_id to\n: SecondaryDatasetConfigurations.create\nClass FeatureEngineeringGraph has been removed. Use datarobot.models.RelationshipsConfiguration instead.\nParam feature_engineering_graphs removed from Project.set_target.\nParam config removed from SecondaryDatasetConfigurations.create.\nDeprecation summary¶\nsupports_binary_classification and  supports_regression are deprecated\n: for datarobot.CustomInferenceModel and will be removed in v2.24\nArgument config and  supports_regression are deprecated\n: for datarobot.models.SecondaryDatasetConfigurations and will be removed in v2.24\nCustomInferenceImage has been deprecated and will be removed in v2.24.\n: datarobot.CustomModelVersion with base_environment_id should be used in their place.\nenvironment_id and environment_version_id are deprecated for CustomModelTest.create\nDocumentation changes¶\nfeature_lineage_id is added as a new parameter in the response for retrieval of a datarobot.models.Feature created by automated feature discovery or time series feature derivation.\nThis id is required to retrieve a datarobot.models.FeatureLineage instance.\n2.22.1¶\nNew features¶\nBatch Prediction jobs now support :ref:dataset <batch_predictions-intake-types-dataset> as intake settings for\nBatchPredictionJob.score.\nCreate a Dataset from DataSource:\nDataset.create_from_data_source\nDataSource.create_dataset\nAdded support for Custom Model Dependency Management.  Please see :ref:custom model documentation<custom_models>.\nNew features added:\nAdded new argument base_environment_id to methods\nCustomModelVersion.create_clean\nand CustomModelVersion.create_from_previous\nNew fields base_environment_id and dependencies to class\ndatarobot.CustomModelVersion\nNew class datarobot.CustomModelVersionDependencyBuild\nto prepare custom model versions with dependencies.\nMade argument environment_id of\nCustomModelTest.create optional to enable using\ncustom model versions with dependencies\nNew field image_type added to class\ndatarobot.CustomModelTest\nDeployment.create_from_custom_model_version can be used to create a deployment from a custom model version.\nAdded new parameters for starting and re-running Autopilot with customizable settings within\nProject.start_autopilot.\nAdded a new method to trigger Feature Impact calculation for a Custom Inference Image:\nCustomInferenceImage.calculate_feature_impact\nAdded new method to retrieve number of iterations trained for early stopping models. Currently supports only tree-based models.\nModel.get_num_iterations_trained.\nEnhancements¶\nA description can now be added or updated for a project.\nProject.set_project_description.\nAdded new parameters read_timeout and max_wait to method Dataset.create_from_file.\nValues larger than the default can be specified for both to avoid timeouts when uploading large files.\nAdded new parameter metric to datarobot.models.deployment.TargetDrift, datarobot.models.deployment.FeatureDrift,\nDeployment.get_target_drift\nand Deployment.get_feature_drift.\nAdded new parameter timeout to BatchPredictionJob.download to indicate\nhow many seconds to wait for the download to start (in case the job doesn’t start processing immediately).\nSet to -1 to disable.\nThis parameter can also be sent as download_timeout to BatchPredictionJob.score\nand BatchPredictionJob.score.\nIf the timeout occurs, the pending job will be aborted.\nAdded new parameter read_timeout to BatchPredictionJob.download to indicate\nhow many seconds to wait between each downloaded chunk.\nThis parameter can also be sent as download_read_timeout to BatchPredictionJob.score\nand BatchPredictionJob.score.\nAdded parameter catalog to BatchPredictionJob to both intake\nand output adapters for type jdbc.\nConsider blenders in recommendation can now be specified in AdvancedOptions.\nBlenders will be included when autopilot chooses a model to prepare and recommend for deployment.\nAdded optional parameter max_wait to Deployment.replace_model to indicate\nthe maximum time to wait for model replacement job to complete before erroring.\nBugfixes¶\nHandle null values in predictionExplanationMetadata[\"shapRemainingTotal\"] while converting a predictions\nresponse to a data frame.\nHandle null values in customModel[\"latestVersion\"]\nRemoved an extra column status from BatchPredictionJob as\nit caused issues with never version of Trafaret validation.\nMake predicted_vs_actual optional in Feature Effects data because a feature may have insufficient qualified samples.\nMake jdbc_url optional in Data Store data because some data stores will not have it.\nThe method Project.get_datetime_models now correctly returns all\nDatetimeModel objects for the project, instead of just the first 100.\nFixed a documentation error related to snake_case vs camelCase in the JDBC settings payload.\nMake trafaret validator for datasets use a syntax that works properly with a wider range of trafaret versions.\nHandle extra keys in CustomModelTests and CustomModelVersions\nImageEmbedding and ImageActivationMap now supports regression projects.\nAPI changes¶\nThe default value for the mode param in Project.set_target has been changed from AUTOPILOT_MODE.FULL_AUTO\nto AUTOPILOT_MODE.QUICK\nDocumentation changes¶\nAdded links to classes with duration parameters such as validation_duration and holdout_duration to\nprovide duration string examples to users.\nThe :ref:models documentation <models> has been revised to include section on how to train a new model and how to run cross-validation\nor backtesting for a model.\n2.21.0¶\nNew features¶\nAdded new arguments explanation_algorithm and max_explanations to method\nModel.request_training_predictions.\nNew fields explanation_algorithm, max_explanations and shap_warnings have been added to class\nTrainingPredictions.\nNew fields prediction_explanations and shap_metadata have been added to class\nTrainingPredictionsIterator that is\nreturned by method\nTrainingPredictions.iterate_rows.\nAdded new arguments explanation_algorithm and max_explanations to method\nModel.request_predictions. New fields explanation_algorithm,\nmax_explanations and shap_warnings have been added to class\nPredictions. Method\nPredictions.get_all_as_dataframe has new argument\nserializer that specifies the retrieval and results validation method (json or csv) for the predictions.\nAdded possibility to compute ShapImpact.create and request\nShapImpact.get SHAP impact scores for features in a model.\nAdded support for accessing Visual AI images and insights. See the DataRobot\nPython Package documentation, Visual AI Projects, section for details.\nUser can specify custom row count when requesting Feature Effects. Extended methods are\nModel.request_feature_effect and\nModel.get_or_request_feature_effect.\nUsers can request SHAP based predictions explanations for a models that support SHAP scores using\nShapMatrix.create.\nAdded two new methods to Dataset to lazily retrieve paginated\nresponses.\nDataset.iterate returns an iterator of the datasets\nthat a user can view.\nDataset.iterate_all_features returns an\niterator of the features of a dataset.\nIt’s possible to create an Interaction feature by combining two categorical features together using\nProject.create_interaction_feature.\nOperation result represented by models.InteractionFeature..\nSpecific information about an interaction feature may be retrieved by its name using\nmodels.InteractionFeature.get\nAdded the DatasetFeaturelist class to support featurelists\non datasets in the AI Catalog. DatasetFeaturelists can be updated or deleted. Two new methods were\nalso added to Dataset to interact with DatasetFeaturelists. These are\nDataset.get_featurelists and\nDataset.create_featurelist which list existing\nfeaturelists and create new featurelists on a dataset, respectively.\nAdded model_splits to DatetimePartitioningSpecification and\nto DatetimePartitioning. This will allow users to control the\njobs per model used when building models. A higher number of model_splits  will result in less downsampling,\nallowing the use of more post-processed data.\nAdded support for :ref:unsupervised projects<unsupervised_anomaly>.\nAdded support for external test set. Please see :ref:testset documentation<external_testset>\nA new workflow is available for assessing models on external test sets in time series unsupervised projects.\nMore information can be found in the :ref:documentation<unsupervised_external_dataset>.\nProject.upload_dataset and\nModel.request_predictions now accept\nactual_value_column - name of the actual value column, can be passed only with date range.\nPredictionDataset objects now contain the following\nnew fields:\nactual_value_column: Actual value column which was selected for this dataset.\ndetected_actual_value_column: A list of detected actual value column info.\nNew warning is added to data_quality_warnings of datarobot.models.PredictionDataset: single_class_actual_value_column.\nScores and insights on external test sets can be retrieved using\nExternalScores, ExternalLiftChart, ExternalRocCurve.\nUsers can create payoff matrices for generating profit curves for binary classification projects\nusing PayoffMatrix.create.\nDeployment Improvements:\ndatarobot.models.deployment.TargetDrift can be used to retrieve target drift information.\ndatarobot.models.deployment.FeatureDrift can be used to retrieve feature drift information.\nDeployment.submit_actuals will submit actuals in batches if the total number of actuals exceeds the limit of one single request.\nDeployment.create_from_custom_model_image can be used to create a deployment from a custom model image.\nDeployments now support predictions data collection that enables prediction requests and results to be saved in Predictions Data Storage. See\nDeployment.get_predictions_data_collection_settings\nand Deployment.update_predictions_data_collection_settings for usage.\nNew arguments send_notification and include_feature_discovery_entities are added to Project.share.\nNow it is possible to specify the number of training rows to use in feature impact computation on supported project\ntypes (that is everything except unsupervised, multi-class, time-series). This does not affect SHAP based feature\nimpact. Extended methods:\nModel.request_feature_impact\nModel.get_or_request_feature_impact\nA new class FeatureImpactJob is added to retrieve Feature Impact\nrecords with metadata. The regular Job still works as before.\nAdded support for custom models. Please see :ref:custom model documentation<custom_models>.\nClasses added:\ndatarobot.ExecutionEnvironment and datarobot.ExecutionEnvironmentVersion to create and manage\ncustom model executions environments\ndatarobot.CustomInferenceModel and datarobot.CustomModelVersion\nto create and manage custom inference models\ndatarobot.CustomModelTest to perform testing of custom models\nBatch Prediction jobs now support forecast and historical Time Series predictions using the new\nargument timeseries_settings for BatchPredictionJob.score.\nBatch Prediction jobs now support scoring to Azure and Google Cloud Storage with methods\nBatchPredictionJob.score_azure and\nBatchPredictionJob.score_gcp.\nNow it’s possible to create Relationships Configurations to introduce secondary datasets to projects. A configuration specifies additional datasets to be included to a project and how these datasets are related to each other, and the primary dataset. When a relationships configuration is specified for a project, Feature Discovery will create features automatically from these datasets.\n: - RelationshipsConfiguration.create creates a new relationships configuration between datasets\n- RelationshipsConfiguration.retrieve retrieve the requested relationships configuration\n- RelationshipsConfiguration.replace replace the relationships configuration details with new one\n- RelationshipsConfiguration.delete delete the relationships configuration\nEnhancements¶\nMade creating projects from a dataset easier through the new\nDataset.create_project.\nThese methods now provide additional metadata fields in Feature Impact results if called with\nwith_metadata=True. Fields added: rowCount, shapBased, ranRedundancyDetection,\ncount.\nModel.get_feature_impact\nModel.request_feature_impact\nModel.get_or_request_feature_impact\nSecondary dataset configuration retrieve and deletion is easier now though new\nSecondaryDatasetConfigurations.delete soft deletes a Secondary dataset configuration.\nSecondaryDatasetConfigurations.get retrieve a Secondary dataset configuration.\nRetrieve relationships configuration which is applied on the given feature discovery project using\nProject.get_relationships_configuration.\nBugfixes¶\nAn issue with input validation of the Batch Prediction module\nparent_model_id was not visible for all frozen models\nBatch Prediction jobs that used other output types than local_file failed when using .wait_for_completion()\nA race condition in the Batch Prediction file scoring logic\nAPI changes¶\nThree new fields were added to the Dataset object. This reflects the\nupdated fields in the public API routes at api/v2/datasets/. The added fields are:\nprocessing_state: Current ingestion process state of the dataset\nrow_count: The number of rows in the dataset.\nsize: The size of the dataset as a CSV in bytes.\nDeprecation summary¶\ndatarobot.enums.VARIABLE_TYPE_TRANSFORM.CATEGORICAL for is deprecated for the following and will be removed in  v2.22.\n: - meth:Project.batch_features_type_transform\n- meth:Project.create_type_transform_feature\n2.20.0¶\nNew features¶\nThere is a new Dataset object that implements some of the\npublic API routes at api/v2/datasets/. This also adds two new feature classes and a details\nclass.\nDatasetFeature\nDatasetFeatureHistogram\nDatasetDetails\nFunctionality:\nCreate a Dataset by uploading from a file, URL or in-memory datasource.\nDataset.create_from_file\nDataset.create_from_in_memory_data\nDataset.create_from_url\nGet Datasets or elements of Dataset with:\nDataset.list lists available Datasets\nDataset.get gets a specified Dataset\nDataset.update updates the Dataset with the latest server information.\nDataset.get_details gets the DatasetDetails of the Dataset.\nDataset.get_all_features gets a list of the Dataset’s Features.\nDataset.get_file downloads the Dataset as a csv file.\nDataset.get_projects gets a list of Projects that use the Dataset.\nModify, delete or un-delete a Dataset:\nDataset.modify Changes the name and categories of the Dataset\nDataset.delete soft deletes a Dataset.\nDataset.un_delete un-deletes the Dataset. You cannot retrieve the\nIDs of deleted Datasets, so if you want to un-delete a Dataset, you need to store its ID before deletion.\nYou can also create a Project using a Dataset with:\nProject.create_from_dataset\nIt is possible to create an alternative configuration for the secondary dataset which can be used during the prediction\nSecondaryDatasetConfigurations.create allow to create secondary dataset configuration\nYou can now filter the deployments returned by the Deployment.list command. You can do this by passing an instance of the DeploymentListFilters class to the filters keyword argument. The currently supported filters are:\nrole\nservice_health\nmodel_health\naccuracy_health\nexecution_environment_type\nmateriality\nA new workflow is available for making predictions in time series projects. To that end,\nPredictionDataset objects now contain the following\nnew fields:\nforecast_point_range: The start and end date of the range of dates available for use as the forecast point,\ndetected based on the uploaded prediction dataset\ndata_start_date: A datestring representing the minimum primary date of the prediction dataset\ndata_end_date: A datestring representing the maximum primary date of the prediction dataset\nmax_forecast_date: A datestring representing the maximum forecast date of this prediction dataset\nAdditionally, users no longer need to specify a forecast_point or predictions_start_date and\npredictions_end_date when uploading datasets for predictions in time series projects. More information can be\nfound in the :ref:time series predictions<new_pred_ux> documentation.\n- Per-class lift chart data is now available for multiclass models using\nModel.get_multiclass_lift_chart.\n- Unsupervised projects can now be created using the Project.start\nand Project.set_target methods by providing unsupervised_mode=True,\nprovided that the user has access to unsupervised machine learning functionality. Contact support for more information.\n- A new boolean attribute unsupervised_mode was added to datarobot.DatetimePartitioningSpecification.\nWhen it is set to True, datetime partitioning for unsupervised time series projects will be constructed for\nnowcasting: forecast_window_start=forecast_window_end=0.\n- Users can now configure the start and end of the training partition as well as the end of the validation partition for\nbacktests in a datetime-partitioned project. More information and example usage can be found in the\n* ref:\nbacktesting documentation <backtest_configuration>.\nEnhancements¶\nUpdated the user agent header to show which python version.\nModel.get_frozen_child_models can be used to retrieve models that are frozen from a given model\nAdded datarobot.enums.TS_BLENDER_METHOD to make it clearer which blender methods are allowed for use in time\nseries projects.\nBugfixes¶\nAn issue where uploaded CSV’s would loose quotes during serialization causing issues when columns containing line terminators where loaded in a dataframe, has been fixed\nProject.get_association_featurelists is now using the correct endpoint name, but the old one will continue to work\nPython API PredictionServer supports now on-premise format of API response.\n2.19.0¶\nNew features¶\nProjects can be cloned using Project.clone_project\nCalendars used in time series projects now support having series-specific events, for instance if a holiday only affects some stores. This can be controlled by using new argument of the CalendarFile.create method.\nIf multiseries id columns are not provided, calendar is considered to be single series and all events are applied to all series.\nWe have expanded prediction intervals availability to the following use-cases:\nTime series model deployments now support prediction intervals. See\nDeployment.get_prediction_intervals_settings\nand Deployment.update_prediction_intervals_settings for usage.\nPrediction intervals are now supported for model exports for time series. To that end, a new optional parameter\nprediction_intervals_size has been added to Model.request_transferable_export <datarobot.models.Model.request_transferable_export>.\nMore details on prediction intervals can be found in the :ref:prediction intervals documentation <prediction_intervals>.\n- Allowed pairwise interaction groups can now be specified in AdvancedOptions.\nThey will be used in GAM models during training.\n- New deployments features:\nUpdate the label and description of a deployment using Deployment.update.\nref:\nAssociation ID setting<deployment_association_id> can be retrieved and updated.\nRegression deployments now support :ref:prediction warnings<deployment_prediction_warning>.\nFor multiclass models now it’s possible to get feature impact for each individual target class using\nModel.get_multiclass_feature_impact\nAdded support for new :ref:Batch Prediction API <batch_predictions>.\nIt is now possible to create and retrieve basic, oauth and s3 credentials with\nCredential.\nIt’s now possible to get feature association statuses for featurelists using\nProject.get_association_featurelists\nYou can also pass a specific featurelist_id into\nProject.get_associations\nEnhancements¶\nAdded documentation to Project.get_metrics to detail the new ascending field that\nindicates how a metric should be sorted.\nRetraining of a model is processed asynchronously and returns a  ModelJob immediately.\nBlender models can be retrained on a different set of data or a different feature list.\nWord cloud ngrams now has variable field representing the source of the ngram.\nMethod WordCloud.ngrams_per_class can be used to\nsplit ngrams for better usability in multiclass projects.\nMethod Project.set_target support new optional parameters featureEngineeringGraphs and credentials.\nMethod Project.upload_dataset and Project.upload_dataset_from_data_source support new optional parameter credentials.\nSeries accuracy retrieval methods (DatetimeModel.get_series_accuracy_as_dataframe\nand DatetimeModel.download_series_accuracy_as_csv)\nfor multiseries time series projects now support additional parameters for specifying what data to retrieve, including:\nmetric: Which metric to retrieve scores for\nmultiseries_value: Only returns series with a matching multiseries ID\norder_by: An attribute by which to sort the results\nBugfixes¶\nAn issue when using Feature.get and ModelingFeature.get to retrieve summarized categorical feature has been fixed.\nAPI changes¶\nThe datarobot package is now no longer a\nnamespace package.\ndatarobot.enums.BLENDER_METHOD.FORECAST_DISTANCE is removed (deprecated in 2.18.0).\nDocumentation changes¶\nUpdated :ref:Residuals charts <residuals_chart> documentation to reflect that the data rows include row numbers from the source dataset for projects\ncreated in DataRobot 5.3 and newer.\n2.18.0¶\nNew features¶\nref:\nResiduals charts <residuals_chart> can now be retrieved for non-time-aware regression models.\nref:\nDeployment monitoring <deployment_monitoring> can now be used to retrieve service stats, service health, accuracy info, permissions, and feature lists for deployments.\nref:\nTime series <time_series> projects now support the Average by Forecast Distance blender, configured with more than one Forecast Distance. The blender blends the selected models, selecting the best three models based on the backtesting score for each Forecast Distance and averaging their predictions. The new blender method FORECAST_DISTANCE_AVG has been added to datarobot.enums.BLENDER_METHOD.\nDeployment.submit_actuals can now be used to submit data about actual results from a deployed model, which can be used to calculate accuracy metrics.\nEnhancements¶\nMonotonic constraints are now supported for OTV projects. To that end, the parameters monotonic_increasing_featurelist_id and monotonic_decreasing_featurelist_id can be specified in calls to Model.train_datetime or Project.train_datetime.\nWhen retrieving information about features, information about summarized categorical variables is now available in a new keySummary.\nFor Word Clouds in multiclass projects, values of the target class for corresponding word or ngram can now be passed using the new class parameter.\nListing deployments using Deployment.list now support sorting and searching the results using the new order_by and search parameters.\nYou can now get the model associated with a model job by getting the model variable on the model job object.\nThe Blueprint class can now retrieve the recommended_featurelist_id, which indicates which feature list is recommended for this blueprint. If the field is not present, then there is no recommended feature list for this blueprint.\nThe Model class now can be used to retrieve the model_number.\nThe method Model.get_supported_capabilities now has an extra field supportsCodeGeneration to explain whether the model supports code generation.\nCalls to Project.start and Project.upload_dataset now support uploading data via S3 URI and pathlib.Path objects.\nErrors upon connecting to DataRobot are now clearer when an incorrect API Token is used.\nThe datarobot package is now a namespace package.\nDeprecation summary¶\ndatarobot.enums.BLENDER_METHOD.FORECAST_DISTANCE is deprecated and will be removed in 2.19. Use FORECAST_DISTANCE_ENET instead.\nDocumentation changes¶\nVarious typo and wording issues have been addressed.\nA new notebook showing regression-specific features is now been added to the examples_index.\nDocumentation for :ref:Access lists <sharing> has been added.\n2.17.0¶\nNew features¶\nref:\nDeployments <deployments_overview> can now be managed via the API by using the new Deployment class.\nUsers can now list available prediction servers using PredictionServer.list.\nWhen specifying datetime partitioning settings , :ref:time series <time_series> projects can now mark individual features as excluded from feature derivation using the\nFeatureSettings.do_not_derive attribute. Any features not specified will be assigned according to the DatetimePartitioningSpecification.default_to_do_not_derive value.\nUsers can now submit multiple feature type transformations in a single batch request using Project.batch_features_type_transform.\nref:\nAdvanced Tuning <advanced_tuning> for non-Eureqa models (beta feature) is now enabled by default for all users.\nAs of v2.17, all models are now supported other than blenders, open source, prime, scaleout, baseline and user-created.\nInformation on feature clustering and the association strength between pairs of numeric or categorical features is now available.\nProject.get_associations can be used to retrieve pairwise feature association statistics and\nProject.get_association_matrix_details can be used to get a sample of the actual values used to measure association strength.\nEnhancements¶\nnumber_of_do_not_derive_features has been added to the datarobot.DatetimePartitioning class to specify the number of features that are marked as excluded from derivation.\nUsers with PyYAML>=5.1 will no longer receive a warning when using the datarobot package\nIt is now possible to use files with unicode names for creating projects and prediction jobs.\nUsers can now embed DataRobot-generated content in a ComplianceDocTemplate using keyword tags. :ref:See here <automated_documentation_overview> for more details.\nThe field calendar_name has been added to datarobot.DatetimePartitioning to display the name of the calendar used for a project.\nref:\nPrediction intervals <prediction_intervals> are now supported for start-end retrained models in a time series project.\nPreviously, all backtests had to be run before :ref:prediction intervals <prediction_intervals> for a time series project could be requested with predictions.\nNow, backtests will be computed automatically if needed when prediction intervals are requested.\nBugfixes¶\nAn issue affecting time series project creation for irregularly spaced dates has been fixed.\nComplianceDocTemplate now supports empty text blocks in user sections.\nAn issue when using Predictions.get to retrieve predictions metadata has been fixed.\nDocumentation changes¶\nAn overview on working with class ComplianceDocumentation and ComplianceDocTemplate has been created. :ref:See here <automated_documentation_overview> for more details.\n2.16.0¶\nNew features¶\nThree new methods for Series Accuracy have been added to the DatetimeModel class.\nStart a request to calculate Series Accuracy with\nDatetimeModel.compute_series_accuracy\nOnce computed, Series Accuracy can be retrieved as a pandas.DataFrame using\nDatetimeModel.get_series_accuracy_as_dataframe\nOr saved as a CSV using\nDatetimeModel.download_series_accuracy_as_csv\nUsers can now access :ref:prediction intervals <prediction_intervals> data for each prediction with a DatetimeModel.\nFor each model, prediction intervals estimate the range of values DataRobot expects actual values of the target to fall within.\nThey are similar to a confidence interval of a prediction, but are based on the residual errors measured during the\nbacktesting for the selected model.\nEnhancements¶\nInformation on the effective feature derivation window is now available for :ref:time series projects <time_series> to specify the full span of historical data\nrequired at prediction time. It may be longer than the feature derivation window of the project depending on the differencing settings used.\nAdditionally, more of the project partitioning settings are also available on the\nDatetimeModel class.  The new attributes are:\neffective_feature_derivation_window_start\neffective_feature_derivation_window_end\nforecast_window_start\nforecast_window_end\nwindows_basis_unit\nPrediction metadata is now included in the return of Predictions.get\nDocumentation changes¶\nVarious typo and wording issues have been addressed.\nThe example data that was meant to accompany the Time Series examples has been added to the\nzip file of the download in the examples_index.\n2.15.1¶\nEnhancements¶\nCalendarFile.get_access_list has been added to the CalendarFile class to return a list of users with access to a calendar file.\nA role attribute has been added to the CalendarFile class to indicate the access level a current user has to a calendar file. For more information on the specific access levels, see the :ref:sharing <sharing> documentation.\nBugfixes¶\nPreviously, attempting to retrieve the calendar_id of a project without a set target would result in an error.\nThis has been fixed to return None instead.\n2.15.0¶\nNew features¶\nPreviously available for only Eureqa models, Advanced Tuning methods and objects, including\nModel.start_advanced_tuning_session,\nModel.get_advanced_tuning_parameters,\nModel.advanced_tune, and\nAdvancedTuningSession,\nnow support all models other than blender, open source, and user-created models.  Use of\nAdvanced Tuning via API for non-Eureqa models is in beta and not available by default, but can be\nenabled.\nCalendar Files for time series projects can now be created and managed through the CalendarFile class.\nEnhancements¶\nThe dataframe returned from\ndatarobot.PredictionExplanations.get_all_as_dataframe() will now have\neach class label class_X be the same from row to row.\nThe client is now more robust to networking issues by default. It will retry on more errors and respects Retry-After headers in HTTP 413, 429, and 503 responses.\nAdded Forecast Distance blender for Time-Series projects configured with more than one Forecast\nDistance. It blends the selected models creating separate linear models for each Forecast Distance.\nProject can now be :ref:shared <sharing> with other users.\nProject.upload_dataset and Project.upload_dataset_from_data_source will return a PredictionDataset with data_quality_warnings if potential problems exist around the uploaded dataset.\nrelax_known_in_advance_features_check has been added to Project.upload_dataset and Project.upload_dataset_from_data_source to allow missing values from the known in advance features in the forecast window at prediction time.\ncross_series_group_by_columns has been added to datarobot.DatetimePartitioning to allow users the ability to indicate how to further split series into related groups.\nInformation retrieval for ROC Curve has been extended to include fraction_predicted_as_positive, fraction_predicted_as_negative, lift_positive and lift_negative\nBugfixes¶\nFixes an issue where the client would not be usable if it could not be sure it was compatible with the configured\nserver\nAPI changes¶\nMethods for creating datarobot.models.Project: create_from_mysql, create_from_oracle, and create_from_postgresql, deprecated in 2.11, have now been removed.\nUse datarobot.models.Project.create_from_data_source() instead.\ndatarobot.FeatureSettings attribute apriori, deprecated in 2.11, has been removed.\nUse datarobot.FeatureSettings.known_in_advance instead.\ndatarobot.DatetimePartitioning attribute default_to_a_priori, deprecated in 2.11, has been removed. Use\ndatarobot.DatetimePartitioning.known_in_advance instead.\ndatarobot.DatetimePartitioningSpecification attribute default_to_a_priori, deprecated in 2.11, has been removed.\nUse datarobot.DatetimePartitioningSpecification.known_in_advance\ninstead.\nConfiguration changes¶\nNow requires dependency on package requests  to be at least version 2.21.\nNow requires dependency on package urllib3  to be at least version 1.24.\nDocumentation changes¶\nAdvanced model insights notebook extended to contain information on visualization of cumulative gains and lift charts.\n2.14.2¶\nBugfixes¶\nFixed an issue where searches of the HTML documentation would sometimes hang indefinitely\nDocumentation changes¶\nPython3 is now the primary interpreter used to build the docs (this does not affect the ability to use the\npackage with Python2)\n2.14.1¶\nDocumentation changes¶\nDocumentation for the Model Deployment interface has been removed after the corresponding interface was removed in 2.13.0.\n2.14.0¶\nNew features¶\nThe new method Model.get_supported_capabilities\nretrieves a summary of the capabilities supported by a particular model,\nsuch as whether it is eligible for Prime and whether it has word cloud data available.\nNew class for working with model compliance documentation feature of DataRobot:\nclass ComplianceDocumentation\nNew class for working with compliance documentation templates:\nComplianceDocTemplate\nNew class FeatureHistogram has been added to\nretrieve feature histograms for a requested maximum bin count\nTime series projects now support binary classification targets.\nCross series features can now be created within time series multiseries projects using the\nuse_cross_series_features and aggregation_type attributes of the\ndatarobot.DatetimePartitioningSpecification.\nSee the :ref:Time Series <time_series> documentation for more info.\nEnhancements¶\nClient instantiation now checks the endpoint configuration and provides more informative error messages.\nIt also automatically corrects HTTP to HTTPS if the server responds with a redirect to HTTPS.\nProject.upload_dataset and Project.create\nnow accept an optional parameter of dataset_filename to specify a file name for the dataset.\nThis is ignored for url and file path sources.\nNew optional parameter fallback_to_parent_insights has been added to Model.get_lift_chart,\nModel.get_all_lift_charts, Model.get_confusion_chart,\nModel.get_all_confusion_charts, Model.get_roc_curve,\nand Model.get_all_roc_curves.  When True, a frozen model with\nmissing insights will attempt to retrieve the missing insight data from its parent model.\nNew number_of_known_in_advance_features attribute has been added to the\ndatarobot.DatetimePartitioning class.\nThe attribute specifies number of features that are marked as known in advance.\nProject.set_worker_count can now update the worker count on\na project to the maximum number available to the user.\nref:\nRecommended Models API <recommended_models> can now be used to retrieve\nmodel recommendations for datetime partitioned projects\nTimeseries projects can now accept feature derivation and forecast windows intervals in terms of\nnumber of the rows rather than a fixed time unit. DatetimePartitioningSpecification\nand Project.set_target support new optional parameter windowsBasisUnit, either ‘ROW’ or detected time unit.\nTimeseries projects can now accept feature derivation intervals, forecast windows, forecast points and prediction start/end dates in milliseconds.\nDataSources and DataStores can now\nbe :ref:shared <sharing> with other users.\nTraining predictions for datetime partitioned projects now support the new data subset\ndr.enums.DATA_SUBSET.ALL_BACKTESTS for requesting the predictions for all backtest validation\nfolds.\nAPI changes¶\nThe model recommendation type “Recommended” (deprecated in version 2.13.0) has been removed.\nDocumentation changes¶\nExample notebooks have been updated:\n: - Notebooks now work in Python 2 and Python 3\n- A notebook illustrating time series capability has been added\n- The financial data example has been replaced with an updated introductory example.\nTo supplement the embedded Python notebooks in both the PDF and HTML docs bundles, the notebook files and supporting data can now be downloaded from the HTML docs bundle.\nFixed a minor typo in the code sample for get_or_request_feature_impact\n2.13.0¶\nNew features¶\nThe new method Model.get_or_request_feature_impact functionality will attempt to request feature impact\nand return the newly created feature impact object or the existing object so two calls are no longer required.\nNew methods and objects, including\nModel.start_advanced_tuning_session,\nModel.get_advanced_tuning_parameters,\nModel.advanced_tune, and\nAdvancedTuningSession,\nwere added to support the setting of Advanced Tuning parameters. This is currently supported for\nEureqa models only.\nNew is_starred attribute has been added to the Model class. The attribute\nspecifies whether a model has been marked as starred by user or not.\nModel can be marked as starred or being unstarred with Model.star_model and Model.unstar_model.\nWhen listing models with Project.get_models, the model list can now be filtered by the is_starred value.\nA custom prediction threshold may now be configured for each model via Model.set_prediction_threshold.  When making\npredictions in binary classification projects, this value will be used when deciding between the positive and negative classes.\nProject.check_blendable can be used to confirm if a particular group of models are eligible for blending as\nsome are not, e.g. scaleout models and datetime models with different training lengths.\nIndividual cross validation scores can be retrieved for new models using Model.get_cross_validation_scores.\nEnhancements¶\nPython 3.7 is now supported.\nFeature impact now returns not only the impact score for the features but also whether they were\ndetected to be redundant with other high-impact features.\nA new is_blocked attribute has been added to the Job\nclass, specifying whether a job is blocked from execution because one or more dependencies are not\nyet met.\nThe Featurelist object now has new attributes reporting\nits creation time, whether it was created by a user or by DataRobot, and the number of models\nusing the featurelist, as well as a new description field.\nFeaturelists can now be renamed and have their descriptions updated with\nFeaturelist.update and\nModelingFeaturelist.update.\nFeaturelists can now be deleted with\nFeaturelist.delete\nand ModelingFeaturelist.delete.\nModelRecommendation.get now accepts an optional\nparameter of type datarobot.enums.RECOMMENDED_MODEL_TYPE which can be used to get a specific\nkind of recommendation.\nPreviously computed predictions can now be listed and retrieved with the\nPredictions class, without requiring a\nreference to the original PredictJob.\nBugfixes¶\nThe Model Deployment interface which was previously visible in the client has been removed to\nallow the interface to mature, although the raw API is available as a “beta” API without full\nbackwards compatibility support.\nAPI changes¶\nAdded support for retrieving the Pareto Front of a Eureqa model. See\nParetoFront.\nA new recommendation type “Recommended for Deployment” has been added to\nModelRecommendation which is now returns as the\ndefault recommended model when available. See :ref:model_recommendation.\nDeprecation summary¶\nThe feature previously referred to as “Reason Codes” has been renamed to “Prediction\nExplanations”, to provide increased clarity and accessibility. The old\nReasonCodes interface has been deprecated and replaced with\nPredictionExplanations.\nThe recommendation type “Recommended” is deprecated and  will no longer be returned\nin v2.14 of the API.\nDocumentation changes¶\nAdded a new documentation section :ref:model_recommendation.\nTime series projects support multiseries as well as single series data. They are now documented in\nthe :ref:Time Series Projects <time_series> documentation.\n2.12.0¶\nNew features¶\nSome models now have Missing Value reports allowing users with access to uncensored blueprints to\nretrieve a detailed breakdown of how numeric imputation and categorical converter tasks handled\nmissing values. See the :ref:documentation <missing_values_report> for more information on the\nreport.\n2.11.0¶\nNew features¶\nThe new ModelRecommendation class can be used to retrieve the recommended models for a\nproject.\nA new helper method cross_validate was added to class Model. This method can be used to request\nModel’s Cross Validation score.\nTraining a model with monotonic constraints is now supported. Training with monotonic\nconstraints allows users to force models to learn monotonic relationships with respect to some features and the target. This helps users create accurate models that comply with regulations (e.g. insurance, banking). Currently, only certain blueprints (e.g. xgboost) support this feature, and it is only supported for regression and binary classification projects.\nDataRobot now supports “Database Connectivity”, allowing databases to be used\nas the source of data for projects and prediction datasets. The feature works\non top of the JDBC standard, so a variety of databases conforming to that standard are available;\na list of databases with tested support for DataRobot is available in the user guide\nin the web application. See :ref:Database Connectivity <database_connectivity_overview>\nfor details.\nAdded a new feature to retrieve feature logs for time series projects. Check\ndatarobot.DatetimePartitioning.feature_log_list() and\ndatarobot.DatetimePartitioning.feature_log_retrieve() for details.\nAPI changes¶\nNew attributes supporting monotonic constraints have been added to the\nAdvancedOptions,\nProject,\nModel, and Blueprint\nclasses. See :ref:monotonic constraints<monotonic_constraints> for more information on how to\nconfigure monotonic constraints.\nNew parameters predictions_start_date and predictions_end_date added to\nProject.upload_dataset to support bulk\npredictions upload for time series projects.\nDeprecation summary¶\nMethods for creating datarobot.models.Project: create_from_mysql, create_from_oracle, and create_from_postgresql, have been deprecated and will be removed in 2.14.\nUse datarobot.models.Project.create_from_data_source() instead.\ndatarobot.FeatureSettings attribute apriori, has been deprecated and will be removed in 2.14.\nUse datarobot.FeatureSettings.known_in_advance instead.\ndatarobot.DatetimePartitioning attribute default_to_a_priori, has been deprecated and will be removed in 2.14.\ndatarobot.DatetimePartitioning.known_in_advance instead.\ndatarobot.DatetimePartitioningSpecification attribute default_to_a_priori, has been deprecated and will be removed in 2.14.\nUse datarobot.DatetimePartitioningSpecification.known_in_advance\ninstead.\nConfiguration changes¶\nRetry settings compatible with those offered by urllib3’s Retry\ninterface can now be configured. By default, we will now retry connection errors that prevented requests from arriving at the server.\nDocumentation changes¶\n“Advanced Model Insights” example has been updated to properly handle bin weights when rebinning.\n2.9.0¶\nNew features¶\nNew ModelDeployment class can be used to track status and health of models deployed for\npredictions.\nEnhancements¶\nDataRobot API now supports creating 3 new blender types - Random Forest, TensorFlow, LightGBM.\nMulticlass projects now support blenders creation for 3 new blender types as well as Average\nand ENET blenders.\nModels can be trained by requesting a particular row count using the new training_row_count\nargument with Project.train, Model.train and Model.request_frozen_model in non-datetime\npartitioned projects, as an alternative to the previous option of specifying a desired\npercentage of the project dataset. Specifying model size by row count is recommended when\nthe float precision of sample_pct could be problematic, e.g. when training on a small\npercentage of the dataset or when training up to partition boundaries.\nNew attributes max_train_rows, scaleout_max_train_pct, and scaleout_max_train_rows\nhave been added to Project. max_train_rows specified the equivalent\nvalue to the existing max_train_pct as a row count. The scaleout fields can be used to see how\nfar scaleout models can be trained on projects, which for projects taking advantage of scalable\ningest may exceed the limits on the data available to non-scaleout blueprints.\nIndividual features can now be marked as a priori or not a priori using the new feature_settings\nattribute when setting the target or specifying datetime partitioning settings on time\nseries projects. Any features not specified in the feature_settings parameter will be\nassigned according to the default_to_a_priori value.\nThree new options have been made available in the\ndatarobot.DatetimePartitioningSpecification class to fine-tune how time-series projects\nderive modeling features. treat_as_exponential can control whether data is analyzed as\nan exponential trend and transformations like log-transform are applied.\ndifferencing_method can control which differencing method to use for stationary data.\nperiodicities can be used to specify periodicities occurring within the data.\nAll are optional and defaults will be chosen automatically if they are unspecified.\nAPI changes¶\nNow training_row_count is available on non-datetime models as well as rowCount based\ndatetime models. It reports the number of rows used to train the model (equivalent to\nsample_pct).\nFeatures retrieved from Feature.get now include target_leakage.\n2.8.1¶\nBugfixes¶\nThe documented default connect_timeout will now be correctly set for all configuration mechanisms,\nso that requests that fail to reach the DataRobot server in a reasonable amount of time will now\nerror instead of hanging indefinitely. If you observe that you have started seeing\nConnectTimeout errors, please configure your connect_timeout to a larger value.\nVersion of trafaret library this package depends on is now pinned to trafaret>=0.7,<1.1\nsince versions outside that range are known to be incompatible.\n2.8.0¶\nNew features¶\nThe DataRobot API supports the creation, training, and predicting of multiclass classification\nprojects. DataRobot, by default, handles a dataset with a numeric target column as regression.\nIf your data has a numeric cardinality of fewer than 11 classes, you can override this behavior to\ninstead create a multiclass classification project from the data. To do so, use the set_target\nfunction, setting target_type=‘Multiclass’. If DataRobot recognizes your data as categorical, and\nit has fewer than 11 classes, using multiclass will create a project that classifies which label\nthe data belongs to.\nThe DataRobot API now includes Rating Tables. A rating table is an exportable csv representation\nof a model. Users can influence predictions by modifying them and creating a new model with the\nmodified table. See the :ref:documentation<rating_table> for more information on how to use\nrating tables.\nscaleout_modeling_mode has been added to the AdvancedOptions class\nused when setting a project target. It can be used to control whether\nscaleout models appear in the autopilot and/or available blueprints.\nScaleout models are only supported in the Hadoop environment with\nthe corresponding user permission set.\nA new premium add-on product, Time Series, is now available. New projects can be created as time series\nprojects which automatically derive features from past data and forecast the future. See the\nref:\ntime series documentation<time_series> for more information.\nThe Feature object now returns the EDA summary statistics (i.e., mean, median, minimum, maximum,\nand standard deviation) for features where this is available (e.g., numeric, date, time,\ncurrency, and length features). These summary statistics will be formatted in the same format\nas the data it summarizes.\nThe DataRobot API now supports Training Predictions workflow. Training predictions are made by a\nmodel for a subset of data from original dataset. User can start a job which will make those\npredictions and retrieve them. See the :ref:documentation<predictions>\nfor more information on how to use training predictions.\nDataRobot now supports retrieving a :ref:model blueprint chart<model_blueprint_chart> and a\nref:\nmodel blueprint docs<model_blueprint_doc>.\nWith the introduction of Multiclass Classification projects, DataRobot needed a better way to\nexplain the performance of a multiclass model so we created a new Confusion Chart. The API\nnow supports retrieving and interacting with confusion charts.\nEnhancements¶\nDatetimePartitioningSpecification now includes the optional disable_holdout flag that can\nbe used to disable the holdout fold when creating a project with datetime partitioning.\nWhen retrieving reason codes on a project using an exposure column, predictions that are adjusted\nfor exposure can be retrieved.\nFile URIs can now be used as sourcedata when creating a project or uploading a prediction dataset.\nThe file URI must refer to an allowed location on the server, which is configured as described in\nthe user guide documentation.\nThe advanced options available when setting the target have been extended to include the new\nparameter ‘events_count’ as a part of the AdvancedOptions object to allow specifying the\nevents count column. See the user guide documentation in the webapp for more information\non events count.\nPredictJob.get_predictions now returns predicted probability for each class in the dataframe.\nPredictJob.get_predictions now accepts prefix parameter to prefix the classes name returned in the\npredictions dataframe.\nAPI changes¶\nAdd target_type parameter to set_target() and start(), used to override the project default.\n2.7.2¶\nDocumentation changes¶\nUpdated link to the publicly hosted documentation.\n2.7.1¶\nDocumentation changes¶\nOnline documentation hosting has migrated from PythonHosted to Read The Docs. Minor code changes\nhave been made to support this.\n2.7.0¶\nNew features¶\nLift chart data for models can be retrieved using the Model.get_lift_chart and\nModel.get_all_lift_charts methods.\nROC curve data for models in classification projects can be retrieved using the\nModel.get_roc_curve and Model.get_all_roc_curves methods.\nSemi-automatic autopilot mode is removed.\nWord cloud data for text processing models can be retrieved using Model.get_word_cloud method.\nScoring code JAR file can be downloaded for models supporting code generation.\nEnhancements¶\nA __repr__ method has been added to the PredictionDataset class to improve readability when\nusing the client interactively.\nModel.get_parameters now includes an additional key in the derived features it includes,\nshowing the coefficients for individual stages of multistage models (e.g. Frequency-Severity\nmodels).\nWhen training a DatetimeModel on a window of data, a time_window_sample_pct can be specified\nto take a uniform random sample of the training data instead of using all data within the window.\nInstalling of DataRobot package now has an “Extra Requirements” section that will install all of\nthe dependencies needed to run the example notebooks.\nDocumentation changes¶\nA new example notebook describing how to visualize some of the newly available model insights\nincluding lift charts, ROC curves, and word clouds has been added to the examples section.\nA new section for Common Issues has been added to Getting Started to help debug issues related to client installation and usage.\n2.6.1¶\nBugfixes¶\nFixed a bug with Model.get_parameters raising an exception on some valid parameter values.\nDocumentation changes¶\nFixed sorting order in Feature Impact example code snippet.\n2.6.0¶\nNew features¶\nA new partitioning method (datetime partitioning) has been added. The recommended workflow is to\npreview the partitioning by creating a DatetimePartitioningSpecification and passing it into\nDatetimePartitioning.generate, inspect the results and adjust as needed for the specific project\ndataset by adjusting the DatetimePartitioningSpecification and re-generating, and then set the\ntarget by passing the final DatetimePartitioningSpecification object to the partitioning_method\nparameter of Project.set_target.\nWhen interacting with datetime partitioned projects, DatetimeModel can be used to access more\ninformation specific to models in datetime partitioned projects. See\nref:\nthe documentation<datetime_modeling_workflow> for more information on differences in the\nmodeling workflow for datetime partitioned projects.\nThe advanced options available when setting the target have been extended to include the new\nparameters ‘offset’ and ‘exposure’ (part of the AdvancedOptions object) to allow specifying\noffset and exposure columns to apply to predictions generated by models within the project.\nSee the user guide documentation in the webapp for more information on offset\nand exposure columns.\nBlueprints can now be retrieved directly by project_id and blueprint_id via Blueprint.get.\nBlueprint charts can now be retrieved directly by project_id and blueprint_id via\nBlueprintChart.get. If you already have an instance of Blueprint you can retrieve its\nchart using Blueprint.get_chart.\nModel parameters can now be retrieved using ModelParameters.get. If you already have an\ninstance of Model you can retrieve its parameters using Model.get_parameters.\nBlueprint documentation can now be retrieved using Blueprint.get_documents. It will contain\ninformation about the task, its parameters and (when available) links and references to\nadditional sources.\nThe DataRobot API now includes Reason Codes. You can now compute reason codes for prediction\ndatasets. You are able to specify thresholds on which rows to compute reason codes for to speed\nup computation by skipping rows based on the predictions they generate. See the reason codes\nref:\ndocumentation<reason_codes> for more information.\nEnhancements¶\nA new parameter has been added to the AdvancedOptions used with Project.set_target. By\nspecifying accuracyOptimizedMb=True when creating AdvancedOptions, longer-running models\nthat may have a high accuracy will be included in the autopilot and made available to run\nmanually.\nA new option for Project.create_type_transform_feature has been added which explicitly\ntruncates data when casting numerical data as categorical data.\nAdded 2 new blenders for projects that use MAD or Weighted MAD as a metric. The MAE blender uses\nBFGS optimization to find linear weights for the blender that minimize mean absolute error\n(compared to the GLM blender, which finds linear weights that minimize RMSE), and the MAEL1\nblender uses BFGS optimization to find linear weights that minimize MAE + a L1 penalty on the\ncoefficients (compared to the ENET blender, which minimizes RMSE + a combination of the L1 and L2\npenalty on the coefficients).\nBugfixes¶\nFixed a bug (affecting Python 2 only) with printing any model (including frozen and prime models)\nwhose model_type is not ascii.\nFrozenModels were unable to correctly use methods inherited from Model. This has been fixed.\nWhen calling get_result for a Job, ModelJob, or PredictJob that has errored, AsyncProcessUnsuccessfulError will now be raised instead of JobNotFinished, consistently with the behavior of get_result_when_complete.\nDeprecation summary¶\nSupport for the experimental Recommender Problems projects has been removed. Any code relying on\nRecommenderSettings or the recommender_settings argument of Project.set_target and\nProject.start will error.\nProject.update, deprecated in v2.2.32, has been removed in favor of specific updates:\nrename, unlock_holdout, set_worker_count.\nDocumentation changes¶\nThe link to Configuration from the Quickstart page has been fixed.\n2.5.1¶\nBugfixes¶\nFixed a bug (affecting Python 2 only) with printing blueprints  whose names are\nnot ascii.\nFixed an issue where the weights column (for weighted projects) did not appear\nin the advanced_options of a Project.\n2.5.0¶\nNew features¶\nMethods to work with blender models have been added. Use Project.blend method to create new blenders,\nProject.get_blenders to get the list of existing blenders and BlenderModel.get to retrieve a model\nwith blender-specific information.\nProjects created via the API can now use smart downsampling when setting the target by passing\nsmart_downsampled and majority_downsampling_rate into the AdvancedOptions object used with\nProject.set_target. The smart sampling options used with an existing project will be available\nas part of Project.advanced_options.\nSupport for frozen models, which use tuning parameters from a parent model for more efficient\ntraining, has been added. Use Model.request_frozen_model to create a new frozen model,\nProject.get_frozen_models to get the list of existing frozen models and FrozenModel.get to\nretrieve a particular frozen model.\nEnhancements¶\nThe inferred date format (e.g. “%Y-%m-%d %H:%M:%S”) is now included in the Feature object. For\nnon-date features, it will be None.\nWhen specifying the API endpoint in the configuration, the client will now behave correctly for\nendpoints with and without trailing slashes.\n2.4.0¶\nNew features¶\nThe premium add-on product DataRobot Prime has been added. You can now approximate a model\non the leaderboard and download executable code for it. See documentation for further details, or\ntalk to your account representative if the feature is not available on your account.\n(Only relevant for on-premise users with a Standalone Scoring cluster.) Methods\n(request_transferable_export and download_export) have been added to the Model class for exporting models (which will only work if model export is turned on). There is a new class ImportedModel for managing imported models on a Standalone\nScoring cluster.\nIt is now possible to create projects from a WebHDFS, PostgreSQL, Oracle or MySQL data source. For more information see the\ndocumentation for the relevant Project classmethods: create_from_hdfs, create_from_postgresql,\ncreate_from_oracle and create_from_mysql.\nJob.wait_for_completion, which waits for a job to complete without returning anything, has been added.\nEnhancements¶\nThe client will now check the API version offered by the server specified in configuration, and\ngive a warning if the client version is newer than the server version. The DataRobot server is\nalways backwards compatible with old clients, but new clients may have functionality that is\nnot implemented on older server versions. This issue mainly affects users with on-premise deployments\nof DataRobot.\nBugfixes¶\nFixed an issue where Model.request_predictions might raise an error when predictions finished\nvery quickly instead of returning the job.\nAPI changes¶\nTo set the target with quickrun autopilot, call Project.set_target with mode=AUTOPILOT_MODE.QUICK instead of\nspecifying quickrun=True.\nDeprecation summary¶\nSemi-automatic mode for autopilot has been deprecated and will be removed in 3.0.\nUse manual or fully automatic instead.\nUse of the quickrun argument in Project.set_target has been deprecated and will be removed in\n3.0. Use mode=AUTOPILOT_MODE.QUICK instead.\nConfiguration changes¶\nIt is now possible to control the SSL certificate verification by setting the parameter\nssl_verify in the config file.\nDocumentation changes¶\nThe “Modeling Airline Delay” example notebook has been updated to work with the new 2.3\nenhancements.\nDocumentation for the generic Job class has been added.\nClass attributes are now documented in the API Reference section of the documentation.\nThe changelog now appears in the documentation.\nThere is a new section dedicated to configuration, which lists all of the configuration\noptions and their meanings.\n2.3.0¶\nNew features¶\nThe DataRobot API now includes Feature Impact, an approach to measuring the relevance of each feature\nthat can be applied to any model. The Model class now includes methods request_feature_impact\n(which creates and returns a feature impact job) and get_feature_impact (which can retrieve completed feature impact results).\nA new improved workflow for predictions now supports first uploading a dataset via Project.upload_dataset,\nthen requesting predictions via Model.request_predictions. This allows us to better support predictions on\nlarger datasets and non-ascii files.\nDatasets previously uploaded for predictions (represented by the PredictionDataset class) can be listed from\nProject.get_datasets and retrieve and deleted via PredictionDataset.get and PredictionDataset.delete.\nYou can now create a new feature by re-interpreting the type of an existing feature in a project by\nusing the Project.create_type_transform_feature method.\nThe Job class now includes a get method for retrieving a job and a cancel method for\ncanceling a job.\nAll of the jobs classes (Job, ModelJob, PredictJob) now include the following new methods:\nrefresh (for refreshing the data in the job object), get_result (for getting the\ncompleted resource resulting from the job), and get_result_when_complete (which waits until the job\nis complete and returns the results, or times out).\nA new method Project.refresh can be used to update\nProject objects with the latest state from the server.\nA new function datarobot.async.wait_for_async_resolution can be\nused to poll for the resolution of any generic asynchronous operation\non the server.\nEnhancements¶\nThe JOB_TYPE enum now includes FEATURE_IMPACT.\nThe QUEUE_STATUS enum now includes ABORTED and COMPLETED.\nThe Project.create method now has a read_timeout parameter which can be used to\nkeep open the connection to DataRobot while an uploaded file is being processed.\nFor very large files this time can be substantial. Appropriately raising this value\ncan help avoid timeouts when uploading large files.\nThe method Project.wait_for_autopilot has been enhanced to error if\nthe project enters a state where autopilot may not finish. This avoids\na situation that existed previously where users could wait\nindefinitely on their project that was not going to finish. However,\nusers are still responsible to make sure a project has more than\nzero workers, and that the queue is not paused.\nFeature.get now supports retrieving features by feature name. (For backwards compatibility,\nfeature IDs are still supported until 3.0.)\nFile paths that have unicode directory names can now be used for\ncreating projects and PredictJobs. The filename itself must still\nbe ascii, but containing directory names can have other encodings.\nNow raises more specific JobAlreadyRequested exception when we refuse a model fitting request as a duplicate.\nUsers can explicitly catch this exception if they want it to be ignored.\nA file_name attribute has been added to the Project class, identifying the file name\nassociated with the original project dataset. Note that if the project was created from\na data frame, the file name may not be helpful.\nThe connect timeout for establishing a connection to the server can now be set directly. This can be done in the\nyaml configuration of the client, or directly in the code. The default timeout has been lowered from 60 seconds\nto 6 seconds, which will make detecting a bad connection happen much quicker.\nBugfixes¶\nFixed a bug (affecting Python 2 only) with printing features and featurelists whose names are\nnot ascii.\nAPI changes¶\nJob class hierarchy is rearranged to better express the relationship between these objects. See\ndocumentation for datarobot.models.job for details.\nFeaturelist objects now have a project_id attribute to indicate which project they belong\nto. Directly accessing the project attribute of a Featurelist object is now deprecated\nSupport INI-style configuration, which was deprecated in v2.1, has been removed. yaml is the only supported\nconfiguration format.\nThe method Project.get_jobs method, which was deprecated in v2.1, has been removed. Users should use\nthe Project.get_model_jobs method instead to get the list of model jobs.\nDeprecation summary¶\nPredictJob.create has been deprecated in favor of the alternate workflow using Model.request_predictions.\nFeature.converter (used internally for object construction) has been made private.\nModel.fetch_resource_data has been deprecated and will be removed in 3.0. To fetch a model from\n: its ID, use Model.get.\nThe ability to use Feature.get with feature IDs (rather than names) is deprecated and will\nbe removed in 3.0.\nInstantiating a Project, Model, Blueprint, Featurelist, or Feature instance from a dict\nof data is now deprecated. Please use the from_data classmethod of these classes instead. Additionally,\ninstantiating a Model from a tuple or by using the keyword argument data is also deprecated.\nUse of the attribute Featurelist.project is now deprecated. You can use the project_id\nattribute of a Featurelist to instantiate a Project instance using Project.get.\nUse of the attributes Model.project, Model.blueprint, and Model.featurelist are all deprecated now\nto avoid use of partially instantiated objects. Please use the ids of these objects instead.\nUsing a Project instance as an argument in Featurelist.get is now deprecated.\nPlease use a project_id instead. Similarly, using a Project instance in Model.get is also deprecated,\nand a project_id should be used in its place.\nConfiguration changes¶\nPreviously it was possible (though unintended) that the client configuration could be mixed through\nenvironment variables, configuration files, and arguments to datarobot.Client. This logic is now\nsimpler - please see the Getting Started section of the documentation for more information.\n2.2.33¶\nBugfixes¶\nFixed a bug with non-ascii project names using the package with Python 2.\nFixed an error that occurred when printing projects that had been constructed from an ID only or\nprinting printing models that had been constructed from a tuple (which impacted printing PredictJobs).\nFixed a bug with project creation from non-ascii file names. Project creation from non-ascii file names\nis not supported, so this now raises a more informative exception. The project name is no longer used as\nthe file name in cases where we do not have a file name, which prevents non-ascii project names from\ncausing problems in those circumstances.\nFixed a bug (affecting Python 2 only) with printing projects, features, and featurelists whose names are\nnot ascii.\n2.2.32¶\nNew features¶\nProject.get_features and Feature.get methods have been added for feature retrieval.\nA generic Job entity has been added for use in retrieving the entire queue at once. Calling\nProject.get_all_jobs will retrieve all (appropriately filtered) jobs from the queue. Those\ncan be cancelled directly as generic jobs, or transformed into instances of the specific\njob class using ModelJob.from_job and PredictJob.from_job, which allow all functionality\npreviously available via the ModelJob and PredictJob interfaces.\nModel.train now supports featurelist_id and scoring_type parameters, similar to\nProject.train.\nEnhancements¶\nDeprecation warning filters have been updated. By default, a filter will be added ensuring that\nusage of deprecated features will display a warning once per new usage location. In order to\nhide deprecation warnings, a filter like\nwarnings.filterwarnings('ignore', category=DataRobotDeprecationWarning)\ncan be added to a script so no such warnings are shown. Watching for deprecation warnings\nto avoid reliance on deprecated features is recommended.\nIf your client is misconfigured and does not specify an endpoint, the cloud production server is\nno longer used as the default as in many cases this is not the correct default.\nThis changelog is now included in the distributable of the client.\nBugfixes¶\nFixed an issue where updating the global client would not affect existing objects with cached clients.\nNow the global client is used for every API call.\nAn issue where mistyping a filepath for use in a file upload has been resolved. Now an error will be\nraised if it looks like the raw string content for modeling or predictions is just one single line.\nAPI changes¶\nUse of username and password to authenticate is no longer supported - use an API token instead.\nUsage of start_time and finish_time parameters in Project.get_models is not\nsupported both in filtering and ordering of models\nDefault value of sample_pct parameter of Model.train method is now None instead of 100.\nIf the default value is used, models will be trained with all of the available training data based on\nproject configuration, rather than with entire dataset including holdout for the previous default value\nof 100.\norder_by parameter of Project.list which was deprecated in v2.0 has been removed.\nrecommendation_settings parameter of Project.start which was deprecated in v0.2 has been removed.\nProject.status method which was deprecated in v0.2 has been removed.\nProject.wait_for_aim_stage method which was deprecated in v0.2 has been removed.\nDelay, ConstantDelay, NoDelay, ExponentialBackoffDelay, RetryManager\nclasses from retry module which were deprecated in v2.1 were removed.\nPackage renamed to datarobot.\nDeprecation summary¶\nProject.update deprecated in favor of specific updates:\nrename, unlock_holdout, set_worker_count.\nDocumentation changes¶\nA new use case involving financial data has been added to the examples directory.\nAdded documentation for the partition methods.\n2.1.31¶\nBugfixes¶\nIn Python 2, using a unicode token to instantiate the client will\nnow work correctly.\n2.1.30¶\nBugfixes¶\nThe minimum required version of trafaret has been upgraded to 0.7.1\nto get around an incompatibility between it and setuptools.\n2.1.29¶\nEnhancements¶\nMinimal used version of requests_toolbelt package changed from 0.4 to 0.6\n2.1.28¶\nNew features¶\nDefault to reading YAML config file from ~/.config/datarobot/drconfig.yaml\nAllow config_path argument to client\nwait_for_autopilot method added to Project. This method can be used to\nblock execution until autopilot has finished running on the project.\nSupport for specifying which featurelist to use with initial autopilot in\nProject.set_target\nProject.get_predict_jobs method has been added, which looks up all prediction jobs for a\nproject\nProject.start_autopilot method has been added, which starts autopilot on\nspecified featurelist\nThe schema for PredictJob in DataRobot API v2.1 now includes a message. This attribute has\nbeen added to the PredictJob class.\nPredictJob.cancel now exists to cancel prediction jobs, mirroring ModelJob.cancel\nProject.from_async is a new classmethod that can be used to wait for an async resolution\nin project creation. Most users will not need to know about it as it is used behind the scenes\nin Project.create and Project.set_target, but power users who may run\ninto periodic connection errors will be able to catch the new ProjectAsyncFailureError\nand decide if they would like to resume waiting for async process to resolve\nEnhancements¶\nAUTOPILOT_MODE enum now uses string names for autopilot modes instead of numbers\nDeprecation summary¶\nConstantDelay, NoDelay, ExponentialBackoffDelay, and RetryManager utils are now deprecated\nINI-style config files are now deprecated (in favor of YAML config files)\nSeveral functions in the utils submodule are now deprecated (they are\nbeing moved elsewhere and are not considered part of the public interface)\nProject.get_jobs has been renamed Project.get_model_jobs for clarity and deprecated\nSupport for the experimental date partitioning has been removed in DataRobot API,\nso it is being removed from the client immediately.\nAPI changes¶\nIn several places where AppPlatformError was being raised, now TypeError, ValueError or\nInputNotUnderstoodError are now used. With this change, one can now safely assume that when\ncatching an AppPlatformError it is because of an unexpected response from the server.\nAppPlatformError has gained a two new attributes, status_code which is the HTTP status code\nof the unexpected response from the server, and error_code which is a DataRobot-defined error\ncode. error_code is not used by any routes in DataRobot API 2.1, but will be in the future.\nIn cases where it is not provided, the instance of AppPlatformError will have the attribute\nerror_code set to None.\nTwo new subclasses of AppPlatformError have been introduced, ClientError (for 400-level\nresponse status codes) and ServerError (for 500-level response status codes). These will make\nit easier to build automated tooling that can recover from periodic connection issues while polling.\nIf a ClientError or ServerError occurs during a call to Project.from_async, then a\nProjectAsyncFailureError (a subclass of AsyncFailureError) will be raised. That exception will\nhave the status_code of the unexpected response from the server, and the location that was being\npolled to wait for the asynchronous process to resolve.\n2.0.27¶\nNew features¶\nPredictJob class was added to work with prediction jobs\nwait_for_async_predictions function added to predict_job module\nDeprecation summary¶\nThe order_by parameter of the Project.list is now deprecated.\n0.2.26¶\nEnhancements¶\nProjet.set_target will re-fetch the project data after it succeeds,\nkeeping the client side in sync with the state of the project on the\nserver\nProject.create_featurelist now throws DuplicateFeaturesError\nexception if passed list of features contains duplicates\nProject.get_models now supports snake_case arguments to its\norder_by keyword\nDeprecation summary¶\nProject.wait_for_aim_stage is now deprecated, as the REST Async\nflow is a more reliable method of determining that project creation has\ncompleted successfully\nProject.status is deprecated in favor of Project.get_status\nrecommendation_settings parameter of Project.start is\ndeprecated in favor of recommender_settings\nBugfixes¶\nProject.wait_for_aim_stage changed to support Python 3\nFixed incorrect value of SCORING_TYPE.cross_validation\nModels returned by Project.get_models will now be correctly\nordered when the order_by keyword is used\n0.2.25¶\nPinned versions of required libraries\n0.2.24¶\nOfficial release of v0.2\n0.1.24¶\nUpdated documentation\nRenamed parameter name of Project.create and Project.start to project_name\nRemoved Model.predict method\nwait_for_async_model_creation function added to modeljob module\nwait_for_async_status_service of Project class renamed to _wait_for_async_status_service\nCan now use auth_token in config file to configure API Client\n0.1.23¶\nFixes a method that pointed to a removed route\n0.1.22¶\nAdded featurelist_id attribute to ModelJob class\n0.1.21¶\nRemoves model attribute from ModelJob class\n0.1.20¶\nProject creation raises AsyncProjectCreationError if it was unsuccessful\nRemoved Model.list_prime_rulesets and Model.get_prime_ruleset methods\nRemoved Model.predict_batch method\nRemoved Project.create_prime_model method\nRemoved PrimeRuleSet model\nAdds backwards compatibility bridge for ModelJob async\nAdds ModelJob.get and ModelJob.get_model\n0.1.19¶\nMinor bugfixes in wait_for_async_status_service\n0.1.18¶\nRemoves submit_model from Project until server-side implementation is improved\nSwitches training URLs for new resource-based route at /projects//models/\nJob renamed to ModelJob, and using modelJobs route\nFixes an inconsistency in argument order for train methods\n0.1.17¶\nwait_for_async_status_service timeout increased from 60s to 600s\n0.1.16¶\nProject.create will now handle both async/sync project creation\n0.1.15¶\nAll routes pluralized to sync with changes in API\nProject.get_jobs will request all jobs when no param specified\ndataframes from predict method will have pythonic names\nProject.get_status created, Project.status now deprecated\nProject.unlock_holdout created.\nAdded quickrun parameter to Project.set_target\nAdded modelCategory to Model schema\nAdd permalinks feature to Project and Model objects.\nProject.create_prime_model created\n0.1.14¶\nProject.set_worker_count fix for compatibility with API change in project update.\n0.1.13¶\nAdd positive class to set_target.\nChange attributes names of Project, Model, Job and Blueprint\n: - features in Model, Job and Blueprint are now processes\n- dataset_id and dataset_name migrated to featurelist_id and featurelist_name.\n- samplepct -> sample_pct\nModel has now blueprint, project, and featurlist attributes.\nMinor bugfixes.\n0.1.12¶\nMinor fixes regarding rename Job attributes. features attributes now named processes, samplepct now is sample_pct.\n0.1.11¶\n(May 27, 2015)\nMinor fixes regarding migrating API from under_score names to camelCase.\n0.1.10¶\n(May 20, 2015)\nRemove Project.upload_file, Project.upload_file_from_url and Project.attach_file methods. Moved all logic that uploading file to Project.create method.\n0.1.9¶\n(May 15, 2015)\nFix uploading file causing a lot of memory usage. Minor bugfixes.\nBack to top",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://docs.datarobot.com/en/docs/api/reference/sdk/CHANGES.html",
      "tags": [
        "api-reference",
        "api_reference",
        "advanced",
        "beginner",
        "example",
        "tutorial"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://docs.datarobot.com/en/docs/api/reference/sdk/CHANGES.html",
        "content_length": 198954
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.predictiondataset",
        "deployment.create_from_registered_model_version",
        "model.request_feature_impact",
        "datarobot.models.model",
        "model.get_all_confusion_charts",
        "datarobot.customtaskversion.get",
        "project.upload_file",
        "model.get_forecast_vs_actual_plot",
        "project.refresh",
        "model.share",
        "project.get_options",
        "model.get_parameters",
        "project.set_datetime_partitioning",
        "datarobot.customtask.share",
        "deployment.list_prediction_data_exports",
        "deployment.predict_batch",
        "model.compute_datetime_trend_plots",
        "model.get_model_blueprint_json",
        "deployment.list",
        "datarobot.customtask.download_latest_version",
        "datarobot.customtaskversion.list",
        "datarobot.combinedmodel.get",
        "deployment.get_bias_and_fairness_settings",
        "model.request_feature_fit",
        "project.list",
        "project.get_bias_mitigated_models",
        "project.get_metrics",
        "datarobot.enums.blender_method",
        "project.train",
        "deployment.open_in_browser",
        "project.set_target",
        "deployment.list_data_quality_exports",
        "datarobot.custommodelversion.create_clean",
        "model.star_model",
        "project.open_in_browser",
        "model.get",
        "project.get_features",
        "model.update",
        "datarobot.datetimepartitioning.known_in_advance",
        "model.get_prime_ruleset",
        "model.get_forecast_vs_actual_plot_preview",
        "deployment.validate_replacement_model",
        "project.get_leaderboard_ui_permalink",
        "deployment.get_secondary_dataset_config",
        "datarobot.dataenginequerygenerator.create",
        "project.create_interaction_feature",
        "model.archive",
        "model.get_access_list",
        "project.blend",
        "model.get_multiclass_feature_impact",
        "model.get_or_request_feature_fit",
        "project.attach_file",
        "deployment.get_challenger_replay_settings",
        "project.create",
        "deployment.get_predictions_vs_actuals_over_time",
        "project.create_from_hdfs",
        "project.train_datetime",
        "project.set_partitioning_method",
        "model.advanced_tune",
        "model.get_all_lift_charts",
        "deployment.get_prediction_results",
        "deployment.capabilities",
        "model.request_frozen_model",
        "datarobot.segmentationtask.get",
        "project.get_predict_jobs",
        "model.train",
        "project.start",
        "deployment.get_default_health_settings",
        "project.create_type_transform_feature",
        "model.get_or_request_feature_effect",
        "model.get_accuracy_over_time_plot_preview",
        "model.request_residuals_chart",
        "model.get_supported_capabilities",
        "datarobot.customtaskversion.create_clean",
        "model.set_prediction_threshold",
        "model.predict",
        "datarobot.models.featurelineage",
        "model.get_uri",
        "project.get_blenders",
        "deployment.get_predictions_data_collection_settings",
        "model.request_transferable_export",
        "datarobot.dataenginequerygenerator.get",
        "datarobot.models.data_slice",
        "deployment.update_secondary_dataset_config",
        "datarobot.models.batchpredictionjobdefinition",
        "model.get_all_multilabel_lift_charts",
        "datarobot.customtaskversion.download",
        "model.get_incremental_learning_metadata",
        "datarobot.target_type.anomaly",
        "datarobot.segmentinfo.list",
        "datarobot.customtask.delete",
        "datarobot.helpers.partitioning_methods",
        "model.get_anomaly_over_time_plot_preview",
        "deployment.get_accuracy_metrics_settings",
        "datarobot.insights.shapimpact",
        "project.set_project_description",
        "deployment.list_challengers",
        "deployment.delete_monitoring_data",
        "datarobot.customtask.copy",
        "model.get_feature_effects_multiclass",
        "model.set_segment_champion",
        "model.get_all_roc_curves",
        "deployment.update_bias_and_fairness_settings",
        "model.request_lift_chart",
        "project.get_jobs",
        "datarobot.datetimepartitioning.feature_log_list",
        "deployment.replace_model",
        "project.start_autopilot",
        "project.batch_features_type_transform",
        "deployment.update_health_settings",
        "datarobot.dataenginequerygenerator.create_dataset",
        "project.get_active_combined_model",
        "project.validate_external_time_series_baseline",
        "datarobot.enums.target_type",
        "datarobot.models.model_registry",
        "project.set_worker_count",
        "datarobot.customtask.refresh",
        "project.get_multiseries_names",
        "datarobot.customtaskversion.create_from_previous",
        "model.get_anomaly_over_time_plot",
        "datarobot.enums.customtaskoutboundnetworkpolicy",
        "model.get_feature_impact",
        "deployment.get_capabilities",
        "datarobot.async.wait_for_async_resolution",
        "project.open_leaderboard_browser",
        "model.get_feature_effect_metadata",
        "datarobot.models.job",
        "datarobot.models.statuscheckjob",
        "datarobot.customtaskversion.update",
        "project.get_datetime_models",
        "deployment.create_from_custom_model_version",
        "model.project",
        "datarobot.models.recipe",
        "model.list_prime_rulesets",
        "datarobot._experimental.models",
        "model.get_version",
        "project.request_bias_mitigation_feature_info",
        "project.get_frozen_models",
        "datarobot.userblueprint.create_from_custom_task_version_id",
        "datarobot.models.datadriver",
        "project.share",
        "deployment.update_retraining_settings",
        "deployment.list_training_data_exports",
        "datarobot.models.shapimpact",
        "project.create_featurelist",
        "deployment.get_champion_model_package",
        "project.recommended_model",
        "deployment.update_prediction_intervals_settings",
        "model.get_segments_as_csv",
        "deployment.featuredrift",
        "datarobot.enums.recommended_model_type",
        "datarobot.rest.restclientobject",
        "deployment.create_from_learning_model",
        "model.get_accuracy_over_time_plots_metadata",
        "deployment.get_predictions_by_forecast_date_settings",
        "model.get_shared_roles",
        "datarobot.datetimepartitioning.feature_log_retrieve",
        "deployment.update_shared_roles",
        "deployment.get_retraining_settings",
        "datarobot.customtaskversion.refresh",
        "model.compute_series_accuracy",
        "datarobot.models.featureassociationmatrix",
        "deployment.get_segment_analysis_settings",
        "model.blueprint",
        "model.get_multiclass_lift_chart",
        "datarobot.client.client",
        "datarobot.calendarfile.create_calendar_from_dataset",
        "datarobot.models.jdbctabledatasourceinput",
        "model.continue_incremental_learning_from_incremental_model",
        "model.request_feature_effects_multiclass",
        "deployment.get_feature_drift",
        "model.get_confusion_chart",
        "deployment.download_model_package_file",
        "model.get_segments_as_dataframe",
        "project.get_model_jobs",
        "project.get_all_jobs",
        "model.get_prime_eligibility",
        "model.list_associated_deployments",
        "project.start_prepare_model_for_deployment",
        "model.get_feature_fit",
        "model.unstar_model",
        "model.get_residuals_chart",
        "project.create_segmented_project_from_clustering_model",
        "datarobot.target_type.unstructured",
        "deployment.perform_model_replace",
        "project.get_dataset",
        "deployment.get_health_settings",
        "datarobot.datetimepartitioningspecification.known_in_advance",
        "project.upload_dataset_from_catalog",
        "model.get_advanced_tuning_parameters",
        "project.from_async",
        "deployment.get_target_drift",
        "datarobot.models.feature",
        "datarobot.enums._shared_target_type",
        "model.get_feature_effect",
        "model.train_first_incremental_from_sample",
        "deployment.update",
        "datarobot.customtask.get_access_list",
        "model.fetch_resource_data",
        "model.get_anomaly_over_time_plots_metadata",
        "datarobot.customtask.get",
        "deployment.targetdrift",
        "model.request_frozen_datetime_model",
        "datarobot.customtask.update",
        "deployment.update_segment_analysis_settings",
        "project.restart_segment",
        "deployment.update_predictions_by_forecast_date_settings",
        "datarobot.models.dataset",
        "datarobot.combinedmodel.get_segments_info",
        "project.apply_bias_mitigation",
        "model.start_incremental_learning_from_sample",
        "project.get_associations",
        "model.get_all_feature_impacts",
        "deployment.get_moderation_events",
        "deployment.get_predictions_over_time",
        "model.get_forecast_vs_actual_plots_metadata",
        "datarobot.models.restore_discarded_features",
        "model.open_in_browser",
        "project.download_feature_discovery_recipe_sqls",
        "deployment.share",
        "deployment.deactivate",
        "datarobot.models.datetimemodel",
        "deployment.submit_actuals_from_catalog_async",
        "deployment.get_fairness_scores_over_time",
        "project.get_top_model",
        "project.set_options",
        "deployment.custom_metrics",
        "datarobot.segmentationtask.create",
        "project.get_status",
        "datarobot.predictionexplanations.get_all_as_dataframe",
        "datarobot.models.project",
        "project.get_models",
        "project.update",
        "datarobot.combinedmodel.get_segments_as_csv",
        "datarobot.models.recipedatasetinput",
        "model.get_or_request_feature_effects_multiclass",
        "deployment.update_accuracy_metrics_settings",
        "project.get_model_records",
        "project.status",
        "datarobot.enums.customtaskoutgoingnetworkpolicy",
        "model.get_feature_fit_metadata",
        "deployment.get_segment_attributes",
        "project.from_data",
        "datarobot.models.connector",
        "datarobot.featuresettings.known_in_advance",
        "project.wait_for_aim_stage",
        "deployment.get_segment_values",
        "deployment.activate",
        "project.upload_dataset_from_data_source",
        "model.initialize_anomaly_assessment",
        "model.get_num_iterations_trained",
        "model.train_datetime",
        "model.retrain",
        "model.start_incremental_learning",
        "project.wait_for_autopilot",
        "datarobot.enums.biasmitigationtechnique",
        "datarobot.enums.ts_blender_method",
        "model.request_training_predictions",
        "project.create_from_data_source",
        "deployment.create_from_custom_model_image",
        "datarobot.models.datastore",
        "datarobot.customtask.create",
        "datarobot.models.secondarydatasetconfigurations",
        "deployment.download_scoring_code",
        "datarobot.enums.predictionenvironmentplatform",
        "deployment.capability",
        "datarobot.predictionexplanations.get_rows",
        "datarobot.enums.variable_type_transform",
        "model.get_or_request_feature_impact",
        "deployment.get_prediction_intervals_settings",
        "deployment.deployment",
        "datarobot.customtask.list",
        "model.request_feature_effect",
        "project.create_from_recipe",
        "deployment.submit_actuals",
        "project.clone_project",
        "datarobot.enums.custom_model_target_type",
        "project.get",
        "model.list_versions",
        "model.create",
        "model.request_roc_curve",
        "model.from_data",
        "project.get_relationships_configuration",
        "project.advanced_options",
        "dr.config.yaml",
        "model.get_segments_info",
        "datarobot.target_type.multiclass",
        "project.get_datasets",
        "model.get_all_labelwise_roc_curves",
        "model.get_anomaly_assessment_records",
        "model.get_cross_validation_scores",
        "model.get_series_accuracy_as_dataframe",
        "dr.enums.data_subset",
        "deployment.list_actuals_data_exports",
        "project.download_feature_discovery_dataset",
        "project.check_blendable",
        "model.request_predictions",
        "model.request_per_class_fairness_insights",
        "project.upload_file_from_url",
        "project.get_association_featurelists",
        "deployment.update_predictions_data_collection_settings",
        "project.get_association_matrix_details",
        "model.get_all_residuals_charts",
        "model.assign_training_data",
        "model.open_model_browser",
        "project.upload_dataset",
        "project.unlock_holdout",
        "datarobot.combinedmodel.get_segments_as_dataframe",
        "deployment.download_prediction_results",
        "deployment.get_uri",
        "model.download_series_accuracy_as_csv",
        "datarobot.segmentationtask.list",
        "datarobot.enums.predictionenvironmenthealthtype",
        "datarobot.models.recipe_operation",
        "model.get_multilabel_lift_charts",
        "model.start_advanced_tuning_session",
        "datarobot.models.relationshipsconfiguration",
        "model.request_fairness_insights",
        "project.create_prime_model",
        "datarobot.models.featureassociationmatrixdetails",
        "project.create_from_dataset",
        "project.get_featurelist_by_name",
        "project.download_multicategorical_data_format_errors",
        "project.create_modeling_featurelist",
        "model.get_roc_curve",
        "project.get_uri",
        "model.get_word_cloud",
        "model.predict_batch",
        "deployment.update_challenger_models_settings",
        "datarobot.models.genai",
        "model.get_frozen_child_models",
        "model.get_lift_chart",
        "datarobot.models.featureassociationfeaturelists",
        "project.get_bias_mitigation_feature_info",
        "deployment.get_challenger_models_settings",
        "model.get_leaderboard_ui_permalink",
        "datarobot.combinedmodel.set_segment_champion",
        "model.get_labelwise_roc_curves",
        "datarobot.models.deployment",
        "project.analyze_and_model",
        "model.list",
        "model.get_accuracy_over_time_plot",
        "model.featurelist"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_3686988720782446270",
      "title": "Applications",
      "content": "Applications\nclass datarobot.Application\nAn entity associated with a DataRobot Application.\nVariables:\nid (str) – The ID of the created application.\napplication_type_id (str) – The ID of the type of the application.\nuser_id (str) – The ID of the user which created the application.\nmodel_deployment_id (str) – The ID of the associated model deployment.\ndeactivation_status_id (str or None) – The ID of the status object to track the asynchronous app deactivation process status.\nWill be None if the app was never deactivated.\nname (str) – The name of the application.\ncreated_by (str) – The username of the user created the application.\ncreated_at (str) – The timestamp when the application was created.\nupdated_at (str) – The timestamp when the application was updated.\ndatasets (List[str]) – The list of datasets IDs associated with the application.\ncreator_first_name (Optional[str]) – Application creator first name. Optional.\ncreator_last_name (Optional[str]) – Application creator last name. Optional.\ncreator_userhash (Optional[str]) – Application creator userhash. Optional.\ndeployment_status_id (str) – The ID of the status object to track the asynchronous deployment process status.\ndescription (str) – A description of the application.\ncloud_provider (str) – The host of this application.\ndeployments (Optional[List[ApplicationDeployment]]) – A list of deployment details. Optional.\ndeployment_ids (List[str]) – A list of deployment IDs for this app.\ndeployment_name (Optional[str]) – Name of the deployment. Optional.\napplication_template_type (Optional[str]) – Application template type, purpose. Optional.\npool_used (bool) – Whether the pool where used for last app deployment.\npermissions (List[str]) – The list of permitted actions, which the authenticated user can perform on this application.\nPermissions should be ApplicationPermission options.\nhas_custom_logo (bool) – Whether the app has a custom logo.\nrelated_entities (Optional[ApplcationRelatedEntity]) – IDs of entities, related to app for easy search.\norg_id (str) – ID of the app’s organization.\nclassmethod list(offset=None, limit=None, use_cases=None)\nRetrieve a list of user applications.\nParameters:\noffset (Optional[int]) – Optional. Retrieve applications in a list after this number.\nlimit (Optional[int]) – Optional. Retrieve only this number of applications.\nuse_cases (Optional[Union[UseCase, List[UseCase], str, List[str]]]) – Optional. Filter available Applications by a specific Use Case or Use Cases.\nAccepts either the entity or the ID.\nIf set to [None], the method filters the application’s datasets by those not linked to a UseCase.\nReturns:\napplications – The requested list of user applications.\nReturn type:\nList[Application]\nclassmethod get(application_id)\nRetrieve a single application.\nParameters:\napplication_id (str) – The ID of the application to retrieve.\nReturns:\napplication – The requested application.\nReturn type:\nApplication",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/applications.html",
      "tags": [
        "advanced",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/applications.html",
        "content_length": 2937
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.7,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2761908927490737949",
      "title": "APIObject",
      "content": "APIObject\nclass datarobot.models.api_object.APIObject\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/api-object.html",
      "tags": [
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/api-object.html",
        "content_length": 744
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.api_object"
      ],
      "complexity_score": 0.15000000000000002,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-4881020179108338308",
      "title": "Custom templates",
      "content": "Custom templates\nclass datarobot.models.custom_templates.CustomTemplate\nTemplate for custom activity (e.g. custom-metrics, applications).\nclassmethod list(search=None, order_by=None, tag=None, template_type=None, template_sub_type=None, offset=None, limit=None)\nList all custom templates.\nAdded in version v3.7.\nParameters:\nsearch (Optional[str]) – Search string.\norder_by (Optional[ListCustomTemplatesSortQueryParams]) – Ordering field.\ntag (Optional[str]) – Tag associated with the template.\ntemplate_type (Optional[str]) – Type of the template.\ntemplate_type – Sub-type of the template.\noffset (Optional[int]) – Offset for pagination.\nlimit (Optional[int]) – Limit for pagination.\nReturns:\ntemplates\nReturn type:\nList[CustomTemplate]\nclassmethod get(template_id)\nGet a custom template by ID.\nAdded in version v3.7.\nParameters:\ntemplate_id (str) – ID of the template.\nReturns:\ntemplate\nReturn type:\nCustomTemplate\nupdate(name=None, description=None, default_resource_bundle_id=None, template_type=None, template_sub_type=None, template_metadata=None, default_environment=None)\nUpdate the custom template.\nAdded in version v3.7.\nParameters:\nname (Optional[str]) – The template name.\ndescription (Optional[str]) – A description of the template.\ndefault_resource_bundle_id (Optional[str]) – The default resource bundle ID.\ntemplate_type (Optional[str]) – The template type.\ntemplate_sub_type (Optional[str]) – The template sub-type.\ntemplate_metadata (Optional[TemplateMetadata|str]) – The metadata associated with the template, provided as TemplateMetadata or a JSON encoded string.\ndefault_environment (Optional[DefaultEnvironment|str]) – The default environment associated with the template, provided as DefaultEnvironment or a JSON encoded\nstring.\nReturn type:\nNone\nExamples\nfrom datarobot import CustomTemplate\nfrom datarobot.models.custom_templates import DefaultEnvironment\nnew_env = DefaultEnvironment(\nenvironment_id='679d47c8ce1ecd17326f3fdf',\nenvironment_version_id='679d47c8ce1ecd17326f3fe3',\n)\ntemplate = CustomTemplate.get(template_id='5c939e08962d741e34f609f0')\ntemplate.update(default_environment=new_env, description='Updated template with environment v17')\ndelete()\nDelete this custom template.\nAdded in version v3.7.\nReturn type:\nNone\nclass datarobot.models.custom_templates.DefaultEnvironment\nDefault execution environment.\nclass datarobot.models.custom_templates.CustomMetricMetadata\nMetadata for custom metrics.\nclass datarobot.models.custom_templates.TemplateMetadata\nMetadata for the custom templates.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/application-templates.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/application-templates.html",
        "content_length": 2533
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.custom_templates"
      ],
      "complexity_score": 0.44999999999999996,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-3173951481685535992",
      "title": "Binary data helpers",
      "content": "Binary data helpers\ndatarobot.helpers.binary_data_utils.get_encoded_image_contents_from_urls(urls, custom_headers=None, image_options=None, continue_on_error=False, n_threads=None)\nReturns base64 encoded string of images located in addresses passed in input collection.\nInput collection should hold data of valid image url addresses reachable from\nlocation where code is being executed. Method will retrieve image, apply specified\nreformatting before converting contents to base64 string. Results will in same\norder as specified in input collection.\nParameters:\nurls (Iterable) – Iterable with url addresses to download images from\ncustom_headers (dict) – Dictionary containing custom headers to use when downloading files using a URL. Detailed\ndata related to supported Headers in HTTP  can be found in the RFC specification for\nheaders: https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html\nWhen used, specified passed values will overwrite default header values.\nimage_options (ImageOptions class) – Class holding parameters for use in image transformation and formatting.\ncontinue_on_error (bool) – If one of rows encounters error while retrieving content (i.e. file does not exist) should\nthis error terminate process of downloading consecutive files or should process continue\nskipping this file.\nn_threads (int or None) – Number of threads to use for processing. If “None” is passed, the number of threads is\ndetermined automatically based on the number of available CPU cores. If this is not\npossible, 4 threads are used.\nRaises:\nContentRetrievalTerminatedError: – The error is raised when the flag continue_on_error is set to` False` and processing has\nbeen terminated due to an exception while loading the contents of the file.\nReturn type:\nList of base64 encoded strings representing reformatted images.\ndatarobot.helpers.binary_data_utils.get_encoded_image_contents_from_paths(paths, image_options=None, continue_on_error=False, n_threads=None, base_path=None)\nReturns base64 encoded string of images located in paths passed in input collection.\nInput collection should hold data of valid image paths reachable from location\nwhere code is being executed. Method will retrieve image, apply specified\nreformatting before converting contents to base64 string. Results will in same\norder as specified in input collection.\nParameters:\npaths (Iterable) – Iterable with path locations to open images from\nimage_options (ImageOptions class) – Class holding parameters for image transformation and formatting\ncontinue_on_error (bool) – If one of rows encounters error while retrieving content (i.e. file does not exist) should\nthis error terminate process of downloading consecutive files or should process continue\nskipping this file.\nn_threads (int or None) – Number of threads to use for processing. If “None” is passed, the number of threads is\ndetermined automatically based on the number of available CPU cores. If this is not\npossible, 4 threads are used.\nbase_path (Optional[str]) – Base path to use when opening files. If specified, this path will be used as a base\ndirectory against which all relative paths will be evaluated. If not specified, the path\nwill be evaluated against the directory where this script is running.\nRaises:\nContentRetrievalTerminatedError: – The error is raised when the flag continue_on_error is set to` False` and processing has\nbeen terminated due to an exception while loading the contents of the file.\nReturn type:\nList of base64 encoded strings representing reformatted images.\ndatarobot.helpers.binary_data_utils.get_encoded_file_contents_from_paths(paths, continue_on_error=False, n_threads=None, base_path=None)\nReturns base64 encoded string for files located under paths passed in input collection.\nInput collection should hold data of valid file paths locations reachable from\nlocation where code is being executed. Method will retrieve file and convert its contents\nto base64 string. Results will be returned in same order as specified in input collection.\nParameters:\npaths (Iterable) – Iterable with path locations to open images from\ncontinue_on_error (bool) – If one of rows encounters error while retrieving content (i.e. file does not exist) should\nthis error terminate process of downloading consecutive files or should process continue\nskipping this file.\nn_threads (int or None) – Number of threads to use for processing. If “None” is passed, the number of threads is\ndetermined automatically based on the number of available CPU cores. If this is not\npossible, 4 threads are used.\nbase_path (Optional[str]) – Base path to use when opening files. If specified, this path will be used as a base\ndirectory against which all relative paths will be evaluated. If not specified, the path\nwill be evaluated against the directory where this script is running.\nRaises:\nContentRetrievalTerminatedError: – The error is raised when the flag continue_on_error is set to` False` and processing has\nbeen terminated due to an exception while loading the contents of the file.\nReturn type:\nList of base64 encoded strings representing files.\ndatarobot.helpers.binary_data_utils.get_encoded_file_contents_from_urls(urls, custom_headers=None, continue_on_error=False, n_threads=None)\nReturns base64-encoded string for files located in the URL addresses passed on input. Input\ncollection holds data of valid file URL addresses reachable from location where code is being\nexecuted. Method will retrieve file and convert its contents to base64 string. Results will\nbe returned in same order as specified in input collection.\nParameters:\nurls (Iterable) – Iterable containing URL addresses to download images from.\ncustom_headers (dict) – Dictionary with headers to use when downloading files using a URL. Detailed data\nrelated to supported Headers in HTTP  can be found in the RFC specification:\nhttps://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html. When specified,\npassed values will overwrite default header values.\ncontinue_on_error (bool) – If a row encounters an error while retrieving content (i.e., file does not exist),\nspecifies whether the error results in terminating the process of downloading\nconsecutive files or the process continues. Skipped files will be marked as missing.\nn_threads (int or None) – Number of threads to use for processing. If “None” is passed, the number of threads is\ndetermined automatically based on the number of available CPU cores. If this is not\npossible, 4 threads are used.\nRaises:\nContentRetrievalTerminatedError: – The error is raised when the flag continue_on_error is set to` False` and processing has\nbeen terminated due to an exception while loading the contents of the file.\nReturn type:\nList of base64 encoded strings representing files.\nclass datarobot.helpers.image_utils.ImageOptions\nImage options class. Class holds image options related to image resizing and image reformatting.\nshould_resize: boolWhether input image should be resized to new dimensions.\nforce_size: boolWhether the image size should fully match the new requested size. If the original\nand new image sizes have different aspect ratios, specifying True will force a resize\nto exactly match the requested size. This may break the aspect ratio of the original\nimage. If False, the resize method modifies the image to contain a thumbnail version\nof itself, no larger than the given size, that preserves the image’s aspect ratio.\nimage_size: Tuple[int, int]New image size (width, height). Both values (width, height) should be specified and contain\na positive value. Depending on the value of force_size, the image will be resized exactly\nto the given image size or will be resized into a thumbnail version of itself, no larger\nthan the given size.\nimage_format: ImageFormat | strWhat image format will be used to save result image after transformations. For example\n(ImageFormat.JPEG, ImageFormat.PNG). Values supported are in line with values supported\nby DataRobot. If no format is specified by passing None value original image format\nwill be preserved.\nimage_quality: int or NoneThe image quality used when saving image. When None is specified, a value will\nnot be passed and Pillow library will use its default.\nresample_method: ImageResampleMethodWhat resampling method should be used when resizing image.\nkeep_quality: boolWhether the image quality is kept (when possible). If True, for JPEG images quality will\nbe preserved. For other types, the value specified in image_quality will be used.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/binary_data_helpers.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/binary_data_helpers.html",
        "content_length": 8468
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.helpers.image_utils",
        "datarobot.helpers.binary_data_utils"
      ],
      "complexity_score": 0.6000000000000001,
      "use_case_category": "time_series"
    },
    {
      "id": "readthedocs_8415989045979731818",
      "title": "Batch monitoring",
      "content": "Batch monitoring\nclass datarobot.models.BatchMonitoringJob\nA Batch Monitoring Job is used to monitor data sets outside DataRobot app.\nVariables:\nid (str) – the id of the job\nclassmethod get(project_id, job_id)\nGet batch monitoring job\nVariables:\njob_id (str) – ID of batch job\nReturns:\nInstance of BatchMonitoringJob\nReturn type:\nBatchMonitoringJob\ndownload(fileobj, timeout=120, read_timeout=660)\nDownloads the results of a monitoring job as a CSV.\nVariables:\nfileobj (A file-like object where the CSV monitoring results will be) – written to. Examples include an in-memory buffer\n(e.g., io.BytesIO) or a file on disk (opened for binary writing).\ntimeout (int (optional, default 120)) – Seconds to wait for the download to become available.\nThe download will not be available before the job has started processing.\nIn case other jobs are occupying the queue, processing may not start\nimmediately.\nIf the timeout is reached, the job will be aborted and RuntimeError\nis raised.\nSet to -1 to wait infinitely.\nread_timeout (int (optional, default 660)) – Seconds to wait for the server to respond between chunks.\nReturn type:\nNone\nclassmethod run(deployment, intake_settings=None, output_settings=None, csv_settings=None, num_concurrent=None, chunk_size=None, abort_on_error=True, monitoring_aggregation=None, monitoring_columns=None, monitoring_output_settings=None, download_timeout=120, download_read_timeout=660, upload_read_timeout=600)\nCreate new batch monitoring job, upload the dataset, and\nreturn a batch monitoring job.\nVariables:\ndeployment (Deployment or string ID) – Deployment which will be used for monitoring.\nintake_settings (dict) – A dict configuring how data is coming from. Supported options:\ntype : string, either localFile, s3, azure, gcp, dataset, jdbc\nsnowflake, synapse, bigquery, or datasphere\nNote that to pass a dataset, you not only need to specify the type parameter\nas dataset, but you must also set the dataset parameter as a\ndr.Dataset object.\nTo monitor from a local file, add this parameter to the\nsettings:\nfile : A file-like object, string path to a file or a\npandas.DataFrame of scoring data.\nTo monitor from S3, add the next parameters to the settings:\nurl : string, the URL to score (e.g.: s3://bucket/key).\ncredential_id : string (optional).\nendpoint_url : string (optional), any non-default endpoint\nURL for S3 access (omit to use the default).\n(batch_monitoring_jdbc_creds_usage)=\nTo monitor from JDBC, add the next parameters to the settings:\ndata_store_id : string, the ID of the external data store connected\nto the JDBC data source (see\nDatabase Connectivity).\nquery : string (optional if table, schema and/or catalog is specified),\na self-supplied SELECT statement of the data set you wish to predict.\ntable : string (optional if query is specified),\nthe name of specified database table.\nschema : string (optional if query is specified),\nthe name of specified database schema.\ncatalog : string  (optional if query is specified),\n(new in v2.22) the name of specified database catalog.\nfetch_size : int (optional),\nChanging the fetchSize can be used to balance throughput and memory\nusage.\ncredential_id : string (optional) the ID of the credentials holding\ninformation about a user with read-access to the JDBC data source (see\nCredentials).\nTo monitor from Datasphere, add the following parameters to the settings:\ndata_store_id : string, the ID of the external data store connected to\nthe Datasphere data source (see\nDatabase Connectivity).\ntable : string,  the name of specified database table.\nschema : string, the name of specified database schema.\ncredential_id : string, the ID of the credentials holding information about\na user with read-access to the Datasphere data source (see\nCredentials).\noutput_settings (dict (optional)) – A dict configuring how monitored data is to be saved. Supported\noptions:\ntype : string, either localFile, s3, azure, gcp, jdbc,\nsnowflake, synapse, bigquery, or datasphere\nTo save monitored data to a local file, add parameters to the\nsettings:\npath : string (optional), path to save the scored data\nas CSV. If a path is not specified, you must download\nthe scored data yourself with job.download().\nIf a path is specified, the call will block until the\njob is done. if there are no other jobs currently\nprocessing for the targeted prediction instance,\nuploading, scoring, downloading will happen in parallel\nwithout waiting for a full job to complete. Otherwise,\nit will still block, but start downloading the scored\ndata as soon as it starts generating data. This is the\nfastest method to get predictions.\nTo save monitored data to S3, add the next parameters to the settings:\nurl : string, the URL for storing the results\n(e.g.: s3://bucket/key).\ncredential_id : string (optional).\nendpoint_url : string (optional), any non-default endpoint\nURL for S3 access (omit to use the default).\nTo save monitored data to JDBC, add the next parameters to the settings:\ndata_store_id : string, the ID of the external data store connected to\nthe JDBC data source (see\nDatabase Connectivity).\ntable : string,  the name of specified database table.\nschema : string (optional), the name of specified database schema.\ncatalog : string (optional), (new in v2.22) the name of specified database\ncatalog.\nstatement_type : string, the type of insertion statement to create,\none of datarobot.enums.AVAILABLE_STATEMENT_TYPES.\nupdate_columns : list(string) (optional),  a list of strings containing\nthose column names to be updated in case statement_type is set to a\nvalue related to update or upsert.\nwhere_columns : list(string) (optional), a list of strings containing\nthose column names to be selected in case statement_type is set to a\nvalue related to insert or update.\ncredential_id : string, the ID of the credentials holding information about\na user with write-access to the JDBC data source (see\nCredentials).\ncreate_table_if_not_exists : bool (optional), If no existing table is detected,\nattempt to create it before writing data with the strategy defined in the\nstatementType parameter.\nTo save monitored data to Datasphere, add the next parameters to the settings:\ndata_store_id : string, the ID of the external data store connected to\nthe Datasphere data source (see\nDatabase Connectivity).\ntable : string,  the name of specified database table.\nschema : string, the name of specified database schema.\ncredential_id : string, the ID of the credentials holding information about\na user with write-access to the Datasphere data source (see\nCredentials).\ncsv_settings (dict (optional)) – CSV intake and output settings. Supported options:\ndelimiter : string (optional, default ,), fields are delimited by\nthis character. Use the string tab to denote TSV (TAB separated values).\nMust be either a one-character string or the string tab.\nquotechar : string (optional, default “), fields containing the\ndelimiter must be quoted using this character.\nencoding : string (optional, default utf-8), encoding for the CSV\nfiles. For example (but not limited to): shift_jis, latin_1 or\nmskanji.\nnum_concurrent (int (optional)) – Number of concurrent chunks to score simultaneously. Defaults to\nthe available number of cores of the deployment. Lower it to leave\nresources for real-time scoring.\nchunk_size (string or int (optional)) – Which strategy should be used to determine the chunk size.\nCan be either a named strategy or a fixed size in bytes.\n- auto: use fixed or dynamic based on flipper.\n- fixed: use 1MB for explanations, 5MB for regular requests.\n- dynamic: use dynamic chunk sizes.\n- int: use this many bytes per chunk.\nabort_on_error (boolean (optional)) – Default behavior is to abort the job if too many rows fail scoring. This will free\nup resources for other jobs that may score successfully. Set to false to\nunconditionally score every row no matter how many errors are encountered.\nDefaults to True.\ndownload_timeout (int (optional)) –\nAdded in version 2.22.\nIf using localFile output, wait this many seconds for the download to become\navailable. See download().\ndownload_read_timeout (int (optional, default 660)) –\nAdded in version 2.22.\nIf using localFile output, wait this many seconds for the server to respond\nbetween chunks.\nupload_read_timeout (int (optional, default 600)) –\nAdded in version 2.28.\nIf using localFile intake, wait this many seconds for the server to respond\nafter whole dataset upload.\nReturns:\nInstance of BatchMonitoringJob\nReturn type:\nBatchMonitoringJob\nExamples\n>>> import datarobot as dr\n>>> job_spec = {\n...     \"intake_settings\": {\n...         \"type\": \"jdbc\",\n...         \"data_store_id\": \"645043933d4fbc3215f17e34\",\n...         \"catalog\": \"SANDBOX\",\n...         \"table\": \"10kDiabetes_output_actuals\",\n...         \"schema\": \"SCORING_CODE_UDF_SCHEMA\",\n...         \"credential_id\": \"645043b61a158045f66fb329\"\n...     },\n>>>     \"monitoring_columns\": {\n...         \"predictions_columns\": [\n...             {\n...                 \"class_name\": \"True\",\n...                 \"column_name\": \"readmitted_True_PREDICTION\"\n...             },\n...             {\n...                 \"class_name\": \"False\",\n...                 \"column_name\": \"readmitted_False_PREDICTION\"\n...             }\n...         ],\n...         \"association_id_column\": \"rowID\",\n...         \"actuals_value_column\": \"ACTUALS\"\n...     }\n... }\n>>> deployment_id = \"foobar\"\n>>> job = dr.BatchMonitoringJob.run(deployment_id, **job_spec)\n>>> job.wait_for_completion()\ncancel(ignore_404_errors=False)\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nReturn type:\nNone\nget_status()\nGet status of batch monitoring job\nReturns:\nDict with job status\nReturn type:\nBatchMonitoringJob status data\nclass datarobot.models.BatchMonitoringJobDefinition\nclassmethod get(batch_monitoring_job_definition_id)\nGet batch monitoring job definition\nVariables:\nbatch_monitoring_job_definition_id (str) – ID of batch monitoring job definition\nReturns:\nInstance of BatchMonitoringJobDefinition\nReturn type:\nBatchMonitoringJobDefinition\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchMonitoringJobDefinition.get('5a8ac9ab07a57a0001be501f')\n>>> definition\nBatchMonitoringJobDefinition(60912e09fd1f04e832a575c1)\nclassmethod list()\nGet job all monitoring job definitions\nReturns:\nList of job definitions the user has access to see\nReturn type:\nList[BatchMonitoringJobDefinition]\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchMonitoringJobDefinition.list()\n>>> definition\n[\nBatchMonitoringJobDefinition(60912e09fd1f04e832a575c1),\nBatchMonitoringJobDefinition(6086ba053f3ef731e81af3ca)\n]\nclassmethod create(enabled, batch_monitoring_job, name=None, schedule=None)\nCreates a new batch monitoring job definition to be run either at scheduled interval or as\na manual run.\nVariables:\nenabled (bool (default False)) – Whether the definition should be active on a scheduled basis. If True,\nschedule is required.\nbatch_monitoring_job (dict) – The job specifications for your batch monitoring job.\nIt requires the same job input parameters as used with BatchMonitoringJob\nname (string (optional)) – The name you want your job to be identified with. Must be unique across the\norganization’s existing jobs.\nIf you don’t supply a name, a random one will be generated for you.\nschedule (dict (optional)) – The schedule payload defines at what intervals the job should run, which can be\ncombined in various ways to construct complex scheduling terms if needed. In all\nthe elements in the objects, you can supply either an asterisk [\"*\"] denoting\n“every” time denomination or an array of integers (e.g. [1, 2, 3]) to define\na specific interval.\nThe schedule payload is split up in the following items:\nMinute:\nThe minute(s) of the day that the job will run. Allowed values are either [\"*\"]\nmeaning every minute of the day or [0 ... 59]\nHour:\nThe hour(s) of the day that the job will run. Allowed values are either [\"*\"]\nmeaning every hour of the day or [0 ... 23].\nDay of Month:\nThe date(s) of the month that the job will run. Allowed values are either\n[1 ... 31] or [\"*\"] for all days of the month. This field is additive with\ndayOfWeek, meaning the job will run both on the date(s) defined in this field\nand the day specified by dayOfWeek (for example, dates 1st, 2nd, 3rd, plus every\nTuesday). If dayOfMonth is set to [\"*\"] and dayOfWeek is defined,\nthe scheduler will trigger on every day of the month that matches dayOfWeek\n(for example, Tuesday the 2nd, 9th, 16th, 23rd, 30th).\nInvalid dates such as February 31st are ignored.\nMonth:\nThe month(s) of the year that the job will run. Allowed values are either\n[1 ... 12] or [\"*\"] for all months of the year. Strings, either\n3-letter abbreviations or the full name of the month, can be used\ninterchangeably (e.g., “jan” or “october”).\nMonths that are not compatible with dayOfMonth are ignored, for example\n{\"dayOfMonth\": [31], \"month\":[\"feb\"]}\nDay of Week:\nThe day(s) of the week that the job will run. Allowed values are [0 .. 6],\nwhere (Sunday=0), or [\"*\"], for all days of the week. Strings, either 3-letter\nabbreviations or the full name of the day, can be used interchangeably\n(e.g., “sunday”, “Sunday”, “sun”, or “Sun”, all map to [0].\nThis field is additive with dayOfMonth, meaning the job will run both on the\ndate specified by dayOfMonth and the day defined in this field.\nReturns:\nInstance of BatchMonitoringJobDefinition\nReturn type:\nBatchMonitoringJobDefinition\nExamples\n>>> import datarobot as dr\n>>> job_spec = {\n...    \"num_concurrent\": 4,\n...    \"deployment_id\": \"foobar\",\n...    \"intake_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...    \"output_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...}\n>>> schedule = {\n...    \"day_of_week\": [\n...        1\n...    ],\n...    \"month\": [\n...        \"*\"\n...    ],\n...    \"hour\": [\n...        16\n...    ],\n...    \"minute\": [\n...        0\n...    ],\n...    \"day_of_month\": [\n...        1\n...    ]\n...}\n>>> definition = BatchMonitoringJobDefinition.create(\n...    enabled=False,\n...    batch_monitoring_job=job_spec,\n...    name=\"some_definition_name\",\n...    schedule=schedule\n... )\n>>> definition\nBatchMonitoringJobDefinition(60912e09fd1f04e832a575c1)\nupdate(enabled, batch_monitoring_job=None, name=None, schedule=None)\nUpdates a job definition with the changed specs.\nTakes the same input as create()\nVariables:\nenabled (bool (default False)) – Same as enabled in create().\nbatch_monitoring_job (dict) – Same as batch_monitoring_job in create().\nname (string (optional)) – Same as name in create().\nschedule (dict) – Same as schedule in create().\nReturns:\nInstance of the updated BatchMonitoringJobDefinition\nReturn type:\nBatchMonitoringJobDefinition\nExamples\n>>> import datarobot as dr\n>>> job_spec = {\n...    \"num_concurrent\": 5,\n...    \"deployment_id\": \"foobar_new\",\n...    \"intake_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...    \"output_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...}\n>>> schedule = {\n...    \"day_of_week\": [\n...        1\n...    ],\n...    \"month\": [\n...        \"*\"\n...    ],\n...    \"hour\": [\n...        \"*\"\n...    ],\n...    \"minute\": [\n...        30, 59\n...    ],\n...    \"day_of_month\": [\n...        1, 2, 6\n...    ]\n...}\n>>> definition = BatchMonitoringJobDefinition.create(\n...    enabled=False,\n...    batch_monitoring_job=job_spec,\n...    name=\"updated_definition_name\",\n...    schedule=schedule\n... )\n>>> definition\nBatchMonitoringJobDefinition(60912e09fd1f04e832a575c1)\nrun_on_schedule(schedule)\nSets the run schedule of an already created job definition.\nIf the job was previously not enabled, this will also set the job to enabled.\nVariables:\nschedule (dict) – Same as schedule in create().\nReturns:\nInstance of the updated BatchMonitoringJobDefinition with the new / updated schedule.\nReturn type:\nBatchMonitoringJobDefinition\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchMonitoringJobDefinition.create('...')\n>>> schedule = {\n...    \"day_of_week\": [\n...        1\n...    ],\n...    \"month\": [\n...        \"*\"\n...    ],\n...    \"hour\": [\n...        \"*\"\n...    ],\n...    \"minute\": [\n...        30, 59\n...    ],\n...    \"day_of_month\": [\n...        1, 2, 6\n...    ]\n...}\n>>> definition.run_on_schedule(schedule)\nBatchMonitoringJobDefinition(60912e09fd1f04e832a575c1)\nrun_once()\nManually submits a batch monitoring job to the queue, based off of an already\ncreated job definition.\nReturns:\nInstance of BatchMonitoringJob\nReturn type:\nBatchMonitoringJob\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchMonitoringJobDefinition.create('...')\n>>> job = definition.run_once()\n>>> job.wait_for_completion()\ndelete()\nDeletes the job definition and disables any future schedules of this job if any.\nIf a scheduled job is currently running, this will not be cancelled.\n:rtype: None\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchMonitoringJobDefinition.get('5a8ac9ab07a57a0001be501f')\n>>> definition.delete()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/batch-monitoring.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/batch-monitoring.html",
        "content_length": 17157
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.enums.available_statement_types",
        "dr.batchmonitoringjobdefinition.create",
        "dr.batchmonitoringjob.run",
        "dr.batchmonitoringjobdefinition.get",
        "dr.batchmonitoringjobdefinition.list",
        "datarobot.models.batchmonitoringjobdefinition",
        "datarobot.models.batchmonitoringjob"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_6511901800129504341",
      "title": "Batch predictions",
      "content": "Batch predictions\nclass datarobot.models.BatchPredictionJob\nA Batch Prediction Job is used to score large data sets on\nprediction servers using the Batch Prediction API.\nVariables:\nid (str) – the id of the job\nclassmethod score(deployment, intake_settings=None, output_settings=None, csv_settings=None, timeseries_settings=None, num_concurrent=None, chunk_size=None, passthrough_columns=None, passthrough_columns_set=None, max_explanations=None, max_ngram_explanations=None, explanation_algorithm=None, threshold_high=None, threshold_low=None, prediction_threshold=None, prediction_warning_enabled=None, include_prediction_status=False, skip_drift_tracking=False, prediction_instance=None, abort_on_error=True, column_names_remapping=None, include_probabilities=True, include_probabilities_classes=None, download_timeout=120, download_read_timeout=660, upload_read_timeout=600, explanations_mode=None)\nCreate new batch prediction job, upload the scoring dataset and\nreturn a batch prediction job.\nThe default intake and output options are both localFile which\nrequires the caller to pass the file parameter and either\ndownload the results using the download() method afterwards or\npass a path to a file where the scored data will be downloaded to\nafterwards.\nVariables:\ndeployment (Deployment or string ID) – Deployment which will be used for scoring.\nintake_settings (Optional[IntakeSettings]) – A dict configuring how data is coming from. Supported options:\ntype : str, either localFile, s3, azure, gcp, dataset, jdbc\nsnowflake, synapse, bigquery, or datasphere\nNote that to pass a dataset, you not only need to specify the type parameter\nas dataset, but you must also set the dataset parameter as a\ndr.Dataset object.\nTo score from a local file, add the this parameter to the\nsettings:\nfile : file-like object, string path to file or a\npandas.DataFrame of scoring data\nTo score from S3, add the next parameters to the settings:\nurl : str, the URL to score (e.g.: s3://bucket/key)\ncredential_id : Optional[str]\nendpoint_url : Optional[str], any non-default endpoint\nURL for S3 access (omit to use the default)\nTo score from JDBC, add the next parameters to the settings:\ndata_store_id : str, the ID of the external data store connected\nto the JDBC data source (see\nDatabase Connectivity).\nquery : str (optional if table, schema and/or catalog is specified),\na self-supplied SELECT statement of the data set you wish to predict.\ntable : str (optional if query is specified),\nthe name of specified database table.\nschema : str (optional if query is specified),\nthe name of specified database schema.\ncatalog : str  (optional if query is specified),\n(new in v2.22) the name of specified database catalog.\nfetch_size : Optional[int],\nChanging the fetchSize can be used to balance throughput and memory\nusage.\ncredential_id : Optional[str] the ID of the credentials holding\ninformation about a user with read-access to the JDBC data source (see\nCredentials).\nTo score from Datasphere, add the next parameters to the settings:\ndata_store_id : str, the ID of the external data store connected to\nthe Datasphere data source (see\nDatabase Connectivity).\ntable : str,  the name of specified database table.\nschema : str, the name of specified database schema.\ncredential_id : str, the ID of the credentials holding information about\na user with read-access to the Datasphere data source (see\nCredentials).\noutput_settings (Optional[OutputSettings]) – A dict configuring how scored data is to be saved. Supported\noptions:\ntype : str, either localFile, s3, azure, gcp, jdbc,\nsnowflake, synapse, bigquery, or datasphere\nTo save scored data to a local file, add this parameters to the\nsettings:\npath : Optional[str], path to save the scored data\nas CSV. If a path is not specified, you must download\nthe scored data yourself with job.download().\nIf a path is specified, the call will block until the\njob is done. if there are no other jobs currently\nprocessing for the targeted prediction instance,\nuploading, scoring, downloading will happen in parallel\nwithout waiting for a full job to complete. Otherwise,\nit will still block, but start downloading the scored\ndata as soon as it starts generating data. This is the\nfastest method to get predictions.\nTo save scored data to S3, add the next parameters to the settings:\nurl : str, the URL for storing the results\n(e.g.: s3://bucket/key)\ncredential_id : Optional[str]\nendpoint_url : Optional[str], any non-default endpoint\nURL for S3 access (omit to use the default)\nTo save scored data to JDBC, add the next parameters to the settings:\ndata_store_id : str, the ID of the external data store connected to\nthe JDBC data source (see\nDatabase Connectivity).\ntable : str,  the name of specified database table.\nschema : Optional[str], the name of specified database schema.\ncatalog : Optional[str], (new in v2.22) the name of specified database\ncatalog.\nstatement_type : str, the type of insertion statement to create,\none of datarobot.enums.AVAILABLE_STATEMENT_TYPES.\nupdate_columns : list(string) (optional),  a list of strings containing\nthose column names to be updated in case statement_type is set to a\nvalue related to update or upsert.\nwhere_columns : list(string) (optional), a list of strings containing\nthose column names to be selected in case statement_type is set to a\nvalue related to insert or update.\ncredential_id : str, the ID of the credentials holding information about\na user with write-access to the JDBC data source (see\nCredentials).\nTo save scored data to Datasphere, add the following parameters to the settings:\ndata_store_id : str, the ID of the external data store connected to\nthe Datasphere data source (see\nDatabase Connectivity).\ntable : str,  the name of specified database table.\nschema : str, the name of specified database schema.\ncredential_id : str, the ID of the credentials holding information about\na user with write-access to the Datasphere data source (see\nCredentials).\ncsv_settings (Optional[CsvSettings]) – CSV intake and output settings. Supported options:\ndelimiter : str (optional, default ,), fields are delimited by\nthis character. Use the string tab to denote TSV (TAB separated values).\nMust be either a one-character string or the string tab.\nquotechar : str (optional, default “), fields containing the\ndelimiter must be quoted using this character.\nencoding : str (optional, default utf-8), encoding for the CSV\nfiles. For example (but not limited to): shift_jis, latin_1 or\nmskanji.\ntimeseries_settings (Optional[TimeSeriesSettings]) – Configuration for time-series scoring. Supported options:\ntype : str, must be forecast or historical (default if\nnot passed is forecast). forecast mode makes predictions using\nforecast_point or rows in the dataset without target. historical\nenables bulk prediction mode which calculates predictions for all\npossible forecast points and forecast distances in the dataset within\npredictions_start_date/predictions_end_date range.\nforecast_point : Optional[datetime.datetime], forecast point for the dataset,\nused for the forecast predictions, by default value will be inferred\nfrom the dataset. May be passed if timeseries_settings.type=forecast.\npredictions_start_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\npredictions_end_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\nrelax_known_in_advance_features_check : bool, (default False).\nIf True, missing values in the known in advance features are allowed\nin the forecast window at the prediction time. If omitted or False,\nmissing values are not allowed.\nnum_concurrent (Optional[int]) – Number of concurrent chunks to score simultaneously. Defaults to\nthe available number of cores of the deployment. Lower it to leave\nresources for real-time scoring.\nchunk_size (str or Optional[int]) – Which strategy should be used to determine the chunk size.\nCan be either a named strategy or a fixed size in bytes.\n- auto: use fixed or dynamic based on flipper\n- fixed: use 1MB for explanations, 5MB for regular requests\n- dynamic: use dynamic chunk sizes\n- int: use this many bytes per chunk\npassthrough_columns (list[string] (optional)) – Keep these columns from the scoring dataset in the scored dataset.\nThis is useful for correlating predictions with source data.\npassthrough_columns_set (Optional[str]) – To pass through every column from the scoring dataset, set this to\nall. Takes precedence over passthrough_columns if set.\nmax_explanations (Optional[int]) – Compute prediction explanations for this amount of features.\nmax_ngram_explanations (int or str (optional)) – Compute text explanations for this amount of ngrams. Set to all to return all ngram\nexplanations, or set to a positive integer value to limit the amount of ngram\nexplanations returned. By default no ngram explanations will be computed and returned.\nthreshold_high (Optional[float]) – Only compute prediction explanations for predictions above this\nthreshold. Can be combined with threshold_low.\nthreshold_low (Optional[float]) – Only compute prediction explanations for predictions below this\nthreshold. Can be combined with threshold_high.\nexplanations_mode (PredictionExplanationsMode, optional) – Mode of prediction explanations calculation for multiclass and clustering models, if not\nspecified - server default is to explain only the predicted class, identical to passing\nTopPredictionsMode(1).\nprediction_warning_enabled (Optional[bool]) – Add prediction warnings to the scored data. Currently only\nsupported for regression models.\ninclude_prediction_status (Optional[bool]) – Include the prediction_status column in the output, defaults to False.\nskip_drift_tracking (Optional[bool]) – Skips drift tracking on any predictions made from this job. This is useful when running\nnon-production workloads to not affect drift tracking and cause unnecessary alerts.\nDefaults to False.\nprediction_instance (Optional[PredictionInstance]) – Defaults to instance specified by deployment or system configuration.\nSupported options:\nhostName : str\nsslEnabled : boolean (optional, default true). Set to false to\nrun prediction requests from the batch prediction job without SSL.\ndatarobotKey : Optional[str], if running a job against a prediction\ninstance in the Managed AI Cloud, you must provide the organization level\nDataRobot-Key\napiKey : Optional[str], by default, prediction requests will use the\nAPI key of the user that created the job. This allows you to make requests\non behalf of other users.\nabort_on_error (Optional[bool]) – Default behavior is to abort the job if too many rows fail scoring. This will free\nup resources for other jobs that may score successfully. Set to false to\nunconditionally score every row no matter how many errors are encountered.\nDefaults to True.\ncolumn_names_remapping (Optional[Dict[str, str]]) – Mapping with column renaming for output table. Defaults to {}.\ninclude_probabilities (Optional[bool]) – Flag that enables returning of all probability columns. Defaults to True.\ninclude_probabilities_classes (list (optional)) – List the subset of classes if a user doesn’t want all the classes. Defaults to [].\ndownload_timeout (Optional[int]) –\nAdded in version 2.22.\nIf using localFile output, wait this many seconds for the download to become\navailable. See download().\ndownload_read_timeout (Optional[int], default 660) –\nAdded in version 2.22.\nIf using localFile output, wait this many seconds for the server to respond\nbetween chunks.\nupload_read_timeout (Optional[int], default 600) –\nAdded in version 2.28.\nIf using localFile intake, wait this many seconds for the server to respond\nafter whole dataset upload.\nprediction_threshold (Optional[float]) –\nAdded in version 3.4.0.\nThreshold is the point that sets the class boundary for a predicted value. The model\nclassifies an observation below the threshold as FALSE, and an observation above the\nthreshold as TRUE. In other words, DataRobot automatically assigns the positive class\nlabel to any prediction exceeding the threshold.\nThis value can be set between 0.0 and 1.0.\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nclassmethod apply_time_series_data_prep_and_score(deployment, intake_settings, timeseries_settings, **kwargs)\nPrepare the dataset with time series data prep, create new batch prediction job,\nupload the scoring dataset, and return a batch prediction job.\nThe supported intake_settings are of type localFile or dataset.\nFor timeseries_settings of type forecast the forecast_point must be specified.\nRefer to the datarobot.models.BatchPredictionJob.score() method for details on the other\nkwargs parameters.\nAdded in version v3.1.\nVariables:\ndeployment (Deployment) – Deployment which will be used for scoring.\nintake_settings (dict) – A dict configuring where data is coming from. Supported options:\ntype : str, either localFile, dataset\nNote that to pass a dataset, you not only need to specify the type parameter\nas dataset, but you must also set the dataset parameter as a\nDataset object.\nTo score from a local file, add this parameter to the\nsettings:\nfile : file-like object, string path to file or a\npandas.DataFrame of scoring data.\ntimeseries_settings (dict) – Configuration for time-series scoring. Supported options:\ntype : str, must be forecast or historical (default if\nnot passed is forecast). forecast mode makes predictions using\nforecast_point. historical enables bulk prediction mode which\ncalculates predictions for all possible forecast points and forecast\ndistances in the dataset within predictions_start_date/predictions_end_date\nrange.\nforecast_point : Optional[datetime.datetime], forecast point for the dataset,\nused for the forecast predictions. Must be passed if\ntimeseries_settings.type=forecast.\npredictions_start_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\npredictions_end_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\nrelax_known_in_advance_features_check : bool, (default False).\nIf True, missing values in the known in advance features are allowed\nin the forecast window at the prediction time. If omitted or False,\nmissing values are not allowed.\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nRaises:\nInvalidUsageError – If the deployment does not support time series data prep.\nIf the intake type is not supported for time series data prep.\nclassmethod score_to_file(deployment, intake_path, output_path, **kwargs)\nCreate new batch prediction job, upload the scoring dataset and\ndownload the scored CSV file concurrently.\nWill block until the entire file is scored.\nRefer to the datarobot.models.BatchPredictionJob.score() method for details on the other\nkwargs parameters.\nVariables:\ndeployment (Deployment or string ID) – Deployment which will be used for scoring.\nintake_path (file-like object/string path to file/pandas.DataFrame) – Scoring data\noutput_path (str) – Filename to save the result under\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nclassmethod apply_time_series_data_prep_and_score_to_file(deployment, intake_path, output_path, timeseries_settings, **kwargs)\nPrepare the input dataset with time series data prep. Then, create a new batch prediction\njob using the prepared AI catalog item as input and concurrently download the scored CSV\nfile.\nThe function call will return when the entire file is scored.\nFor timeseries_settings of type forecast the forecast_point must be specified.\nRefer to the datarobot.models.BatchPredictionJob.score() method for details on the other\nkwargs parameters.\nAdded in version v3.1.\nVariables:\ndeployment (Deployment) – The deployment which will be used for scoring.\nintake_path (file-like object/string path to file/pandas.DataFrame) – The scoring data.\noutput_path (str) – The filename under which you save the result.\ntimeseries_settings (dict) – Configuration for time-series scoring. Supported options:\ntype : str, must be forecast or historical (default if\nnot passed is forecast). forecast mode makes predictions using\nforecast_point. historical enables bulk prediction mode which\ncalculates predictions for all possible forecast points and forecast\ndistances in the dataset within predictions_start_date/predictions_end_date\nrange.\nforecast_point : Optional[datetime.datetime], forecast point for the dataset,\nused for the forecast predictions. Must be passed if\ntimeseries_settings.type=forecast.\npredictions_start_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\npredictions_end_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\nrelax_known_in_advance_features_check : bool, (default False).\nIf True, missing values in the known in advance features are allowed\nin the forecast window at the prediction time. If omitted or False,\nmissing values are not allowed.\nReturns:\nInstance of BatchPredictionJob.\nReturn type:\nBatchPredictionJob\nRaises:\nInvalidUsageError – If the deployment does not support time series data prep.\nclassmethod score_s3(deployment, source_url, destination_url, credential=None, endpoint_url=None, **kwargs)\nCreate new batch prediction job, with a scoring dataset from S3\nand writing the result back to S3.\nThis returns immediately after the job has been created. You\nmust poll for job completion using get_status() or\nwait_for_completion() (see datarobot.models.Job)\nRefer to the datarobot.models.BatchPredictionJob.score() method for details on the other\nkwargs parameters.\nVariables:\ndeployment (Deployment or string ID) – Deployment which will be used for scoring.\nsource_url (str) – The URL for the prediction dataset (e.g.: s3://bucket/key)\ndestination_url (str) – The URL for the scored dataset (e.g.: s3://bucket/key)\ncredential (str or Credential (optional)) – The AWS Credential object or credential id\nendpoint_url (Optional[str]) – Any non-default endpoint URL for S3 access (omit to use the default)\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nclassmethod score_azure(deployment, source_url, destination_url, credential=None, **kwargs)\nCreate new batch prediction job, with a scoring dataset from Azure blob\nstorage and writing the result back to Azure blob storage.\nThis returns immediately after the job has been created. You\nmust poll for job completion using get_status() or\nwait_for_completion() (see datarobot.models.Job).\nRefer to the datarobot.models.BatchPredictionJob.score() method for details on the other\nkwargs parameters.\nVariables:\ndeployment (Deployment or string ID) – Deployment which will be used for scoring.\nsource_url (str) – The URL for the prediction dataset\n(e.g.: https://storage_account.blob.endpoint/container/blob_name)\ndestination_url (str) – The URL for the scored dataset\n(e.g.: https://storage_account.blob.endpoint/container/blob_name)\ncredential (str or Credential (optional)) – The Azure Credential object or credential id\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nclassmethod score_gcp(deployment, source_url, destination_url, credential=None, **kwargs)\nCreate new batch prediction job, with a scoring dataset from Google Cloud Storage\nand writing the result back to one.\nThis returns immediately after the job has been created. You\nmust poll for job completion using get_status() or\nwait_for_completion() (see datarobot.models.Job).\nRefer to the datarobot.models.BatchPredictionJob.score() method for details on the other\nkwargs parameters.\nVariables:\ndeployment (Deployment or string ID) – Deployment which will be used for scoring.\nsource_url (str) – The URL for the prediction dataset\n(e.g.: http(s)://storage.googleapis.com/[bucket]/[object])\ndestination_url (str) – The URL for the scored dataset\n(e.g.: http(s)://storage.googleapis.com/[bucket]/[object])\ncredential (str or Credential (optional)) – The GCP Credential object or credential id\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nclassmethod score_from_existing(batch_prediction_job_id)\nCreate a new batch prediction job based on the settings from a previously created one\nVariables:\nbatch_prediction_job_id (str) – ID of the previous batch prediction job\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nclassmethod score_pandas(deployment, df, read_timeout=660, **kwargs)\nRun a batch prediction job, with a scoring dataset from a\npandas dataframe. The output from the prediction will be joined\nto the passed DataFrame and returned.\nUse columnNamesRemapping to drop or rename columns in the\noutput\nThis method blocks until the job has completed or raises an\nexception on errors.\nRefer to the datarobot.models.BatchPredictionJob.score() method for details on the other\nkwargs parameters.\nVariables:\ndeployment (Deployment or string ID) – Deployment which will be used for scoring.\ndf (pandas.DataFrame) – The dataframe to score\nReturn type:\nTuple[BatchPredictionJob, DataFrame]\nReturns:\nBatchPredictionJob – Instance of BatchPredictonJob\npandas.DataFrame – The original dataframe merged with the predictions\nclassmethod score_with_leaderboard_model(model, intake_settings=None, output_settings=None, csv_settings=None, timeseries_settings=None, passthrough_columns=None, passthrough_columns_set=None, max_explanations=None, max_ngram_explanations=None, explanation_algorithm=None, threshold_high=None, threshold_low=None, prediction_threshold=None, prediction_warning_enabled=None, include_prediction_status=False, abort_on_error=True, column_names_remapping=None, include_probabilities=True, include_probabilities_classes=None, download_timeout=120, download_read_timeout=660, upload_read_timeout=600, explanations_mode=None)\nCreates a new batch prediction job for a Leaderboard model by\nuploading the scoring dataset. Returns a batch prediction job.\nThe default intake and output options are both localFile, which\nrequires the caller to pass the file parameter and either\ndownload the results using the download() method afterwards or\npass a path to a file where the scored data will be downloaded to.\nVariables:\nmodel (Model or DatetimeModel or string ID) – Model which will be used for scoring.\nintake_settings (Optional[IntakeSettings]) – A dict configuring how data is coming from. Supported options:\ntype : str, either localFile, dataset, or dss.\nNote that to pass a dataset, you not only need to specify the type parameter\nas dataset, but you must also set the dataset parameter as a\ndr.Dataset object.\nTo score from a local file, add the this parameter to the\nsettings:\nfile : file-like object, string path to file or a\npandas.DataFrame of scoring data.\nTo score subset of training data, use dss intake type and specify\nfollowing parameters:\nproject_id : project to fetch training data from. Access to project is required.\npartition : subset of training data to score,\none of datarobot.enums.TrainingDataSubsets.\noutput_settings (Optional[OutputSettings]) – A dict configuring how scored data is to be saved. Supported\noptions:\ntype : str, localFile\nTo save scored data to a local file, add this parameters to the\nsettings:\npath : Optional[str] The path to save the scored data\nas a CSV file. If a path is not specified, you must download\nthe scored data yourself with job.download().\nIf a path is specified, the call is blocked until the\njob is done. If there are no other jobs currently\nprocessing for the targeted prediction instance,\nuploading, scoring, and downloading will happen in parallel\nwithout waiting for a full job to complete. Otherwise,\nit will still block, but start downloading the scored\ndata as soon as it starts generating data. This is the\nfastest method to get predictions.\ncsv_settings (Optional[CsvSettings]) – CSV intake and output settings. Supported options:\ndelimiter : str (optional, default ,), fields are delimited by\nthis character. Use the string tab to denote TSV (TAB separated values).\nMust be either a one-character string or the string tab.\nquotechar : str (optional, default “), fields containing the\ndelimiter must be quoted using this character.\nencoding : str (optional, default utf-8), encoding for the CSV\nfiles. For example (but not limited to): shift_jis, latin_1 or\nmskanji.\ntimeseries_settings (Optional[TimeSeriesSettings]) – Configuration for time-series scoring. Supported options:\ntype : str, must be forecast, historical (default if\nnot passed is forecast), or training. forecast mode makes predictions using\nforecast_point or rows in the dataset without target. historical\nenables bulk prediction mode which calculates predictions for all\npossible forecast points and forecast distances in the dataset within\npredictions_start_date/predictions_end_date range. training mode is\na special case for predictions on subsets of training data. Note, that it must\nbe used in conjunction with dss intake type only.\nforecast_point : Optional[datetime.datetime], forecast point for the dataset,\nused for the forecast predictions, by default value will be inferred\nfrom the dataset. May be passed if timeseries_settings.type=forecast.\npredictions_start_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\npredictions_end_date : Optional[datetime.datetime], used for historical\npredictions in order to override date from which predictions should be\ncalculated. By default value will be inferred automatically from the\ndataset. May be passed if timeseries_settings.type=historical.\nrelax_known_in_advance_features_check : bool, (default False).\nIf True, missing values in the known in advance features are allowed\nin the forecast window at the prediction time. If omitted or False,\nmissing values are not allowed.\npassthrough_columns (list[string] (optional)) – Keep these columns from the scoring dataset in the scored dataset.\nThis is useful for correlating predictions with source data.\npassthrough_columns_set (Optional[str]) – To pass through every column from the scoring dataset, set this to\nall. Takes precedence over passthrough_columns if set.\nmax_explanations (Optional[int]) – Compute prediction explanations for this amount of features.\nmax_ngram_explanations (int or str (optional)) – Compute text explanations for this amount of ngrams. Set to all to return all ngram\nexplanations, or set to a positive integer value to limit the amount of ngram\nexplanations returned. By default no ngram explanations will be computed and returned.\nthreshold_high (Optional[float]) – Only compute prediction explanations for predictions above this\nthreshold. Can be combined with threshold_low.\nthreshold_low (Optional[float]) – Only compute prediction explanations for predictions below this\nthreshold. Can be combined with threshold_high.\nexplanations_mode (PredictionExplanationsMode, optional) – Mode of prediction explanations calculation for multiclass and clustering models, if not\nspecified - server default is to explain only the predicted class, identical to passing\nTopPredictionsMode(1).\nprediction_warning_enabled (Optional[bool]) – Add prediction warnings to the scored data. Currently only\nsupported for regression models.\ninclude_prediction_status (Optional[bool]) – Include the prediction_status column in the output, defaults to False.\nabort_on_error (Optional[bool]) – Default behavior is to abort the job if too many rows fail scoring. This will free\nup resources for other jobs that may score successfully. Set to false to\nunconditionally score every row no matter how many errors are encountered.\nDefaults to True.\ncolumn_names_remapping (Optional[Dict]) – Mapping with column renaming for output table. Defaults to {}.\ninclude_probabilities (Optional[bool]) – Flag that enables returning of all probability columns. Defaults to True.\ninclude_probabilities_classes (list (optional)) – List the subset of classes if you do not want all the classes. Defaults to [].\ndownload_timeout (Optional[int]) –\nAdded in version 2.22.\nIf using localFile output, wait this many seconds for the download to become\navailable. See download().\ndownload_read_timeout (int (optional, default 660)) –\nAdded in version 2.22.\nIf using localFile output, wait this many seconds for the server to respond\nbetween chunks.\nupload_read_timeout (int (optional, default 600)) –\nAdded in version 2.28.\nIf using localFile intake, wait this many seconds for the server to respond\nafter whole dataset upload.\nprediction_threshold (Optional[float]) –\nAdded in version 3.4.0.\nThreshold is the point that sets the class boundary for a predicted value. The model\nclassifies an observation below the threshold as FALSE, and an observation above the\nthreshold as TRUE. In other words, DataRobot automatically assigns the positive class\nlabel to any prediction exceeding the threshold.\nThis value can be set between 0.0 and 1.0.\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nclassmethod get(batch_prediction_job_id)\nGet batch prediction job\nVariables:\nbatch_prediction_job_id (str) – ID of batch prediction job\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\ndownload(fileobj, timeout=120, read_timeout=660)\nDownloads the CSV result of a prediction job\nVariables:\nfileobj (A file-like object where the CSV prediction results will be) – written to. Examples include an in-memory buffer\n(e.g., io.BytesIO) or a file on disk (opened for binary writing).\ntimeout (int (optional, default 120)) –\nAdded in version 2.22.\nSeconds to wait for the download to become available.\nThe download will not be available before the job has started processing.\nIn case other jobs are occupying the queue, processing may not start\nimmediately.\nIf the timeout is reached, the job will be aborted and RuntimeError\nis raised.\nSet to -1 to wait infinitely.\nread_timeout (int (optional, default 660)) –\nAdded in version 2.22.\nSeconds to wait for the server to respond between chunks.\nReturn type:\nNone\ndelete(ignore_404_errors=False)\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nReturn type:\nNone\nget_status()\nGet status of batch prediction job\nReturns:\nDict with job status\nReturn type:\nBatchPredictionJob status data\nclassmethod list_by_status(statuses=None)\nGet jobs collection for specific set of statuses\nVariables:\nstatuses – List of statuses to filter jobs ([ABORTED|COMPLETED…])\nif statuses is not provided, returns all jobs for user\nReturns:\nList of job statuses dicts with specific statuses\nReturn type:\nBatchPredictionJob statuses\nclass datarobot.models.BatchPredictionJobDefinition\nclassmethod get(batch_prediction_job_definition_id)\nGet batch prediction job definition\nVariables:\nbatch_prediction_job_definition_id (str) – ID of batch prediction job definition\nReturns:\nInstance of BatchPredictionJobDefinition\nReturn type:\nBatchPredictionJobDefinition\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchPredictionJobDefinition.get('5a8ac9ab07a57a0001be501f')\n>>> definition\nBatchPredictionJobDefinition(60912e09fd1f04e832a575c1)\nclassmethod list(search_name=None, deployment_id=None, limit=<datarobot.models.batch_prediction_job.MissingType object>, offset=0)\nGet job all definitions\nParameters:\nsearch_name (Optional[str]) – String for filtering job definitions\nJob definitions that contain the string in name will be returned.\nIf not specified, all available job definitions will be returned.\ndeployment_id (str) – The ID of the deployment record belongs to.\nlimit (Optional[int]) – 0 by default. At most this many results are returned.\noffset (Optional[int]) – This many results will be skipped.\nReturns:\nList of job definitions the user has access to see\nReturn type:\nList[BatchPredictionJobDefinition]\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchPredictionJobDefinition.list()\n>>> definition\n[\nBatchPredictionJobDefinition(60912e09fd1f04e832a575c1),\nBatchPredictionJobDefinition(6086ba053f3ef731e81af3ca)\n]\nclassmethod create(enabled, batch_prediction_job, name=None, schedule=None)\nCreates a new batch prediction job definition to be run either at scheduled interval or as\na manual run.\nVariables:\nenabled (bool (default False)) – Whether or not the definition should be active on a scheduled basis. If True,\nschedule is required.\nbatch_prediction_job (dict) – The job specifications for your batch prediction job.\nIt requires the same job input parameters as used with\nscore(), only it will not initialize a job scoring,\nonly store it as a definition for later use.\nname (Optional[str]) – The name you want your job to be identified with. Must be unique across the\norganization’s existing jobs.\nIf you don’t supply a name, a random one will be generated for you.\nschedule (Optional[Dict]) – The schedule payload defines at what intervals the job should run, which can be\ncombined in various ways to construct complex scheduling terms if needed. In all of\nthe elements in the objects, you can supply either an asterisk [\"*\"] denoting\n“every” time denomination or an array of integers (e.g. [1, 2, 3]) to define\na specific interval.\nThe schedule payload is split up in the following items:\nMinute:\nThe minute(s) of the day that the job will run. Allowed values are either [\"*\"]\nmeaning every minute of the day or [0 ... 59]\nHour:\nThe hour(s) of the day that the job will run. Allowed values are either [\"*\"]\nmeaning every hour of the day or [0 ... 23].\nDay of Month:\nThe date(s) of the month that the job will run. Allowed values are either\n[1 ... 31] or [\"*\"] for all days of the month. This field is additive with\ndayOfWeek, meaning the job will run both on the date(s) defined in this field\nand the day specified by dayOfWeek (for example, dates 1st, 2nd, 3rd, plus every\nTuesday). If dayOfMonth is set to [\"*\"] and dayOfWeek is defined,\nthe scheduler will trigger on every day of the month that matches dayOfWeek\n(for example, Tuesday the 2nd, 9th, 16th, 23rd, 30th).\nInvalid dates such as February 31st are ignored.\nMonth:\nThe month(s) of the year that the job will run. Allowed values are either\n[1 ... 12] or [\"*\"] for all months of the year. Strings, either\n3-letter abbreviations or the full name of the month, can be used\ninterchangeably (e.g., “jan” or “october”).\nMonths that are not compatible with dayOfMonth are ignored, for example\n{\"dayOfMonth\": [31], \"month\":[\"feb\"]}\nDay of Week:\nThe day(s) of the week that the job will run. Allowed values are [0 .. 6],\nwhere (Sunday=0), or [\"*\"], for all days of the week. Strings, either 3-letter\nabbreviations or the full name of the day, can be used interchangeably\n(e.g., “sunday”, “Sunday”, “sun”, or “Sun”, all map to [0].\nThis field is additive with dayOfMonth, meaning the job will run both on the\ndate specified by dayOfMonth and the day defined in this field.\nReturns:\nInstance of BatchPredictionJobDefinition\nReturn type:\nBatchPredictionJobDefinition\nExamples\n>>> import datarobot as dr\n>>> job_spec = {\n...    \"num_concurrent\": 4,\n...    \"deployment_id\": \"foobar\",\n...    \"intake_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...    \"output_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...}\n>>> schedule = {\n...    \"day_of_week\": [\n...        1\n...    ],\n...    \"month\": [\n...        \"*\"\n...    ],\n...    \"hour\": [\n...        16\n...    ],\n...    \"minute\": [\n...        0\n...    ],\n...    \"day_of_month\": [\n...        1\n...    ]\n...}\n>>> definition = BatchPredictionJobDefinition.create(\n...    enabled=False,\n...    batch_prediction_job=job_spec,\n...    name=\"some_definition_name\",\n...    schedule=schedule\n... )\n>>> definition\nBatchPredictionJobDefinition(60912e09fd1f04e832a575c1)\nupdate(enabled, batch_prediction_job=None, name=None, schedule=None)\nUpdates a job definition with the changed specs.\nTakes the same input as create()\nVariables:\nenabled (bool (default False)) – Same as enabled in create().\nbatch_prediction_job (dict) – Same as batch_prediction_job in create().\nname (Optional[str]) – Same as name in create().\nschedule (dict) – Same as schedule in create().\nReturns:\nInstance of the updated BatchPredictionJobDefinition\nReturn type:\nBatchPredictionJobDefinition\nExamples\n>>> import datarobot as dr\n>>> job_spec = {\n...    \"num_concurrent\": 5,\n...    \"deployment_id\": \"foobar_new\",\n...    \"intake_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...    \"output_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\"\n...    },\n...}\n>>> schedule = {\n...    \"day_of_week\": [\n...        1\n...    ],\n...    \"month\": [\n...        \"*\"\n...    ],\n...    \"hour\": [\n...        \"*\"\n...    ],\n...    \"minute\": [\n...        30, 59\n...    ],\n...    \"day_of_month\": [\n...        1, 2, 6\n...    ]\n...}\n>>> definition = BatchPredictionJobDefinition.create(\n...    enabled=False,\n...    batch_prediction_job=job_spec,\n...    name=\"updated_definition_name\",\n...    schedule=schedule\n... )\n>>> definition\nBatchPredictionJobDefinition(60912e09fd1f04e832a575c1)\nrun_on_schedule(schedule)\nSets the run schedule of an already created job definition.\nIf the job was previously not enabled, this will also set the job to enabled.\nVariables:\nschedule (dict) – Same as schedule in create().\nReturns:\nInstance of the updated BatchPredictionJobDefinition with the new / updated schedule.\nReturn type:\nBatchPredictionJobDefinition\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchPredictionJobDefinition.create('...')\n>>> schedule = {\n...    \"day_of_week\": [\n...        1\n...    ],\n...    \"month\": [\n...        \"*\"\n...    ],\n...    \"hour\": [\n...        \"*\"\n...    ],\n...    \"minute\": [\n...        30, 59\n...    ],\n...    \"day_of_month\": [\n...        1, 2, 6\n...    ]\n...}\n>>> definition.run_on_schedule(schedule)\nBatchPredictionJobDefinition(60912e09fd1f04e832a575c1)\nrun_once()\nManually submits a batch prediction job to the queue, based off of an already\ncreated job definition.\nReturns:\nInstance of BatchPredictionJob\nReturn type:\nBatchPredictionJob\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchPredictionJobDefinition.create('...')\n>>> job = definition.run_once()\n>>> job.wait_for_completion()\ndelete()\nDeletes the job definition and disables any future schedules of this job if any.\nIf a scheduled job is currently running, this will not be cancelled.\n:rtype: None\nExamples\n>>> import datarobot as dr\n>>> definition = dr.BatchPredictionJobDefinition.get('5a8ac9ab07a57a0001be501f')\n>>> definition.delete()\nBatch job\nclass datarobot.models.batch_job.IntakeSettings\nIntake settings typed dict\nclass datarobot.models.batch_job.OutputSettings\nOutput settings typed dict\nPredict job\ndatarobot.models.predict_job.wait_for_async_predictions(project_id, predict_job_id, max_wait=600)\nGiven a Project id and PredictJob id poll for status of process\nresponsible for predictions generation until it’s finished\nParameters:\nproject_id (str) – The identifier of the project\npredict_job_id (str) – The identifier of the PredictJob\nmax_wait (Optional[int]) – Time in seconds after which predictions creation is considered\nunsuccessful\nReturns:\npredictions – Generated predictions.\nReturn type:\npandas.DataFrame\nRaises:\nAsyncPredictionsGenerationError – Raised if status of fetched PredictJob object is error\nAsyncTimeoutError – Predictions weren’t generated in time, specified by max_wait\nparameter\nclass datarobot.models.PredictJob\nTracks asynchronous work being done within a project\nVariables:\nid (int) – the id of the job\nproject_id (str) – the id of the project the job belongs to\nstatus (str) – the status of the job - will be one of datarobot.enums.QUEUE_STATUS\njob_type (str) – what kind of work the job is doing - will be ‘predict’ for predict jobs\nis_blocked (bool) – if true, the job is blocked (cannot be executed) until its dependencies are resolved\nmessage (str) – a message about the state of the job, typically explaining why an error occurred\nclassmethod from_job(job)\nTransforms a generic Job into a PredictJob\nParameters:\njob (Job) – A generic job representing a PredictJob\nReturns:\npredict_job – A fully populated PredictJob with all the details of the job\nReturn type:\nPredictJob\nRaises:\nValueError: – If the generic Job was not a predict job, e.g. job_type != JOB_TYPE.PREDICT\nclassmethod get(project_id, predict_job_id)\nFetches one PredictJob. If the job finished, raises PendingJobFinished\nexception.\nParameters:\nproject_id (str) – The identifier of the project the model on which prediction\nwas started belongs to\npredict_job_id (str) – The identifier of the predict_job\nReturns:\npredict_job – The pending PredictJob\nReturn type:\nPredictJob\nRaises:\nPendingJobFinished – If the job being queried already finished, and the server is\nre-routing to the finished predictions.\nAsyncFailureError – Querying this resource gave a status code other than 200 or 303\nclassmethod get_predictions(project_id, predict_job_id, class_prefix='class_')\nFetches finished predictions from the job used to generate them.\nNotes\nThe prediction API for classifications now returns an additional prediction_values\ndictionary that is converted into a series of class_prefixed columns in the final\ndataframe. For example, <label> = 1.0 is converted to ‘class_1.0’. If you are on an\nolder version of the client (prior to v2.8), you must update to v2.8 to correctly pivot\nthis data.\nParameters:\nproject_id (str) – The identifier of the project to which belongs the model used\nfor predictions generation\npredict_job_id (str) – The identifier of the predict_job\nclass_prefix (str) – The prefix to append to labels in the final dataframe (e.g., apple -> class_apple)\nReturns:\npredictions – Generated predictions\nReturn type:\npandas.DataFrame\nRaises:\nJobNotFinished – If the job has not finished yet\nAsyncFailureError – Querying the predict_job in question gave a status code other than 200 or 303\ncancel()\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nget_result(params=None)\nParameters:\nparams (dict or None) – Query parameters to be added to request to get results.\nNotes\nFor featureEffects, source param is required to define source,\notherwise the default is training.\nReturns:\nresult –\nReturn type depends on the job type\nfor model jobs, a Model is returned\nfor predict jobs, a pandas.DataFrame (with predictions) is returned\nfor featureImpact jobs, a list of dicts by default (see with_metadata\nparameter of the FeatureImpactJob class and its get() method).\nfor primeRulesets jobs, a list of Rulesets\nfor primeModel jobs, a PrimeModel\nfor primeDownloadValidation jobs, a PrimeFile\nfor predictionExplanationInitialization jobs, a PredictionExplanationsInitialization\nfor predictionExplanations jobs, a PredictionExplanations\nfor featureEffects, a FeatureEffects.\nReturn type:\nobject\nRaises:\nJobNotFinished – If the job is not finished, the result is not available.\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nget_result_when_complete(max_wait=600, params=None)\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nparams (dict, optional) – Query parameters to be added to request.\nReturns:\nresult – Return type is the same as would be returned by Job.get_result.\nReturn type:\nobject\nRaises:\nAsyncTimeoutError – If the job does not finish in time\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nrefresh()\nUpdate this object with the latest job data from the server.\nwait_for_completion(max_wait=600)\nWaits for job to complete.\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nReturn type:\nNone\nPrediction dataset\nclass datarobot.models.PredictionDataset\nA dataset uploaded to make predictions\nTypically created via project.upload_dataset\nVariables:\nid (str) – the id of the dataset\nproject_id (str) – the id of the project the dataset belongs to\ncreated (str) – the time the dataset was created\nname (str) – the name of the dataset\nnum_rows (int) – the number of rows in the dataset\nnum_columns (int) – the number of columns in the dataset\nforecast_point (datetime.datetime or None) – For time series projects only. This is the default point relative to which predictions will\nbe generated, based on the forecast window of the project.  See the time series\npredictions documentation for more information.\npredictions_start_date (datetime.datetime or None, optional) – For time series projects only. The start date for bulk predictions. Note that this\nparameter is for generating historical predictions using the training data. This parameter\nshould be provided in conjunction with predictions_end_date. Can’t be provided with the\nforecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – For time series projects only. The end date for bulk predictions, exclusive. Note that this\nparameter is for generating historical predictions using the training data. This parameter\nshould be provided in conjunction with predictions_start_date. Can’t be provided with\nthe forecast_point parameter.\nrelax_known_in_advance_features_check (Optional[bool]) – (New in version v2.15) For time series projects only. If True, missing values in the\nknown in advance features are allowed in the forecast window at the prediction time.\nIf omitted or False, missing values are not allowed.\ndata_quality_warnings (dict, optional) – (New in version v2.15) A dictionary that contains available warnings about potential\nproblems in this prediction dataset. Available warnings include:\nhas_kia_missing_values_in_forecast_window (bool)Applicable for time series projects. If True, known in advance features\nhave missing values in forecast window which may decrease prediction accuracy.\ninsufficient_rows_for_evaluating_models  (bool)Applicable for datasets which are used as external test sets. If True, there is not\nenough rows in dataset to calculate insights.\nsingle_class_actual_value_column (bool)Applicable for datasets which are used as external test sets. If True, actual value\ncolumn has only one class and such insights as ROC curve can not be calculated.\nOnly applies for binary classification projects or unsupervised projects.\nforecast_point_range (list[datetime.datetime] or None, optional) – (New in version v2.20) For time series projects only. Specifies the range of dates available\nfor use as a forecast point.\ndata_start_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The minimum primary date of this\nprediction dataset.\ndata_end_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The maximum primary date of this\nprediction dataset.\nmax_forecast_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The maximum forecast date of this\nprediction dataset.\nactual_value_column (string, optional) – (New in version v2.21) Optional, only available for unsupervised projects,\nin case dataset was uploaded with actual value column specified. Name of the\ncolumn which will be used to calculate the classification metrics and insights.\ndetected_actual_value_columns (list of dict, optional) – (New in version v2.21) For unsupervised projects only, list of detected actual value\ncolumns information containing missing count and name for each column.\ncontains_target_values (Optional[bool]) – (New in version v2.21)  Only for supervised projects. If True, dataset contains target\nvalues and can be used to calculate the classification metrics and insights.\nsecondary_datasets_config_id (string or None, optional) – (New in version v2.23) The Id of the alternative secondary dataset config\nto use during prediction for Feature discovery project.\nclassmethod get(project_id, dataset_id)\nRetrieve information about a dataset uploaded for predictions\nParameters:\nproject_id (str) – the id of the project to query\ndataset_id (str) – the id of the dataset to retrieve\nReturns:\ndataset – A dataset uploaded to make predictions\nReturn type:\nPredictionDataset\ndelete()\nDelete a dataset uploaded for predictions\nWill also delete predictions made using this dataset and cancel any predict jobs using\nthis dataset.\nReturn type:\nNone",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/batch-predictions.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/batch-predictions.html",
        "content_length": 49457
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.predictiondataset",
        "datarobot.enums.available_statement_types",
        "datarobot.models.batch_prediction_job",
        "dr.batchpredictionjobdefinition.list",
        "dr.batchpredictionjobdefinition.create",
        "datarobot.models.batch_job",
        "datarobot.enums.trainingdatasubsets",
        "project.upload_dataset",
        "datarobot.models.batchpredictionjobdefinition",
        "datarobot.models.predict_job",
        "datarobot.models.predictjob",
        "datarobot.enums.queue_status",
        "dr.batchpredictionjobdefinition.get",
        "datarobot.models.batchpredictionjob",
        "datarobot.models.job"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-9148082547541740688",
      "title": "Challenger",
      "content": "Challenger\nclass datarobot.models.deployment.challenger.Challenger\nA challenger is an alternative model being compared to the model currently deployed\nVariables:\nid (str) – The ID of the challenger.\ndeployment_id (str) – The ID of the deployment.\nname (str) – The name of the challenger.\nmodel (dict) – The model of the challenger.\nmodel_package (dict) – The model package of the challenger.\nprediction_environment (dict) – The prediction environment of the challenger.\nclassmethod create(deployment_id, model_package_id, prediction_environment_id, name, max_wait=600)\nCreate a challenger for a deployment\nParameters:\ndeployment_id (str) – The ID of the deployment\nmodel_package_id (str) – The model package id of the challenger model\nprediction_environment_id (str) – The prediction environment id of the challenger model\nname (str) – The name of the challenger model\nmax_wait (Optional[int]) – The amount of seconds to wait for successful resolution of a challenger creation job.\nReturn type:\nChallenger\nExamples\nfrom datarobot import Challenger\nchallenger = Challenger.create(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nname=\"Elastic-Net Classifier\",\nmodel_package_id=\"5c0a969859b00004ba52e41b\",\nprediction_environment_id=\"60b012436635fc00909df555\"\n)\nclassmethod get(deployment_id, challenger_id)\nGet a challenger for a deployment\nParameters:\ndeployment_id (str) – The ID of the deployment\nchallenger_id (str) – The ID of the challenger\nReturns:\nThe challenger object\nReturn type:\nChallenger\nExamples\nfrom datarobot import Challenger\nchallenger = Challenger.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nchallenger_id=\"5c939e08962d741e34f609f0\"\n)\nchallenger.id\n>>>'5c939e08962d741e34f609f0'\nchallenger.model_package['name']\n>>> 'Elastic-Net Classifier'\nclassmethod list(deployment_id)\nList all challengers for a deployment\nParameters:\ndeployment_id (str) – The ID of the deployment\nReturns:\nchallengers – A list of challenger objects\nReturn type:\nlist\nExamples\nfrom datarobot import Challenger\nchallengers = Challenger.list(deployment_id=\"5c939e08962d741e34f609f0\")\nchallengers[0].id\n>>>'5c939e08962d741e34f609f0'\nchallengers[0].model_package['name']\n>>> 'Elastic-Net Classifier'\ndelete()\nDelete a challenger for a deployment\nReturn type:\nNone\nupdate(name=None, prediction_environment_id=None)\nUpdate name and prediction environment of a challenger\nParameters:\nname (Optional[str]) – The name of the challenger model\nprediction_environment_id (Optional[str]) – The prediction environment id of the challenger model\nReturn type:\nNone\nclass datarobot.models.deployment.champion_model_package.ChampionModelPackage\nRepresents a champion model package.\nParameters:\nid (str) – The ID of the registered model version.\nregistered_model_id (str) – The ID of the parent registered model.\nregistered_model_version (int) – The version of the registered model.\nname (str) – The name of the registered model version.\nmodel_id (str) – The ID of the model.\nmodel_execution_type (str) – The type of model package (version). dedicated (native DataRobot models) and\ncustom_inference_model` (user added inference models) both execute on DataRobot\nprediction servers, while external does not.\nis_archived (bool) – Whether the model package (version) is permanently archived\n(cannot be used in deployment or replacement).\nimport_meta (ImportMeta) – Information from when this model package (version) was first saved.\nsource_meta (SourceMeta) – Meta information from where the model was generated.\nmodel_kind (ModelKind) – Model attribute information.\ntarget (Target) – Target information for the registered model version.\nmodel_description (ModelDescription) – Model description information.\ndatasets (Dataset) – Dataset information for the registered model version.\ntimeseries (Timeseries) – Time series information for the registered model version.\nbias_and_fairness (BiasAndFairness) – Bias and fairness information for the registered model version.\nis_deprecated (bool) – Whether the model package (version) is deprecated\n(cannot be used in deployment or replacement).\nbuild_status (str or None) – Model package (version) build status. One of complete, inProgress, failed.\nuser_provided_id (str or None) – User provided ID for the registered model version.\nupdated_at (str or None) – The time the registered model version was last updated.\nupdated_by (UserMetadata or None) – The user who last updated the registered model version.\ntags (List[TagWithId] or None) – The tags associated with the registered model version.\nmlpkg_file_contents (str or None) – The contents of the model package file.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/challenger-models.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/challenger-models.html",
        "content_length": 4588
      },
      "code_examples": [],
      "api_methods": [
        "deployment.champion_model_package",
        "deployment.challenger",
        "datarobot.models.deployment"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-1415749445508163803",
      "title": "Client setup",
      "content": "Client setup\ndatarobot.client.Client(token=None, endpoint=None, config_path=None, connect_timeout=None, ssl_verify=None, max_retries=None, use_tcp_keepalive=None, token_type=None, default_use_case=None, enable_api_consumer_tracking=None, trace_context=None)\nConfigures the global API client for the Python SDK. The client will be configured in one of\nthe following ways, in order of priority.\nNotes\nToken and endpoint must be specified from one source only. This is a restriction\nto prevent token leakage if environment variables or config file are used.\nThe DataRobotClientConfig params will be looking up to find the configuration parameters\nin one of the following ways,\nFrom call kwargs if specified;\nFrom a YAML file at the path specified in the config_path kwarg;\nFrom a YAML file at the path specified in the environment variables DATAROBOT_CONFIG_FILE;\nFrom environment variables;\nFrom the default values in the default YAML file\nat the path $HOME/.config/datarobot/drconfig.yaml.\nThis can also have the side effect of setting a default Use Case for client API requests.\nParameters:\ntoken (Optional[str]) – API token.\nendpoint (Optional[str]) – Base URL of API.\nconfig_path (Optional[str]) – An alternate location of the config file.\nconnect_timeout (Optional[int]) – How long the client should be willing to wait before giving up on establishing\na connection with the server.\nssl_verify (bool or Optional[str]) – Whether to check SSL certificate.\nCould be set to path with certificates of trusted certification authorities. Default: True.\nmax_retries (int or urllib3.util.retry.Retry, optional) – Either an integer number of times to retry connection errors,\nor a urllib3.util.retry.Retry object to configure retries.\nuse_tcp_keepalive (Optional[bool]) – Enable TCP keepalive for the client. If None is set to True. Default: None.\ntoken_type (Optional[str]) – Authentication token type: Token, Bearer.\n“Bearer” is for DataRobot OAuth2 token, “Token” for token generated in Developer Tools.\nDefault: “Token”.\ndefault_use_case (Optional[str]) – The entity ID of the default Use Case to use with any requests made by the client.\nenable_api_consumer_tracking (Optional[bool]) – Enable and disable user metrics tracking within the datarobot module. Default: False.\ntrace_context (Optional[str]) – An ID or other string for identifying which code template or AI Accelerator was used to make\na request.\nReturn type:\nRESTClientObject\nReturns:\nThe RESTClientObject instance created.\ndatarobot.client.get_client()\nReturns the global HTTP client for the Python SDK, instantiating it\nif necessary.\nReturn type:\nRESTClientObject\ndatarobot.client.set_client(client)\nConfigure the global HTTP client for the Python SDK.\nReturns previous instance.\nReturn type:\nOptional[RESTClientObject]\ndatarobot.client.client_configuration(*args, **kwargs)\nThis context manager can be used to temporarily change the global HTTP client.\nIn multithreaded scenarios, it is highly recommended to use a fresh manager object\nper thread.\nDataRobot does not recommend nesting these contexts.\nParameters:\nargs (Parameters passed to datarobot.client.Client())\nkwargs (Keyword arguments passed to datarobot.client.Client())\nExamples\nfrom datarobot.client import client_configuration\nfrom datarobot.models import Project\nwith client_configuration(default_use_case=[]):\n# Interact with all accessible projects, not just those associated\n# with the current use case.\nProject.list()\nwith client_configuration(token=\"api-key-here\", endpoint=\"https://host-name.com\"):\n# Interact with projects on a different DataRobot instance.\nProject.list()\nfrom datarobot.client import Client, client_configuration\nfrom datarobot.models import Project\nClient()  # Interact with DataRobot using the default configuration.\nProject.list()\nwith client_configuration(config_path=\"/path/to/a/drconfig.yaml\"):\n# Interact with DataRobot using a different configuration.\nProject.list()\nclass datarobot.rest.RESTClientObject\nParameters:\nconnect_timeout (Optional[int]) – timeout for http request and connection\nheaders – headers for outgoing requests\nopen_in_browser()\nOpens the DataRobot app in a web browser, or logs the\nURL if a browser is not available.\nReturn type:\nNone",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/client-setup.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/client-setup.html",
        "content_length": 4220
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.client.set_client",
        "project.list",
        "datarobot.client.get_client",
        "datarobot.client.client_configuration",
        "datarobot.client.client",
        "datarobot.rest.restclientobject"
      ],
      "complexity_score": 0.55,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-7660086027565860956",
      "title": "Blueprints",
      "content": "Blueprints\nBlueprint\nclass datarobot.models.Blueprint\nA Blueprint which can be used to fit models\nVariables:\nid (str) – the id of the blueprint\nprocesses (List[str]) – the processes used by the blueprint\nmodel_type (str) – the model produced by the blueprint\nproject_id (str) – the project the blueprint belongs to\nblueprint_category (str) – (New in version v2.6) Describes the category of the blueprint and the kind of model it\nproduces.\nrecommended_featurelist_id (str or null) – (New in v2.18) The ID of the feature list recommended for this blueprint.\nIf this field is not present, then there is no recommended feature list.\nsupports_composable_ml (bool or None) – (New in version v2.26)\nwhether this blueprint is supported in the Composable ML.\nsupports_incremental_learning (bool or None) – (New in version v3.3)\nwhether this blueprint supports incremental learning.\nclassmethod get(project_id, blueprint_id)\nRetrieve a blueprint.\nParameters:\nproject_id (str) – The project’s id.\nblueprint_id (str) – Id of blueprint to retrieve.\nReturns:\nblueprint – The queried blueprint.\nReturn type:\nBlueprint\nget_json()\nGet the blueprint json representation used by this model.\nReturns:\nJson representation of the blueprint stages.\nReturn type:\nBlueprintJson\nget_chart()\nRetrieve a chart.\nReturns:\nThe current blueprint chart.\nReturn type:\nBlueprintChart\nget_documents()\nGet documentation for tasks used in the blueprint.\nReturns:\nAll documents available for blueprint.\nReturn type:\nlist of BlueprintTaskDocument\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nclass datarobot.models.BlueprintTaskDocument\nDocument describing a task from a blueprint.\nVariables:\ntitle (str) – Title of document.\ntask (str) – Name of the task described in document.\ndescription (str) – Task description.\nparameters (list of dict(name, type, description)) – Parameters that task can receive in human-readable format.\nlinks (list of dict(name, url)) – External links used in document\nreferences (list of dict(name, url)) – References used in document. When no link available url equals None.\nclass datarobot.models.BlueprintChart\nA Blueprint chart that can be used to understand data flow in blueprint.\nVariables:\nnodes (list of dict (id, label)) – Chart nodes, id unique in chart.\nedges (list of tuple (id1, id2)) – Directions of data flow between blueprint chart nodes.\nclassmethod get(project_id, blueprint_id)\nRetrieve a blueprint chart.\nParameters:\nproject_id (str) – The project’s id.\nblueprint_id (str) – Id of blueprint to retrieve chart.\nReturns:\nThe queried blueprint chart.\nReturn type:\nBlueprintChart\nto_graphviz()\nGet blueprint chart in graphviz DOT format.\nReturns:\nString representation of chart in graphviz DOT language.\nReturn type:\nunicode\nclass datarobot.models.ModelBlueprintChart\nA Blueprint chart that can be used to understand data flow in model.\nModel blueprint chart represents reduced repository blueprint chart with\nonly elements that used to build this particular model.\nVariables:\nnodes (list of dict (id, label)) – Chart nodes, id unique in chart.\nedges (list of tuple (id1, id2)) – Directions of data flow between blueprint chart nodes.\nclassmethod get(project_id, model_id)\nRetrieve a model blueprint chart.\nParameters:\nproject_id (str) – The project’s id.\nmodel_id (str) – Id of model to retrieve model blueprint chart.\nReturns:\nThe queried model blueprint chart.\nReturn type:\nModelBlueprintChart\nto_graphviz()\nGet blueprint chart in graphviz DOT format.\nReturns:\nString representation of chart in graphviz DOT language.\nReturn type:\nunicode\nUser blueprints\nclass datarobot.UserBlueprint\nA representation of a blueprint which may be modified by the user,\nsaved to a user’s AI Catalog, trained on projects, and shared with others.\nIt is recommended to install the python library called datarobot_bp_workshop,\navailable via pip, for the best experience when building blueprints.\nPlease refer to http://blueprint-workshop.datarobot.com for tutorials,\nexamples, and other documentation.\nParameters:\nblender (bool) – Whether the blueprint is a blender.\nblueprint_id (string) – The deterministic id of the blueprint, based on its content.\ncustom_task_version_metadata (list[list[string]], Optional) – An association of custom entity ids and task ids.\ndiagram (string) – The diagram used by the UI to display the blueprint.\nfeatures (list[string]) – A list of the names of tasks used in the blueprint.\nfeatures_text (string) – A description of the blueprint via the names of tasks used.\nhex_column_name_lookup (list[UserBlueprintsHexColumnNameLookupEntry], Optional) – A lookup between hex values and data column names used in the blueprint.\nicons (list[int]) – The icon(s) associated with the blueprint.\ninsights (string) – An indication of the insights generated by the blueprint.\nis_time_series (bool (Default=False)) – Whether the blueprint contains time-series tasks.\nmodel_type (string) – The generated or provided title of the blueprint.\nproject_id (string, Optional) – The id of the project the blueprint was originally created with, if applicable.\nreference_model (bool (Default=False)) – Whether the blueprint is a reference model.\nshap_support (bool (Default=False)) – Whether the blueprint supports shapley additive explanations.\nsupported_target_types (list[enum(``’binary’, ``'multiclass', 'multilabel', 'nonnegative',)\n'regression' – The list of supported targets of the current blueprint.\n'unsupervised' – The list of supported targets of the current blueprint.\n'unsupervisedclustering')] – The list of supported targets of the current blueprint.\nsupports_gpu (bool (Default=False)) – Whether the blueprint supports execution on the GPU.\nuser_blueprint_id (string) – The unique id associated with the user blueprint.\nuser_id (string) – The id of the user who owns the blueprint.\nblueprint (list[dict] or list[UserBlueprintTask], Optional) – The representation of a directed acyclic graph defining a pipeline of data through tasks\nand a final estimator.\nvertex_context (list[VertexContextItem], Optional) – Info about, warnings about, and errors with a specific vertex in the blueprint.\nblueprint_context (VertexContextItemMessages) – Warnings and errors which may describe or summarize warnings or errors in the blueprint’s\nvertices\nclassmethod list(limit=100, offset=0, project_id=None)\nFetch a list of the user blueprints the current user created\nParameters:\nlimit (int (Default=100)) – The max number of results to return.\noffset (int (Default=0)) – The number of results to skip (for pagination).\nproject_id (string, Optional) – The id of the project, used to filter for original project_id.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nlist[UserBlueprint]\nclassmethod get(user_blueprint_id, project_id=None)\nRetrieve a user blueprint\nParameters:\nuser_blueprint_id (string) – Used to identify a specific user-owned blueprint.\nproject_id (string (optional, default is None)) – String representation of ObjectId for a given project. Used to validate selected\ncolumns in the user blueprint.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprint\nclassmethod create(blueprint, model_type=None, project_id=None, save_to_catalog=True)\nCreate a user blueprint\nParameters:\nblueprint (list[dict] or list[UserBlueprintTask]) – A list of tasks in the form of dictionaries which define a blueprint.\nmodel_type (string, Optional) – The title to give to the blueprint.\nproject_id (string, Optional) – The project associated with the blueprint. Necessary in the event of project specific\ntasks, such as column selection tasks.\nsave_to_catalog (bool, (Default=True)) – Whether the blueprint being created should be saved to the catalog.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprint\nclassmethod create_from_custom_task_version_id(custom_task_version_id, save_to_catalog=True, description=None)\nCreate a user blueprint with a single custom task version\nParameters:\ncustom_task_version_id (string) – Id of custom task version from which the user blueprint is created\nsave_to_catalog (bool, (Default=True)) – Whether the blueprint being created should be saved to the catalog\ndescription (string (Default=None)) – The description for the user blueprint that will be created from the\ncustom task version.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprint\nclassmethod clone_project_blueprint(blueprint_id, project_id, model_type=None, save_to_catalog=True)\nClone a blueprint from a project.\nParameters:\nblueprint_id (string) – The id associated with the blueprint to create the user blueprint from.\nmodel_type (string, Optional) – The title to give to the blueprint.\nproject_id (string) – The id of the project which the blueprint to copy comes from.\nsave_to_catalog (bool, (Default=True)) – Whether the blueprint being created should be saved to the catalog.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprint\nclassmethod clone_user_blueprint(user_blueprint_id, model_type=None, project_id=None, save_to_catalog=True)\nClone a user blueprint.\nParameters:\nmodel_type (string, Optional) – The title to give to the blueprint.\nproject_id (string, Optional) – String representation of ObjectId for a given project. Used to validate selected\ncolumns in the user blueprint.\nuser_blueprint_id (string) – The id of the existing user blueprint to copy.\nsave_to_catalog (bool, (Default=True)) – Whether the blueprint being created should be saved to the catalog.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprint\nclassmethod update(blueprint, user_blueprint_id, model_type=None, project_id=None, include_project_id_if_none=False)\nUpdate a user blueprint\nParameters:\nblueprint (list(dict) or list(UserBlueprintTask)) – A list of tasks in the form of dictionaries which define a blueprint.\nIf None, will not be passed.\nmodel_type (string, Optional) – The title to give to the blueprint. If None, will not be passed.\nproject_id (string, Optional) – The project associated with the blueprint. Necessary in the event of project specific\ntasks, such as column selection tasks.\nIf None, will not be passed. To explicitly pass None, pass True to\ninclude_project_id_if_none (useful if unlinking a blueprint from a project)\nuser_blueprint_id (string) – Used to identify a specific user-owned blueprint.\ninclude_project_id_if_none (bool (Default=False)) – Allows project_id to be passed as None, instead of ignored.\nIf set to False, will not pass project_id in the API request if it is set to None.\nIf True, the project id will be passed even if it is set to None.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprint\nclassmethod delete(user_blueprint_id)\nDelete a user blueprint, specified by the userBlueprintId.\nParameters:\nuser_blueprint_id (string) – Used to identify a specific user-owned blueprint.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nrequests.models.Response\nclassmethod get_input_types()\nRetrieve the input types which can be used with User Blueprints.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintAvailableInput\nclassmethod add_to_project(project_id, user_blueprint_ids)\nAdd a list of user blueprints, by id, to a specified (by id) project’s repository.\nParameters:\nproject_id (string) – The projectId of the project for the repository to add the specified user blueprints\nto.\nuser_blueprint_ids (list(string) or string) – The ids of the user blueprints to add to the specified project’s repository.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintAddToProjectMenu\nclassmethod get_available_tasks(project_id=None, user_blueprint_id=None)\nRetrieve the available tasks, organized into categories, which can be used to create or\nmodify User Blueprints.\nParameters:\nproject_id (string, Optional)\nuser_blueprint_id (string, Optional)\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintAvailableTasks\nclassmethod validate_task_parameters(output_method, task_code, task_parameters, project_id=None)\nValidate that each value assigned to specified task parameters are valid.\nParameters:\noutput_method (enum(``’P’, ``'Pm', 'S', 'Sm', 'T', 'TS')) – The method representing how the task will output data.\ntask_code (string) – The task code representing the task to validate parameter values.\ntask_parameters (list(UserBlueprintTaskParameterValidationRequestParamItem)) – A list of task parameters and proposed values to be validated.\nproject_id (string (optional, default is None)) – The projectId representing the project where this user blueprint is edited.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintValidateTaskParameters\nclassmethod list_shared_roles(user_blueprint_id, limit=100, offset=0, id=None, name=None, share_recipient_type=None)\nGet a list of users, groups and organizations that have an access to this user blueprint\nParameters:\nid (Optional[str]) – Only return the access control information for a organization, group or user with this\nID.\nlimit (int (Default=100)) – At most this many results are returned.\nname (string, Optional) – Only return the access control information for a organization, group or user with this\nname.\noffset (int (Default=0)) – This many results will be skipped.\nshare_recipient_type (enum(``’user’, ``'group', 'organization'), Optional) – Describes the recipient type, either user, group, or organization.\nuser_blueprint_id (str) – Used to identify a specific user-owned blueprint.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nlist[UserBlueprintSharedRolesResponseValidator]\nclassmethod validate_blueprint(blueprint, project_id=None)\nValidate a user blueprint and return information about the inputs expected and outputs\nprovided by each task.\nParameters:\nblueprint (list(dict) or list(UserBlueprintTask)) – The representation of a directed acyclic graph defining a pipeline of data through tasks\nand a final estimator.\nproject_id (string (optional, default is None)) – The projectId representing the project where this user blueprint is edited.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nlist[VertexContextItem]\nclassmethod update_shared_roles(user_blueprint_id, roles)\nShare a user blueprint with a user, group, or organization\nParameters:\nuser_blueprint_id (str) – Used to identify a specific user-owned blueprint.\nroles (list(or(GrantAccessControlWithUsernameValidator, GrantAccessControlWithIdValidator))) – Array of GrantAccessControl objects., up to maximum 100 objects.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nrequests.models.Response\nclassmethod search_catalog(search=None, tag=None, limit=100, offset=0, owner_user_id=None, owner_username=None, order_by='-created')\nFetch a list of the user blueprint catalog entries the current user has access to\nbased on an optional search term, tags, owner user info, or sort order.\nParameters:\nsearch (string, Optional.) – A value to search for in the dataset’s name, description, tags, column names,\ncategories, and latest error. The search is case insensitive. If no value is provided\nfor this parameter, or if the empty string is used, or if the string contains only\nwhitespace, no filtering will be done. Partial matching is performed on dataset name\nand description fields while all other fields will only match if the search matches the\nwhole value exactly.\ntag (string, Optional.) – If provided, the results will be filtered to include only items with the specified tag.\nlimit (int, Optional. (default: 0), at most this many results are returned. To specify no) – limit, use 0. The default may change and a maximum limit may be imposed without notice.\noffset (int, Optional. (default: 0), this many results will be skipped.)\nowner_user_id (string, Optional.) – Filter results to those owned by one or more owner identified by UID.\nowner_username (string,  Optional.) – Filter results to those owned by one or more owner identified by username.\norder_by (string, Optional. Defaults to '-created') – Sort order which will be applied to catalog list, valid options are “catalogName”,\n“originalName”, “description”, “created”, and “relevance”. For all options other\nthan relevance, you may prefix the attribute name with a dash to sort\nin descending order. e.g. orderBy=’-catalogName’.\nReturn type:\nUserBlueprintCatalogSearch\nclass datarobot.models.user_blueprints.models.UserBlueprintAvailableInput\nRetrieve the input types which can be used with User Blueprints.\nParameters:\ninput_types (list(UserBlueprintsInputType)) – A list of associated pairs of an input types and their human-readable names.\nclassmethod get_input_types()\nRetrieve the input types which can be used with User Blueprints.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintAvailableInput\nclass datarobot.models.user_blueprints.models.UserBlueprintAddToProjectMenu\nAdd a list of user blueprints, by id, to a specified (by id) project’s repository.\nParameters:\nadded_to_menu (list(UserBlueprintAddedToMenuItem)) – The list of userBlueprintId and blueprintId pairs representing blueprints successfully\nadded to the project repository.\nnot_added_to_menu (list(UserBlueprintNotAddedToMenuItem)) – The list of userBlueprintId and error message representing blueprints which failed to be\nadded to the project repository.\nmessage (string) – A success message or a list of reasons why the list of blueprints could not be added\nto the project repository.\nclassmethod add_to_project(project_id, user_blueprint_ids)\nAdd a list of user blueprints, by id, to a specified (by id) project’s repository.\nParameters:\nproject_id (string) – The projectId of the project for the repository to add the specified user blueprints\nto.\nuser_blueprint_ids (list(string)) – The ids of the user blueprints to add to the specified project’s repository.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintAddToProjectMenu\nclass datarobot.models.user_blueprints.models.UserBlueprintAvailableTasks\nRetrieve the available tasks, organized into categories, which can be used to create or modify\nUser Blueprints.\nParameters:\ncategories (list(UserBlueprintTaskCategoryItem)) – A list of the available task categories, sub-categories, and tasks.\ntasks (list(UserBlueprintTaskLookupEntry)) – A list of task codes and their task definitions.\nclassmethod get_available_tasks(project_id=None, user_blueprint_id=None)\nRetrieve the available tasks, organized into categories, which can be used to create or\nmodify User Blueprints.\nParameters:\nproject_id (string, Optional)\nuser_blueprint_id (string, Optional)\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintAvailableTasks\nclass datarobot.models.user_blueprints.models.UserBlueprintValidateTaskParameters\nValidate that each value assigned to specified task parameters are valid.\nParameters:\nerrors (list(UserBlueprintsValidateTaskParameter)) – A list of the task parameters, their proposed values, and messages describing why each is\nnot valid.\nclassmethod validate_task_parameters(output_method, task_code, task_parameters, project_id=None)\nValidate that each value assigned to specified task parameters are valid.\nParameters:\noutput_method (enum(``’P’, ``'Pm', 'S', 'Sm', 'T', 'TS')) – The method representing how the task will output data.\ntask_code (string) – The task code representing the task to validate parameter values.\ntask_parameters (list(UserBlueprintTaskParameterValidationRequestParamItem)) – A list of task parameters and proposed values to be validated.\nproject_id (string (optional, default is None)) – The projectId representing the project where this user blueprint is edited.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nUserBlueprintValidateTaskParameters\nclass datarobot.models.user_blueprints.models.UserBlueprintSharedRolesResponseValidator\nA list of SharedRoles objects.\nParameters:\nshare_recipient_type (enum(``’user’, ``'group', 'organization')) – Describes the recipient type, either user, group, or organization.\nrole (str, one of enum(``’CONSUMER’, ``'EDITOR', 'OWNER')) – The role of the org/group/user on this dataset or “NO_ROLE” for removing access when used\nwith route to modify access.\nid (str) – The ID of the recipient organization, group or user.\nname (string) – The name of the recipient organization, group or user.\nclass datarobot.models.user_blueprints.models.VertexContextItem\nInfo about, warnings about, and errors with a specific vertex in the blueprint.\nParameters:\ntask_id (string) – The id associated with a specific vertex in the blueprint.\ninformation (VertexContextItemInfo)\nmessages (VertexContextItemMessages)\nclass datarobot.models.user_blueprints.models.UserBlueprintCatalogSearch\nAn APIObject representing a user blueprint catalog entry the current\nuser has access to based on an optional search term and/or tags.\nParameters:\nid (str) – The ID of the catalog entry linked to the user blueprint.\ncatalog_name (str) – The name of the user blueprint.\ncreator (str) – The name of the user that created the user blueprint.\nuser_blueprint_id (str) – The ID of the user blueprint.\ndescription (Optional[str] (Default=None)) – The description of the user blueprint.\nlast_modifier_name (Optional[str] (Default=None)) – The name of the user that last modified the user blueprint.\nclassmethod search_catalog(search=None, tag=None, limit=100, offset=0, owner_user_id=None, owner_username=None, order_by='-created')\nFetch a list of the user blueprint catalog entries the current user has access to\nbased on an optional search term, tags, owner user info, or sort order.\nParameters:\nsearch (string, Optional.) – A value to search for in the dataset’s name, description, tags, column names,\ncategories, and latest error. The search is case insensitive. If no value is provided\nfor this parameter, or if the empty string is used, or if the string contains only\nwhitespace, no filtering will be done. Partial matching is performed on dataset name\nand description fields while all other fields will only match if the search matches the\nwhole value exactly.\ntag (string, Optional.) – If provided, the results will be filtered to include only items with the specified tag.\nlimit (int, Optional. (default: 0), at most this many results are returned. To specify no) – limit, use 0. The default may change and a maximum limit may be imposed without notice.\noffset (int, Optional. (default: 0), this many results will be skipped.)\nowner_user_id (string, Optional.) – Filter results to those owned by one or more owner identified by UID.\nowner_username (string,  Optional.) – Filter results to those owned by one or more owner identified by username.\norder_by (string, Optional. Defaults to '-created') – Sort order which will be applied to catalog list, valid options are “catalogName”,\n“originalName”, “description”, “created”, and “relevance”. For all options other\nthan relevance, you may prefix the attribute name with a dash to sort\nin descending order. e.g. orderBy=’-catalogName’.\nReturn type:\nList[UserBlueprintCatalogSearch]\nCustom tasks\nclass datarobot.CustomTask\nA custom task. This can be in a partial state or a complete state.\nWhen the latest_version is None, the empty task has been initialized with\nsome metadata.  It is not yet use-able for actual training.  Once the first\nCustomTaskVersion has been created, you can put the CustomTask in UserBlueprints to\ntrain Models in Projects\nAdded in version v2.26.\nVariables:\nid (str) – id of the custom task\nname (str) – name of the custom task\nlanguage (str) – programming language of the custom task.\nCan be “python”, “r”, “java” or “other”\ndescription (str) – description of the custom task\ntarget_type (datarobot.enums.CUSTOM_TASK_TARGET_TYPE) – the target type of the custom task. One of:\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.BINARY\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.REGRESSION\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.MULTICLASS\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.ANOMALY\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.TRANSFORM\nlatest_version (datarobot.CustomTaskVersion or None) – latest version of the custom task if the task has a latest version. If the\nlatest version is None, the custom task is not ready for use in user blueprints.\nYou must create its first CustomTaskVersion before you can use the CustomTask\ncreated_by (str) – The username of the user who created the custom task.\nupdated_at (str) – An ISO-8601 formatted timestamp of when the custom task was updated.\ncreated_at (str) – ISO-8601 formatted timestamp of when the custom task was created\ncalibrate_predictions (bool) – whether anomaly predictions should be calibrated to be between 0 and 1 by DR.\nonly applies to custom estimators with target type\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.ANOMALY\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nCustomTask\nclassmethod list(order_by=None, search_for=None)\nList custom tasks available to the user.\nAdded in version v2.26.\nParameters:\nsearch_for (Optional[str]) – string for filtering custom tasks - only tasks that contain the\nstring in name or description will be returned.\nIf not specified, all custom task will be returned\norder_by (Optional[str]) – property to sort custom tasks by.\nSupported properties are “created” and “updated”.\nPrefix the attribute name with a dash to sort in descending order,\ne.g. order_by=’-created’.\nBy default, the order_by parameter is None which will result in\ncustom tasks being returned in order of creation time descending\nReturns:\na list of custom tasks.\nReturn type:\nList[CustomTask]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(custom_task_id)\nGet custom task by id.\nAdded in version v2.26.\nParameters:\ncustom_task_id (str) – id of the custom task\nReturns:\nretrieved custom task\nReturn type:\nCustomTask\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nclassmethod copy(custom_task_id)\nCreate a custom task by copying existing one.\nAdded in version v2.26.\nParameters:\ncustom_task_id (str) – id of the custom task to copy\nReturn type:\nCustomTask\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod create(name, target_type, language=None, description=None, calibrate_predictions=None, **kwargs)\nCreates only the metadata for a custom task.  This task will\nnot be use-able until you have created a CustomTaskVersion attached to this task.\nAdded in version v2.26.\nParameters:\nname (str) – name of the custom task\ntarget_type (datarobot.enums.CUSTOM_TASK_TARGET_TYPE) – the target typed based on the following values. Anything else will raise an error\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.BINARY\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.REGRESSION\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.MULTICLASS\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.ANOMALY\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.TRANSFORM\nlanguage (Optional[str]) – programming language of the custom task.\nCan be “python”, “r”, “java” or “other”\ndescription (Optional[str]) – description of the custom task\ncalibrate_predictions (Optional[bool]) – whether anomaly predictions should be calibrated to be between 0 and 1 by DR.\nif None, uses default value from DR app (True).\nonly applies to custom estimators with target type\ndatarobot.enums.CUSTOM_TASK_TARGET_TYPE.ANOMALY\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nCustomTask\nupdate(name=None, language=None, description=None, **kwargs)\nUpdate custom task properties.\nAdded in version v2.26.\nParameters:\nname (Optional[str]) – new custom task name\nlanguage (Optional[str]) – new custom task programming language\ndescription (Optional[str]) – new custom task description\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nNone\nrefresh()\nUpdate custom task with the latest data from server.\n:rtype: None\nAdded in version v2.26.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndelete()\nDelete custom task.\n:rtype: None\nAdded in version v2.26.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndownload_latest_version(file_path)\nDownload the latest custom task version.\nAdded in version v2.26.\nParameters:\nfile_path (str) – the full path of the target zip file\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nNone\nget_access_list()\nRetrieve access control settings of this custom task.\nAdded in version v2.27.\nReturn type:\nlist of SharingAccess\nshare(access_list)\nUpdate the access control settings of this custom task.\nAdded in version v2.27.\nParameters:\naccess_list (list of SharingAccess) – A list of SharingAccess to update.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nNone\nExamples\nTransfer access to the custom task from old_user@datarobot.com to new_user@datarobot.com\nimport datarobot as dr\nnew_access = dr.SharingAccess(new_user@datarobot.com,\ndr.enums.SHARING_ROLE.OWNER, can_share=True)\naccess_list = [dr.SharingAccess(old_user@datarobot.com, None), new_access]\ndr.CustomTask.get('custom-task-id').share(access_list)\nclass datarobot.models.custom_task_version.CustomTaskFileItem\nA file item attached to a DataRobot custom task version.\nAdded in version v2.26.\nVariables:\nid (str) – id of the file item\nfile_name (str) – name of the file item\nfile_path (str) – path of the file item\nfile_source (str) – source of the file item\ncreated_at (str) – ISO-8601 formatted timestamp of when the version was created\nclass datarobot.enums.CustomTaskOutboundNetworkPolicy\nThe way to set and view a CustomTaskVersions outbound network policy.\nclass datarobot.CustomTaskVersion\nA version of a DataRobot custom task.\nAdded in version v2.26.\nVariables:\nid (str) – id of the custom task version\ncustom_task_id (str) – id of the custom task\nversion_minor (int) – a minor version number of custom task version\nversion_major (int) – a major version number of custom task version\nlabel (str) – short human readable string to label the version\ncreated_at (str) – ISO-8601 formatted timestamp of when the version was created\nis_frozen (bool) – a flag if the custom task version is frozen\nitems (List[CustomTaskFileItem]) – a list of file items attached to the custom task version\ndescription (Optional[str]) – custom task version description\nbase_environment_id (Optional[str]) – id of the environment to use with the task\nbase_environment_version_id (Optional[str]) – id of the environment version to use with the task\ndependencies (List[CustomDependency]) – the parsed dependencies of the custom task version if the\nversion has a valid requirements.txt file\nrequired_metadata_values (List[RequiredMetadataValue]) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\narguments (List[UserBlueprintTaskArgument]) – A list of custom task version arguments.\noutbound_network_policy (CustomTaskOutboundNetworkPolicy)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nclassmethod create_clean(custom_task_id, base_environment_id, maximum_memory=None, is_major_update=True, folder_path=None, required_metadata_values=None, outbound_network_policy=None)\nCreate a custom task version without files from previous versions.\nAdded in version v2.26.\nParameters:\ncustom_task_id (str) – the id of the custom task\nbase_environment_id (str) – the id of the base environment to use with the custom task version\nmaximum_memory (Optional[int]) – A number in bytes about how much memory custom tasks’ inference containers can run with.\nis_major_update (bool) – If the current version is 2.3, True would set the new version at 3.0.\nFalse would set the new version at 2.4.\nDefaults to True.\nfolder_path (Optional[str]) – The path to a folder containing files to be uploaded.\nEach file in the folder is uploaded under path relative\nto a folder path.\nrequired_metadata_values (Optional[List[RequiredMetadataValue]]) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\noutbound_network_policy (Optional[CustomTaskOutboundNetworkPolicy]) – You must enable custom task network access permissions to pass any value other than None!\nSpecifies if you custom task version is able to make network calls. None will set the value\nto DataRobot’s default.\nReturns:\ncreated custom task version\nReturn type:\nCustomTaskVersion\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod create_from_previous(custom_task_id, base_environment_id, maximum_memory=None, is_major_update=True, folder_path=None, files_to_delete=None, required_metadata_values=None, outbound_network_policy=None)\nCreate a custom task version containing files from a previous version.\nAdded in version v2.26.\nParameters:\ncustom_task_id (str) – the id of the custom task\nbase_environment_id (str) – the id of the base environment to use with the custom task version\nmaximum_memory (Optional[int]) – A number in bytes about how much memory custom tasks’ inference containers can run with.\nis_major_update (bool) – If the current version is 2.3, True would set the new version at 3.0.\nFalse would set the new version at 2.4.\nDefaults to True.\nfolder_path (Optional[str]) – The path to a folder containing files to be uploaded.\nEach file in the folder is uploaded under path relative\nto a folder path.\nfiles_to_delete (Optional[List[str]]) – the list of a file items ids to be deleted\nExample: [“5ea95f7a4024030aba48e4f9”, “5ea6b5da402403181895cc51”]\nrequired_metadata_values (Optional[List[RequiredMetadataValue]]) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\noutbound_network_policy (Optional[CustomTaskOutboundNetworkPolicy]) – You must enable custom task network access permissions to pass any value other than None!\nSpecifies if you custom task version is able to make network calls. None will get the value\nfrom the previous version if you have the proper permissions or use DataRobot’s default.\nReturns:\ncreated custom task version\nReturn type:\nCustomTaskVersion\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod list(custom_task_id)\nList custom task versions.\nAdded in version v2.26.\nParameters:\ncustom_task_id (str) – the id of the custom task\nReturns:\na list of custom task versions\nReturn type:\nList[CustomTaskVersion]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(custom_task_id, custom_task_version_id)\nGet custom task version by id.\nAdded in version v2.26.\nParameters:\ncustom_task_id (str) – the id of the custom task\ncustom_task_version_id (str) – the id of the custom task version to retrieve\nReturns:\nretrieved custom task version\nReturn type:\nCustomTaskVersion\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\ndownload(file_path)\nDownload custom task version.\nAdded in version v2.26.\nParameters:\nfile_path (str) – path to create a file with custom task version content\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nupdate(description=None, required_metadata_values=None)\nUpdate custom task version properties.\nAdded in version v2.26.\nParameters:\ndescription (str) – new custom task version description\nrequired_metadata_values (List[RequiredMetadataValue]) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nrefresh()\nUpdate custom task version with the latest data from server.\nAdded in version v2.26.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nstart_dependency_build()\nStart the dependency build for a custom task version and return build status.\n.. versionadded:: v2.27\nReturns:\nDTO of custom task version dependency build.\nReturn type:\nCustomTaskVersionDependencyBuild\nstart_dependency_build_and_wait(max_wait)\nStart the dependency build for a custom task version and wait while pulling status.\n.. versionadded:: v2.27\nParameters:\nmax_wait (int) – max time to wait for a build completion\nReturns:\nDTO of custom task version dependency build.\nReturn type:\nCustomTaskVersionDependencyBuild\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndatarobot.errors.AsyncTimeoutError – Raised if the dependency build is not finished after max_wait.\ncancel_dependency_build()\nCancel custom task version dependency build that is in progress.\n.. versionadded:: v2.27\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nget_dependency_build()\nRetrieve information about a custom task version’s dependency build.\n.. versionadded:: v2.27\nReturns:\nDTO of custom task version dependency build.\nReturn type:\nCustomTaskVersionDependencyBuild\ndownload_dependency_build_log(file_directory='.')\nGet log of a custom task version dependency build.\n.. versionadded:: v2.27\nParameters:\nfile_directory (str (optional, default is \".\")) – Directory path where downloaded file is to save.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nVisual AI\nclass datarobot.models.visualai.images.Image\nAn image stored in a project’s dataset.\nVariables:\nid (str) – Image ID for this image.\nimage_type (str) – Image media type. Accessing this may require a server request\nand an associated delay in returning.\nimage_bytes (bytes) – Raw bytes of this image. Accessing this may require a server request\nand an associated delay in returning.\nheight (int) – Height of the image in pixels.\nwidth (int) – Width of the image in pixels.\nclassmethod get(project_id, image_id)\nGet a single image object from project.\nParameters:\nproject_id (str) – Id of the project that contains the images.\nimage_id (str) – ID of image to load from the project.\nReturn type:\nImage\nclass datarobot.models.visualai.images.SampleImage\nA sample image in a project’s dataset.\nIf Project.stage is datarobot.enums.PROJECT_STAGE.EDA2 then\nthe target_* attributes of this class will have values, otherwise\nthe values will all be None.\nVariables:\nimage (Image) – Image object.\ntarget_value (TargetValue) – Value associated with the feature_name.\nproject_id (str) – Id of the project that contains the images.\nclassmethod list(project_id, feature_name, target_value=None, target_bin_start=None, target_bin_end=None, offset=None, limit=None)\nGet sample images from a project.\nParameters:\nproject_id (str) – Project that contains the images.\nfeature_name (str) – Name of feature column that contains images.\ntarget_value (TargetValue) – For classification projects - target value to filter images.\nPlease note that you can only use this parameter when the project has finished the EDA2\nstage.\ntarget_bin_start (Optional[Union[int, float]]) – For regression projects - only images corresponding to the target values above\n(inclusive) this value will be returned. Must be specified together with target_bin_end.\nPlease note that you can only use this parameter when the project has finished the EDA2\nstage.\ntarget_bin_end (Optional[Union[int, float]]) – For regression projects - only images corresponding to the target values below\n(exclusive) this value will be returned. Must be specified together with\ntarget_bin_start.\nPlease note that you can only use this parameter when the project has finished the EDA2\nstage.\noffset (Optional[int]) – Number of images to be skipped.\nlimit (Optional[int]) – Number of images to be returned.\nReturn type:\nList[SampleImage]\nclass datarobot.models.visualai.images.DuplicateImage\nAn image that was duplicated in the project dataset.\nVariables:\nimage (Image) – Image object.\ncount (int) – Number of times the image was duplicated.\nclassmethod list(project_id, feature_name, offset=None, limit=None)\nGet all duplicate images in a project.\nParameters:\nproject_id (str) – Project that contains the images.\nfeature_name (str) – Name of feature column that contains images.\noffset (Optional[int]) – Number of images to be skipped.\nlimit (Optional[int]) – Number of images to be returned.\nReturn type:\nList[DuplicateImage]\nclass datarobot.models.visualai.insights.ImageEmbedding\nVector representation of an image in an embedding space.\nA vector in an embedding space will allow linear computations to\nbe carried out between images: for example computing the Euclidean\ndistance of the images.\nVariables:\nimage (Image) – Image object used to create this map.\nfeature_name (str) – Name of the feature column this embedding is associated with.\nposition_x (int) – X coordinate of the image in the embedding space.\nposition_y (int) – Y coordinate of the image in the embedding space.\nactual_target_value (object) – Actual target value of the dataset row.\ntarget_values (Optional[List[str]]) – For classification projects, a list of target values of this project.\ntarget_bins (Optional[List[Dict[str, float]]]) – For regression projects, a list of target bins of this project.\nproject_id (str) – Id of the project this Image Embedding belongs to.\nmodel_id (str) – Id of the model this Image Embedding belongs to.\nclassmethod compute(project_id, model_id)\nStart the computation of image embeddings for the model.\nParameters:\nproject_id (str) – Project to start creation in.\nmodel_id (str) – Project’s model to start creation in.\nReturns:\nURL to check for image embeddings progress.\nReturn type:\nstr\nRaises:\ndatarobot.errors.ClientError – Server rejected creation due to client error. Most likely\ncause is bad project_id or model_id.\nclassmethod models(project_id)\nFor a given project_id, list all model_id - feature_name pairs with available\nImage Embeddings.\nParameters:\nproject_id (str) – Id of the project to list model_id - feature_name pairs with available Image Embeddings\nfor.\nReturns:\nList of model and feature name pairs.\nReturn type:\nlist( tuple(model_id, feature_name) )\nclassmethod list(project_id, model_id, feature_name)\nReturn a list of ImageEmbedding objects.\nParameters:\nproject_id (str) – Id of the project the model belongs to.\nmodel_id (str) – Id of the model to list Image Embeddings for.\nfeature_name (str) – Name of feature column to list Image Embeddings for.\nReturn type:\nList[ImageEmbedding]\nclass datarobot.models.visualai.insights.ImageActivationMap\nMark areas of image with weight of impact on training.\nThis is a technique to display how various areas of the region were\nused in training, and their effect on predictions. Larger values in\nactivation_values indicates a larger impact.\nVariables:\nimage (Image) – Image object used to create this map.\noverlay_image (Image) – Image object containing the original image overlaid by the activation heatmap.\nfeature_name (str) – Name of the feature column that contains the value this map is based on.\nactivation_values (List[List[int]]) – A row-column matrix that contains the activation strengths for\nimage regions. Values are integers in the range [0, 255].\nactual_target_value (TargetValue) – Actual target value of the dataset row.\npredicted_target_value (TargetValue) – Predicted target value of the dataset row that contains this image.\ntarget_values (Optional[List[str]]) – For classification projects a list of target values of this project.\ntarget_bins (Optional[List[Dict[str, float]]]) – For regression projects a list of target bins.\nproject_id (str) – Id of the project this Activation Map belongs to.\nmodel_id (str) – Id of the model this Activation Map belongs to.\nclassmethod compute(project_id, model_id)\nStart the computation of activation maps for the given model.\nParameters:\nproject_id (str) – Project to start creation in.\nmodel_id (str) – Project’s model to start creation in.\nReturns:\nURL to check for image embeddings progress.\nReturn type:\nstr\nRaises:\ndatarobot.errors.ClientError – Server rejected creation due to client error. Most likely\ncause is bad project_id or model_id.\nclassmethod models(project_id)\nFor a given project_id, list all model_id - feature_name pairs with available\nImage Activation Maps.\nParameters:\nproject_id (str) – Id of the project to list model_id - feature_name pairs with available\nImage Activation Maps for.\nReturns:\nList of model and feature name pairs.\nReturn type:\nlist( tuple(model_id, feature_name) )\nclassmethod list(project_id, model_id, feature_name, offset=None, limit=None)\nReturn a list of ImageActivationMap objects.\nParameters:\nproject_id (str) – Project that contains the images.\nmodel_id (str) – Model that contains the images.\nfeature_name (str) – Name of feature column that contains images.\noffset (Optional[int]) – Number of images to be skipped.\nlimit (Optional[int]) – Number of images to be returned.\nReturn type:\nList[ImageActivationMap]\nclass datarobot.models.visualai.augmentation.ImageAugmentationOptions\nA List of all supported Image Augmentation Transformations for a project.\nIncludes additional information about minimum, maximum, and default values\nfor a transformation.\nVariables:\nname (str) – The name of the augmentation list\nproject_id (str) – The project containing the image data to be augmented\nmin_transformation_probability (float) – The minimum allowed value for transformation probability.\ncurrent_transformation_probability (float) – Default setting for probability that each transformation will be applied to an image.\nmax_transformation_probability (float) – The maximum allowed value for transformation probability.\nmin_number_of_new_images (int) – The minimum allowed number of new rows to add for each existing row\ncurrent_number_of_new_images (int) – The default number of new rows to add for each existing row\nmax_number_of_new_images (int) – The maximum allowed number of new rows to add for each existing row\ntransformations (list[dict]) – List of transformations to possibly apply to each image\nclassmethod get(project_id)\nReturns a list of all supported transformations for the given\nproject\nParameters:\nproject_id (str) – sting\nThe id of the project for which to return the list of supported transformations.\nReturn type:\nImageAugmentationOptions\nReturns:\nImageAugmentationOptionsA list containing all the supported transformations for the project.\nclass datarobot.models.visualai.augmentation.ImageAugmentationList\nA List of Image Augmentation Transformations\nVariables:\nname (str) – The name of the augmentation list\nproject_id (str) – The project containing the image data to be augmented\nfeature_name (Optional[str]) – name of the feature that the augmentation list is associated with\nin_use (bool) – Whether this is the list that will passed in to every blueprint during blueprint generation\nbefore autopilot\ninitial_list (bool) – True if this is the list to be used during training to produce augmentations\ntransformation_probability (float) – Probability that each transformation will be applied to an image.  Value should be\nbetween 0.01 - 1.0.\nnumber_of_new_images (int) – Number of new rows to add for each existing row\ntransformations (List[Dict]) – List of transformations to possibly apply to each image\nsamples_id (str) – Id of last image augmentation sample generated for image augmentation list.\nclassmethod create(name, project_id, feature_name=None, initial_list=False, transformation_probability=0.0, number_of_new_images=1, transformations=None, samples_id=None)\ncreate a new image augmentation list\nReturn type:\nImageAugmentationList\nclassmethod list(project_id, feature_name=None)\nList Image Augmentation Lists present in a project.\nParameters:\nproject_id (str) – Project Id to retrieve augmentation lists for.\nfeature_name (Optional[str]) – If passed, the response will only include Image Augmentation Lists active for the\nprovided feature name.\nReturn type:\nlist[ImageAugmentationList]\nupdate(name=None, feature_name=None, initial_list=None, transformation_probability=None, number_of_new_images=None, transformations=None)\nUpdate one or multiple attributes of the Image Augmentation List in the DataRobot backend\nas well on this object.\nParameters:\nname (Optional[str]) – New name of the feature list.\nfeature_name (Optional[str]) – The new feature name for which the Image Augmentation List is effective.\ninitial_list (Optional[bool]) – New flag that indicates whether this list will be used during Autopilot to perform\nimage augmentation.\ntransformation_probability (Optional[float]) – New probability that each enabled transformation will be applied to an image.\nThis does not apply to Horizontal or Vertical Flip, which are always set to 50%.\nnumber_of_new_images (Optional[int]) – New number of new rows to add for each existing row, updating the existing augmentation\nlist.\ntransformations (Optional[list]) – New list of Transformations to possibly apply to each image.\nReturns:\nReference to self. The passed values will be updated in place.\nReturn type:\nImageAugmentationList\nretrieve_samples()\nLists already computed image augmentation sample for image augmentation list.\nReturns samples only if they have been already computed. It does not initialize computation.\nReturn type:\nList of class ImageAugmentationSample\ncompute_samples(max_wait=600)\nInitializes computation and retrieves list of image augmentation samples\nfor image augmentation list. If samples exited prior to this call method,\nthis will compute fresh samples and return latest version of samples.\nReturn type:\nList of class ImageAugmentationSample\nclass datarobot.models.visualai.augmentation.ImageAugmentationSample\nA preview of the type of images that augmentations will create during training.\nVariables:\nsample_id (ObjectId) – The id of the augmentation sample, used to group related images together\nimage_id (ObjectId) – A reference to the Image which can be used to retrieve the image binary\nproject_id (ObjectId) – A reference to the project containing the image\noriginal_image_id (ObjectId) – A reference to the original image that generated this image in the case of an augmented\nimage.  If this is None it signifies this is an original image\nheight (int) – Image height in pixels\nwidth (int) – Image width in pixels\nclassmethod list(auglist_id=None)\nReturn a list of ImageAugmentationSample objects.\nParameters:\nauglist_id (str) – ID for augmentation list to retrieve samples for\nReturn type:\nList of class ImageAugmentationSample",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/blueprints.html",
      "tags": [
        "api-reference",
        "advanced",
        "documentation",
        "example",
        "tutorial"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/blueprints.html",
        "content_length": 55309
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.enums.custom_task_target_type",
        "datarobot.models.custom_task_version",
        "datarobot.enums.customtaskoutboundnetworkpolicy",
        "datarobot.models.visualai",
        "dr.customtask.get",
        "datarobot.models.blueprintchart",
        "datarobot.models.blueprint",
        "datarobot.models.modelblueprintchart",
        "datarobot.errors.servererror",
        "datarobot.errors.asynctimeouterror",
        "dr.enums.sharing_role",
        "datarobot.enums.project_stage",
        "datarobot.models.blueprinttaskdocument",
        "datarobot.models.user_blueprints",
        "project.stage",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-8279356882944042319",
      "title": "Compliance Documentation",
      "content": "Compliance Documentation\nAutomated documentation\nclass datarobot.models.automated_documentation.AutomatedDocument\nAn automated documentation object.\nAdded in version v2.24.\nVariables:\ndocument_type (str or None) – Type of automated document. You can specify: MODEL_COMPLIANCE, AUTOPILOT_SUMMARY\ndepending on your account settings. Required for document generation.\nentity_id (str or None) – ID of the entity to generate the document for. It can be model ID or project ID.\nRequired for document generation.\noutput_format (str or None) – Format of the generate document, either docx or html.\nRequired for document generation.\nlocale (str or None) – Localization of the document, dependent on your account settings.\nDefault setting is EN_US.\ntemplate_id (str or None) – Template ID to use for the document outline. Defaults to standard DataRobot template.\nSee the documentation for ComplianceDocTemplate for more information.\nid (str or None) – ID of the document. Required to download or delete a document.\nfilepath (str or None) – Path to save a downloaded document to. Either include a file path and name or the file\nwill be saved to the directory from which the script is launched.\ncreated_at (datetime or None) – Document creation timestamp.\nclassmethod list_available_document_types(cls)\nGet a list of all available document types and locales. This method is deprecated.\nReturns:\n{“data”: List of dicts}\nReturn type:\nList[DocumentOption]\nExamples\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc_types = dr.AutomatedDocument.list_available_document_types()\nclassmethod list_all_available_document_types()\nGet a list of all available document types and locales.\nThis method is direct replacement of list_available_document_types().\nReturn type:\nList of dicts\nExamples\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc_types = dr.AutomatedDocument.list_all_available_document_types()\nproperty is_model_compliance_initialized: Tuple[bool, str]\nCheck if model compliance documentation pre-processing is initialized.\nModel compliance documentation pre-processing must be initialized before\ngenerating documentation for a custom model.\nReturns:\nboolean flag is whether model compliance documentation pre-processing is initialized\nstring value is the initialization status\nReturn type:\nTuple of (boolean, string)\ninitialize_model_compliance()\nInitialize model compliance documentation pre-processing.\nMust be called before generating documentation for a custom model.\nReturns:\nboolean flag is whether model compliance documentation pre-processing is initialized\nstring value is the initialization status\nReturn type:\nTuple of (boolean, string)\nExamples\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\n# NOTE: entity_id is either a model id or a model package (version) id\ndoc = dr.AutomatedDocument(\ndocument_type=\"MODEL_COMPLIANCE\",\nentity_id=\"6f50cdb77cc4f8d1560c3ed5\",\noutput_format=\"docx\",\nlocale=\"EN_US\")\ndoc.initialize_model_compliance()\ngenerate(max_wait=600)\nRequest generation of an automated document.\nRequired attributes to request document generation: document_type, entity_id,\nand output_format.\nReturn type:\nrequests.models.Response\nExamples\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc = dr.AutomatedDocument(\ndocument_type=\"MODEL_COMPLIANCE\",\nentity_id=\"6f50cdb77cc4f8d1560c3ed5\",\noutput_format=\"docx\",\nlocale=\"EN_US\",\ntemplate_id=\"50efc9db8aff6c81a374aeec\",\nfilepath=\"/Users/username/Documents/example.docx\"\n)\ndoc.generate()\ndoc.download()\ndownload()\nDownload a generated Automated Document.\nDocument ID is required to download a file.\nReturn type:\nrequests.models.Response\nExamples\nGenerating and downloading the generated document:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc = dr.AutomatedDocument(\ndocument_type=\"AUTOPILOT_SUMMARY\",\nentity_id=\"6050d07d9da9053ebb002ef7\",\noutput_format=\"docx\",\nfilepath=\"/Users/username/Documents/Project_Report_1.docx\"\n)\ndoc.generate()\ndoc.download()\nDownloading an earlier generated document when you know the document ID:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc = dr.AutomatedDocument(id='5e8b6a34d2426053ab9a39ed')\ndoc.download()\nNotice that filepath was not set for this document. In this case, the file is saved\nto the directory from which the script was launched.\nDownloading a document chosen from a list of earlier generated documents:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\nmodel_id = \"6f5ed3de855962e0a72a96fe\"\ndocs = dr.AutomatedDocument.list_generated_documents(entity_ids=[model_id])\ndoc = docs[0]\ndoc.filepath = \"/Users/me/Desktop/Recommended_model_doc.docx\"\ndoc.download()\ndelete()\nDelete a document using its ID.\nReturn type:\nrequests.models.Response\nExamples\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc = dr.AutomatedDocument(id=\"5e8b6a34d2426053ab9a39ed\")\ndoc.delete()\nIf you don’t know the document ID, you can follow the same workflow to get the ID as in\nthe examples for the AutomatedDocument.download method.\nclassmethod list_generated_documents(document_types=None, entity_ids=None, output_formats=None, locales=None, offset=None, limit=None)\nGet information about all previously generated documents available for your account. The\ninformation includes document ID and type, ID of the entity it was generated for, time of\ncreation, and other information.\nParameters:\ndocument_types (List of str or None) – Query for one or more document types.\nentity_ids (List of str or None) – Query generated documents by one or more entity IDs.\noutput_formats (List of str or None) – Query for one or more output formats.\nlocales (List of str or None) – Query generated documents by one or more locales.\noffset (int or None) – Number of items to skip. Defaults to 0 if not provided.\nlimit (int or None) – Number of items to return, maximum number of items is 1000.\nReturn type:\nList[AutomatedDocument]\nReturns:\nList of AutomatedDocument objects, where each object contains attributes described in\nAutomatedDocument\nExamples\nTo get a list of all generated documents:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndocs = AutomatedDocument.list_generated_documents()\nTo get a list of all AUTOPILOT_SUMMARY documents:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndocs = AutomatedDocument.list_generated_documents(document_types=[\"AUTOPILOT_SUMMARY\"])\nTo get a list of 5 recently created automated documents in html format:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndocs = AutomatedDocument.list_generated_documents(output_formats=[\"html\"], limit=5)\nTo get a list of automated documents created for specific entities (projects or models):\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndocs = AutomatedDocument.list_generated_documents(\nentity_ids=[\"6051d3dbef875eb3be1be036\",\n\"6051d3e1fbe65cd7a5f6fde6\",\n\"6051d3e7f86c04486c2f9584\"]\n)\nNote, that the list of results contains AutomatedDocument objects, which means that you\ncan execute class-related methods on them. Here’s how you can list, download, and then\ndelete from the server all automated documents related to a certain entity:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\nids = [\"6051d3dbef875eb3be1be036\", \"5fe1d3d55cd810ebdb60c517f\"]\ndocs = AutomatedDocument.list_generated_documents(entity_ids=ids)\nfor doc in docs:\ndoc.download()\ndoc.delete()\nclass datarobot.models.automated_documentation.DocumentOption\nCompliance documentation templates\nclass datarobot.models.compliance_doc_template.ComplianceDocTemplate\nA compliance documentation template. Templates\nare used to customize contents of AutomatedDocument.\nAdded in version v2.14.\nNotes\nEach section dictionary has the following schema:\ntitle : title of the section\ntype : type of section. Must be one of “datarobot”, “user” or “table_of_contents”.\nEach type of section has a different set of attributes described bellow.\nSection of type \"datarobot\" represent a section owned by DataRobot. DataRobot\nsections have the following additional attributes:\ncontent_id : The identifier of the content in this section.\nYou can get the default template with get_default\nfor a complete list of possible DataRobot section content ids.\nsections :  list of sub-section dicts nested under the parent section.\nSection of type \"user\" represent a section with user-defined content.\nThose sections may contain text generated by user and have the following additional fields:\nregularText : regular text of the section, optionally separated by\n\\n to split paragraphs.\nhighlightedText : highlighted text of the section, optionally separated\nby \\n to split paragraphs.\nsections :  list of sub-section dicts nested under the parent section.\nSection of type \"table_of_contents\" represent a table of contents and has\nno additional attributes.\nVariables:\nid (str) – The ID of the template.\nname (str) – The name of the template.\ncreator_id (str) – The ID of the user who created the template.\ncreator_username (str) – The username of the user who created the template.\norg_id (str) – The ID of the organization the template belongs to.\nsections (list of dicts) – The sections of the template describing the structure of the document. The section schema\nis described in Notes section, above.\nproject_type (ComplianceDocTemplateProjectType) – The project type of the template.\nclassmethod get_default(template_type=None)\nGet a default DataRobot template. This template is used for generating\ncompliance documentation when no template is specified.\nParameters:\ntemplate_type (str or None) – Type of the template. Currently supported values are “normal” and “time_series”\nReturns:\ntemplate – the default template object with sections attribute populated with default sections.\nReturn type:\nComplianceDocTemplate\nclassmethod create_from_json_file(name, path, project_type=None)\nCreate a template with the specified name and sections in a JSON file.\nThis is useful when working with sections in a JSON file. Example:\ndefault_template = ComplianceDocTemplate.get_default()\ndefault_template.sections_to_json_file('path/to/example.json')\n# ... edit example.json in your editor\nmy_template = ComplianceDocTemplate.create_from_json_file(\nname='my template',\npath='path/to/example.json'\n)\nParameters:\nname (str) – the name of the template, which must be unique.\npath (str) – the path to find the JSON file at\nproject_type (ComplianceDocTemplateProjectType) – The project type of the template.\nReturns:\ntemplate – The created template.\nReturn type:\nComplianceDocTemplate\nclassmethod create(name, sections, project_type=None)\nCreate a template with the specified name and sections.\nParameters:\nname (str) – The name of the template, which must be unique.\nsections (list) – List of section objects\nproject_type (ComplianceDocTemplateProjectType) – The project type of the template.\nReturns:\ntemplate – The created template.\nReturn type:\nComplianceDocTemplate\nclassmethod get(template_id)\nRetrieve a specific template.\nParameters:\ntemplate_id (str) – the id of the template to retrieve\nReturns:\ntemplate – the retrieved template\nReturn type:\nComplianceDocTemplate\nclassmethod list(name_part=None, limit=None, offset=None, project_type=None)\nGet a paginated list of compliance documentation template objects.\nParameters:\nname_part (str or None) – Return only the templates with names matching specified string. The matching is\ncase-insensitive.\nlimit (int) – The number of records to return. The server will use a (possibly finite) default if not\nspecified.\noffset (int) – The number of records to skip.\nproject_type (ComplianceDocTemplateProjectType) – The project type of the template.\nReturns:\ntemplates – The list of template objects.\nReturn type:\nlist of ComplianceDocTemplate\nsections_to_json_file(path, indent=2)\nSave sections of the template to a json file at the specified path\nParameters:\npath (str) – the path to save the file to\nindent (int) – indentation to use in the json file.\nReturn type:\nNone\nupdate(name=None, sections=None, project_type=None)\nUpdate the name or sections of an existing doc template.\nNote that default or non-existent templates can not be updated.\nParameters:\nname (Optional[str]) – the new name for the template\nsections (list of dicts) – The list of sections within the template.\nproject_type (ComplianceDocTemplateProjectType) – The project type of the template\nReturn type:\nNone\ndelete()\nDelete the compliance documentation template.\nReturn type:\nNone\nclass datarobot.enums.ComplianceDocTemplateProjectType\nThe project type supported by the template.\nclass datarobot.enums.ComplianceDocTemplateType\nThe type of default template and sections to create a template.\nclassmethod to_project_type(template_type)\nMap from template type to project type supported by the template.\nReturn type:\nOptional[ComplianceDocTemplateProjectType]",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/compliance-documentation.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/compliance-documentation.html",
        "content_length": 12943
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.compliance_doc_template",
        "datarobot.enums.compliancedoctemplatetype",
        "dr.automateddocument.list_available_document_types",
        "datarobot.enums.compliancedoctemplateprojecttype",
        "dr.automateddocument.list_all_available_document_types",
        "datarobot.models.automated_documentation",
        "dr.automateddocument.list_generated_documents"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_574993271577659054",
      "title": "Credentials",
      "content": "Credentials\nclass datarobot.models.Credential\nclassmethod list()\nReturns list of available credentials.\nReturns:\ncredentials – contains a list of available credentials.\nReturn type:\nlist of Credential instances\nExamples\n>>> import datarobot as dr\n>>> data_sources = dr.Credential.list()\n>>> data_sources\n[\nCredential('5e429d6ecf8a5f36c5693e03', 'my_s3_cred', 's3'),\nCredential('5e42cc4dcf8a5f3256865840', 'my_jdbc_cred', 'jdbc'),\n]\nclassmethod get(credential_id)\nGets the Credential.\nParameters:\ncredential_id (str) – the identifier of the credential.\nReturns:\ncredential – the requested credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.get('5a8ac9ab07a57a0001be501f')\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'my_s3_cred', 's3'),\ndelete()\nDeletes the Credential the store.\nParameters:\ncredential_id (str) – the identifier of the credential.\nReturns:\ncredential – the requested credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.get('5a8ac9ab07a57a0001be501f')\n>>> cred.delete()\nclassmethod create_basic(name, user, password, description=None)\nCreates the credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\nuser (str) – the username to store for this set of credentials.\npassword (str) – the password to store for this set of credentials.\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_basic(\n...     name='my_basic_cred',\n...     user='username',\n...     password='password',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'my_basic_cred', 'basic'),\nclassmethod create_oauth(name, token, refresh_token, description=None)\nCreates the OAUTH credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\ntoken (str) – the OAUTH token\nrefresh_token (str) – The OAUTH token\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_oauth(\n...     name='my_oauth_cred',\n...     token='XXX',\n...     refresh_token='YYY',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'my_oauth_cred', 'oauth'),\nclassmethod create_s3(name, aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, config_id=None, description=None)\nCreates the S3 credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\naws_access_key_id (Optional[str]) – the AWS access key id.\naws_secret_access_key (Optional[str]) – the AWS secret access key.\naws_session_token (Optional[str]) – the AWS session token.\nconfig_id (Optional[str]) – The ID of the saved shared secure configuration. If specified, cannot include awsAccessKeyId,\nawsSecretAccessKey or awsSessionToken.\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_s3(\n...     name='my_s3_cred',\n...     aws_access_key_id='XXX',\n...     aws_secret_access_key='YYY',\n...     aws_session_token='ZZZ',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'my_s3_cred', 's3'),\nclassmethod create_azure(name, azure_connection_string, description=None)\nCreates the Azure storage credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\nazure_connection_string (str) – the Azure connection string.\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_azure(\n...     name='my_azure_cred',\n...     azure_connection_string='XXX',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'my_azure_cred', 'azure'),\nclassmethod create_snowflake_key_pair(name, user=None, private_key=None, passphrase=None, config_id=None, description=None)\nCreates the Snowflake Key Pair credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\nuser (Optional[str]) – the Snowflake login name\nprivate_key (Optional[str]) – the private key copied exactly from user private key file. Since it contains multiple\nlines, when assign to a variable, put the key string inside triple-quotes\npassphrase (Optional[str]) – the string used to encrypt the private key\nconfig_id (Optional[str]) – The ID of the saved shared secure configuration. If specified, cannot include user,\nprivateKeyStr or passphrase.\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_snowflake_key_pair(\n...     name='key_pair_cred',\n...     user='XXX',\n...     private_key='YYY',\n...     passphrase='ZZZ',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'key_pair_cred', 'snowflake_key_pair_user_account'),\nclassmethod create_databricks_access_token(name, databricks_access_token, description=None)\nCreates the Databricks access token credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\ndatabricks_access_token (Optional[str]) – the Databricks personal access token\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_databricks_access_token(\n...     name='access_token_cred',\n...     databricks_access_token='XXX',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'access_token_cred', 'databricks_access_token_account'),\nclassmethod create_databricks_service_principal(name, client_id=None, client_secret=None, config_id=None, description=None)\nCreates the Databricks access token credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\nclient_id (Optional[str]) – the client ID for Databricks Service Principal\nclient_secret (Optional[str]) – the client secret for Databricks Service Principal\nconfig_id (Optional[str]) – The ID of the saved shared secure configuration. If specified, cannot include clientId\nand clientSecret.\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_databricks_service_principal(\n...     name='svc_principal_cred',\n...     client_id='XXX',\n...     client_secret='XXX',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'svc_principal_cred', 'databricks_service_principal_account'),\nclassmethod create_azure_service_principal(name, client_id=None, client_secret=None, azure_tenant_id=None, config_id=None, description=None)\nCreates the Azure service principal credentials.\nParameters:\nname (str) – The name to use for these credentials.\nclient_id (Optional[str]) – The client ID.\nclient_secret (Optional[str]) – The client secret.\nazure_tenant_id (Optional[str]) – The Azure tenant ID.\nconfig_id (Optional[str]) – The ID of the saved secure configuration. If specified, the ID cannot include clientId,\nclientSecret, or azureTenantId.\ndescription (Optional[str]) – The description to use for these credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_azure_service_principal(\n...     name='my_azure_service_principal_cred',\n...     client_id='XXX',\n...     client_secret='YYY',\n...     azure_tenant_id='ZZZ',\n... )\n>>> cred\nCredential('66c9172d8b7a361cda126f5c', 'my_azure_service_principal_cred', 'azure_service_principal')\nclassmethod create_adls_oauth(name, client_id=None, client_secret=None, oauth_scopes=None, config_id=None, description=None)\nCreates the ADLS OAuth credentials.\nParameters:\nname (str) – The name to use for these credentials.\nclient_id (Optional[str]) – The client ID.\nclient_secret (Optional[str]) – The client secret.\noauth_scopes (List[str], optional) – The OAuth scopes.\nconfig_id (Optional[str]) – The ID of the saved shared secure configuration. If specified, cannot include clientId,\nclientSecret, or oauthScopes.\ndescription (Optional[str]) – The description to use for the ADLS OAuth credentials.\nReturns:\ncredential – The created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_adls_oauth(\n...     name='my_adls_oauth_cred',\n...     client_id='XXX',\n...     client_secret='YYY',\n...     oauth_scopes=['ZZZ'],\n... )\n>>> cred\nCredential('66c91e0f03010d4790735220', 'my_adls_oauth_cred', 'adls_gen2_oauth')\nclassmethod create_gcp(name, gcp_key=None, description=None)\nCreates the GCP credentials.\nParameters:\nname (str) – the name to use for this set of credentials.\ngcp_key (str | dict) – the GCP key in json format or parsed as dict.\ndescription (Optional[str]) – the description to use for this set of credentials.\nReturns:\ncredential – the created credential.\nReturn type:\nCredential\nExamples\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_gcp(\n...     name='my_gcp_cred',\n...     gcp_key='XXX',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'my_gcp_cred', 'gcp'),\nupdate(name=None, description=None, **kwargs)\nUpdate the credential values of an existing credential. Updates this object in place.\nAdded in version v3.2.\nParameters:\nname (str) – The name to use for this set of credentials.\ndescription (Optional[str]) – The description to use for this set of credentials; if omitted, and name is not\nomitted, then it clears any previous description for that name.\nkwargs (Keyword arguments specific to the given credential_type that should be updated.)\nReturn type:\nNone",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/credentials.html",
      "tags": [
        "beginner",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/credentials.html",
        "content_length": 9983
      },
      "code_examples": [],
      "api_methods": [
        "dr.credential.list",
        "dr.credential.get",
        "dr.credential.create_azure_service_principal",
        "dr.credential.create_snowflake_key_pair",
        "dr.credential.create_databricks_access_token",
        "dr.credential.create_databricks_service_principal",
        "dr.credential.create_s3",
        "dr.credential.create_gcp",
        "dr.credential.create_basic",
        "dr.credential.create_oauth",
        "dr.credential.create_adls_oauth",
        "datarobot.models.credential",
        "dr.credential.create_azure"
      ],
      "complexity_score": 0.6000000000000001,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-551006351488834207",
      "title": "Custom metrics",
      "content": "Custom metrics\nclass datarobot.models.deployment.custom_metrics.CustomMetric\nA DataRobot custom metric.\nAdded in version v3.4.\nVariables:\nid (str) – The ID of the custom metric.\ndeployment_id (str) – The ID of the deployment.\nname (str) – The name of the custom metric.\nunits (str) – The units, or the y-axis label, of the given custom metric.\nbaseline_values (BaselinesValues) – The baseline value used to add “reference dots” to the values over time chart.\nis_model_specific (bool) – Determines whether the metric is related to the model or deployment.\ntype (CustomMetricAggregationType) – The aggregation type of the custom metric.\ndirectionality (CustomMetricDirectionality) – The directionality of the custom metric.\ntime_step (CustomMetricBucketTimeStep) – Custom metric time bucket size.\ndescription (str) – A description of the custom metric.\nassociation_id (DatasetColumn) – A custom metric association_id column source when reading values from columnar dataset.\ntimestamp (DatasetColumn) – A custom metric timestamp column source when reading values from columnar dataset.\nvalue (DatasetColumn) – A custom metric value source when reading values from columnar dataset.\nsample_count (DatasetColumn) – A custom metric sample source when reading values from columnar dataset.\nbatch (str) – A custom metric batch ID source when reading values from columnar dataset.\nclassmethod create(name, deployment_id, units, is_model_specific, aggregation_type, time_step='hour', directionality=None, description=None, baseline_value=None, value_column_name=None, sample_count_column_name=None, timestamp_column_name=None, timestamp_format=None, batch_column_name=None, categories=None, is_geospatial=None, geospatial_segment_attribute=None)\nCreate a custom metric for a deployment\nParameters:\nname (str) – The name of the custom metric.\ndeployment_id (str) – The id of the deployment.\nunits (str) – The units, or the y-axis label, of the given custom metric.\nbaseline_value (float) – The baseline value used to add “reference dots” to the values over time chart.\nis_model_specific (bool) – Determines whether the metric is related to the model or deployment.\naggregation_type (CustomMetricAggregationType) – The aggregation type of the custom metric.\ndirectionality (CustomMetricDirectionality) – The directionality of the custom metric.\ntime_step (CustomMetricBucketTimeStep) – Custom metric time bucket size.\ndescription (Optional[str]) – A description of the custom metric.\nvalue_column_name (Optional[str]) – A custom metric value column name when reading values from columnar dataset.\nsample_count_column_name (Optional[str]) – Points to a weight column name if users provide pre-aggregated metric values from columnar dataset.\ntimestamp_column_name (Optional[str]) – A custom metric timestamp column name when reading values from columnar dataset.\ntimestamp_format (Optional[str]) – A custom metric timestamp format when reading values from columnar dataset.\nbatch_column_name (Optional[str]) – A custom metric batch ID column name when reading values from columnar dataset.\nis_geospatial (Optional[bool]) – Determines whether the metric is geospatial or not.\ngeospatial_segment_attribute (Optional[str]) – The name of  the geospatial segment attribute.\nReturns:\nThe custom metric object.\nReturn type:\nCustomMetric\nExamples\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import CustomMetricAggregationType, CustomMetricDirectionality\ncustom_metric = CustomMetric.create(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nname=\"Sample metric\",\nunits=\"Y\",\nbaseline_value=12,\nis_model_specific=True,\naggregation_type=CustomMetricAggregationType.AVERAGE,\ndirectionality=CustomMetricDirectionality.HIGHER_IS_BETTER\n)\nclassmethod get(deployment_id, custom_metric_id)\nGet a custom metric for a deployment\nParameters:\ndeployment_id (str) – The ID of the deployment.\ncustom_metric_id (str) – The ID of the custom metric.\nReturns:\nThe custom metric object.\nReturn type:\nCustomMetric\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\ncustom_metric.id\n>>>'65f17bdcd2d66683cdfc1113'\nclassmethod list(deployment_id)\nList all custom metrics for a deployment\nParameters:\ndeployment_id (str) – The ID of the deployment.\nReturns:\ncustom_metrics – A list of custom metrics objects.\nReturn type:\nlist\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metrics = CustomMetric.list(deployment_id=\"5c939e08962d741e34f609f0\")\ncustom_metrics[0].id\n>>>'65f17bdcd2d66683cdfc1113'\nclassmethod delete(deployment_id, custom_metric_id)\nDelete a custom metric associated with a deployment.\nParameters:\ndeployment_id (str) – The ID of the deployment.\ncustom_metric_id (str) – The ID of the custom metric.\nReturn type:\nNone\nExamples\nfrom datarobot.models.deployment import CustomMetric\nCustomMetric.delete(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\nupdate(name=None, units=None, aggregation_type=None, directionality=None, time_step=None, description=None, baseline_value=None, value_column_name=None, sample_count_column_name=None, timestamp_column_name=None, timestamp_format=None, batch_column_name=None)\nUpdate metadata of a custom metric\nParameters:\nname (Optional[str]) – The name of the custom metric.\nunits (Optional[str]) – The units, or the y-axis label, of the given custom metric.\nbaseline_value (Optional[float]) – The baseline value used to add “reference dots” to the values over time chart.\naggregation_type (Optional[CustomMetricAggregationType]) – The aggregation type of the custom metric.\ndirectionality (Optional[CustomMetricDirectionality]) – The directionality of the custom metric.\ntime_step (Optional[CustomMetricBucketTimeStep]) – Custom metric time bucket size.\ndescription (Optional[str]) – A description of the custom metric.\nvalue_column_name (Optional[str]) – A custom metric value column name when reading values from columnar dataset.\nsample_count_column_name (Optional[str]) – Points to a weight column name if users provide pre-aggregated metric values from columnar dataset.\ntimestamp_column_name (Optional[str]) – A custom metric timestamp column name when reading values from columnar dataset.\ntimestamp_format (Optional[str]) – A custom metric timestamp format when reading values from columnar dataset.\nbatch_column_name (Optional[str]) – A custom metric batch ID column name when reading values from columnar dataset.\nReturns:\nThe custom metric object.\nReturn type:\nCustomMetric\nExamples\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import CustomMetricAggregationType, CustomMetricDirectionality\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\ncustom_metric = custom_metric.update(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nname=\"Sample metric\",\nunits=\"Y\",\nbaseline_value=12,\nis_model_specific=True,\naggregation_type=CustomMetricAggregationType.AVERAGE,\ndirectionality=CustomMetricDirectionality.HIGHER_IS_BETTER\n)\nunset_baseline()\nUnset the baseline value of a custom metric\nReturn type:\nNone\nExamples\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import CustomMetricAggregationType, CustomMetricDirectionality\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\ncustom_metric.baseline_values\n>>> [{'value': 12.0}]\ncustom_metric.unset_baseline()\ncustom_metric.baseline_values\n>>> []\nsubmit_values(data, model_id=None, model_package_id=None, dry_run=False)\nSubmit aggregated custom metrics values from JSON.\nParameters:\ndata (pd.DataFrame or List[CustomMetricBucket]) – The data containing aggregated custom metric values.\nmodel_id (Optional[str]) – For a model metric: the ID of the associated champion/challenger model, used to update the metric values.\nFor a deployment metric: the ID of the model is not needed.\nmodel_package_id (Optional[str]) – For a model metric: the ID of the associated champion/challenger model, used to update the metric values.\nFor a deployment metric: the ID of the model package is not needed.\ndry_run (Optional[bool]) – Specifies whether or not metric data is submitted in production mode (where data is saved).\nReturn type:\nNone\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\n# data for values over time\ndata = [{\n'value': 12,\n'sample_size': 3,\n'timestamp': '2024-03-15T14:00:00'\n}]\n# data witch association ID\ndata = [{\n'value': 12,\n'sample_size': 3,\n'timestamp': '2024-03-15T14:00:00',\n'association_id': '65f44d04dbe192b552e752ed'\n}]\n# data for batches\ndata = [{\n'value': 12,\n'sample_size': 3,\n'batch': '65f44c93fedc5de16b673a0d'\n}]\n# for deployment specific metrics\ncustom_metric.submit_values(data=data)\n# for model specific metrics pass model_package_id or model_id\ncustom_metric.submit_values(data=data, model_package_id=\"6421df32525c58cc6f991f25\")\n# dry run\ncustom_metric.submit_values(data=data, model_package_id=\"6421df32525c58cc6f991f25\", dry_run=True)\nsubmit_single_value(value, model_id=None, model_package_id=None, dry_run=False, segments=None)\nSubmit a single custom metric value at the current moment.\nParameters:\nvalue (float) – Single numeric custom metric value.\nmodel_id (Optional[str]) – For a model metric: the ID of the associated champion/challenger model, used to update the metric values.\nFor a deployment metric: the ID of the model is not needed.\nmodel_package_id (Optional[str]) – For a model metric: the ID of the associated champion/challenger model, used to update the metric values.\nFor a deployment metric: the ID of the model package is not needed.\ndry_run (Optional[bool]) – Specifies whether or not metric data is submitted in production mode (where data is saved).\nsegments (Optional[CustomMetricSegmentFromJSON]) – A list of segments for a custom metric used in segmented analysis.\nReturn type:\nNone\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\n# for deployment specific metrics\ncustom_metric.submit_single_value(value=121)\n# for model specific metrics pass model_package_id or model_id\ncustom_metric.submit_single_value(value=121, model_package_id=\"6421df32525c58cc6f991f25\")\n# dry run\ncustom_metric.submit_single_value(value=121, model_package_id=\"6421df32525c58cc6f991f25\", dry_run=True)\n# for segmented analysis\nsegments = [{\"name\": \"custom_seg\", \"value\": \"val_1\"}]\ncustom_metric.submit_single_value(value=121, model_package_id=\"6421df32525c58cc6f991f25\", segments=segments)\nsubmit_values_from_catalog(dataset_id, model_id=None, model_package_id=None, batch_id=None, segments=None, geospatial=None)\nSubmit aggregated custom metrics values from dataset (AI catalog).\nThe names of the columns in the dataset should correspond to the names of the columns that were defined in\nthe custom metric. In addition, the format of the timestamps should also be the same as defined in the metric.\nParameters:\ndataset_id (str) – The ID of the source dataset.\nmodel_id (Optional[str]) – For a model metric: the ID of the associated champion/challenger model, used to update the metric values.\nFor a deployment metric: the ID of the model is not needed.\nmodel_package_id (Optional[str]) – For a model metric: the ID of the associated champion/challenger model, used to update the metric values.\nFor a deployment metric: the ID of the model package is not needed.\nbatch_id (Optional[str]) – Specifies a batch ID associated with all values provided by this dataset, an alternative\nto providing batch IDs as a column within a dataset (at the record level).\nsegments (Optional[CustomMetricSegmentFromDataset]) – A list of segments for a custom metric used in segmented analysis.\ngeospatial (Optional[Geospatial]) – A geospatial column source when reading values from columnar dataset.\nReturn type:\nNone\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\n# for deployment specific metrics\ncustom_metric.submit_values_from_catalog(dataset_id=\"61093144cabd630828bca321\")\n# for model specific metrics pass model_package_id or model_id\ncustom_metric.submit_values_from_catalog(\ndataset_id=\"61093144cabd630828bca321\",\nmodel_package_id=\"6421df32525c58cc6f991f25\"\n)\n# for segmented analysis\nsegments = [{\"name\": \"custom_seg\", \"column\": \"column_with_segment_values\"}]\ncustom_metric.submit_values_from_catalog(\ndataset_id=\"61093144cabd630828bca321\",\nmodel_package_id=\"6421df32525c58cc6f991f25\",\nsegments=segments\n)\nget_values_over_time(start, end, model_package_id=None, model_id=None, segment_attribute=None, segment_value=None, bucket_size='P7D')\nRetrieve values of a single custom metric over a time period.\nParameters:\nstart (datetime or str) – Start of the time period.\nend (datetime or str) – End of the time period.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nbucket_size (Optional[str]) – Time duration of a bucket, in ISO 8601 time duration format.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_over_time – The queried custom metric values over time information.\nReturn type:\nCustomMetricValuesOverTime\nExamples\nfrom datarobot.models.deployment import CustomMetric\nfrom datetime import datetime, timedelta\nnow=datetime.now()\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\nvalues_over_time = custom_metric.get_values_over_time(start=now - timedelta(days=7), end=now)\nvalues_over_time.bucket_values\n>>> {datetime.datetime(2024, 3, 22, 14, 0, tzinfo=tzutc()): 1.0,\n>>> datetime.datetime(2024, 3, 22, 15, 0, tzinfo=tzutc()): 123.0}}\nvalues_over_time.bucket_sample_sizes\n>>> {datetime.datetime(2024, 3, 22, 14, 0, tzinfo=tzutc()): 1,\n>>>  datetime.datetime(2024, 3, 22, 15, 0, tzinfo=tzutc()): 1}}\nvalues_over_time.get_buckets_as_dataframe()\n>>>                        start                       end  value  sample_size\n>>> 0  2024-03-21 16:00:00+00:00 2024-03-21 17:00:00+00:00    NaN          NaN\n>>> 1  2024-03-21 17:00:00+00:00 2024-03-21 18:00:00+00:00    NaN          NaN\nget_values_over_space(start=None, end=None, model_id=None, model_package_id=None)\nRetrieve values of a custom metric over space.\nParameters:\nstart (Optional[datetime]) – Start of the time period.\nend (Optional[datetime]) – End of the time period.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nReturns:\ncustom_metric_over_space – The queried custom metric values over space information.\nReturn type:\nCustomMetricValuesOverSpace\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\nvalues_over_space = custom_metric.get_values_over_space(model_package_id='6421df32525c58cc6f991f25')\nget_summary(start, end, model_package_id=None, model_id=None, segment_attribute=None, segment_value=None)\nRetrieve the summary of a custom metric over a time period.\nParameters:\nstart (datetime or str) – Start of the time period.\nend (datetime or str) – End of the time period.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_summary – The summary of the custom metric.\nReturn type:\nCustomMetricSummary\nExamples\nfrom datarobot.models.deployment import CustomMetric\nfrom datetime import datetime, timedelta\nnow=datetime.now()\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\nsummary = custom_metric.get_summary(start=now - timedelta(days=7), end=now)\nprint(summary)\n>> \"CustomMetricSummary(2024-03-21 15:52:13.392178+00:00 - 2024-03-22 15:52:13.392168+00:00:\n{'id': '65fd9b1c0c1a840bc6751ce0', 'name': 'Test METRIC', 'value': 215.0, 'sample_count': 13,\n'baseline_value': 12.0, 'percent_change': 24.02})\"\nget_values_over_batch(batch_ids=None, model_package_id=None, model_id=None, segment_attribute=None, segment_value=None)\nRetrieve values of a single custom metric over batches.\nParameters:\nbatch_ids (Optional[List[str]]) – Specify a list of batch IDs to pull the data for.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_over_batch – The queried custom metric values over batch information.\nReturn type:\nCustomMetricValuesOverBatch\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\n# all batch metrics all model specific\nvalues_over_batch = custom_metric.get_values_over_batch(model_package_id='6421df32525c58cc6f991f25')\nvalues_over_batch.bucket_values\n>>> {'6572db2c9f9d4ad3b9de33d0': 35.0, '6572db2c9f9d4ad3b9de44e1': 105.0}\nvalues_over_batch.bucket_sample_sizes\n>>> {'6572db2c9f9d4ad3b9de33d0': 6, '6572db2c9f9d4ad3b9de44e1': 8}\nvalues_over_batch.get_buckets_as_dataframe()\n>>>                    batch_id                     batch_name  value  sample_size\n>>> 0  6572db2c9f9d4ad3b9de33d0  Batch 1 - 03/26/2024 13:04:46   35.0            6\n>>> 1  6572db2c9f9d4ad3b9de44e1  Batch 2 - 03/26/2024 13:06:04  105.0            8\nget_batch_summary(batch_ids=None, model_package_id=None, model_id=None, segment_attribute=None, segment_value=None)\nRetrieve the summary of a custom metric over a batch.\nParameters:\nbatch_ids (Optional[List[str]]) – Specify a list of batch IDs to pull the data for.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_summary – The batch summary of the custom metric.\nReturn type:\nCustomMetricBatchSummary\nExamples\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\n# all batch metrics all model specific\nbatch_summary = custom_metric.get_batch_summary(model_package_id='6421df32525c58cc6f991f25')\nprint(batch_summary)\n>> CustomMetricBatchSummary({'id': '6605396413434b3a7b74342c', 'name': 'batch metric', 'value': 41.25,\n'sample_count': 28, 'baseline_value': 123.0, 'percent_change': -66.46})\nclass datarobot.models.deployment.custom_metrics.CustomMetricValuesOverTime\nCustom metric over time information.\nAdded in version v3.4.\nVariables:\nbuckets (List[Bucket]) – A list of bucketed time periods and the custom metric values aggregated over that period.\nsummary (Summary) – The summary of values over time retrieval.\nmetric (Dict) – A custom metric definition.\ndeployment_id (str) – The ID of the deployment.\nsegment_attribute (str) – The name of the segment on which segment analysis is being performed.\nsegment_value (str) – The value of the segment_attribute to segment on.\nclassmethod get(deployment_id, custom_metric_id, start, end, model_id=None, model_package_id=None, segment_attribute=None, segment_value=None, bucket_size='P7D')\nRetrieve values of a single custom metric over a time period.\nParameters:\ncustom_metric_id (str) – The ID of the custom metric.\ndeployment_id (str) – The ID of the deployment.\nstart (datetime or str) – Start of the time period.\nend (datetime or str) – End of the time period.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nbucket_size (Optional[str]) – Time duration of a bucket, in ISO 8601 time duration format.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_over_time – The queried custom metric values over time information.\nReturn type:\nCustomMetricValuesOverTime\nproperty bucket_values: Dict[datetime, int]\nThe metric value for all time buckets, keyed by start time of the bucket.\nReturns:\nbucket_values\nReturn type:\nDict\nproperty bucket_sample_sizes: Dict[datetime, int]\nThe sample size for all time buckets, keyed by start time of the bucket.\nReturns:\nbucket_sample_sizes\nReturn type:\nDict\nget_buckets_as_dataframe()\nRetrieves all custom metrics buckets in a pandas DataFrame.\nReturns:\nbuckets\nReturn type:\npd.DataFrame\nclass datarobot.models.deployment.custom_metrics.CustomMetricSummary\nThe summary of a custom metric.\nAdded in version v3.4.\nVariables:\nperiod (Period) – A time period defined by a start and end tie\nmetric (Dict) – The summary of the custom metric.\nclassmethod get(deployment_id, custom_metric_id, start, end, model_id=None, model_package_id=None, segment_attribute=None, segment_value=None)\nRetrieve the summary of a custom metric over a time period.\nParameters:\ncustom_metric_id (str) – The ID of the custom metric.\ndeployment_id (str) – The ID of the deployment.\nstart (datetime or str) – Start of the time period.\nend (datetime or str) – End of the time period.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_summary – The summary of the custom metric.\nReturn type:\nCustomMetricSummary\nclass datarobot.models.deployment.custom_metrics.CustomMetricValuesOverBatch\nCustom metric over batch information.\nAdded in version v3.4.\nVariables:\nbuckets (List[BatchBucket]) – A list of buckets with custom metric values aggregated over batches.\nmetric (Dict) – A custom metric definition.\ndeployment_id (str) – The ID of the deployment.\nsegment_attribute (str) – The name of the segment on which segment analysis is being performed.\nsegment_value (str) – The value of the segment_attribute to segment on.\nclassmethod get(deployment_id, custom_metric_id, batch_ids=None, model_id=None, model_package_id=None, segment_attribute=None, segment_value=None)\nRetrieve values of a single custom metric over batches.\nParameters:\ncustom_metric_id (str) – The ID of the custom metric.\ndeployment_id (str) – The ID of the deployment.\nbatch_ids (Optional[List[str]]) – Specify a list of batch IDs to pull the data for.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_over_batch – The queried custom metric values over batch information.\nReturn type:\nCustomMetricValuesOverBatch\nproperty bucket_values: Dict[str, int]\nThe metric value for all batch buckets, keyed by batch ID\nReturns:\nbucket_values\nReturn type:\nDict\nproperty bucket_sample_sizes: Dict[str, int]\nThe sample size for all batch buckets, keyed by batch ID.\nReturns:\nbucket_sample_sizes\nReturn type:\nDict\nget_buckets_as_dataframe()\nRetrieves all custom metrics buckets in a pandas DataFrame.\nReturns:\nbuckets\nReturn type:\npd.DataFrame\nclass datarobot.models.deployment.custom_metrics.CustomMetricBatchSummary\nThe batch summary of a custom metric.\nAdded in version v3.4.\nVariables:\nmetric (Dict) – The summary of the batch custom metric.\nclassmethod get(deployment_id, custom_metric_id, batch_ids=None, model_id=None, model_package_id=None, segment_attribute=None, segment_value=None)\nRetrieve the summary of a custom metric over a batch.\nParameters:\ncustom_metric_id (str) – The ID of the custom metric.\ndeployment_id (str) – The ID of the deployment.\nbatch_ids (Optional[List[str]]) – Specify a list of batch IDs to pull the data for.\nmodel_id (Optional[str]) – The ID of the model.\nmodel_package_id (Optional[str]) – The ID of the model package.\nsegment_attribute (Optional[str]) – The name of the segment on which segment analysis is being performed.\nsegment_value (Optional[str]) – The value of the segment_attribute to segment on.\nReturns:\ncustom_metric_summary – The batch summary of the custom metric.\nReturn type:\nCustomMetricBatchSummary\nclass datarobot.models.deployment.custom_metrics.HostedCustomMetricTemplate\nTemplate for hosted custom metric.\nclassmethod list(search=None, order_by=None, metric_type=None, offset=None, limit=None)\nList all hosted custom metric templates.\nParameters:\nsearch (Optional[str]) – Search string.\norder_by (Optional[ListHostedCustomMetricTemplatesSortQueryParams]) – Ordering field.\nmetric_type (Optional[HostedCustomMetricsTemplateMetricTypeQueryParams]) – Type of the metric.\noffset (Optional[int]) – Offset for pagination.\nlimit (Optional[int]) – Limit for pagination.\nReturns:\ntemplates\nReturn type:\nList[HostedCustomMetricTemplate]\nclassmethod get(template_id)\nGet a hosted custom metric template by ID.\nParameters:\ntemplate_id (str) – ID of the template.\nReturns:\ntemplate\nReturn type:\nHostedCustomMetricTemplate\nclass datarobot.models.deployment.custom_metrics.HostedCustomMetric\nHosted custom metric.\nclassmethod list(job_id, skip=None, limit=None)\nList all hosted custom metrics for a job.\nParameters:\njob_id (str) – ID of the job.\nReturns:\nmetrics\nReturn type:\nList[HostedCustomMetric]\nclassmethod create_from_template(template_id, deployment_id, job_name, custom_metric_name, job_description=None, custom_metric_description=None, sidecar_deployment_id=None, baseline_value=None, timestamp=None, value=None, sample_count=None, batch=None, schedule=None, parameter_overrides=None)\nCreate a hosted custom metric from a template.\nA shortcut for 2 calls:\nJob.from_custom_metric_template(template_id)\nHostedCustomMetrics.create_from_custom_job()\nParameters:\ntemplate_id (str) – ID of the template.\ndeployment_id (str) – ID of the deployment.\njob_name (str) – Name of the job.\ncustom_metric_name (str) – Name of the metric.\njob_description (Optional[str]) – Description of the job.\ncustom_metric_description (Optional[str]) – Description of the metric.\nsidecar_deployment_id (Optional[str]) – ID of the sidecar deployment.\nbaseline_value (Optional[float]) – Baseline value.\ntimestamp (Optional[MetricTimestampSpoofing]) – Timestamp details.\nvalue (Optional[ValueField]) – Value details.\nsample_count (Optional[SampleCountField]) – Sample count details.\nbatch (Optional[BatchField]) – Batch details.\nschedule (Optional[Schedule]) – Schedule details.\nparameter_overrides (Optional[List[RuntimeParameterValue]]) – Parameter overrides.\nReturns:\nmetric\nReturn type:\nHostedCustomMetric\nclassmethod create_from_custom_job(custom_job_id, deployment_id, name, description=None, baseline_value=None, timestamp=None, value=None, sample_count=None, batch=None, schedule=None, parameter_overrides=None, geospatial_segment_attribute=None)\nCreate a hosted custom metric from existing custom job.\nParameters:\ncustom_job_id (str) – ID of the custom job.\ndeployment_id (str) – ID of the deployment.\nname (str) – Name of the metric.\ndescription (Optional[str]) – Description of the metric.\nbaseline_value (Optional[float]) – Baseline value.\ntimestamp (Optional[MetricTimestampSpoofing]) – Timestamp details.\nvalue (Optional[ValueField]) – Value details.\nsample_count (Optional[SampleCountField]) – Sample count details.\nbatch (Optional[BatchField]) – Batch details.\nschedule (Optional[Schedule]) – Schedule details.\nparameter_overrides (Optional[List[RuntimeParameterValue]]) – Parameter overrides.\ngeospatial_segment_attribute (Optional[str]) – The name of the geospatial segment attribute. Only applicable for geospatial custom metrics.\nReturns:\nmetric\nReturn type:\nHostedCustomMetric\nupdate(name=None, description=None, units=None, directionality=None, aggregation_type=None, baseline_value=None, timestamp=None, value=None, sample_count=None, batch=None, schedule=None, parameter_overrides=None)\nUpdate the hosted custom metric.\nParameters:\nname (Optional[str]) – Name of the metric.\ndescription (Optional[str]) – Description of the metric.\nunits (Optional[str]) – Units of the metric.\ndirectionality (Optional[str]) – Directionality of the metric.\naggregation_type (Optional[CustomMetricAggregationType]) – Aggregation type of the metric.\nbaseline_value (Optional[float]) – Baseline values.\ntimestamp (Optional[MetricTimestampSpoofing]) – Timestamp details.\nvalue (Optional[ValueField]) – Value details.\nsample_count (Optional[SampleCountField]) – Sample count details.\nbatch (Optional[BatchField]) – Batch details.\nschedule (Optional[Schedule]) – Schedule details.\nparameter_overrides (Optional[List[RuntimeParameterValue]]) – Parameter overrides.\nReturns:\nmetric\nReturn type:\nHostedCustomMetric\ndelete()\nDelete the hosted custom metric.\nReturn type:\nNone\nclass datarobot.models.deployment.custom_metrics.DeploymentDetails\nInformation about a hosted custom metric deployment.\nclass datarobot.models.deployment.custom_metrics.MetricBaselineValue\nThe baseline values for a custom metric.\nclass datarobot.models.deployment.custom_metrics.SampleCountField\nA weight column used with columnar datasets if pre-aggregated metric values are provided.\nclass datarobot.models.deployment.custom_metrics.ValueField\nA custom metric value source for when reading values from a columnar dataset like a file.\nclass datarobot.models.deployment.custom_metrics.MetricTimestampSpoofing\nCustom metric timestamp spoofing. Occurs when reading values from a file, like a dataset.\nBy default, replicates pd.to_datetime formatting behavior.\nclass datarobot.models.deployment.custom_metrics.BatchField\nA custom metric batch ID source for when reading values from a columnar dataset like a file.\nclass datarobot.models.deployment.custom_metrics.HostedCustomMetricBlueprint\nHosted custom metric blueprints provide an option to share custom metric settings between multiple\ncustom metrics sharing the same custom jobs. When a custom job of a hosted custom metric type is connected to the\ndeployment, all the custom metric parameters from the blueprint are automatically copied.\nclassmethod get(custom_job_id)\nGet a hosted custom metric blueprint.\nParameters:\ncustom_job_id (str) – ID of the custom job.\nReturns:\nblueprint\nReturn type:\nHostedCustomMetricBlueprint\nclassmethod create(custom_job_id, directionality, units, type, time_step, is_model_specific, is_geospatial=None)\nCreate a hosted custom metric blueprint.\nParameters:\ncustom_job_id (str) – ID of the custom job.\ndirectionality (str) – Directionality of the metric.\nunits (str) – Units of the metric.\ntype (str) – Type of the metric.\ntime_step (str) – Time step of the metric.\nis_model_specific (bool) – Whether the metric is model specific.\nis_geospatial (Optional[bool]) – Determines whether the metric is geospatial.\nReturns:\nblueprint\nReturn type:\nHostedCustomMetricBlueprint\nupdate(directionality=None, units=None, type=None, time_step=None, is_model_specific=None, is_geospatial=None)\nUpdate a hosted custom metric blueprint.\nParameters:\ndirectionality (Optional[str]) – Directionality of the metric.\nunits (Optional[str]) – Units of the metric.\ntype (Optional[str]) – Type of the metric.\ntime_step (Optional[str]) – Time step of the metric.\nis_model_specific (Optional[bool]) – Determines whether the metric is model specific.\nis_geospatial (Optional[bool]) – Determines whether the metric is geospatial.\nReturns:\nupdated_blueprint\nReturn type:\nHostedCustomMetricBlueprint\nclass datarobot.models.deployment.custom_metrics.CustomMetricValuesOverSpace\nCustom metric values over space.\nAdded in version v3.7.\nVariables:\nbuckets (List[BatchBucket]) – A list of buckets with custom metric values aggregated over geospatial hexagons.\nmetric (Dict) – A custom metric definition.\nmodel_id (str) – The ID of the model.\nmodel_package_id (str) – The ID of the model package (also known as registered model version id).\nsummary (Dict) – Start-end interval over which data is retrieved.\nclassmethod get(deployment_id, custom_metric_id, start=None, end=None, model_package_id=None, model_id=None)\nRetrieve custom metric values over space.\nParameters:\ndeployment_id (str) – The id of the deployment.\ncustom_metric_id (str) – The id of the custom metric.\nstart (datetime) – The start time of the interval.\nend (datetime) – The end time of the interval.\nmodel_package_id (str) – The id of the model package.\nmodel_id (str) – The id of the model.\nReturns:\nvalues_over_space – Custom metric values over geospatial hexagons.\nReturn type:\nCustomMetricValuesOverSpace",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/custom-metrics.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/custom-metrics.html",
        "content_length": 33584
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.deployment",
        "deployment.custom_metrics"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_7982641439401376495",
      "title": "Custom models",
      "content": "Custom models\nclass datarobot.models.custom_model_version.CustomModelFileItem\nA file item attached to a DataRobot custom model version.\nAdded in version v2.21.\nVariables:\nid (str) – The ID of the file item.\nfile_name (str) – The name of the file item.\nfile_path (str) – The path of the file item.\nfile_source (str) – The source of the file item.\ncreated_at (Optional[str]) – ISO-8601 formatted timestamp of when the version was created.\nclass datarobot.CustomInferenceModel\nA custom inference model.\nAdded in version v2.21.\nVariables:\nid (str) – The ID of the custom model.\nname (str) – The name of the custom model.\nlanguage (str) – The programming language of the custom inference model.\nCan be “python”, “r”, “java” or “other”.\ndescription (str) – The description of the custom inference model.\ntarget_type (datarobot.TARGET_TYPE) – Target type of the custom inference model.\nValues: [datarobot.TARGET_TYPE.BINARY, datarobot.TARGET_TYPE.REGRESSION,\ndatarobot.TARGET_TYPE.MULTICLASS, datarobot.TARGET_TYPE.UNSTRUCTURED,\ndatarobot.TARGET_TYPE.ANOMALY, datarobot.TARGET_TYPE.TEXT_GENERATION]\ntarget_name (str) – Target feature name.\nIt is optional(ignored if provided) for datarobot.TARGET_TYPE.UNSTRUCTURED\nor datarobot.TARGET_TYPE.ANOMALY target type.\nlatest_version (datarobot.CustomModelVersion or None) – The latest version of the custom model if the model has a latest version.\ndeployments_count (int) – Number of a deployments of the custom models.\ntarget_name – The custom model target name.\npositive_class_label (str) – For binary classification projects, a label of a positive class.\nnegative_class_label (str) – For binary classification projects, a label of a negative class.\nprediction_threshold (float) – For binary classification projects, a threshold used for predictions.\ntraining_data_assignment_in_progress (bool) – Flag describing if training data assignment is in progress.\ntraining_dataset_id (Optional[str]) – The ID of a dataset assigned to the custom model.\ntraining_dataset_version_id (Optional[str]) – The ID of a dataset version assigned to the custom model.\ntraining_data_file_name (Optional[str]) – The name of assigned training data file.\ntraining_data_partition_column (Optional[str]) – The name of a partition column in a training dataset assigned to the custom model.\ncreated_by (str) – The username of a user who created the custom model.\nupdated_at (str) – ISO-8601 formatted timestamp of when the custom model was updated\ncreated_at (str) – ISO-8601 formatted timestamp of when the custom model was created\nnetwork_egress_policy (datarobot.NETWORK_EGRESS_POLICY, optional) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE, datarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nmaximum_memory (Optional[int]) – The maximum memory that might be allocated by the custom-model.\nIf exceeded, the custom-model will be killed by k8s.\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed in the cluster\nis_training_data_for_versions_permanently_enabled (Optional[bool]) – Whether training data assignment on the version level is permanently enabled for the model.\nclassmethod list(is_deployed=None, search_for=None, order_by=None)\nList custom inference models available to the user.\nAdded in version v2.21.\nParameters:\nis_deployed (Optional[bool]) – Flag for filtering custom inference models.\nIf set to True, only deployed custom inference models are returned.\nIf set to False, only not deployed custom inference models are returned.\nsearch_for (Optional[str]) – String for filtering custom inference models - only custom\ninference models that contain the string in name or description will\nbe returned.\nIf not specified, all custom models will be returned\norder_by (Optional[str]) – Property to sort custom inference models by.\nSupported properties are “created” and “updated”.\nPrefix the attribute name with a dash to sort in descending order,\ne.g. order_by=’-created’.\nBy default, the order_by parameter is None which will result in\ncustom models being returned in order of creation time descending.\nReturns:\nA list of custom inference models.\nReturn type:\nList[CustomInferenceModel]\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status\ndatarobot.errors.ServerError – If the server responded with 5xx status\nclassmethod get(custom_model_id)\nGet custom inference model by id.\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – The ID of the custom inference model.\nReturns:\nRetrieved custom inference model.\nReturn type:\nCustomInferenceModel\nRaises:\ndatarobot.errors.ClientError – The ID the server responded with 4xx status.\ndatarobot.errors.ServerError – The ID the server responded with 5xx status.\ndownload_latest_version(file_path)\nDownload the latest custom inference model version.\nAdded in version v2.21.\nParameters:\nfile_path (str) – Path to create a file with custom model version content.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nReturn type:\nNone\nclassmethod create(name, target_type, target_name=None, language=None, description=None, positive_class_label=None, negative_class_label=None, prediction_threshold=None, class_labels=None, class_labels_file=None, network_egress_policy=None, maximum_memory=None, replicas=None, is_training_data_for_versions_permanently_enabled=None)\nCreate a custom inference model.\nAdded in version v2.21.\nParameters:\nname (str) – Name of the custom inference model.\ntarget_type (datarobot.TARGET_TYPE) – Target type of the custom inference model.\nValues: [datarobot.TARGET_TYPE.BINARY, datarobot.TARGET_TYPE.REGRESSION,\ndatarobot.TARGET_TYPE.MULTICLASS, datarobot.TARGET_TYPE.UNSTRUCTURED,\ndatarobot.TARGET_TYPE.TEXT_GENERATION]\ntarget_name (Optional[str]) – Target feature name.\nIt is optional(ignored if provided) for datarobot.TARGET_TYPE.UNSTRUCTURED target type.\nlanguage (Optional[str]) – Programming language of the custom learning model.\ndescription (Optional[str]) – Description of the custom learning model.\npositive_class_label (Optional[str]) – Custom inference model positive class label for binary classification.\nnegative_class_label (Optional[str]) – Custom inference model negative class label for binary classification.\nprediction_threshold (Optional[float]) – Custom inference model prediction threshold.\nclass_labels (List[str], optional) – Custom inference model class labels for multiclass classification.\nCannot be used with class_labels_file.\nclass_labels_file (Optional[str]) – Path to file containing newline separated class labels for multiclass classification.\nCannot be used with class_labels.\nnetwork_egress_policy (datarobot.NETWORK_EGRESS_POLICY, optional) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE, datarobot.NETWORK_EGRESS_POLICY.PUBLIC]\nmaximum_memory (Optional[int]) – The maximum memory that might be allocated by the custom-model.\nIf exceeded, the custom-model will be killed by k8s.\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed in the cluster.\nis_training_data_for_versions_permanently_enabled (Optional[bool]) – Permanently enable training data assignment on the version level for the current model,\ninstead of training data assignment on the model level.\nReturns:\nCreated a custom inference model.\nReturn type:\nCustomInferenceModel\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclassmethod copy_custom_model(custom_model_id)\nCreate a custom inference model by copying existing one.\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – The ID of the custom inference model to copy.\nReturns:\nCreated a custom inference model.\nReturn type:\nCustomInferenceModel\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nupdate(name=None, language=None, description=None, target_name=None, positive_class_label=None, negative_class_label=None, prediction_threshold=None, class_labels=None, class_labels_file=None, is_training_data_for_versions_permanently_enabled=None)\nUpdate custom inference model properties.\nAdded in version v2.21.\nParameters:\nname (Optional[str]) – New custom inference model name.\nlanguage (Optional[str]) – New custom inference model programming language.\ndescription (Optional[str]) – New custom inference model description.\ntarget_name (Optional[str]) – New custom inference model target name.\npositive_class_label (Optional[str]) – New custom inference model positive class label.\nnegative_class_label (Optional[str]) – New custom inference model negative class label.\nprediction_threshold (Optional[float]) – New custom inference model prediction threshold.\nclass_labels (List[str], optional) – custom inference model class labels for multiclass classification\nCannot be used with class_labels_file\nclass_labels_file (Optional[str]) – Path to file containing newline separated class labels for multiclass classification.\nCannot be used with class_labels\nis_training_data_for_versions_permanently_enabled (Optional[bool]) – Permanently enable training data assignment on the version level for the current model,\ninstead of training data assignment on the model level.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nReturn type:\nNone\nrefresh()\nUpdate custom inference model with the latest data from server.\n:rtype: None\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\ndelete()\nDelete custom inference model.\n:rtype: None\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nassign_training_data(dataset_id, partition_column=None, max_wait=600)\nAssign training data to the custom inference model.\nAdded in version v2.21.\nParameters:\ndataset_id (str) – The ID of the training dataset to be assigned.\npartition_column (Optional[str]) – The name of a partition column in the training dataset.\nmax_wait (Optional[int]) – The max time to wait for a training data assignment.\nIf set to None, then method will return without waiting.\nDefaults to 10 min.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status\ndatarobot.errors.ServerError – If the server responded with 5xx status\nReturn type:\nNone\nget_access_list()\nRetrieve access control settings of this custom model.\nAdded in version v2.36.\nReturn type:\nlist of SharingAccess\nshare(access_list)\nUpdate the access control settings of this custom model.\nAdded in version v2.36.\nParameters:\naccess_list (list of SharingAccess) – A list of SharingAccess to update.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nNone\nExamples\nTransfer access to the custom model from old_user@datarobot.com to new_user@datarobot.com\nimport datarobot as dr\nnew_access = dr.SharingAccess(new_user@datarobot.com,\ndr.enums.SHARING_ROLE.OWNER, can_share=True)\naccess_list = [dr.SharingAccess(old_user@datarobot.com, None), new_access]\ndr.CustomInferenceModel.get('custom-model-id').share(access_list)\nclass datarobot.CustomModelTest\nAn custom model test.\nAdded in version v2.21.\nVariables:\nid (str) – test id\ncustom_model_image_id (str) – id of a custom model image\nimage_type (str) – the type of the image, either CUSTOM_MODEL_IMAGE_TYPE.CUSTOM_MODEL_IMAGE if the testing\nattempt is using a CustomModelImage as its model or\nCUSTOM_MODEL_IMAGE_TYPE.CUSTOM_MODEL_VERSION if the testing attempt is\nusing a CustomModelVersion with dependency management\noverall_status (str) – a string representing testing status.\nStatus can be\n- ‘not_tested’: the check not run\n- ‘failed’: the check failed\n- ‘succeeded’: the check succeeded\n- ‘warning’: the check resulted in a warning, or in non-critical failure\n- ‘in_progress’: the check is in progress\ndetailed_status (dict) – detailed testing status - maps the testing types to their status and message.\nThe keys of the dict are one of ‘errorCheck’, ‘nullValueImputation’,\n‘longRunningService’, ‘sideEffects’.\nThe values are dict with ‘message’ and ‘status’ keys.\ncreated_by (str) – a user who created a test\ndataset_id (Optional[str]) – id of a dataset used for testing\ndataset_version_id (Optional[str]) – id of a dataset version used for testing\ncompleted_at (Optional[str]) – ISO-8601 formatted timestamp of when the test has completed\ncreated_at (Optional[str]) – ISO-8601 formatted timestamp of when the version was created\nnetwork_egress_policy (datarobot.NETWORK_EGRESS_POLICY, optional) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE, datarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nmaximum_memory (Optional[int]) – The maximum memory that might be allocated by the custom-model.\nIf exceeded, the custom-model will be killed by k8s\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed in the cluster\nclassmethod create(custom_model_id, custom_model_version_id, dataset_id=None, max_wait=600, network_egress_policy=None, maximum_memory=None, replicas=None)\nCreate and start a custom model test.\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – the id of the custom model\ncustom_model_version_id (str) – the id of the custom model version\ndataset_id (Optional[str]) – The id of the testing dataset for non-unstructured custom models.\nIgnored and not required for unstructured models.\nmax_wait (Optional[int]) – max time to wait for a test completion.\nIf set to None - method will return without waiting.\nnetwork_egress_policy (datarobot.NETWORK_EGRESS_POLICY, optional) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE, datarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nmaximum_memory (Optional[int]) – The maximum memory that might be allocated by the custom-model.\nIf exceeded, the custom-model will be killed by k8s\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed in the cluster\nReturns:\ncreated custom model test\nReturn type:\nCustomModelTest\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod list(custom_model_id)\nList custom model tests.\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – the id of the custom model\nReturns:\na list of custom model tests\nReturn type:\nList[CustomModelTest]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(custom_model_test_id)\nGet custom model test by id.\nAdded in version v2.21.\nParameters:\ncustom_model_test_id (str) – the id of the custom model test\nReturns:\nretrieved custom model test\nReturn type:\nCustomModelTest\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nget_log()\nGet log of a custom model test.\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nget_log_tail()\nGet log tail of a custom model test.\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ncancel()\nCancel custom model test that is in progress.\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nrefresh()\nUpdate custom model test with the latest data from server.\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclass datarobot.CustomModelVersion\nA version of a DataRobot custom model.\nAdded in version v2.21.\nVariables:\nid (str) – The ID of the custom model version.\ncustom_model_id (str) – The ID of the custom model.\nversion_minor (int) – A minor version number of the custom model version.\nversion_major (int) – A major version number of the custom model version.\nis_frozen (bool) – A flag if the custom model version is frozen.\nitems (List[CustomModelFileItem]) – A list of file items attached to the custom model version.\nbase_environment_id (str) – The ID of the environment to use with the model.\nbase_environment_version_id (str) – The ID of the environment version to use with the model.\nlabel (Optional[str]) – A short human readable string to label the version.\ndescription (Optional[str]) – The custom model version description.\ncreated_at (Optional[str]) – ISO-8601 formatted timestamp of when the version was created.\ndependencies (List[CustomDependency]) – The parsed dependencies of the custom model version if the\nversion has a valid requirements.txt file.\nnetwork_egress_policy (datarobot.NETWORK_EGRESS_POLICY, optional) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE, datarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nmaximum_memory (Optional[int]) – The maximum memory that might be allocated by the custom-model.\nIf exceeded, the custom-model will be killed by k8s.\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed in the cluster.\nrequired_metadata_values (List[RequiredMetadataValue]) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\ntraining_data (TrainingData, optional) – The information about the training data assigned to the model version.\nholdout_data (HoldoutData, optional) – The information about the holdout data assigned to the model version.\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nCustomModelVersion\nclassmethod create_clean(custom_model_id, base_environment_id=None, is_major_update=True, folder_path=None, files=None, network_egress_policy=None, maximum_memory=None, replicas=None, required_metadata_values=None, training_dataset_id=None, partition_column=None, holdout_dataset_id=None, keep_training_holdout_data=None, max_wait=600, runtime_parameter_values=None, base_environment_version_id=None)\nCreate a custom model version without files from previous versions.\nCreate a version with training or holdout data:\nIf training/holdout data related parameters are provided,\nthe training data is assigned asynchronously.\nIn this case:\n* if max_wait is not None, the function returns once the job is finished.\n* if max_wait is None, the function returns immediately. Progress can be polled by the user (see examples).\nIf training data assignment fails, new version is still created,\nbut it is not allowed to create a model package (version) for the model version and to deploy it.\nTo check for training data assignment error, check version.training_data.assignment_error[“message”].\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\nbase_environment_id (str) – The base environment to use with this model version.\nAt least one of “base_environment_id” and “base_environment_version_id” must be provided.\nIf both are specified, the version must belong to the environment.\nbase_environment_version_id (str) – The base environment version ID to use with this model version.\nAt least one of “base_environment_id” and “base_environment_version_id” must be provided.\nIf both are specified, the version must belong to the environment.\nIf not specified: in case previous model versions exist, the value from the latest model\nversion is inherited, otherwise, latest successfully built version of the environment\nspecified in “base_environment_id” is used.\nis_major_update (Optional[bool]) – The flag defining if a custom model version will be a minor or a major version.\nDefault to True\nfolder_path (Optional[str]) – The path to a folder containing files to be uploaded.\nEach file in the folder is uploaded under path relative to a folder path.\nfiles (Optional[List]) – The list of tuples, where values in each tuple are the local filesystem path and\nthe path the file should be placed in the model.\nIf the list is of strings, then basenames will be used for tuples.\nExample:\n[(“/home/user/Documents/myModel/file1.txt”, “file1.txt”),\n(“/home/user/Documents/myModel/folder/file2.txt”, “folder/file2.txt”)]\nor\n[“/home/user/Documents/myModel/file1.txt”,\n“/home/user/Documents/myModel/folder/file2.txt”]\nnetwork_egress_policy (datarobot.NETWORK_EGRESS_POLICY, optional) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE, datarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nmaximum_memory (Optional[int]) – The maximum memory that might be allocated by the custom-model.\nIf exceeded, the custom-model will be killed by k8s.\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed in the cluster.\nrequired_metadata_values (List[RequiredMetadataValue]) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\ntraining_dataset_id (Optional[str]) – The ID of the training dataset to assign to the custom model.\npartition_column (Optional[str]) – Name of a partition column in a training dataset assigned to the custom model.\nCan only be assigned for structured models.\nholdout_dataset_id (Optional[str]) – The ID of the holdout dataset to assign to the custom model.\nCan only be assigned for unstructured models.\nkeep_training_holdout_data (Optional[bool]) – If the version should inherit training and holdout data from the previous version.\nDefaults to True.\nThis field is only applicable if the model has training data for versions enabled,\notherwise the field value will be ignored.\nmax_wait (Optional[int]) – Max time to wait for training data assignment.\nIf set to None - method will return without waiting.\nDefaults to 10 minutes.\nruntime_parameter_values (List[RuntimeParameterValue]) – Additional parameters to be injected into a model at runtime. The fieldName\nmust match a fieldName that is listed in the runtimeParameterDefinitions section\nof the model-metadata.yaml file.\nReturns:\nCreated custom model version.\nReturn type:\nCustomModelVersion\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\ndatarobot.errors.InvalidUsageError – If wrong parameters are provided.\ndatarobot.errors.TrainingDataAssignmentError – If training data assignment fails.\nExamples\nCreate a version with blocking (default max_wait=600) training data assignment:\nimport datarobot as dr\nfrom datarobot.errors import TrainingDataAssignmentError\ndr.Client(token=my_token, endpoint=endpoint)\ntry:\nversion = dr.CustomModelVersion.create_clean(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\n)\nexcept TrainingDataAssignmentError as e:\nprint(e)\nCreate a version with non-blocking training data assignment:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\nversion = dr.CustomModelVersion.create_clean(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\nmax_wait=None,\n)\nwhile version.training_data.assignment_in_progress:\ntime.sleep(10)\nversion.refresh()\nif version.training_data.assignment_error:\nprint(version.training_data.assignment_error[\"message\"])\nclassmethod create_from_previous(custom_model_id, base_environment_id=None, is_major_update=True, folder_path=None, files=None, files_to_delete=None, network_egress_policy=None, maximum_memory=None, replicas=None, required_metadata_values=None, training_dataset_id=None, partition_column=None, holdout_dataset_id=None, keep_training_holdout_data=None, max_wait=600, runtime_parameter_values=None, base_environment_version_id=None)\nCreate a custom model version containing files from a previous version.\nCreate a version with training/holdout data:\nIf training/holdout data related parameters are provided,\nthe training data is assigned asynchronously.\nIn this case:\n* if max_wait is not None, function returns once job is finished.\n* if max_wait is None, function returns immediately, progress can be polled by the user, see examples.\nIf training data assignment fails, new version is still created,\nbut it is not allowed to create a model package (version) for the model version and to deploy it.\nTo check for training data assignment error, check version.training_data.assignment_error[“message”].\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\nbase_environment_id (str) – The base environment to use with this model version.\nAt least one of “base_environment_id” and “base_environment_version_id” must be provided.\nIf both are specified, the version must belong to the environment.\nbase_environment_version_id (str) – The base environment version ID to use with this model version.\nAt least one of “base_environment_id” and “base_environment_version_id” must be provided.\nIf both are specified, the version must belong to the environment.\nIf not specified: in case previous model versions exist, the value from the latest model\nversion is inherited, otherwise, latest successfully built version of the environment\nspecified in “base_environment_id” is used.\nis_major_update (Optional[bool]) – The flag defining if a custom model version will be a minor or a major version.\nDefaults to True.\nfolder_path (Optional[str]) – The path to a folder containing files to be uploaded.\nEach file in the folder is uploaded under path relative to a folder path.\nfiles (Optional[List]) – The list of tuples, where values in each tuple are the local filesystem path and\nthe path the file should be placed in the model.\nIf list is of strings, then basenames will be used for tuples\nExample:\n[(“/home/user/Documents/myModel/file1.txt”, “file1.txt”),\n(“/home/user/Documents/myModel/folder/file2.txt”, “folder/file2.txt”)]\nor\n[“/home/user/Documents/myModel/file1.txt”,\n“/home/user/Documents/myModel/folder/file2.txt”]\nfiles_to_delete (Optional[List]) – The list of a file items ids to be deleted.\nExample: [“5ea95f7a4024030aba48e4f9”, “5ea6b5da402403181895cc51”]\nnetwork_egress_policy (datarobot.NETWORK_EGRESS_POLICY, optional) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE, datarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nmaximum_memory (Optional[int]) – The maximum memory that might be allocated by the custom-model.\nIf exceeded, the custom-model will be killed by k8s\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed in the cluster\nrequired_metadata_values (List[RequiredMetadataValue]) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\ntraining_dataset_id (Optional[str]) – The ID of the training dataset to assign to the custom model.\npartition_column (Optional[str]) – Name of a partition column in a training dataset assigned to the custom model.\nCan only be assigned for structured models.\nholdout_dataset_id (Optional[str]) – The ID of the holdout dataset to assign to the custom model.\nCan only be assigned for unstructured models.\nkeep_training_holdout_data (Optional[bool]) – If the version should inherit training and holdout data from the previous version.\nDefaults to True.\nThis field is only applicable if the model has training data for versions enabled,\notherwise the field value will be ignored.\nmax_wait (Optional[int]) – Max time to wait for training data assignment.\nIf set to None - method will return without waiting.\nDefaults to 10 minutes.\nruntime_parameter_values (List[RuntimeParameterValue]) – Additional parameters to be injected into the model at runtime. The fieldName\nmust match a fieldName that is listed in the runtimeParameterDefinitions section\nof the model-metadata.yaml file. This list will be merged with any existing\nruntime values set from the prior version, so it is possible to specify a null value\nto unset specific parameters and fall back to the defaultValue from the definition.\nReturns:\ncreated custom model version\nReturn type:\nCustomModelVersion\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\ndatarobot.errors.InvalidUsageError – If wrong parameters are provided.\ndatarobot.errors.TrainingDataAssignmentError – If training data assignment fails.\nExamples\nCreate a version with blocking (default max_wait=600) training data assignment:\nimport datarobot as dr\nfrom datarobot.errors import TrainingDataAssignmentError\ndr.Client(token=my_token, endpoint=endpoint)\ntry:\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\n)\nexcept TrainingDataAssignmentError as e:\nprint(e)\nCreate a version with non-blocking training data assignment:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\nmax_wait=None,\n)\nwhile version.training_data.assignment_in_progress:\ntime.sleep(10)\nversion.refresh()\nif version.training_data.assignment_error:\nprint(version.training_data.assignment_error[\"message\"])\nclassmethod list(custom_model_id)\nList custom model versions.\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\nReturns:\nA list of custom model versions.\nReturn type:\nList[CustomModelVersion]\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclassmethod get(custom_model_id, custom_model_version_id)\nGet custom model version by id.\nAdded in version v2.21.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\ncustom_model_version_id (str) – The id of the custom model version to retrieve.\nReturns:\nRetrieved custom model version.\nReturn type:\nCustomModelVersion\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\ndownload(file_path)\nDownload custom model version.\nAdded in version v2.21.\nParameters:\nfile_path (str) – Path to create a file with custom model version content.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nReturn type:\nNone\nupdate(description=None, required_metadata_values=None)\nUpdate custom model version properties.\nAdded in version v2.21.\nParameters:\ndescription (Optional[str]) – New custom model version description.\nrequired_metadata_values (List[RequiredMetadataValue], optional) – Additional parameters required by the execution environment. The required keys are\ndefined by the fieldNames in the base environment’s requiredMetadataKeys.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nReturn type:\nNone\nrefresh()\nUpdate custom model version with the latest data from server.\n:rtype: None\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nget_feature_impact(with_metadata=False)\nGet custom model feature impact.\nAdded in version v2.23.\nParameters:\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\nReturns:\nfeature_impacts – The feature impact data. Each item is a dict with the keys ‘featureName’,\n‘impactNormalized’, and ‘impactUnnormalized’, and ‘redundantWith’.\nReturn type:\nlist of dict\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\ncalculate_feature_impact(max_wait=600)\nCalculate custom model feature impact.\nAdded in version v2.23.\nParameters:\nmax_wait (Optional[int]) – Max time to wait for feature impact calculation.\nIf set to None - method will return without waiting.\nDefaults to 10 min\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nNone\nclass datarobot.models.execution_environment.RequiredMetadataKey\nDefinition of a metadata key that custom models using this environment must define\nAdded in version v2.25.\nVariables:\nfield_name (str) – The required field key. This value will be added as an environment\nvariable when running custom models.\ndisplay_name (str) – A human readable name for the required field.\nclass datarobot.models.CustomModelVersionConversion\nA conversion of a DataRobot custom model version.\nAdded in version v2.27.\nVariables:\nid (str) – The ID of the custom model version conversion.\ncustom_model_version_id (str) – The ID of the custom model version.\ncreated (str) – ISO-8601 timestamp of when the custom model conversion created.\nmain_program_item_id (str or None) – The ID of the main program item.\nlog_message (str or None) – The conversion output log message.\ngenerated_metadata (dict or None) – The dict contains two items: ‘outputDataset’ & ‘outputColumns’.\nconversion_succeeded (bool) – Whether the conversion succeeded or not.\nconversion_in_progress (bool) – Whether a given conversion is in progress or not.\nshould_stop (bool) – Whether the user asked to stop a conversion.\nclassmethod run_conversion(custom_model_id, custom_model_version_id, main_program_item_id, max_wait=None)\nInitiate a new custom model version conversion.\nParameters:\ncustom_model_id (str) – The associated custom model ID.\ncustom_model_version_id (str) – The associated custom model version ID.\nmain_program_item_id (str) – The selected main program item ID. This should be one of the SAS items in the\nassociated custom model version.\nmax_wait (int or None) – Max wait time in seconds. If None, then don’t wait.\nReturns:\nconversion_id – The ID of the newly created conversion entity.\nReturn type:\nstr\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclassmethod stop_conversion(custom_model_id, custom_model_version_id, conversion_id)\nStop a conversion that is in progress.\nParameters:\ncustom_model_id (str) – The ID of the associated custom model.\ncustom_model_version_id (str) – The ID of the associated custom model version.\nconversion_id (str) – The ID of a conversion that is in-progress.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nReturn type:\nResponse\nclassmethod get(custom_model_id, custom_model_version_id, conversion_id)\nGet custom model version conversion by id.\nAdded in version v2.27.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\ncustom_model_version_id (str) – The ID of the custom model version.\nconversion_id (str) – The ID of the conversion to retrieve.\nReturns:\nRetrieved custom model version conversion.\nReturn type:\nCustomModelVersionConversion\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclassmethod get_latest(custom_model_id, custom_model_version_id)\nGet latest custom model version conversion for a given custom model version.\nAdded in version v2.27.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\ncustom_model_version_id (str) – The ID of the custom model version.\nReturns:\nRetrieved latest conversion for a given custom model version.\nReturn type:\nCustomModelVersionConversion or None\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclassmethod list(custom_model_id, custom_model_version_id)\nGet custom model version conversions list per custom model version.\nAdded in version v2.27.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\ncustom_model_version_id (str) – The ID of the custom model version.\nReturns:\nRetrieved conversions for a given custom model version.\nReturn type:\nList[CustomModelVersionConversion]\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclass datarobot.CustomModelVersionDependencyBuild\nMetadata about a DataRobot custom model version’s dependency build\nAdded in version v2.22.\nVariables:\ncustom_model_id (str) – The ID of the custom model.\ncustom_model_version_id (str) – The ID of the custom model version.\nbuild_status (str) – The status of the custom model version’s dependency build.\nstarted_at (str) – ISO-8601 formatted timestamp of when the build was started.\ncompleted_at (Optional[str]) – ISO-8601 formatted timestamp of when the build has completed.\nclassmethod get_build_info(custom_model_id, custom_model_version_id)\nRetrieve information about a custom model version’s dependency build\nAdded in version v2.22.\nParameters:\ncustom_model_id (str) – The ID of the custom model.\ncustom_model_version_id (str) – The ID of the custom model version.\nReturns:\nThe dependency build information.\nReturn type:\nCustomModelVersionDependencyBuild\nclassmethod start_build(custom_model_id, custom_model_version_id, max_wait=600)\nStart the dependency build for a custom model version  dependency build\nAdded in version v2.22.\nParameters:\ncustom_model_id (str) – The ID of the custom model\ncustom_model_version_id (str) – the ID of the custom model version\nmax_wait (Optional[int]) – Max time to wait for a build completion.\nIf set to None - method will return without waiting.\nReturn type:\nOptional[CustomModelVersionDependencyBuild]\nget_log()\nGet log of a custom model version dependency build.\n:rtype: str\nAdded in version v2.22.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\ncancel()\nCancel custom model version dependency build that is in progress.\n:rtype: None\nAdded in version v2.22.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nrefresh()\nUpdate custom model version dependency build with the latest data from server.\n:rtype: None\nAdded in version v2.22.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclass datarobot.ExecutionEnvironment\nAn execution environment entity.\nAdded in version v2.21.\nVariables:\nid (str) – The ID of the execution environment.\nname (str) – The name of the execution environment.\ndescription (Optional[str]) – The description of the execution environment.\nprogramming_language (Optional[str]) – The programming language of the execution environment.\nCan be “python”, “r”, “java” or “other”.\nis_public (Optional[bool]) – Public accessibility of environment, visible only for admin user.\ncreated_at (Optional[str]) – ISO-8601 formatted timestamp of when the execution environment version was created.\nlatest_version (ExecutionEnvironmentVersion, optional) – The latest version of the execution environment.\nlatest_successful_version (ExecutionEnvironmentVersion, optional) – The latest version of the execution environment, which contains a successfully built image.\nrequired_metadata_keys (Optional[List[RequiredMetadataKey]]) – The definition of metadata keys that custom models using this environment must define.\nuse_cases (Optional[List[str]]) – A list of use-cases this environment may be used for.\nclassmethod create(name, description=None, programming_language=None, required_metadata_keys=None, is_public=None, use_cases=None)\nCreate an execution environment.\nAdded in version v2.21.\nParameters:\nname (str) – execution environment name\ndescription (Optional[str]) – execution environment description\nprogramming_language (Optional[str]) – programming language of the environment to be created.\nCan be “python”, “r”, “java” or “other”. Default value - “other”\nrequired_metadata_keys (List[RequiredMetadataKey]) – Definition of a metadata keys that custom models using this environment must define\nis_public (bool, optional) – public accessibility of environment\nuse_cases (List[str], optional) – List of use-cases this environment may be used for\nReturns:\ncreated execution environment\nReturn type:\nExecutionEnvironment\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod list(search_for=None, is_own=None, use_cases=None, offset=0, limit=0)\nList execution environments available to the user.\nAdded in version v2.21.\nParameters:\nsearch_for (Optional[str]) – the string for filtering execution environment - only execution\nenvironments that contain the string in name or description will\nbe returned.\nis_own (bool, optional) – Only return execution environments that were created by the current user.\nuse_cases (str, optional) – Only return execution environments that contain the specified use case\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 0 to maintain previous behavior.\nThe default on the server is 20, with a maximum of 100.\nReturns:\na list of execution environments.\nReturn type:\nList[ExecutionEnvironment]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(execution_environment_id)\nGet execution environment by its ID.\nAdded in version v2.21.\nParameters:\nexecution_environment_id (str) – ID of the execution environment to retrieve\nReturns:\nretrieved execution environment\nReturn type:\nExecutionEnvironment\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndelete()\nDelete execution environment.\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nupdate(name=None, description=None, required_metadata_keys=None, is_public=None, use_cases=None)\nUpdate execution environment properties.\nAdded in version v2.21.\nParameters:\nname (Optional[str]) – new execution environment name\ndescription (Optional[str]) – new execution environment description\nrequired_metadata_keys (List[RequiredMetadataKey]) – Definition of a metadata keys that custom models using this environment must define\nis_public (bool, optional) – public accessibility of environment\nuse_cases (List[str], optional) – List of use-cases this environment may be used for\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nrefresh()\nUpdate execution environment with the latest data from server.\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nget_access_list()\nRetrieve access control settings of this environment.\nAdded in version v2.36.\nReturn type:\nlist of SharingAccess\nshare(access_list)\nUpdate the access control settings of this execution environment.\nAdded in version v2.36.\nParameters:\naccess_list (list of SharingAccess) – A list of SharingAccess to update.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nReturn type:\nNone\nExamples\nTransfer access to the execution environment from old_user@datarobot.com to new_user@datarobot.com\nimport datarobot as dr\nnew_access = dr.SharingAccess(new_user@datarobot.com,\ndr.enums.SHARING_ROLE.OWNER, can_share=True)\naccess_list = [dr.SharingAccess(old_user@datarobot.com, None), new_access]\ndr.ExecutionEnvironment.get('environment-id').share(access_list)\nclass datarobot.ExecutionEnvironmentVersion\nA version of a DataRobot execution environment.\nAdded in version v2.21.\nVariables:\nid (str) – the id of the execution environment version\nenvironment_id (str) – the id of the execution environment the version belongs to\nbuild_status (str) – the status of the execution environment version build\nimage_id (str) – The Docker image ID of the environment version.\nlabel (Optional[str]) – the label of the execution environment version\ndescription (Optional[str]) – the description of the execution environment version\ncreated_at (Optional[str]) – ISO-8601 formatted timestamp of when the execution environment version was created\ndocker_context_size (Optional[int]) – The size of the uploaded Docker context in bytes if available or None if not\ndocker_image_size (Optional[int]) – The size of the built Docker image in bytes if available or None if not\ndocker_image_uri (Optional[str]) – The URI that the source Docker image execution environment version is based on.\nSet to None if there is not one provided.\nclassmethod create(execution_environment_id, docker_context_path=None, docker_image_uri=None, label=None, description=None, max_wait=600)\nCreate an execution environment version.\nAdded in version v2.21.\nParameters:\nexecution_environment_id (str) – the id of the execution environment\ndocker_context_path (Optional[str]) – The path to a Docker context archive or folder. This parameter has lower priority\nthan docker_image_uri if they are both provided.\ndocker_image_uri (Optional[str]) – The docker_image_uri to be used as an environment.\nIt has priority over the docker_context_path. If both are provided,\nthe environment is created from docker_image_uri, and context is uploaded for\ninformation purposes.\nlabel (Optional[str]) – A human-readable string to label the version.\ndescription (Optional[str]) – execution environment version description\nmax_wait (Optional[int]) – max time to wait for a final build status (“success” or “failed”).\nIf set to None - method will return without waiting.\nReturns:\ncreated execution environment version\nReturn type:\nExecutionEnvironmentVersion\nRaises:\ndatarobot.errors.AsyncTimeoutError – if version did not reach final state during timeout seconds\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod list(execution_environment_id, build_status=None)\nList execution environment versions available to the user.\n.. versionadded:: v2.21\nParameters:\nexecution_environment_id (str) – the id of the execution environment\nbuild_status (Optional[str]) – build status of the execution environment version to filter by.\nSee datarobot.enums.EXECUTION_ENVIRONMENT_VERSION_BUILD_STATUS for valid options\nReturns:\na list of execution environment versions.\nReturn type:\nList[ExecutionEnvironmentVersion]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(execution_environment_id, version_id)\nGet execution environment version by id.\nAdded in version v2.21.\nParameters:\nexecution_environment_id (str) – the id of the execution environment\nversion_id (str) – the id of the execution environment version to retrieve\nReturns:\nretrieved execution environment version\nReturn type:\nExecutionEnvironmentVersion\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\ndownload(file_path)\nDownload execution environment version.\nAdded in version v2.21.\nParameters:\nfile_path (str) – path to create a file with execution environment version content\nReturns:\nretrieved execution environment version\nReturn type:\nExecutionEnvironmentVersion\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nget_build_log()\nGet execution environment version build log and error.\nAdded in version v2.21.\nReturns:\nretrieved execution environment version build log and error.\nIf there is no build error - None is returned.\nReturn type:\nTuple[str, str]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nrefresh()\nUpdate execution environment version with the latest data from server.\nAdded in version v2.21.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclass datarobot.models.custom_model_version.HoldoutData\nHoldout data assigned to a DataRobot custom model version.\nAdded in version v3.2.\nVariables:\ndataset_id (str) – The ID of the dataset.\ndataset_version_id (str) – The ID of the dataset version.\ndataset_name (str) – The name of the dataset.\npartition_column (str) – The name of the partitions column.\nclass datarobot.models.custom_model_version.TrainingData\nTraining data assigned to a DataRobot custom model version.\nAdded in version v3.2.\nVariables:\ndataset_id (str) – The ID of the dataset.\ndataset_version_id (str) – The ID of the dataset version.\ndataset_name (str) – The name of the dataset.\nassignment_in_progress (bool) – The status of the assignment in progress.\nassignment_error (dict) – The assignment error message.\nclass datarobot.models.custom_model_version.RuntimeParameter\nDefinition of a runtime parameter used for the custom model version, it includesthe override value if provided.\nAdded in version v3.4.0.\nVariables:\nfield_name (str) – The runtime parameter name. This value is added as an environment variable when\nrunning custom models.\ntype (str) – The value type accepted by the runtime parameter.\ndescription (str) – Describes how the runtime parameter impacts the running model.\nallow_empty (bool) – Indicates if the runtime parameter must be set before registration.\nmin_value (float) – The minimum value for a numeric field.\nmax_value (float) – The maximum value for a numeric field.\ndefault_value (str, bool, float or None) – The default value for the given field.\noverride_value (str, bool, float or None) – The value set by the user that overrides the default set in the runtime parameter\ndefinition.\ncurrent_value (str, bool, float or None) – After the default and the override values are applied, this is the value of the\nruntime parameter.\ncredential_type (str) – Describes the type of credential, used only for credentials parameters.\nclass datarobot.models.custom_model_version.RuntimeParameterValue\nThe definition of a runtime parameter value used for the custom model version, this defines\nthe runtime parameter override.\nAdded in version v3.4.0.\nVariables:\nfield_name (str) – The runtime parameter name. This value is added as an environment variable when\nrunning custom models.\ntype (str) – The value type accepted by the runtime parameter.\nvalue (str, bool or float) – After the default and the override values are applied, this is the value of the\nruntime parameter.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/custom-models.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/custom-models.html",
        "content_length": 52912
      },
      "code_examples": [],
      "api_methods": [
        "dr.custommodelversion.create_clean",
        "datarobot.target_type.anomaly",
        "datarobot.errors.invalidusageerror",
        "datarobot.models.custommodelversionconversion",
        "dr.custommodelversion.create_from_previous",
        "datarobot.errors.asynctimeouterror",
        "datarobot.errors.trainingdataassignmenterror",
        "dr.enums.sharing_role",
        "dr.executionenvironment.get",
        "datarobot.models.custom_model_version",
        "datarobot.target_type.unstructured",
        "datarobot.target_type.text_generation",
        "datarobot.target_type.binary",
        "datarobot.network_egress_policy.public",
        "datarobot.errors.servererror",
        "datarobot.target_type.regression",
        "dr.custominferencemodel.get",
        "datarobot.models.execution_environment",
        "model.get",
        "datarobot.network_egress_policy.none",
        "datarobot.enums.execution_environment_version_build_status",
        "datarobot.target_type.multiclass",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_7026574108989237941",
      "title": "Database connectivity",
      "content": "Database connectivity\nclass datarobot.DataDriver\nA data driver\nVariables:\nid (str) – the id of the driver.\nclass_name (str) – the Java class name for the driver.\ncanonical_name (str) – the user-friendly name of the driver.\ncreator (str) – the id of the user who created the driver.\nbase_names (List[str]) – a list of the file name(s) of the jar files.\nclassmethod list(typ=None)\nReturns list of available drivers.\nParameters:\ntyp (DataDriverListTypes) – If specified, filters by specified driver type.\nReturns:\ndrivers – contains a list of available drivers.\nReturn type:\nlist of DataDriver instances\nExamples\n>>> import datarobot as dr\n>>> drivers = dr.DataDriver.list()\n>>> drivers\n[DataDriver('mysql'), DataDriver('RedShift'), DataDriver('PostgreSQL')]\nclassmethod get(driver_id)\nGets the driver.\nParameters:\ndriver_id (str) – the identifier of the driver.\nReturns:\ndriver – the required driver.\nReturn type:\nDataDriver\nExamples\n>>> import datarobot as dr\n>>> driver = dr.DataDriver.get('5ad08a1889453d0001ea7c5c')\n>>> driver\nDataDriver('PostgreSQL')\nclassmethod create(class_name, canonical_name, files=None, typ=None, database_driver=None)\nCreates the driver. Only available to admin users.\nParameters:\nclass_name (str) – the Java class name for the driver. Specify None if typ is DataDriverTypes.DR_DATABASE_V1`.\ncanonical_name (str) – the user-friendly name of the driver.\nfiles (List[str]) – a list of the file paths on file system file_path(s) for the driver.\ntyp (str) – Optional. Specify the type of the driver. Defaults to DataDriverTypes.JDBC, may also be\nDataDriverTypes.DR_DATABASE_V1.\ndatabase_driver (str) – Optional. Specify when typ is DataDriverTypes.DR_DATABASE_V1 to create a native database\ndriver. See DrDatabaseV1Types enum for some of the types, but that list may not be exhaustive.\nReturns:\ndriver – the created driver.\nReturn type:\nDataDriver\nRaises:\nClientError – raised if user is not granted for Can manage JDBC database drivers feature\nExamples\n>>> import datarobot as dr\n>>> driver = dr.DataDriver.create(\n...     class_name='org.postgresql.Driver',\n...     canonical_name='PostgreSQL',\n...     files=['/tmp/postgresql-42.2.2.jar']\n... )\n>>> driver\nDataDriver('PostgreSQL')\nupdate(class_name=None, canonical_name=None)\nUpdates the driver. Only available to admin users.\nParameters:\nclass_name (str) – the Java class name for the driver.\ncanonical_name (str) – the user-friendly name of the driver.\nRaises:\nClientError – raised if user is not granted for Can manage JDBC database drivers feature\nReturn type:\nNone\nExamples\n>>> import datarobot as dr\n>>> driver = dr.DataDriver.get('5ad08a1889453d0001ea7c5c')\n>>> driver.canonical_name\n'PostgreSQL'\n>>> driver.update(canonical_name='postgres')\n>>> driver.canonical_name\n'postgres'\ndelete()\nRemoves the driver. Only available to admin users.\nRaises:\nClientError – raised if user is not granted for Can manage JDBC database drivers feature\nReturn type:\nNone\nclass datarobot.Connector\nA connector\nVariables:\nid (str) – the id of the connector.\ncreator_id (str) – the id of the user who created the connector.\nbase_name (str) – the file name of the jar file.\ncanonical_name (str) – the user-friendly name of the connector.\nconfiguration_id (str) – the id of the configuration of the connector.\nclassmethod list()\nReturns list of available connectors.\nReturns:\nconnectors – contains a list of available connectors.\nReturn type:\nlist of Connector instances\nExamples\n>>> import datarobot as dr\n>>> connectors = dr.Connector.list()\n>>> connectors\n[Connector('ADLS Gen2 Connector'), Connector('S3 Connector')]\nclassmethod get(connector_id)\nGets the connector.\nParameters:\nconnector_id (str) – the identifier of the connector.\nReturns:\nconnector – the required connector.\nReturn type:\nConnector\nExamples\n>>> import datarobot as dr\n>>> connector = dr.Connector.get('5fe1063e1c075e0245071446')\n>>> connector\nConnector('ADLS Gen2 Connector')\nclassmethod create(file_path=None, connector_type=None)\nCreates the connector from a jar file. Only available to admin users.\nParameters:\nfile_path (str) – (Deprecated in version v3.6)\nthe file path on file system file_path(s) for the java-based connector.\nconnector_type (str) – The type of the native connector to create\nReturns:\nconnector – the created connector.\nReturn type:\nConnector\nRaises:\nClientError – raised if user is not granted for Can manage connectors feature\nExamples\n>>> import datarobot as dr\n>>> connector = dr.Connector.create('/tmp/connector-adls-gen2.jar')\n>>> connector\nConnector('ADLS Gen2 Connector')\nupdate(file_path)\nUpdates the connector with new jar file. Only available to admin users.\nParameters:\nfile_path (str) – (Deprecated in version v3.6)\nthe file path on file system file_path(s) for the java-based connector.\nReturns:\nconnector – the updated connector.\nReturn type:\nConnector\nRaises:\nClientError – raised if user is not granted for Can manage connectors feature\nExamples\n>>> import datarobot as dr\n>>> connector = dr.Connector.get('5fe1063e1c075e0245071446')\n>>> connector.base_name\n'connector-adls-gen2.jar'\n>>> connector.update('/tmp/connector-s3.jar')\n>>> connector.base_name\n'connector-s3.jar'\ndelete()\nRemoves the connector. Only available to admin users.\nRaises:\nClientError – raised if user is not granted for Can manage connectors feature\nReturn type:\nNone\nclass datarobot.DataStore\nA data store. Represents database\nVariables:\nid (str) – The id of the data store.\ndata_store_type (str) – The type of data store.\ncanonical_name (str) – The user-friendly name of the data store.\ncreator (str) – The id of the user who created the data store.\nupdated (datetime.datetime) – The time of the last update\nparams (DataStoreParameters) – A list specifying data store parameters.\nrole (str) – Your access role for this data store.\nclassmethod list(typ=None, name=None, substitute_url_parameters=False)\nReturns list of available data stores.\nParameters:\ntyp (str) – If specified, filters by specified data store type. If not specified, the default\nis DataStoreListTypes.JDBC.\nname (str) – If specified, filters by data store names that match or contain this name.\nThe search is case-insensitive.\nsubstitute_url_parameters (bool) – If specified, dynamic parameters in the URL will be substituted.\nReturns:\ndata_stores – contains a list of available data stores.\nReturn type:\nlist of DataStore instances\nExamples\n>>> import datarobot as dr\n>>> data_stores = dr.DataStore.list()\n>>> data_stores\n[DataStore('Demo'), DataStore('Airlines')]\nclassmethod get(data_store_id, substitute_url_parameters=False)\nGets the data store.\nParameters:\ndata_store_id (str) – the identifier of the data store.\nsubstitute_url_parameters (bool) – If specified, dynamic parameters in the URL will be substituted.\nReturns:\ndata_store – the required data store.\nReturn type:\nDataStore\nExamples\n>>> import datarobot as dr\n>>> data_store = dr.DataStore.get('5a8ac90b07a57a0001be501e')\n>>> data_store\nDataStore('Demo')\nclassmethod create(data_store_type, canonical_name, driver_id=None, jdbc_url=None, fields=None, connector_id=None)\nCreates the data store.\nParameters:\ndata_store_type (str or DataStoreTypes) – the type of data store.\ncanonical_name (str) – the user-friendly name of the data store.\ndriver_id (str) – Optional. The identifier of the DataDriver if data_store_type is DataStoreListTypes.JDBC or\nDataStoreListTypes.DR_DATABASE_V1.\njdbc_url (str) – Optional. The full JDBC URL (for example: jdbc:postgresql://my.dbaddress.org:5432/my_db).\nfields (list) – Optional. If the type is dr-database-v1, then the fields specify the configuration.\nconnector_id (str) – Optional. The identifier of the Connector if data_store_type is DataStoreListTypes.DR_CONNECTOR_V1\nReturns:\ndata_store – the created data store.\nReturn type:\nDataStore\nExamples\n>>> import datarobot as dr\n>>> data_store = dr.DataStore.create(\n...     data_store_type='jdbc',\n...     canonical_name='Demo DB',\n...     driver_id='5a6af02eb15372000117c040',\n...     jdbc_url='jdbc:postgresql://my.db.address.org:5432/perftest'\n... )\n>>> data_store\nDataStore('Demo DB')\nupdate(canonical_name=None, driver_id=None, connector_id=None, jdbc_url=None, fields=None)\nUpdates the data store.\nParameters:\ncanonical_name (str) – optional, the user-friendly name of the data store.\ndriver_id (str) – Optional. The identifier of the DataDriver. if the type is one of DataStoreTypes.DR_DATABASE_V1\nor DataStoreTypes.JDBC.\nconnector_id (str) – Optional. The identifier of the Connector. if the type is DataStoreTypes.DR_CONNECTOR_V1.\njdbc_url (str) – Optional. The full JDBC URL (for example: jdbc:postgresql://my.dbaddress.org:5432/my_db).\nfields (list) – Optional. If the type is dr-database-v1, then the fields specify the configuration.\nReturn type:\nNone\nExamples\n>>> import datarobot as dr\n>>> data_store = dr.DataStore.get('5ad5d2afef5cd700014d3cae')\n>>> data_store\nDataStore('Demo DB')\n>>> data_store.update(canonical_name='Demo DB updated')\n>>> data_store\nDataStore('Demo DB updated')\ndelete()\nRemoves the DataStore\nReturn type:\nNone\ntest(username=None, password=None, credential_id=None, use_kerberos=None, credential_data=None)\nTests database connection.\nChanged in version v3.2: Added credential_id, use_kerberos and credential_data optional params and made\nusername and password optional.\nParameters:\nusername (str) – optional, the username for database authentication.\npassword (str) – optional, the password for database authentication. The password is encrypted\nat server side and never saved / stored\ncredential_id (str) – optional, id of the set of credentials to use instead of username and password\nuse_kerberos (bool) – optional, whether to use Kerberos for data store authentication\ncredential_data (dict) – optional, the credentials to authenticate with the database, to use instead of\nuser/password or credential ID\nReturns:\nmessage – message with status.\nReturn type:\ndict\nExamples\n>>> import datarobot as dr\n>>> data_store = dr.DataStore.get('5ad5d2afef5cd700014d3cae')\n>>> data_store.test(username='db_username', password='db_password')\n{'message': 'Connection successful'}\nschemas(username, password)\nReturns list of available schemas.\nParameters:\nusername (str) – the username for database authentication.\npassword (str) – the password for database authentication. The password is encrypted\nat server side and never saved / stored\nReturns:\nresponse – dict with database name and list of str - available schemas\nReturn type:\ndict\nExamples\n>>> import datarobot as dr\n>>> data_store = dr.DataStore.get('5ad5d2afef5cd700014d3cae')\n>>> data_store.schemas(username='db_username', password='db_password')\n{'catalog': 'perftest', 'schemas': ['demo', 'information_schema', 'public']}\ntables(username, password, schema=None)\nReturns list of available tables in schema.\nParameters:\nusername (str) – optional, the username for database authentication.\npassword (str) – optional, the password for database authentication. The password is encrypted\nat server side and never saved / stored\nschema (str) – optional, the schema name.\nReturns:\nresponse – dict with catalog name and tables info\nReturn type:\ndict\nExamples\n>>> import datarobot as dr\n>>> data_store = dr.DataStore.get('5ad5d2afef5cd700014d3cae')\n>>> data_store.tables(username='db_username', password='db_password', schema='demo')\n{'tables': [{'type': 'TABLE', 'name': 'diagnosis', 'schema': 'demo'}, {'type': 'TABLE',\n'name': 'kickcars', 'schema': 'demo'}, {'type': 'TABLE', 'name': 'patient',\n'schema': 'demo'}, {'type': 'TABLE', 'name': 'transcript', 'schema': 'demo'}],\n'catalog': 'perftest'}\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nDataStore\nget_shared_roles()\nRetrieve what users have access to this data store\nAdded in version v3.2.\nReturn type:\nlist of SharingRole\nshare(access_list)\nModify the ability of users to access this data store\nAdded in version v2.14.\nParameters:\naccess_list (list of SharingRole) – the modifications to make.\nReturn type:\nNone\nRaises:\ndatarobot.ClientError : – if you do not have permission to share this data store, if the user you’re sharing with\ndoesn’t exist, if the same user appears multiple times in the access_list, or if these\nchanges would leave the data store without an owner.\nExamples\nThe SharingRole class is needed in order to\nshare a Data Store with one or more users.\nFor example, suppose you had a list of user IDs you wanted to share this DataStore with. You could use\na loop to generate a list of SharingRole objects for them,\nand bulk share this Data Store.\n>>> import datarobot as dr\n>>> from datarobot.models.sharing import SharingRole\n>>> from datarobot.enums import SHARING_ROLE, SHARING_RECIPIENT_TYPE\n>>>\n>>> user_ids = [\"60912e09fd1f04e832a575c1\", \"639ce542862e9b1b1bfa8f1b\", \"63e185e7cd3a5f8e190c6393\"]\n>>> sharing_roles = []\n>>> for user_id in user_ids:\n...     new_sharing_role = SharingRole(\n...         role=SHARING_ROLE.CONSUMER,\n...         share_recipient_type=SHARING_RECIPIENT_TYPE.USER,\n...         id=user_id,\n...         can_share=True,\n...     )\n...     sharing_roles.append(new_sharing_role)\n>>> dr.DataStore.get('my-data-store-id').share(access_list)\nSimilarly, a SharingRole instance can be used to\nremove a user’s access if the role is set to SHARING_ROLE.NO_ROLE, like in this example:\n>>> import datarobot as dr\n>>> from datarobot.models.sharing import SharingRole\n>>> from datarobot.enums import SHARING_ROLE, SHARING_RECIPIENT_TYPE\n>>>\n>>> user_to_remove = \"[email protected]\"\n... remove_sharing_role = SharingRole(\n...     role=SHARING_ROLE.NO_ROLE,\n...     share_recipient_type=SHARING_RECIPIENT_TYPE.USER,\n...     username=user_to_remove,\n...     can_share=False,\n... )\n>>> dr.DataStore.get('my-data-store-id').share(roles=[remove_sharing_role])\nclass datarobot.DataSource\nA data source. Represents data request\nVariables:\nid (str) – the id of the data source.\ntype (str) – the type of data source.\ncanonical_name (str) – the user-friendly name of the data source.\ncreator (str) – the id of the user who created the data source.\nupdated (datetime.datetime) – the time of the last update.\nparams (DataSourceParameters) – a list specifying data source parameters.\nrole (str or None) – if a string, represents a particular level of access and should be one of\ndatarobot.enums.SHARING_ROLE.  For more information on the specific access levels, see\nthe sharing documentation.  If None, can be passed to a share\nfunction to revoke access for a specific user.\nclassmethod list(typ=None)\nReturns list of available data sources.\nParameters:\ntyp (DataStoreListTypes) – If specified, filters by specified datasource type. If not specified it will\ndefault to DataStoreListTypes.DATABASES\nReturns:\ndata_sources – contains a list of available data sources.\nReturn type:\nlist of DataSource instances\nExamples\n>>> import datarobot as dr\n>>> data_sources = dr.DataSource.list()\n>>> data_sources\n[DataSource('Diagnostics'), DataSource('Airlines 100mb'), DataSource('Airlines 10mb')]\nclassmethod get(data_source_id)\nGets the data source.\nParameters:\ndata_source_id (str) – the identifier of the data source.\nReturns:\ndata_source – the requested data source.\nReturn type:\nDataSource\nExamples\n>>> import datarobot as dr\n>>> data_source = dr.DataSource.get('5a8ac9ab07a57a0001be501f')\n>>> data_source\nDataSource('Diagnostics')\nclassmethod create(data_source_type, canonical_name, params)\nCreates the data source.\nParameters:\ndata_source_type (str or DataStoreTypes) – the type of data source.\ncanonical_name (str) – the user-friendly name of the data source.\nparams (DataSourceParameters) – a list specifying data source parameters.\nReturns:\ndata_source – the created data source.\nReturn type:\nDataSource\nExamples\n>>> import datarobot as dr\n>>> params = dr.DataSourceParameters(\n...     data_store_id='5a8ac90b07a57a0001be501e',\n...     query='SELECT * FROM airlines10mb WHERE \"Year\" >= 1995;'\n... )\n>>> data_source = dr.DataSource.create(\n...     data_source_type='jdbc',\n...     canonical_name='airlines stats after 1995',\n...     params=params\n... )\n>>> data_source\nDataSource('airlines stats after 1995')\nupdate(canonical_name=None, params=None)\nCreates the data source.\nParameters:\ncanonical_name (str) – optional, the user-friendly name of the data source.\nparams (DataSourceParameters) – optional, the identifier of the DataDriver.\nReturn type:\nNone\nExamples\n>>> import datarobot as dr\n>>> data_source = dr.DataSource.get('5ad840cc613b480001570953')\n>>> data_source\nDataSource('airlines stats after 1995')\n>>> params = dr.DataSourceParameters(\n...     query='SELECT * FROM airlines10mb WHERE \"Year\" >= 1990;'\n... )\n>>> data_source.update(\n...     canonical_name='airlines stats after 1990',\n...     params=params\n... )\n>>> data_source\nDataSource('airlines stats after 1990')\ndelete()\nRemoves the DataSource\nReturn type:\nNone\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(TDataSource, bound= DataSource)\nget_access_list()\nRetrieve what users have access to this data source\nAdded in version v2.14.\nReturn type:\nlist of SharingAccess\nshare(access_list)\nModify the ability of users to access this data source\nAdded in version v2.14.\nParameters:\naccess_list (list of SharingAccess) – The modifications to make.\nReturn type:\nNone\nRaises:\ndatarobot.ClientError: – If you do not have permission to share this data source, if the user you’re sharing with\ndoesn’t exist, if the same user appears multiple times in the access_list, or if these\nchanges would leave the data source without an owner.\nExamples\nTransfer access to the data source from old_user@datarobot.com to new_user@datarobot.com\nfrom datarobot.enums import SHARING_ROLE\nfrom datarobot.models.data_source import DataSource\nfrom datarobot.models.sharing import SharingAccess\nnew_access = SharingAccess(\n\"[email protected]\",\nSHARING_ROLE.OWNER,\ncan_share=True,\n)\naccess_list = [\nSharingAccess(\"[email protected]\", SHARING_ROLE.OWNER, can_share=True),\nnew_access,\n]\nDataSource.get('my-data-source-id').share(access_list)\ncreate_dataset(username=None, password=None, do_snapshot=None, persist_data_after_ingestion=None, categories=None, credential_id=None, use_kerberos=None)\nCreate a Dataset from this data source.\nAdded in version v2.22.\nParameters:\nusername (string, optional) – The username for database authentication.\npassword (string, optional) – The password (in cleartext) for database authentication. The password\nwill be encrypted on the server side in scope of HTTP request and never saved or stored.\ndo_snapshot (Optional[bool]) – If unset, uses the server default: True.\nIf true, creates a snapshot dataset; if\nfalse, creates a remote dataset. Creating snapshots from non-file sources requires an\nadditional permission, Enable Create Snapshot Data Source.\npersist_data_after_ingestion (Optional[bool]) – If unset, uses the server default: True.\nIf true, will enforce saving all data\n(for download and sampling) and will allow a user to view extended data profile\n(which includes data statistics like min/max/median/mean, histogram, etc.). If false,\nwill not enforce saving data. The data schema (feature names and types) still will be\navailable. Specifying this parameter to false and doSnapshot to true will result in\nan error.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\ncredential_id (string, optional) – The ID of the set of credentials to\nuse instead of user and password. Note that with this change, username and password\nwill become optional.\nuse_kerberos (Optional[bool]) – If unset, uses the server default: False.\nIf true, use kerberos authentication for database authentication.\nReturns:\nresponse – The Dataset created from the uploaded data\nReturn type:\nDataset\nclass datarobot.DataSourceParameters\nData request configuration\nVariables:\ndata_store_id (str) – the id of the DataStore.\ntable (str) – Optional. The name of specified database table.\nschema (str) – Optional. The name of the schema associated with the table.\npartition_column (str) – Optional. The name of the partition column.\nquery (str) – Optional. The user specified SQL query.\nfetch_size (int) – Optional. A user specified fetch size in the range [1, 20000].\nBy default a fetchSize will be assigned to balance throughput and memory usage\npath (str) – Optional. The user-specified path for BLOB storage\nData store\nclass datarobot.models.data_store.TestResponse\nclass datarobot.models.data_store.SchemasResponse\nclass datarobot.models.data_store.TablesResponse",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-connectivity.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-connectivity.html",
        "content_length": 21341
      },
      "code_examples": [],
      "api_methods": [
        "dr.datasource.get",
        "dr.datasource.list",
        "datarobot.models.data_source",
        "dr.datasource.create",
        "dr.datastore.get",
        "dr.connector.get",
        "dr.datastore.create",
        "dr.datadriver.create",
        "dr.connector.list",
        "datarobot.enums.sharing_role",
        "dr.datadriver.get",
        "datarobot.models.data_store",
        "dr.connector.create",
        "datarobot.models.sharing",
        "dr.datadriver.list",
        "dr.datastore.list"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2738859079800320384",
      "title": "Data exports",
      "content": "Data exports\nclass datarobot.models.deployment.data_exports.PredictionDataExport\nA prediction data export.\nAdded in version v3.4.\nVariables:\nid (str) – The ID of the prediction data export.\nmodel_id (str) – The ID of the model (or null if not specified).\ncreated_at (datetime) – Prediction data export creation timestamp.\nperiod (Period) – A prediction data time range definition.\nstatus (ExportStatus) – A prediction data export processing state.\nerror (ExportError) – Error description, appears when prediction data export job failed (status is FAILED).\nbatches (ExportBatches) – Metadata associated with exported batch.\ndeployment_id (str) – The ID of the deployment.\nclassmethod list(deployment_id, status=None, model_id=None, batch=None, offset=0, limit=100)\nRetrieve a list of prediction data exports.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nmodel_id (Optional[str]) – The ID of the model used for prediction data export.\nstatus (Optional[ExportStatus]) – A prediction data export processing state.\nbatch (Optional[bool]) – If true, only return batch exports.\nIf false, only return real-time exports.\nIf not provided, return both real-time and batch exports.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 100 (0 means no limit).\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nReturns:\nprediction_data_exports – A list of PredictionDataExport objects.\nReturn type:\nlist\nExamples\nfrom datarobot.models.deployment import PredictionDataExport\nprediction_data_exports = PredictionDataExport.list(deployment_id='5c939e08962d741e34f609f0')\nclassmethod get(deployment_id, export_id)\nRetrieve a single prediction data export.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nexport_id (str) – The ID of the prediction data export.\nReturns:\nprediction_data_export – A prediction data export.\nReturn type:\nPredictionDataExport\nExamples\nfrom datarobot.models.deployment import PredictionDataExport\nprediction_data_export = PredictionDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fbe59aaa3f847bd5acc75b'\n)\nclassmethod create(deployment_id, start, end, model_id=None, batch_ids=None, max_wait=600)\nCreate a deployment prediction data export.\nWaits until ready and fetches PredictionDataExport after the export finishes. This method is blocking.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nstart (Union[datetime, str]) – Inclusive start of the time range.\nend (Union[datetime, str]) – Exclusive end of the time range.\nmodel_id (Optional[str]) – The ID of the model.\nbatch_ids (Optional[List[str]]) – IDs of batches to export. Null for real-time data exports.\nmax_wait (int,) – Seconds to wait for successful resolution.\nReturns:\nprediction_data_export – A prediction data export.\nReturn type:\nPredictionDataExport\nExamples\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import PredictionDataExport\nnow=datetime.now()\nprediction_data_export = PredictionDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0', start=now - timedelta(days=7), end=now\n)\nfetch_data()\nReturn data from prediction export as datarobot Dataset.\nReturns:\nprediction_datasets – List of datasets for a given export, most often it is just one.\nReturn type:\nList[Dataset]\nExamples\nfrom datarobot.models.deployment import PredictionDataExport\nprediction_data_export = PredictionDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fbe59aaa3f847bd5acc75b'\n)\nprediction_datasets = prediction_data_export.fetch_data()\nclass datarobot.models.deployment.data_exports.ActualsDataExport\nAn actuals data export.\nAdded in version v3.4.\nVariables:\nid (str) – The ID of the actuals data export.\nmodel_id (str) – The ID of the model (or null if not specified).\ncreated_at (datetime) – Actuals data export creation timestamp.\nperiod (Period) – A actuals data time range definition.\nstatus (ExportStatus) – A data export processing state.\nerror (ExportError) – Error description, appears when actuals data export job failed (status is FAILED).\nonly_matched_predictions (bool) – If true, exports actuals with matching predictions only.\ndeployment_id (str) – The ID of the deployment.\nclassmethod list(deployment_id, status=None, offset=0, limit=100)\nRetrieve a list of actuals data exports.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nstatus (Optional[ExportStatus]) – Actuals data export processing state.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 100 (0 means no limit).\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nReturns:\nactuals_data_exports – A list of ActualsDataExport objects.\nReturn type:\nlist\nExamples\nfrom datarobot.models.deployment import ActualsDataExport\nactuals_data_exports = ActualsDataExport.list(deployment_id='5c939e08962d741e34f609f0')\nclassmethod get(deployment_id, export_id)\nRetrieve a single actuals data export.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nexport_id (str) – The ID of the actuals data export.\nReturns:\nactuals_data_export – An actuals data export.\nReturn type:\nActualsDataExport\nExamples\nfrom datarobot.models.deployment import ActualsDataExport\nactuals_data_export = ActualsDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fb0a6c9bb187781cfdea36'\n)\nclassmethod create(deployment_id, start, end, model_id=None, only_matched_predictions=None, max_wait=600)\nCreate a deployment actuals data export.\nWaits until ready and fetches ActualsDataExport after the export finishes. This method is blocking.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nstart (Union[datetime, str]) – Inclusive start of the time range.\nend (Union[datetime, str]) – Exclusive end of the time range.\nmodel_id (Optional[str]) – The ID of the model.\nonly_matched_predictions (Optional[bool]) – If true, exports actuals with matching predictions only.\nmax_wait (int) – Seconds to wait for successful resolution.\nReturns:\nactuals_data_export – An actuals data export.\nReturn type:\nActualsDataExport\nExamples\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import ActualsDataExport\nnow=datetime.now()\nactuals_data_export = ActualsDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0', start=now - timedelta(days=7), end=now\n)\nfetch_data()\nReturn data from actuals export as datarobot Dataset.\nReturns:\nactuals_datasets – List of datasets for a given export, most often it is just one.\nReturn type:\nList[Dataset]\nExamples\nfrom datarobot.models.deployment import ActualsDataExport\nactuals_data_export = ActualsDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fb0a6c9bb187781cfdea36'\n)\nactuals_datasets = actuals_data_export.fetch_data()\nclass datarobot.models.deployment.data_exports.TrainingDataExport\nA training data export.\nAdded in version v3.4.\nVariables:\nid (str) – The ID of the training data export.\nmodel_id (str) – The ID of the model (or null if not specified).\nmodel_package_id (str) – The ID of the model package.\ncreated_at (datetime) – Training data export creation timestamp.\ndeployment_id (str) – The ID of the deployment.\nclassmethod list(deployment_id)\nRetrieve a list of successful training data exports.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nReturns:\ntraining_data_exports – A list of TrainingDataExport objects.\nReturn type:\nlist\nExamples\nfrom datarobot.models.deployment import TrainingDataExport\ntraining_data_exports = TrainingDataExport.list(deployment_id='5c939e08962d741e34f609f0')\nclassmethod get(deployment_id, export_id)\nRetrieve a single training data export.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nexport_id (str) – The ID of the training data export.\nReturns:\ntraining_data_export – A training data export.\nReturn type:\nTrainingDataExport\nExamples\nfrom datarobot.models.deployment import TrainingDataExport\ntraining_data_export = TrainingDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fbf2356124f1daa3acc522'\n)\nclassmethod create(deployment_id, model_id=None, max_wait=600)\nCreate a single training data export.\nWaits until ready and fetches TrainingDataExport after the export finishes. This method is blocking.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nmodel_id (Optional[str]) – The ID of the model.\nmax_wait (int) – Seconds to wait for successful resolution.\nReturn type:\nstr\nReturns:\ndataset_id (str) –    A created dataset with training data.\nExamples\n--------\ncode-block:: python – from datarobot.models.deployment import TrainingDataExport\ndataset_id = TrainingDataExport.create(deployment_id=’5c939e08962d741e34f609f0’)\nfetch_data()\nReturn data from training data export as datarobot Dataset.\nReturns:\ntraining_dataset – A datasets for a given export.\nReturn type:\nDataset\nExamples\nfrom datarobot.models.deployment import TrainingDataExport\ntraining_data_export = TrainingDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fbf2356124f1daa3acc522'\n)\ntraining_data_export = training_data_export.fetch_data()\nclass datarobot.models.deployment.data_exports.DataQualityExport\nA data quality export record.\nAdded in version v3.6.\nVariables:\nassociation_id (str) – The association ID of the data quality export.\ntimestamp (datetime) – The data quality export creation timestamp.\ndeployment_id (str) – The ID of the deployment.\nprompt (Optional[str]) – The LLM prompt of the data quality export.\npredicted_value (str) – The predicted value of the data quality export.\nactual_value (Optional[str]) – The actual value (if available) of the data quality export.\ncontext (List[Dict[str, str]]) – Context data (context and link data) for the contexts associated with the data quality export.\nmetrics (List[Dict[str, Any]]) – Custom-metrics data for the data quality export.\nclassmethod list(deployment_id, start, end, model_id=None, prediction_pattern=None, prompt_pattern=None, actual_pattern=None, order_by=None, order_metric=None, filter_metric=None, filter_value=None, offset=0, limit=100)\nRetrieve a list of data-quality export records for a given deployment.\nAdded in version v3.6.\nParameters:\ndeployment_id (str) – The ID of the deployment.\nstart (Union[str, datetime]) – The earliest time of the objects to return.\nend (Union[str, datetime]) – The latest time of the objects to return.\nmodel_id (Optional[str]) – The ID of the model.\nprediction_pattern (Optional[str]) – The keywords to search in a predicted value for a text generation target.\nprompt_pattern (Optional[str]) – The keywords to search in a prompt value for a text generation target.\nactual_pattern (Optional[str]) – The keywords to search in an actual value for a text generation target.\norder_by (Optional[str]) – The field to sort by (e.g. associationId, timestamp, or customMetrics). Use a leading ‘-’\nto indicate descending order. When ordering by a custom-metric, must also specify ‘order_metric’.\nThe default is None, which equates to ‘-timestamp’.\norder_metric (Optional[str]) – When ‘order_by’ is a custom-metric, this specifies the custom-metric name or ID to use for ordering.\nThe default is None.\nfilter_metric (Optional[str]) – Specifies the metric name or ID to use for matching. Must also use ‘filter_value’ to specify\nthe value that must be matched. The default is None.\nfilter_value (Optional[str]) – Specifies the value associated with ‘filter_metric’ that must be matched. The default\nis None.\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 100 (which is maximum).\nReturns:\ndata_quality_exports – A list of DataQualityExport objects.\nReturn type:\nlist\nExamples\nfrom datarobot.models.deployment import DataQualityExport\ndata_quality_exports = DataQualityExport.list(\ndeployment_id='5c939e08962d741e34f609f0', start_time='2024-07-01', end_time='2024-08-01\n)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-exploration.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-exploration.html",
        "content_length": 11947
      },
      "code_examples": [],
      "api_methods": [
        "deployment.data_exports",
        "datarobot.models.deployment"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_1758552012976174452",
      "title": "Recipes",
      "content": "Recipes\nclass datarobot.models.recipe.Recipe\nData wrangling entity, which contains all information needed to transform dataset and generate SQL.\nclassmethod update_downsampling(recipe_id, downsampling)\nSet downsampling for the recipe, applied during publishing.\nReturn type:\nRecipe\nretrieve_preview(max_wait=600, number_of_operations_to_use=None)\nRetrieve preview and compute it, if absent.\nParameters:\nmax_wait (int) – The number of seconds to wait for the result.\nnumber_of_operations_to_use (Optional[int]) – Request preview for particular number of operations.\nReturns:\npreview\nReturn type:\ndict\nretrieve_insights(max_wait=600, number_of_operations_to_use=None)\nRetrieve insights for the sample. When preview is requested, the insights job starts automatically.\nParameters:\nmax_wait (int) – The number of seconds to wait for the result.\nnumber_of_operations_to_use (Optional[int]) – Retrieves insights for the specified number of operations. First, preview computation for the same\nnumber of operations must be submitted.\nReturn type:\nAny\nclassmethod set_inputs(recipe_id, inputs)\nSet inputs for the recipe.\nReturn type:\nRecipe\nclassmethod set_operations(recipe_id, operations)\nSet operations for the recipe.\nReturn type:\nRecipe\nclassmethod set_recipe_metadata(recipe_id, metadata)\nUpdate metadata for the recipe.\nParameters:\nrecipe_id (str) – Recipe ID.\nmetadata (Dict[str, str]) – Dictionary of metadata to be updated.\nReturns:\nrecipe – New recipe with updated metadata.\nReturn type:\nRecipe\nget_sql(operations=None)\nGenerate sql for the given recipe in a transient way, recipe is not modified.\nif operations is None, recipe operations are used to generate sql.\nif operations = [], recipe operations are ignored during sql generation.\nif operations is not empty list, generate sql for them.\nReturn type:\nstr\nclassmethod from_data_store(use_case, data_store, data_source_type, dialect, data_source_inputs, recipe_type=RecipeType.WRANGLING)\nCreate a wrangling recipe from data store.\nReturn type:\nRecipe\nclassmethod from_dataset(use_case, dataset, dialect=None, inputs=None, recipe_type=RecipeType.WRANGLING, snapshot_policy=DataWranglingSnapshotPolicy.LATEST)\nCreate a wrangling recipe from dataset.\nReturn type:\nRecipe\nclass datarobot.models.recipe.RecipeSettings\nSettings, for example to apply at downsampling stage.\nclass datarobot.models.recipe.RecipeDatasetInput\nObject, describing inputs for recipe transformations.\nclass datarobot.models.recipe.DatasetInput\nclass datarobot.models.recipe.DataSourceInput\nInputs required to create a new recipe from data store.\nRecipe Operations\nclass datarobot.models.recipe_operation.WranglingOperation\nclass datarobot.models.recipe_operation.DownsamplingOperation\nclass datarobot.models.recipe_operation.SamplingOperation\nclass datarobot.models.recipe_operation.BaseTimeAwareTask\nclass datarobot.models.recipe_operation.TaskPlanElement\nclass datarobot.models.recipe_operation.CategoricalStats\nclass datarobot.models.recipe_operation.NumericStats\nclass datarobot.models.recipe_operation.Lags\nclass datarobot.models.recipe_operation.LagsOperation\nGenerate lags in a window.\nclass datarobot.models.recipe_operation.WindowCategoricalStatsOperation\nGenerate rolling statistics in a window for categorical features.\nclass datarobot.models.recipe_operation.WindowNumericStatsOperation\nGenerate various rolling numeric statistics in a window. Output could be a several columns.\nclass datarobot.models.recipe_operation.TimeSeriesOperation\nOperation to generate a dataset ready for time series modeling: with forecast point, forecast distances,\nknown in advance columns, etc.\nclass datarobot.models.recipe_operation.ComputeNewOperation\nclass datarobot.models.recipe_operation.RenameColumnsOperation\nclass datarobot.models.recipe_operation.FilterCondition\nclass datarobot.models.recipe_operation.FilterOperation\nFilter rows.\nclass datarobot.models.recipe_operation.DropColumnsOperation\nclass datarobot.models.recipe_operation.RandomSamplingOperation\nclass datarobot.models.recipe_operation.DatetimeSamplingOperation",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-wrangling.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-wrangling.html",
        "content_length": 4085
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.recipe",
        "datarobot.models.recipe_operation"
      ],
      "complexity_score": 0.5,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_9063696407140453864",
      "title": "Datasets",
      "content": "Datasets\nclass datarobot.models.Dataset\nRepresents a Dataset returned from the api/v2/datasets/ endpoints.\nVariables:\nid (string) – The ID of this dataset\nname (string) – The name of this dataset in the catalog\nis_latest_version (bool) – Whether this dataset version is the latest version\nof this dataset\nversion_id (string) – The object ID of the catalog_version the dataset belongs to\ncategories (list(string)) – An array of strings describing the intended use of the dataset. The\nsupported options are “TRAINING” and “PREDICTION”.\ncreated_at (string) – The date when the dataset was created\ncreated_by (string, optional) – Username of the user who created the dataset\nis_snapshot (bool) – Whether the dataset version is an immutable snapshot of data\nwhich has previously been retrieved and saved to Data_robot\ndata_persisted (Optional[bool]) – If true, user is allowed to view extended data profile\n(which includes data statistics like min/max/median/mean, histogram, etc.) and download\ndata. If false, download is not allowed and only the data schema (feature names and types)\nwill be available.\nis_data_engine_eligible (bool) – Whether this dataset can be\na data source of a data engine query.\nprocessing_state (string) – Current ingestion process state of\nthe dataset\nrow_count (Optional[int]) – The number of rows in the dataset.\nsize (Optional[int]) – The size of the dataset as a CSV in bytes.\nsample_size (dict, optional) – The size of data fetched during dataset registration. For example, to fetch the first 95\nrows,  the sample_size value is {‘type’: ‘rows’, ‘value’: 95}.\nCurrently only ‘rows’ type is supported.\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this dataset in AI Catalog.\nReturn type:\nstr\nclassmethod upload(source)\nThis method covers Dataset creation from local materials (file & DataFrame) and a URL.\nParameters:\nsource (str, pd.DataFrame or file object) – Pass a URL, filepath, file or DataFrame to create and return a Dataset.\nReturns:\nresponse – The Dataset created from the uploaded data source.\nReturn type:\nDataset\nRaises:\nInvalidUsageError – If the source parameter cannot be determined to be a URL, filepath, file or DataFrame.\nExamples\n# Upload a local file\ndataset_one = Dataset.upload(\"./data/examples.csv\")\n# Create a dataset via URL\ndataset_two = Dataset.upload(\n\"https://raw.githubusercontent.com/curran/data/gh-pages/dbpedia/cities/data.csv\"\n)\n# Create dataset with a pandas Dataframe\ndataset_three = Dataset.upload(my_df)\n# Create dataset using a local file\nwith open(\"./data/examples.csv\", \"rb\") as file_pointer:\ndataset_four = Dataset.create_from_file(filelike=file_pointer)\nclassmethod create_from_file(cls, file_path=None, filelike=None, categories=None, read_timeout=600, max_wait=600, *, use_cases=None)\nA blocking call that creates a new Dataset from a file. Returns when the dataset has\nbeen successfully uploaded and processed.\nWarning: This function does not clean up it’s open files. If you pass a filelike, you are\nresponsible for closing it. If you pass a file_path, this will create a file object from\nthe file_path but will not close it.\nParameters:\nfile_path (string, optional) – The path to the file. This will create a file object pointing to that file but will\nnot close it.\nfilelike (file, optional) – An open and readable file object.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nread_timeout (Optional[int]) – The maximum number of seconds to wait for the server to respond indicating that the\ninitial upload is complete\nmax_wait (Optional[int]) – Time in seconds after which dataset creation is considered unsuccessful\nuse_cases (list[UseCase] | UseCase | list[string] | string, optional) – A list of UseCase objects, UseCase object,\nlist of Use Case ids or a single Use Case id to add this new Dataset to. Must be a kwarg.\nReturns:\nresponse – A fully armed and operational Dataset\nReturn type:\nDataset\nclassmethod create_from_in_memory_data(cls, data_frame=None, records=None, categories=None, read_timeout=600, max_wait=600, fname=None, *, use_cases=None)\nA blocking call that creates a new Dataset from in-memory data. Returns when the dataset has\nbeen successfully uploaded and processed.\nThe data can be either a pandas DataFrame or a list of dictionaries with identical keys.\nParameters:\ndata_frame (DataFrame, optional) – The data frame to upload\nrecords (list[dict], optional) – A list of dictionaries with identical keys to upload\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nread_timeout (Optional[int]) – The maximum number of seconds to wait for the server to respond indicating that the\ninitial upload is complete\nmax_wait (Optional[int]) – Time in seconds after which dataset creation is considered unsuccessful\nfname (string, optional) – The file name, “data.csv” by default\nuse_cases (list[UseCase] | UseCase | list[string] | string, optional) – A list of UseCase objects, UseCase object,\nlist of Use Case IDs or a single Use Case ID to add this new dataset to. Must be a kwarg.\nReturns:\nresponse – The Dataset created from the uploaded data.\nReturn type:\nDataset\nRaises:\nInvalidUsageError – If neither a DataFrame or list of records is passed.\nclassmethod create_from_url(cls, url, do_snapshot=None, persist_data_after_ingestion=None, categories=None, sample_size=None, max_wait=600, *, use_cases=None)\nA blocking call that creates a new Dataset from data stored at a url.\nReturns when the dataset has been successfully uploaded and processed.\nParameters:\nurl (string) – The URL to use as the source of data for the dataset being created.\ndo_snapshot (Optional[bool]) – If unset, uses the server default: True.\nIf true, creates a snapshot dataset; if\nfalse, creates a remote dataset. Creating snapshots from non-file sources may be\ndisabled by the permission, Disable AI Catalog Snapshots.\npersist_data_after_ingestion (Optional[bool]) – If unset, uses the server default: True.\nIf true, will enforce saving all data\n(for download and sampling) and will allow a user to view extended data profile\n(which includes data statistics like min/max/median/mean, histogram, etc.). If false,\nwill not enforce saving data. The data schema (feature names and types) still will be\navailable. Specifying this parameter to false and doSnapshot to true will result in\nan error.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nsample_size (dict, optional) – The size of data fetched during dataset registration.\nFor example, to fetch the first 95 rows,  the sample_size value would be:\n{‘type’: ‘rows’, ‘value’: 95}.\nCurrently only ‘rows’ type is supported.\nmax_wait (Optional[int]) – Time in seconds after which dataset creation is considered unsuccessful.\nuse_cases (list[UseCase] | UseCase | list[string] | string, optional) – A list of UseCase objects, UseCase object,\nlist of Use Case IDs or a single Use Case ID to add this new dataset to. Must be a kwarg.\nReturns:\nresponse – The Dataset created from the uploaded data\nReturn type:\nDataset\nclassmethod create_from_project(cls, project_id, categories=None, max_wait=600, *, use_cases=None)\nA blocking call that creates a new dataset from project data.\nReturns when the dataset has been successfully created.\nParameters:\nproject_id (string) – The project to create the dataset from.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nmax_wait (Optional[int]) – Time in seconds after which dataset creation is considered unsuccessful.\nuse_cases (list[UseCase] | UseCase | list[string] | string, optional) – A list of UseCase objects, UseCase object,\nlist of Use Case IDs or a single Use Case ID to add this new dataset to. Must be a kwarg.\nReturns:\nresponse – The dataset created from the project dataset.\nReturn type:\nDataset\nclassmethod create_from_datastage(cls, datastage_id, categories=None, max_wait=600, *, use_cases=None)\nA blocking call that creates a new Dataset from data stored as a DataStage.\nReturns when the dataset has been successfully uploaded and processed.\nParameters:\ndatastage_id (string) – The ID of the DataStage to use as the source of data for the dataset being created.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nmax_wait (Optional[int]) – Time in seconds after which dataset creation is considered unsuccessful.\nReturns:\nresponse – The Dataset created from the uploaded data\nReturn type:\nDataset\nclassmethod create_from_data_source(cls, data_source_id, username=None, password=None, do_snapshot=None, persist_data_after_ingestion=None, categories=None, credential_id=None, use_kerberos=None, credential_data=None, sample_size=None, max_wait=600, *, use_cases=None)\nA blocking call that creates a new Dataset from data stored at a DataSource.\nReturns when the dataset has been successfully uploaded and processed.\nAdded in version v2.22.\nParameters:\ndata_source_id (string) – The ID of the DataSource to use as the source of data.\nusername (string, optional) – The username for database authentication.\npassword (string, optional) – The password (in cleartext) for database authentication. The password\nwill be encrypted on the server side in scope of HTTP request and never saved or stored.\ndo_snapshot (Optional[bool]) – If unset, uses the server default: True.\nIf true, creates a snapshot dataset; if\nfalse, creates a remote dataset. Creating snapshots from non-file sources requires may\nbe disabled by the permission, Disable AI Catalog Snapshots.\npersist_data_after_ingestion (Optional[bool]) – If unset, uses the server default: True.\nIf true, will enforce saving all data\n(for download and sampling) and will allow a user to view extended data profile\n(which includes data statistics like min/max/median/mean, histogram, etc.). If false,\nwill not enforce saving data. The data schema (feature names and types) still will be\navailable. Specifying this parameter to false and doSnapshot to true will result in\nan error.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\ncredential_id (string, optional) – The ID of the set of credentials to\nuse instead of user and password. Note that with this change, username and password\nwill become optional.\nuse_kerberos (Optional[bool]) – If unset, uses the server default: False.\nIf true, use kerberos authentication for database authentication.\ncredential_data (dict, optional) – The credentials to authenticate with the database, to use instead of user/password or\ncredential ID.\nsample_size (dict, optional) – The size of data fetched during dataset registration.\nFor example, to fetch the first 95 rows,  the sample_size value would be:\n{‘type’: ‘rows’, ‘value’: 95}.\nCurrently only ‘rows’ type is supported.\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful.\nuse_cases (list[UseCase] | UseCase | list[string] | string, optional) – A list of UseCase objects, UseCase object,\nlist of Use Case IDs or a single Use Case ID to add this new dataset to. Must be a kwarg.\nReturns:\nresponse – The Dataset created from the uploaded data\nReturn type:\nDataset\nclassmethod create_from_query_generator(cls, generator_id, dataset_id=None, dataset_version_id=None, max_wait=600, *, use_cases=None)\nA blocking call that creates a new Dataset from the query generator.\nReturns when the dataset has been successfully processed. If optional\nparameters are not specified the query is applied to the dataset_id\nand dataset_version_id stored in the query generator. If specified they\nwill override the stored dataset_id/dataset_version_id, e.g. to prep a\nprediction dataset.\nParameters:\ngenerator_id (str) – The id of the query generator to use.\ndataset_id (Optional[str]) – The id of the dataset to apply the query to.\ndataset_version_id (Optional[str]) – The id of the dataset version to apply the query to. If not specified the\nlatest version associated with dataset_id (if specified) is used.\nmax_wait (int) – optional, the maximum number of seconds to wait before giving up.\nuse_cases (list[UseCase] | UseCase | list[string] | string, optional) – A list of UseCase objects, UseCase object,\nlist of Use Case IDs or a single Use Case ID to add this new dataset to. Must be a kwarg.\nReturns:\nresponse – The Dataset created from the query generator\nReturn type:\nDataset\nclassmethod create_from_recipe(cls, recipe, name=None, do_snapshot=None, persist_data_after_ingestion=None, categories=None, credential=None, credential_id=None, use_kerberos=None, materialization_destination=None, max_wait=600, *, use_cases=None)\nA blocking call that creates a new Dataset from the recipe.\nReturns when the dataset has been successfully uploaded and processed.\nAdded in version 3.6.\nReturns:\nresponse – The Dataset created from the uploaded data\nReturn type:\nDataset\nclassmethod get(dataset_id)\nGet information about a dataset.\nParameters:\ndataset_id (string) – the id of the dataset\nReturns:\ndataset – the queried dataset\nReturn type:\nDataset\nclassmethod delete(dataset_id)\nSoft deletes a dataset.  You cannot get it or list it or do actions with it, except for\nun-deleting it.\nParameters:\ndataset_id (string) – The id of the dataset to mark for deletion\nReturn type:\nNone\nclassmethod un_delete(dataset_id)\nUn-deletes a previously deleted dataset.  If the dataset was not deleted, nothing happens.\nParameters:\ndataset_id (string) – The id of the dataset to un-delete\nReturn type:\nNone\nclassmethod list(category=None, filter_failed=None, order_by=None, use_cases=None)\nList all datasets a user can view.\nParameters:\ncategory (string, optional) – Optional. If specified, only dataset versions that have the specified category will be\nincluded in the results. Categories identify the intended use of the dataset; supported\ncategories are “TRAINING” and “PREDICTION”.\nfilter_failed (Optional[bool]) – If unset, uses the server default: False.\nWhether datasets that failed during import should be excluded from the results.\nIf True invalid datasets will be excluded.\norder_by (string, optional) – If unset, uses the server default: “-created”.\nSorting order which will be applied to catalog list, valid options are:\n- “created” – ascending order by creation datetime;\n- “-created” – descending order by creation datetime.\nuse_cases (Union[UseCase, List[UseCase], str, List[str]], optional) – Filter available datasets by a specific Use Case or Cases. Accepts either the entity or the ID.\nIf set to [None], the method filters the project’s datasets by those not linked to a UseCase.\nReturns:\na list of datasets the user can view\nReturn type:\nlist[Dataset]\nclassmethod iterate(offset=None, limit=None, category=None, order_by=None, filter_failed=None, use_cases=None)\nGet an iterator for the requested datasets a user can view.\nThis lazily retrieves results. It does not get the next page from the server until the\ncurrent page is exhausted.\nParameters:\noffset (Optional[int]) – If set, this many results will be skipped\nlimit (Optional[int]) – Specifies the size of each page retrieved from the server.  If unset, uses the server\ndefault.\ncategory (string, optional) – Optional. If specified, only dataset versions that have the specified category will be\nincluded in the results. Categories identify the intended use of the dataset; supported\ncategories are “TRAINING” and “PREDICTION”.\nfilter_failed (Optional[bool]) – If unset, uses the server default: False.\nWhether datasets that failed during import should be excluded from the results.\nIf True invalid datasets will be excluded.\norder_by (string, optional) – If unset, uses the server default: “-created”.\nSorting order which will be applied to catalog list, valid options are:\n- “created” – ascending order by creation datetime;\n- “-created” – descending order by creation datetime.\nuse_cases (Union[UseCase, List[UseCase], str, List[str]], optional) – Filter available datasets by a specific Use Case or Cases. Accepts either the entity or the ID.\nIf set to [None], the method filters the project’s datasets by those not linked to a UseCase.\nYields:\nDataset – An iterator of the datasets the user can view.\nReturn type:\nGenerator[TypeVar(TDataset, bound= Dataset), None, None]\nupdate()\nUpdates the Dataset attributes in place with the latest information from the server.\nReturn type:\nNone\nmodify(name=None, categories=None)\nModifies the Dataset name and/or categories.  Updates the object in place.\nParameters:\nname (string, optional) – The new name of the dataset\ncategories (list[string], optional) – A list of strings describing the intended use of the\ndataset. The supported options are “TRAINING” and “PREDICTION”. If any\ncategories were previously specified for the dataset, they will be overwritten.\nIf omitted or None, keep previous categories. To clear them specify []\nReturn type:\nNone\nshare(access_list, apply_grant_to_linked_objects=False)\nModify the ability of users to access this dataset\nParameters:\naccess_list (list of SharingAccess) – The modifications to make.\napply_grant_to_linked_objects (bool) – If true for any users being granted access to the dataset, grant the user read access to\nany linked objects such as DataSources and DataStores that may be used by this dataset.\nIgnored if no such objects are relevant for dataset, defaults to False.\nReturn type:\nNone\nRaises:\ndatarobot.ClientError: – If you do not have permission to share this dataset, if the user you’re sharing with\ndoesn’t exist, if the same user appears multiple times in the access_list, or if these\nchanges would leave the dataset without an owner.\nExamples\nTransfer access to the dataset from old_user@datarobot.com to new_user@datarobot.com\nfrom datarobot.enums import SHARING_ROLE\nfrom datarobot.models.dataset import Dataset\nfrom datarobot.models.sharing import SharingAccess\nnew_access = SharingAccess(\n\"[email protected]\",\nSHARING_ROLE.OWNER,\ncan_share=True,\n)\naccess_list = [\nSharingAccess(\n\"[email protected]\",\nSHARING_ROLE.OWNER,\ncan_share=True,\ncan_use_data=True,\n),\nnew_access,\n]\nDataset.get('my-dataset-id').share(access_list)\nget_details()\nGets the details for this Dataset\nReturn type:\nDatasetDetails\nget_all_features(order_by=None)\nGet a list of all the features for this dataset.\nParameters:\norder_by (string, optional) – If unset, uses the server default: ‘name’.\nHow the features should be ordered. Can be ‘name’ or ‘featureType’.\nReturn type:\nlist[DatasetFeature]\niterate_all_features(offset=None, limit=None, order_by=None)\nGet an iterator for the requested features of a dataset.\nThis lazily retrieves results. It does not get the next page from the server until the\ncurrent page is exhausted.\nParameters:\noffset (Optional[int]) – If set, this many results will be skipped.\nlimit (Optional[int]) – Specifies the size of each page retrieved from the server.  If unset, uses the server\ndefault.\norder_by (string, optional) – If unset, uses the server default: ‘name’.\nHow the features should be ordered. Can be ‘name’ or ‘featureType’.\nYields:\nDatasetFeature\nReturn type:\nGenerator[DatasetFeature, None, None]\nget_featurelists()\nGet DatasetFeaturelists created on this Dataset\nReturns:\nfeature_lists\nReturn type:\nlist[DatasetFeaturelist]\ncreate_featurelist(name, features)\nCreate a new dataset featurelist\nParameters:\nname (str) – the name of the modeling featurelist to create. Names must be unique within the\ndataset, or the server will return an error.\nfeatures (List[str]) – the names of the features to include in the dataset featurelist. Each feature must\nbe a dataset feature.\nReturns:\nfeaturelist – the newly created featurelist\nReturn type:\nDatasetFeaturelist\nExamples\ndataset = Dataset.get('1234deadbeeffeeddead4321')\ndataset_features = dataset.get_all_features()\nselected_features = [feat.name for feat in dataset_features][:5]  # select first five\nnew_flist = dataset.create_featurelist('Simple Features', selected_features)\nget_file(file_path=None, filelike=None)\nRetrieves all the originally uploaded data in CSV form.\nWrites it to either the file or a filelike object that can write bytes.\nOnly one of file_path or filelike can be provided and it must be provided as a\nkeyword argument (i.e. file_path=’path-to-write-to’). If a file-like object is\nprovided, the user is responsible for closing it when they are done.\nThe user must also have permission to download data.\nParameters:\nfile_path (string, optional) – The destination to write the file to.\nfilelike (file, optional) – A file-like object to write to.  The object must be able to write bytes. The user is\nresponsible for closing the object\nReturn type:\nNone\nget_as_dataframe(low_memory=False)\nRetrieves all the originally uploaded data in a pandas DataFrame.\nAdded in version v3.0.\nParameters:\nlow_memory (Optional[bool]) – If True, use local files to reduce memory usage which will be slower.\nReturn type:\npd.DataFrame\nget_projects()\nRetrieves the Dataset’s projects as ProjectLocation named tuples.\nReturns:\nlocations\nReturn type:\nlist[ProjectLocation]\ncreate_project(project_name=None, user=None, password=None, credential_id=None, use_kerberos=None, credential_data=None, *, use_cases=None)\nCreate a datarobot.models.Project from this dataset\nParameters:\nproject_name (string, optional) – The name of the project to be created.\nIf not specified, will be “Untitled Project” for database connections, otherwise\nthe project name will be based on the file used.\nuser (string, optional) – The username for database authentication.\npassword (string, optional) – The password (in cleartext) for database authentication. The password\nwill be encrypted on the server side in scope of HTTP request and never saved or stored\ncredential_id (string, optional) – The ID of the set of credentials to use instead of user and password.\nuse_kerberos (Optional[bool]) – Server default is False.\nIf true, use kerberos authentication for database authentication.\ncredential_data (dict, optional) – The credentials to authenticate with the database, to use instead of user/password or\ncredential ID.\nuse_cases (list[UseCase] | UseCase | list[string] | string, optional) – A list of UseCase objects, UseCase object,\nlist of Use Case ids or a single Use Case id to add this new Dataset to. Must be a kwarg.\nReturn type:\nProject\nclassmethod create_version_from_file(dataset_id, file_path=None, filelike=None, categories=None, read_timeout=600, max_wait=600)\nA blocking call that creates a new Dataset version from a file. Returns when the new dataset\nversion has been successfully uploaded and processed.\nWarning: This function does not clean up it’s open files. If you pass a filelike, you are\nresponsible for closing it. If you pass a file_path, this will create a file object from\nthe file_path but will not close it.\nAdded in version v2.23.\nParameters:\ndataset_id (string) – The ID of the dataset for which new version to be created\nfile_path (string, optional) – The path to the file. This will create a file object pointing to that file but will\nnot close it.\nfilelike (file, optional) – An open and readable file object.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nread_timeout (Optional[int]) – The maximum number of seconds to wait for the server to respond indicating that the\ninitial upload is complete\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful\nReturns:\nresponse – A fully armed and operational Dataset version\nReturn type:\nDataset\nclassmethod create_version_from_in_memory_data(dataset_id, data_frame=None, records=None, categories=None, read_timeout=600, max_wait=600)\nA blocking call that creates a new Dataset version for a dataset from in-memory data.\nReturns when the dataset has been successfully uploaded and processed.\nThe data can be either a pandas DataFrame or a list of dictionaries with identical keys.\nAdded in version v2.23.\nParameters:\ndataset_id (string) – The ID of the dataset for which new version to be created\ndata_frame (DataFrame, optional) – The data frame to upload\nrecords (list[dict], optional) – A list of dictionaries with identical keys to upload\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nread_timeout (Optional[int]) – The maximum number of seconds to wait for the server to respond indicating that the\ninitial upload is complete\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful\nReturns:\nresponse – The Dataset version created from the uploaded data\nReturn type:\nDataset\nRaises:\nInvalidUsageError – If neither a DataFrame or list of records is passed.\nclassmethod create_version_from_url(dataset_id, url, categories=None, max_wait=600)\nA blocking call that creates a new Dataset from data stored at a url for a given dataset.\nReturns when the dataset has been successfully uploaded and processed.\nAdded in version v2.23.\nParameters:\ndataset_id (string) – The ID of the dataset for which new version to be created\nurl (string) – The URL to use as the source of data for the dataset being created.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful\nReturns:\nresponse – The Dataset version created from the uploaded data\nReturn type:\nDataset\nclassmethod create_version_from_datastage(dataset_id, datastage_id, categories=None, max_wait=600)\nA blocking call that creates a new Dataset from data stored as a DataStage for a given dataset.\nReturns when the dataset has been successfully uploaded and processed.\nParameters:\ndataset_id (string) – The ID of the dataset for which new version to be created\ndatastage_id (string) – The ID of the DataStage to use as the source of data for the dataset being created.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful\nReturns:\nresponse – The Dataset version created from the uploaded data\nReturn type:\nDataset\nclassmethod create_version_from_data_source(dataset_id, data_source_id, username=None, password=None, categories=None, credential_id=None, use_kerberos=None, credential_data=None, max_wait=600)\nA blocking call that creates a new Dataset from data stored at a DataSource.\nReturns when the dataset has been successfully uploaded and processed.\nAdded in version v2.23.\nParameters:\ndataset_id (string) – The ID of the dataset for which new version to be created\ndata_source_id (string) – The ID of the DataSource to use as the source of data.\nusername (string, optional) – The username for database authentication.\npassword (string, optional) – The password (in cleartext) for database authentication. The password\nwill be encrypted on the server side in scope of HTTP request and never saved or stored.\ncategories (list[string], optional) – An array of strings describing the intended use of the dataset. The\ncurrent supported options are “TRAINING” and “PREDICTION”.\ncredential_id (string, optional) – The ID of the set of credentials to\nuse instead of user and password. Note that with this change, username and password\nwill become optional.\nuse_kerberos (Optional[bool]) – If unset, uses the server default: False.\nIf true, use kerberos authentication for database authentication.\ncredential_data (dict, optional) – The credentials to authenticate with the database, to use instead of user/password or\ncredential ID.\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful\nReturns:\nresponse – The Dataset version created from the uploaded data\nReturn type:\nDataset\nclassmethod create_version_from_recipe(dataset_id, recipe, credential=None, credential_id=None, use_kerberos=None, max_wait=600)\nA blocking call that creates a new Dataset version from Recipe.\nReturns when the dataset has been successfully uploaded and processed.\nAdded in version v3.8.\nParameters:\ndataset_id (string) – The ID of the dataset for which a new version will be created.\nrecipe (Recipe) – The Recipe to use to create a new dataset version.\ncredential (Credential, optional) – The credentials to authenticate with the database.\ncredential_id (string, optional) – The ID of the set of credentials to use instead of Credential object.\nuse_kerberos (Optional[bool]) – If unset, uses the server default: False.\nIf true, use kerberos authentication for database authentication.\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful.\nReturns:\nresponse – The Dataset version created from the uploaded data\nReturn type:\nDataset\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nclass datarobot.DatasetDetails\nRepresents a detailed view of a Dataset. The to_dataset method creates a Dataset\nfrom this details view.\nVariables:\ndataset_id (string) – The ID of this dataset\nname (string) – The name of this dataset in the catalog\nis_latest_version (bool) – Whether this dataset version is the latest version\nof this dataset\nversion_id (string) – The object ID of the catalog_version the dataset belongs to\ncategories (list(string)) – An array of strings describing the intended use of the dataset. The\nsupported options are “TRAINING” and “PREDICTION”.\ncreated_at (string) – The date when the dataset was created\ncreated_by (string) – Username of the user who created the dataset\nis_snapshot (bool) – Whether the dataset version is an immutable snapshot of data\nwhich has previously been retrieved and saved to Data_robot\ndata_persisted (Optional[bool]) – If true, user is allowed to view extended data profile\n(which includes data statistics like min/max/median/mean, histogram, etc.) and download\ndata. If false, download is not allowed and only the data schema (feature names and types)\nwill be available.\nis_data_engine_eligible (bool) – Whether this dataset can be\na data source of a data engine query.\nprocessing_state (string) – Current ingestion process state of\nthe dataset\nrow_count (Optional[int]) – The number of rows in the dataset.\nsize (Optional[int]) – The size of the dataset as a CSV in bytes.\ndata_engine_query_id (string, optional) – ID of the source data engine query\ndata_source_id (string, optional) – ID of the datasource used as the source of the dataset\ndata_source_type (string) – the type of the datasource that was used as the source of the\ndataset\ndescription (string, optional) – the description of the dataset\neda1_modification_date (string, optional) – the ISO 8601 formatted date and time when the EDA1 for\nthe dataset was updated\neda1_modifier_full_name (string, optional) – the user who was the last to update EDA1 for the\ndataset\nerror (string) – details of exception raised during ingestion process, if any\nfeature_count (Optional[int]) – total number of features in the dataset\nfeature_count_by_type (list[FeatureTypeCount]) – number of features in the dataset grouped by feature type\nlast_modification_date (string) – the ISO 8601 formatted date and time when the dataset\nwas last modified\nlast_modifier_full_name (string) – full name of user who was the last to modify the\ndataset\ntags (list[string]) – list of tags attached to the item\nuri (string) – the uri to datasource like:\n- ‘file_name.csv’\n- ‘jdbc:DATA_SOURCE_GIVEN_NAME/SCHEMA.TABLE_NAME’\n- ‘jdbc:DATA_SOURCE_GIVEN_NAME/<query>’ - for query based datasources\n- ‘https://s3.amazonaws.com/my_data/my_dataset.csv’\n- etc.\nsample_size (dict, optional) – The size of data fetched during dataset registration. For example, to fetch the first 95\nrows,  the sample_size value is {‘type’: ‘rows’, ‘value’: 95}.\nCurrently only ‘rows’ type is supported.\nclassmethod get(dataset_id)\nGet details for a Dataset from the server\nParameters:\ndataset_id (str) – The id for the Dataset from which to get details\nReturn type:\nDatasetDetails\nto_dataset()\nBuild a Dataset object from the information in this object\nReturn type:\nDataset\nclass datarobot.models.dataset.ProjectLocation\nProjectLocation(url, id)\nid\nAlias for field number 1\nurl\nAlias for field number 0\nSecondary datasets\nclass datarobot.helpers.feature_discovery.SecondaryDataset\nA secondary dataset to be used for feature discovery\nAdded in version v2.25.\nVariables:\nidentifier (str) – Alias of the dataset (used directly as part of the generated feature names)\ncatalog_id (str) – Identifier of the catalog item\ncatalog_version_id (str) – Identifier of the catalog item version\nsnapshot_policy (Optional[str]) – Policy to use while creating a project or making predictions.\nIf omitted, by default endpoint will use ‘latest’.\nMust be one of the following values:\n‘specified’: Use specific snapshot specified by catalogVersionId\n‘latest’: Use latest snapshot from the same catalog item\n‘dynamic’: Get data from the source (only applicable for JDBC datasets)\nExamples\nimport datarobot as dr\ndataset_definition = dr.SecondaryDataset(\nidentifier='profile',\ncatalog_id='5ec4aec1f072bc028e3471ae',\ncatalog_version_id='5ec4aec2f072bc028e3471b1',\n)\nSecondary dataset configurations\nclass datarobot.models.SecondaryDatasetConfigurations\nCreate secondary dataset configurations for a given project\nAdded in version v2.20.\nVariables:\nid (str) – Id of this secondary dataset configuration\nproject_id (str) – Id of the associated project.\nconfig (list of DatasetConfiguration (Deprecated in version v2.23)) – List of secondary dataset configurations\nsecondary_datasets (list of SecondaryDataset (new in v2.23)) – List of secondary datasets (secondaryDataset)\nname (str) – Verbose name of the SecondaryDatasetConfig. null if it wasn’t specified.\ncreated (datetime.datetime) – DR-formatted datetime. null for legacy (before DR 6.0) db records.\ncreator_user_id (str) – Id of the user created this config.\ncreator_full_name (str) – fullname or email of the user created this config.\nfeaturelist_id (Optional[str]) – Id of the feature list. null if it wasn’t specified.\ncredential_ids (Optional[list of DatasetsCredentials]) – credentials used by the secondary datasets if the datasets used\nin the configuration are from datasource\nis_default (Optional[bool]) – Boolean flag if default config created during feature discovery aim\nproject_version (Optional[str]) – Version of project when its created (Release version)\nclassmethod create(project_id, secondary_datasets, name, featurelist_id=None)\ncreate secondary dataset configurations\nAdded in version v2.20.\nParameters:\nproject_id (str) – id of the associated project.\nsecondary_datasets (list of SecondaryDataset (New in version v2.23)) – list of secondary datasets used by the configuration\neach element is a datarobot.helpers.feature_discovery.SecondaryDataset\nname (str (New in version v2.23)) – Name of the secondary datasets configuration\nfeaturelist_id (str, or None (New in version v2.23)) – Id of the featurelist\nReturn type:\nan instance of SecondaryDatasetConfigurations\nRaises:\nClientError – raised if incorrect configuration parameters are provided\nExamples\nprofile_secondary_dataset = dr.SecondaryDataset(\nidentifier='profile',\ncatalog_id='5ec4aec1f072bc028e3471ae',\ncatalog_version_id='5ec4aec2f072bc028e3471b1',\nsnapshot_policy='latest'\n)\ntransaction_secondary_dataset = dr.SecondaryDataset(\nidentifier='transaction',\ncatalog_id='5ec4aec268f0f30289a03901',\ncatalog_version_id='5ec4aec268f0f30289a03900',\nsnapshot_policy='latest'\n)\nsecondary_datasets = [profile_secondary_dataset, transaction_secondary_dataset]\nnew_secondary_dataset_config = dr.SecondaryDatasetConfigurations.create(\nproject_id=project.id,\nname='My config',\nsecondary_datasets=secondary_datasets\n)\n>>> new_secondary_dataset_config.id\n'5fd1e86c589238a4e635e93d'\ndelete()\nRemoves the Secondary datasets configuration\n:rtype: None\nAdded in version v2.21.\nRaises:\nClientError – Raised if an invalid or already deleted secondary dataset config id is provided\nExamples\n# Deleting with a valid secondary_dataset_config id\nstatus_code = dr.SecondaryDatasetConfigurations.delete(some_config_id)\nstatus_code\n>>> 204\nget()\nRetrieve a single secondary dataset configuration for a given id\nAdded in version v2.21.\nReturns:\nsecondary_dataset_configurations – The requested secondary dataset configurations\nReturn type:\nSecondaryDatasetConfigurations\nExamples\nconfig_id = '5fd1e86c589238a4e635e93d'\nsecondary_dataset_config = dr.SecondaryDatasetConfigurations(id=config_id).get()\n>>> secondary_dataset_config\n{\n'created': datetime.datetime(2020, 12, 9, 6, 16, 22, tzinfo=tzutc()),\n'creator_full_name': u'[email protected]',\n'creator_user_id': u'asdf4af1gf4bdsd2fba1de0a',\n'credential_ids': None,\n'featurelist_id': None,\n'id': u'5fd1e86c589238a4e635e93d',\n'is_default': True,\n'name': u'My config',\n'project_id': u'5fd06afce2456ec1e9d20457',\n'project_version': None,\n'secondary_datasets': [\n{\n'snapshot_policy': u'latest',\n'identifier': u'profile',\n'catalog_version_id': u'5fd06b4af24c641b68e4d88f',\n'catalog_id': u'5fd06b4af24c641b68e4d88e'\n},\n{\n'snapshot_policy': u'dynamic',\n'identifier': u'transaction',\n'catalog_version_id': u'5fd1e86c589238a4e635e98e',\n'catalog_id': u'5fd1e86c589238a4e635e98d'\n}\n]\n}\nclassmethod list(project_id, featurelist_id=None, limit=None, offset=None)\nReturns list of secondary dataset configurations.\nAdded in version v2.23.\nParameters:\nproject_id (str) – The Id of project\nfeaturelist_id (Optional[str]) – Id of the feature list to filter the secondary datasets configurations\nReturns:\nsecondary_dataset_configurations – The requested list of secondary dataset configurations for a given project\nReturn type:\nlist of SecondaryDatasetConfigurations\nExamples\npid = '5fd06afce2456ec1e9d20457'\nsecondary_dataset_configs = dr.SecondaryDatasetConfigurations.list(pid)\n>>> secondary_dataset_configs[0]\n{\n'created': datetime.datetime(2020, 12, 9, 6, 16, 22, tzinfo=tzutc()),\n'creator_full_name': u'[email protected]',\n'creator_user_id': u'asdf4af1gf4bdsd2fba1de0a',\n'credential_ids': None,\n'featurelist_id': None,\n'id': u'5fd1e86c589238a4e635e93d',\n'is_default': True,\n'name': u'My config',\n'project_id': u'5fd06afce2456ec1e9d20457',\n'project_version': None,\n'secondary_datasets': [\n{\n'snapshot_policy': u'latest',\n'identifier': u'profile',\n'catalog_version_id': u'5fd06b4af24c641b68e4d88f',\n'catalog_id': u'5fd06b4af24c641b68e4d88e'\n},\n{\n'snapshot_policy': u'dynamic',\n'identifier': u'transaction',\n'catalog_version_id': u'5fd1e86c589238a4e635e98e',\n'catalog_id': u'5fd1e86c589238a4e635e98d'\n}\n]\n}\nData engine query generator\nclass datarobot.DataEngineQueryGenerator\nDataEngineQueryGenerator is used to set up time series data prep.\nAdded in version v2.27.\nVariables:\nid (str) – id of the query generator\nquery (str) – text of the generated Spark SQL query\ndatasets (list(QueryGeneratorDataset)) – datasets associated with the query generator\ngenerator_settings (QueryGeneratorSettings) – the settings used to define the query\ngenerator_type (str) – “TimeSeries” is the only supported type\nclassmethod create(generator_type, datasets, generator_settings)\nCreates a query generator entity.\nAdded in version v2.27.\nParameters:\ngenerator_type (str) – Type of data engine query generator\ndatasets (List[QueryGeneratorDataset]) – Source datasets in the Data Engine workspace.\ngenerator_settings (dict) – Data engine generator settings of the given generator_type.\nReturns:\nquery_generator – The created generator\nReturn type:\nDataEngineQueryGenerator\nExamples\nimport datarobot as dr\nfrom datarobot.models.data_engine_query_generator import (\nQueryGeneratorDataset,\nQueryGeneratorSettings,\n)\ndataset = QueryGeneratorDataset(\nalias='My_Awesome_Dataset_csv',\ndataset_id='61093144cabd630828bca321',\ndataset_version_id=1,\n)\nsettings = QueryGeneratorSettings(\ndatetime_partition_column='date',\ntime_unit='DAY',\ntime_step=1,\ndefault_numeric_aggregation_method='sum',\ndefault_categorical_aggregation_method='mostFrequent',\n)\ng = dr.DataEngineQueryGenerator.create(\ngenerator_type='TimeSeries',\ndatasets=[dataset],\ngenerator_settings=settings,\n)\ng.id\n>>>'54e639a18bd88f08078ca831'\ng.generator_type\n>>>'TimeSeries'\nclassmethod get(generator_id)\nGets information about a query generator.\nParameters:\ngenerator_id (str) – The identifier of the query generator you want to load.\nReturns:\nquery_generator – The queried generator\nReturn type:\nDataEngineQueryGenerator\nExamples\nimport datarobot as dr\ng = dr.DataEngineQueryGenerator.get(generator_id='54e639a18bd88f08078ca831')\ng.id\n>>>'54e639a18bd88f08078ca831'\ng.generator_type\n>>>'TimeSeries'\ncreate_dataset(dataset_id=None, dataset_version_id=None, max_wait=600)\nA blocking call that creates a new Dataset from the query generator.\nReturns when the dataset has been successfully processed. If optional\nparameters are not specified the query is applied to the dataset_id\nand dataset_version_id stored in the query generator. If specified they\nwill override the stored dataset_id/dataset_version_id, i.e. to prep a\nprediction dataset.\nParameters:\ndataset_id (Optional[str]) – The id of the unprepped dataset to apply the query to\ndataset_version_id (Optional[str]) – The version_id of the unprepped dataset to apply the query to\nReturns:\nresponse – The Dataset created from the query generator\nReturn type:\nDataset\nprepare_prediction_dataset_from_catalog(project_id, dataset_id, dataset_version_id=None, max_wait=600, relax_known_in_advance_features_check=None)\nApply time series data prep to a catalog dataset and upload it to the project\nas a PredictionDataset.\nAdded in version v3.1.\nParameters:\nproject_id (str) – The id of the project to which you upload the prediction dataset.\ndataset_id (str) – The identifier of the dataset.\ndataset_version_id (Optional[str]) – The version id of the dataset to use.\nmax_wait (Optional[int]) – Optional, the maximum number of seconds to wait before giving up.\nrelax_known_in_advance_features_check (Optional[bool]) – For time series projects only. If True, missing values in the\nknown in advance features are allowed in the forecast window at the prediction time.\nIf omitted or False, missing values are not allowed.\nReturns:\ndataset – The newly uploaded dataset.\nReturn type:\nPredictionDataset\nprepare_prediction_dataset(sourcedata, project_id, max_wait=600, relax_known_in_advance_features_check=None)\nApply time series data prep and upload the PredictionDataset to the project.\nAdded in version v3.1.\nParameters:\nsourcedata (str, file or pandas.DataFrame) – Data to be used for predictions. If it is a string, it can be either a path to a local file,\nor raw file content. If using a file on disk, the filename must consist of ASCII\ncharacters only.\nproject_id (str) – The id of the project to which you upload the prediction dataset.\nmax_wait (Optional[int]) – The maximum number of seconds to wait for the uploaded dataset to be processed before\nraising an error.\nrelax_known_in_advance_features_check (Optional[bool]) – For time series projects only. If True, missing values in the\nknown in advance features are allowed in the forecast window at the prediction time.\nIf omitted or False, missing values are not allowed.\nReturns:\ndataset – The newly uploaded dataset.\nReturn type:\nPredictionDataset\nRaises:\nInputNotUnderstoodError – Raised if sourcedata isn’t one of supported types.\nAsyncFailureError – Raised if polling for the status of an async process resulted in a response with an\nunsupported status code.\nAsyncProcessUnsuccessfulError – Raised if project creation was unsuccessful (i.e. the server reported an error in\nuploading the dataset).\nAsyncTimeoutError – Raised if processing the uploaded dataset took more time than specified\nby the max_wait parameter.\nSharing access\nclass datarobot.SharingAccess\nRepresents metadata about whom a entity (e.g. a data store) has been shared with\nAdded in version v2.14.\nCurrently DataStores,\nDataSources,\nDatasets,\nProjects (new in version v2.15) and\nCalendarFiles (new in version 2.15) can be shared.\nThis class can represent either access that has already been granted, or be used to grant access\nto additional users.\nVariables:\nusername (str) – a particular user\nrole (str or None) – if a string, represents a particular level of access and should be one of\ndatarobot.enums.SHARING_ROLE.  For more information on the specific access levels, see\nthe sharing documentation.  If None, can be passed to a share\nfunction to revoke access for a specific user.\ncan_share (bool or None) – if a bool, indicates whether this user is permitted to further share.  When False, the\nuser has access to the entity, but can only revoke their own access but not modify any\nuser’s access role.  When True, the user can share with any other user at a access role up\nto their own.  May be None if the SharingAccess was not retrieved from the DataRobot server\nbut intended to be passed into a share function; this will be equivalent to passing True.\ncan_use_data (bool or None) – if a bool, indicates whether this user should be able to view, download and process data\n(use to create projects, predictions, etc). For OWNER can_use_data is always True. If role\nis empty canUseData is ignored.\nuser_id (str or None) – the id of the user\nSharing role\nclass datarobot.models.sharing.SharingRole\nRepresents metadata about a user who has been granted access to an entity.\nAt least one of id or username must be set.\nVariables:\nid (str or None) – The ID of the user.\nrole (str) – Represents a particular level of access. Should be one of\ndatarobot.enums.SHARING_ROLE.\nshare_recipient_type (SHARING_RECIPIENT_TYPE) – The type of user for the object of the method. Can be user or organization.\nuser_full_name (str or None) – The full name of the user.\nusername (str or None) – The username (usually the email) of the user.\ncan_share (bool or None) – Indicates whether this user is permitted to share with other users. When False, the\nuser has access to the entity, but can only revoke their own access. They cannot not modify\nany user’s access role. When True, the user can share with any other user at an access\nrole up to their own.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-registry.html",
      "tags": [
        "beginner",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/data-registry.html",
        "content_length": 47611
      },
      "code_examples": [],
      "api_methods": [
        "dr.dataenginequerygenerator.get",
        "datarobot.models.dataset",
        "datarobot.models.data_engine_query_generator",
        "project.id",
        "dr.secondarydatasetconfigurations.list",
        "datarobot.enums.sharing_role",
        "datarobot.models.project",
        "datarobot.helpers.feature_discovery",
        "dr.secondarydatasetconfigurations.delete",
        "dr.dataenginequerygenerator.create",
        "dr.secondarydatasetconfigurations.create",
        "datarobot.models.secondarydatasetconfigurations",
        "datarobot.models.sharing"
      ],
      "complexity_score": 0.95,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-3071577430992067889",
      "title": "Exceptions",
      "content": "Exceptions\nexception datarobot.errors.AppPlatformError\nRaised by Client.request() for requests that:\nReturn a non-200 HTTP response, or\nConnection refused/timeout or\nResponse timeout or\nMalformed request\nHave a malformed/missing header in the response.\nexception datarobot.errors.ServerError\nFor 500-level responses from the server\nexception datarobot.errors.ClientError\nFor 400-level responses from the server\nhas json parameter for additional information to be stored about error\nif need be\nexception datarobot.errors.InputNotUnderstoodError\nRaised if a method is called in a way that cannot be understood\nexception datarobot.errors.InvalidUsageError\nRaised when methods are called with invalid or incompatible arguments\nexception datarobot.errors.AllRetriesFailedError\nRaised when the retry manager does not successfully make a request\nexception datarobot.errors.InvalidModelCategoryError\nRaised when method specific for model category was called from wrong model\nexception datarobot.errors.AsyncTimeoutError\nRaised when an asynchronous operation did not successfully get resolved\nwithin a specified time limit\nexception datarobot.errors.AsyncFailureError\nRaised when querying an asynchronous status resulted in an exceptional\nstatus code (not 200 and not 303)\nexception datarobot.errors.ProjectAsyncFailureError\nWhen an AsyncFailureError occurs during project creation or finalizing the project\nsettings for modeling. This exception will have the attributes status_code\nindicating the unexpected status code from the server, and async_location indicating\nwhich asynchronous status object was being polled when the failure happened.\nexception datarobot.errors.AsyncProcessUnsuccessfulError\nRaised when querying an asynchronous status showed that async process\nwas not successful\nexception datarobot.errors.AsyncModelCreationError\nRaised when querying an asynchronous status showed that model creation\nwas not successful\nexception datarobot.errors.AsyncPredictionsGenerationError\nRaised when querying an asynchronous status showed that predictions\ngeneration was not successful\nexception datarobot.errors.PendingJobFinished\nRaised when the server responds with a 303 for the pending creation of a\nresource.\nexception datarobot.errors.JobNotFinished\nRaised when execution was trying to get a finished resource from a pending\njob, but the job is not finished\nexception datarobot.errors.DuplicateFeaturesError\nRaised when trying to create featurelist with duplicating features\nexception datarobot.errors.TrainingDataAssignmentError\nRaised when the training data assignment for a custom model version fails\nexception datarobot.errors.DataRobotDeprecationWarning\nRaised when using deprecated functions or using functions in a deprecated way\nSee also\nPlatformDeprecationWarning\nexception datarobot.errors.IllegalFileName\nRaised when trying to use a filename we can’t handle.\nexception datarobot.errors.JobAlreadyRequested\nRaised when the requested model has already been requested.\nexception datarobot.errors.ContentRetrievalTerminatedError\nRaised when due to content retrieval error process of data retrieval was terminated.\nexception datarobot.errors.UpdateAttributesError\nexception datarobot.errors.InvalidRatingTableWarning\nRaised when using interacting with rating tables that failed validation\nexception datarobot.errors.PartitioningMethodWarning\nRaised when interacting with project methods related to partition classes, i.e.\nProject.set_partitioning_method() or Project.set_datetime_partitioning().\nexception datarobot.errors.NonPersistableProjectOptionWarning\nRaised when setting project options via Project.set_options if any of the options\npassed are not supported for POST requests to /api/v2/project/{project_id}/options/.\nAll options that fall under this category can be found here:\ndatarobot.enums.NonPersistableProjectOptions().\nexception datarobot.errors.OverwritingProjectOptionWarning\nRaised when setting project options via Project.set_options if any of the options\npassed have already been set to a value in Project.advanced_options, or if\na different value is already stored in the endpoint /api/v2/project/{project_id}/options/.\nPrecedence is given to the new value you passed in.\nexception datarobot.errors.NoRedundancyImpactAvailable\nRaised when retrieving old feature impact data\nRedundancy detection was added in v2.13 of the API, and some projects, e.g. multiclass projects\ndo not support redundancy detection. This warning is raised to make\nclear that redundancy detection is unavailable.\nexception datarobot.errors.ParentModelInsightFallbackWarning\nRaised when insights are unavailable for a model and\ninsight retrieval falls back to retrieving insights\nfor a model’s parent model\nexception datarobot.errors.ProjectHasNoRecommendedModelWarning\nRaised when a project has no recommended model.\nexception datarobot.errors.PlatformDeprecationWarning\nRaised when Deprecation header is returned in the API, for example a project may be\ndeprecated as part of the 2022 Python 3 platform migration.\nSee also\nDataRobotDeprecationWarning\nexception datarobot.errors.MultipleUseCasesNotAllowed\nRaised when a method decorated with add_to_use_case(allow_multiple=True) calls a method\ndecorated with add_to_use_case(allow_multiple=False) with multiple UseCases passed",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/errors.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/errors.html",
        "content_length": 5309
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.errors.multipleusecasesnotallowed",
        "datarobot.errors.appplatformerror",
        "datarobot.errors.asyncprocessunsuccessfulerror",
        "datarobot.errors.invalidratingtablewarning",
        "datarobot.errors.allretriesfailederror",
        "datarobot.errors.invalidmodelcategoryerror",
        "datarobot.enums.nonpersistableprojectoptions",
        "datarobot.errors.asyncmodelcreationerror",
        "datarobot.errors.invalidusageerror",
        "datarobot.errors.asyncfailureerror",
        "datarobot.errors.datarobotdeprecationwarning",
        "datarobot.errors.inputnotunderstooderror",
        "datarobot.errors.noredundancyimpactavailable",
        "datarobot.errors.platformdeprecationwarning",
        "datarobot.errors.jobalreadyrequested",
        "datarobot.errors.asynctimeouterror",
        "datarobot.errors.trainingdataassignmenterror",
        "datarobot.errors.updateattributeserror",
        "datarobot.errors.overwritingprojectoptionwarning",
        "datarobot.errors.asyncpredictionsgenerationerror",
        "datarobot.errors.parentmodelinsightfallbackwarning",
        "datarobot.errors.illegalfilename",
        "project.set_options",
        "project.set_partitioning_method",
        "datarobot.errors.servererror",
        "datarobot.errors.projecthasnorecommendedmodelwarning",
        "datarobot.errors.pendingjobfinished",
        "project.set_datetime_partitioning",
        "datarobot.errors.partitioningmethodwarning",
        "datarobot.errors.duplicatefeatureserror",
        "project.advanced_options",
        "datarobot.errors.contentretrievalterminatederror",
        "datarobot.errors.projectasyncfailureerror",
        "datarobot.errors.jobnotfinished",
        "datarobot.errors.nonpersistableprojectoptionwarning",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_2356858551056069241",
      "title": "Features",
      "content": "Features\nclass datarobot.models.Feature\nA feature from a project’s dataset\nThese are features either included in the originally uploaded dataset or added to it via\nfeature transformations.  In time series projects, these will be distinct from the\nModelingFeature s created during partitioning;\notherwise, they will correspond to the same features.  For more information about input and\nmodeling features, see the time series documentation.\nThe min, max, mean, median, and std_dev attributes provide information about\nthe distribution of the feature in the EDA sample data.  For non-numeric features or features\ncreated prior to these summary statistics becoming available, they will be None.  For features\nwhere the summary statistics are available, they will be in a format compatible with the data\ntype, i.e. date type features will have their summary statistics expressed as ISO-8601\nformatted date strings.\nVariables:\nid (int) – the id for the feature - note that name is used to reference the feature instead of id\nproject_id (str) – the id of the project the feature belongs to\nname (str) – the name of the feature\nfeature_type (str) – the type of the feature, e.g. ‘Categorical’, ‘Text’\nimportance (float or None) – numeric measure of the strength of relationship between the feature and target (independent\nof any model or other features); may be None for non-modeling features such as partition\ncolumns\nlow_information (bool) – whether a feature is considered too uninformative for modeling (e.g. because it has too few\nvalues)\nunique_count (int) – number of unique values\nna_count (int or None) – number of missing values\ndate_format (str or None) – For Date features, the date format string for how this feature\nwas interpreted, compatible with https://docs.python.org/2/library/time.html#time.strftime .\nFor other feature types, None.\nmin (str, int, float, or None) – The minimum value of the source data in the EDA sample\nmax (str, int, float, or None) – The maximum value of the source data in the EDA sample\nmean (str, int, or, float) – The arithmetic mean of the source data in the EDA sample\nmedian (str, int, float, or None) – The median of the source data in the EDA sample\nstd_dev (str, int, float, or None) – The standard deviation of the source data in the EDA sample\ntime_series_eligible (bool) – Whether this feature can be used as the datetime partition column in a time series project.\ntime_series_eligibility_reason (str) – Why the feature is ineligible for the datetime partition column in a time series project,\nor ‘suitable’ when it is eligible.\ntime_step (int or None) – For time series eligible features, a positive integer determining the interval at which\nwindows can be specified. If used as the datetime partition column on a time series\nproject, the feature derivation and forecast windows must start and end at an integer\nmultiple of this value. None for features that are not time series eligible.\ntime_unit (str or None) – For time series eligible features, the time unit covered by a single time step, e.g. ‘HOUR’,\nor None for features that are not time series eligible.\ntarget_leakage (str) – Whether a feature is considered to have target leakage or not.  A value of\n‘SKIPPED_DETECTION’ indicates that target leakage detection was not run on the feature.\n‘FALSE’ indicates no leakage, ‘MODERATE’ indicates a moderate risk of target leakage, and\n‘HIGH_RISK’ indicates a high risk of target leakage\nfeature_lineage_id (str) – id of a lineage for automatically discovered features or derived time series features.\nkey_summary (list of dict) – Statistics for top 50 keys (truncated to 103 characters) of\nSummarized Categorical column example:\n{‘key’:’DataRobot’,\n‘summary’:{‘min’:0, ‘max’:29815.0, ‘stdDev’:6498.029, ‘mean’:1490.75,\n‘median’:0.0, ‘pctRows’:5.0}}\nwhere,\nkey: string or Nonename of the key\nsummary: dictstatistics of the key\nmax: maximum value of the key.\nmin: minimum value of the key.\nmean: mean value of the key.\nmedian: median value of the key.\nstdDev: standard deviation of the key.\npctRows: percentage occurrence of key in the EDA sample of the feature.\nmultilabel_insights_key (str or None) – For multicategorical columns this will contain a key for multilabel insights. The key is\nunique for a project, feature and EDA stage combination. This will be the key for the most\nrecent, finished EDA stage.\nclassmethod get(project_id, feature_name)\nRetrieve a single feature\nParameters:\nproject_id (str) – The ID of the project the feature is associated with.\nfeature_name (str) – The name of the feature to retrieve\nReturns:\nfeature – The queried instance\nReturn type:\nFeature\nget_multiseries_properties(multiseries_id_columns, max_wait=600)\nRetrieve time series properties for a potential multiseries datetime partition column\nMultiseries time series projects use multiseries id columns to model multiple distinct\nseries within a single project.  This function returns the time series properties (time step\nand time unit) of this column if it were used as a datetime partition column with the\nspecified multiseries id columns, running multiseries detection automatically if it had not\npreviously been successfully ran.\nParameters:\nmultiseries_id_columns (List[str]) – the name(s) of the multiseries id columns to use with this datetime partition column.\nCurrently only one multiseries id column is supported.\nmax_wait (Optional[int]) – if a multiseries detection task is run, the maximum amount of time to wait for it to\ncomplete before giving up\nReturns:\nproperties –\nA dict with three keys:\ntime_series_eligible : bool, whether the column can be used as a partition column\ntime_unit : str or null, the inferred time unit if used as a partition column\ntime_step : int or null, the inferred time step if used as a partition column\nReturn type:\ndict\nget_cross_series_properties(datetime_partition_column, cross_series_group_by_columns, max_wait=600)\nRetrieve cross-series properties for multiseries ID column.\nThis function returns the cross-series properties (eligibility\nas group-by column) of this column if it were used with specified datetime partition column\nand with current multiseries id column, running cross-series group-by validation\nautomatically if it had not previously been successfully ran.\nParameters:\ndatetime_partition_column (datetime partition column)\ncross_series_group_by_columns (List[str]) – the name(s) of the columns to use with this multiseries ID column.\nCurrently only one cross-series group-by column is supported.\nmax_wait (Optional[int]) – if a multiseries detection task is run, the maximum amount of time to wait for it to\ncomplete before giving up\nReturns:\nproperties –\nA dict with three keys:\nname : str, column name\neligibility : str, reason for column eligibility\nisEligible : bool, is column eligible as cross-series group-by\nReturn type:\ndict\nget_multicategorical_histogram()\nRetrieve multicategorical histogram for this feature\nAdded in version v2.24.\nReturn type:\ndatarobot.models.MulticategoricalHistogram\nRaises:\ndatarobot.errors.InvalidUsageError – if this method is called on a unsuited feature\nValueError – if no multilabel_insights_key is present for this feature\nget_pairwise_correlations()\nRetrieve pairwise label correlation for multicategorical features\nAdded in version v2.24.\nReturn type:\ndatarobot.models.PairwiseCorrelations\nRaises:\ndatarobot.errors.InvalidUsageError – if this method is called on a unsuited feature\nValueError – if no multilabel_insights_key is present for this feature\nget_pairwise_joint_probabilities()\nRetrieve pairwise label joint probabilities for multicategorical features\nAdded in version v2.24.\nReturn type:\ndatarobot.models.PairwiseJointProbabilities\nRaises:\ndatarobot.errors.InvalidUsageError – if this method is called on a unsuited feature\nValueError – if no multilabel_insights_key is present for this feature\nget_pairwise_conditional_probabilities()\nRetrieve pairwise label conditional probabilities for multicategorical features\nAdded in version v2.24.\nReturn type:\ndatarobot.models.PairwiseConditionalProbabilities\nRaises:\ndatarobot.errors.InvalidUsageError – if this method is called on a unsuited feature\nValueError – if no multilabel_insights_key is present for this feature\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nget_histogram(bin_limit=None)\nRetrieve a feature histogram\nParameters:\nbin_limit (int or None) – Desired max number of histogram bins. If omitted, by default\nendpoint will use 60.\nReturns:\nfeatureHistogram – The requested histogram with desired number or bins\nReturn type:\nFeatureHistogram\nclass datarobot.models.ModelingFeature\nA feature used for modeling\nIn time series projects, a new set of modeling features is created after setting the\npartitioning options.  These features are automatically derived from those in the project’s\ndataset and are the features used for modeling.  Modeling features are only accessible once\nthe target and partitioning options have been set.  In projects that don’t use time series\nmodeling, once the target has been set, ModelingFeatures and Features will behave\nthe same.\nFor more information about input and modeling features, see the\ntime series documentation.\nAs with the Feature object, the min, max, `mean,\nmedian, and std_dev attributes provide information about the distribution of the feature in\nthe EDA sample data.  For non-numeric features, they will be None.  For features where the\nsummary statistics are available, they will be in a format compatible with the data type, i.e.\ndate type features will have their summary statistics expressed as ISO-8601 formatted date\nstrings.\nVariables:\nproject_id (str) – the id of the project the feature belongs to\nname (str) – the name of the feature\nfeature_type (str) – the type of the feature, e.g. ‘Categorical’, ‘Text’\nimportance (float or None) – numeric measure of the strength of relationship between the feature and target (independent\nof any model or other features); may be None for non-modeling features such as partition\ncolumns\nlow_information (bool) – whether a feature is considered too uninformative for modeling (e.g. because it has too few\nvalues)\nunique_count (int) – number of unique values\nna_count (int or None) – number of missing values\ndate_format (str or None) – For Date features, the date format string for how this feature\nwas interpreted, compatible with https://docs.python.org/2/library/time.html#time.strftime .\nFor other feature types, None.\nmin (str, int, float, or None) – The minimum value of the source data in the EDA sample\nmax (str, int, float, or None) – The maximum value of the source data in the EDA sample\nmean (str, int, or, float) – The arithmetic mean of the source data in the EDA sample\nmedian (str, int, float, or None) – The median of the source data in the EDA sample\nstd_dev (str, int, float, or None) – The standard deviation of the source data in the EDA sample\nparent_feature_names (List[str]) – A list of the names of input features used to derive this modeling feature.  In cases where\nthe input features and modeling features are the same, this will simply contain the\nfeature’s name.  Note that if a derived feature was used to create this modeling feature,\nthe values here will not necessarily correspond to the features that must be supplied at\nprediction time.\nkey_summary (list of dict) – Statistics for top 50 keys (truncated to 103 characters) of\nSummarized Categorical column example:\n{‘key’:’DataRobot’,\n‘summary’:{‘min’:0, ‘max’:29815.0, ‘stdDev’:6498.029, ‘mean’:1490.75,\n‘median’:0.0, ‘pctRows’:5.0}}\nwhere,\nkey: string or Nonename of the key\nsummary: dictstatistics of the key\nmax: maximum value of the key.\nmin: minimum value of the key.\nmean: mean value of the key.\nmedian: median value of the key.\nstdDev: standard deviation of the key.\npctRows: percentage occurrence of key in the EDA sample of the feature.\nclassmethod get(project_id, feature_name)\nRetrieve a single modeling feature\nParameters:\nproject_id (str) – The ID of the project the feature is associated with.\nfeature_name (str) – The name of the feature to retrieve\nReturns:\nfeature – The requested feature\nReturn type:\nModelingFeature\nclass datarobot.models.DatasetFeature\nA feature from a project’s dataset\nThese are features either included in the originally uploaded dataset or added to it via\nfeature transformations.\nThe min, max, mean, median, and std_dev attributes provide information about\nthe distribution of the feature in the EDA sample data.  For non-numeric features or features\ncreated prior to these summary statistics becoming available, they will be None.  For features\nwhere the summary statistics are available, they will be in a format compatible with the data\ntype, i.e. date type features will have their summary statistics expressed as ISO-8601\nformatted date strings.\nVariables:\nid (int) – the id for the feature - note that name is used to reference the feature instead of id\ndataset_id (str) – the id of the dataset the feature belongs to\ndataset_version_id (str) – the id of the dataset version the feature belongs to\nname (str) – the name of the feature\nfeature_type (Optional[str]) – the type of the feature, e.g. ‘Categorical’, ‘Text’\nlow_information (Optional[bool]) – whether a feature is considered too uninformative for modeling (e.g. because it has too few\nvalues)\nunique_count (Optional[int]) – number of unique values\nna_count (Optional[int]) – number of missing values\ndate_format (Optional[str]) – For Date features, the date format string for how this feature\nwas interpreted, compatible with https://docs.python.org/2/library/time.html#time.strftime .\nFor other feature types, None.\nmin (str, int, Optional[float]) – The minimum value of the source data in the EDA sample\nmax (str, int, Optional[float]) – The maximum value of the source data in the EDA sample\nmean (str, int, Optional[float]) – The arithmetic mean of the source data in the EDA sample\nmedian (str, int, Optional[float]) – The median of the source data in the EDA sample\nstd_dev (str, int, Optional[float]) – The standard deviation of the source data in the EDA sample\ntime_series_eligible (Optional[bool]) – Whether this feature can be used as the datetime partition column in a time series project.\ntime_series_eligibility_reason (Optional[str]) – Why the feature is ineligible for the datetime partition column in a time series project,\nor ‘suitable’ when it is eligible.\ntime_step (Optional[int]) – For time series eligible features, a positive integer determining the interval at which\nwindows can be specified. If used as the datetime partition column on a time series\nproject, the feature derivation and forecast windows must start and end at an integer\nmultiple of this value. None for features that are not time series eligible.\ntime_unit (Optional[str]) – For time series eligible features, the time unit covered by a single time step, e.g. ‘HOUR’,\nor None for features that are not time series eligible.\ntarget_leakage (Optional[str]) – Whether a feature is considered to have target leakage or not.  A value of\n‘SKIPPED_DETECTION’ indicates that target leakage detection was not run on the feature.\n‘FALSE’ indicates no leakage, ‘MODERATE’ indicates a moderate risk of target leakage, and\n‘HIGH_RISK’ indicates a high risk of target leakage\ntarget_leakage_reason (string, optional) – The descriptive text explaining the reason for target leakage, if any.\nget_histogram(bin_limit=None)\nRetrieve a feature histogram\nParameters:\nbin_limit (int or None) – Desired max number of histogram bins. If omitted, by default\nendpoint will use 60.\nReturns:\nfeatureHistogram – The requested histogram with desired number or bins\nReturn type:\nDatasetFeatureHistogram\nclass datarobot.models.DatasetFeatureHistogram\nclassmethod get(dataset_id, feature_name, bin_limit=None, key_name=None)\nRetrieve a single feature histogram\nParameters:\ndataset_id (str) – The ID of the Dataset the feature is associated with.\nfeature_name (str) – The name of the feature to retrieve\nbin_limit (int or None) – Desired max number of histogram bins. If omitted, by default\nthe endpoint will use 60.\nkey_name (string or None) – (Only required for summarized categorical feature)\nName of the top 50 keys for which plot to be retrieved\nReturns:\nfeatureHistogram – The queried instance with plot attribute in it.\nReturn type:\nFeatureHistogram\nclass datarobot.models.FeatureHistogram\nclassmethod get(project_id, feature_name, bin_limit=None, key_name=None)\nRetrieve a single feature histogram\nParameters:\nproject_id (str) – The ID of the project the feature is associated with.\nfeature_name (str) – The name of the feature to retrieve\nbin_limit (int or None) – Desired max number of histogram bins. If omitted, by default\nendpoint will use 60.\nkey_name (string or None) – (Only required for summarized categorical feature)\nName of the top 50 keys for which plot to be retrieved\nReturns:\nfeatureHistogram – The queried instance with plot attribute in it.\nReturn type:\nFeatureHistogram\nclass datarobot.models.InteractionFeature\nInteraction feature data\nAdded in version v2.21.\nVariables:\nrows (int) – Total number of rows\nsource_columns (list(str)) – names of two categorical features which were combined into this one\nbars (list(dict)) – dictionaries representing frequencies of each independent value from the source columns\nbubbles (list(dict)) – dictionaries representing frequencies of each combined value in the interaction feature.\nclassmethod get(project_id, feature_name)\nRetrieve a single Interaction feature\nParameters:\nproject_id (str) – The id of the project the feature belongs to\nfeature_name (str) – The name of the Interaction feature to retrieve\nReturns:\nfeature – The queried instance\nReturn type:\nInteractionFeature\nclass datarobot.models.MulticategoricalHistogram\nHistogram for Multicategorical feature.\nAdded in version v2.24.\nNotes\nHistogramValues contains:\nvalues.[].label : string - Label name\nvalues.[].plot : list - Histogram for label\nvalues.[].plot.[].label_relevance : int - Label relevance value\nvalues.[].plot.[].row_count : int - Row count where label has given relevance\nvalues.[].plot.[].row_pct : float - Percentage of rows where label has given relevance\nVariables:\nfeature_name (str) – Name of the feature\nvalues (list(dict)) – List of Histogram values with a schema described as HistogramValues\nclassmethod get(multilabel_insights_key)\nRetrieves multicategorical histogram\nYou might find it more convenient to use\nFeature.get_multicategorical_histogram\ninstead.\nParameters:\nmultilabel_insights_key (string) – Key for multilabel insights, unique for a project, feature and EDA stage combination.\nThe multilabel_insights_key can be retrieved via\nFeature.multilabel_insights_key.\nReturns:\nThe multicategorical histogram for multilabel_insights_key\nReturn type:\nMulticategoricalHistogram\nto_dataframe()\nConvenience method to get all the information from this multicategorical_histogram instance\nin form of a pandas.DataFrame.\nReturns:\nHistogram information as a multicategorical_histogram. The dataframe will contain these\ncolumns: feature_name, label, label_relevance, row_count and row_pct\nReturn type:\npandas.DataFrame\nclass datarobot.models.PairwiseCorrelations\nCorrelation of label pairs for multicategorical feature.\nAdded in version v2.24.\nNotes\nCorrelationValues contain:\nvalues.[].label_configuration : list of length 2 - Configuration of the label pair\nvalues.[].label_configuration.[].label : str – Label name\nvalues.[].statistic_value : float – Statistic value\nVariables:\nfeature_name (str) – Name of the feature\nvalues (list(dict)) – List of correlation values with a schema described as CorrelationValues\nstatistic_dataframe (pandas.DataFrame) – Correlation values for all label pairs as a DataFrame\nclassmethod get(multilabel_insights_key)\nRetrieves pairwise correlations\nYou might find it more convenient to use\nFeature.get_pairwise_correlations\ninstead.\nParameters:\nmultilabel_insights_key (string) – Key for multilabel insights, unique for a project, feature and EDA stage combination.\nThe multilabel_insights_key can be retrieved via\nFeature.multilabel_insights_key.\nReturns:\nThe pairwise label correlations\nReturn type:\nPairwiseCorrelations\nas_dataframe()\nThe pairwise label correlations as a (num_labels x num_labels) DataFrame.\nReturns:\nThe pairwise label correlations. Index and column names allow the interpretation of the\nvalues.\nReturn type:\npandas.DataFrame\nclass datarobot.models.PairwiseJointProbabilities\nJoint probabilities of label pairs for multicategorical feature.\nAdded in version v2.24.\nNotes\nProbabilityValues contain:\nvalues.[].label_configuration : list of length 2 - Configuration of the label pair\nvalues.[].label_configuration.[].relevance : int – 0 for absence of the labels,\n1 for the presence of labels\nvalues.[].label_configuration.[].label : str – Label name\nvalues.[].statistic_value : float – Statistic value\nVariables:\nfeature_name (str) – Name of the feature\nvalues (list(dict)) – List of joint probability values with a schema described as ProbabilityValues\nstatistic_dataframes (dict(pandas.DataFrame)) – Joint Probability values as DataFrames for different relevance combinations.\nE.g. The probability P(A=0,B=1) can be retrieved via:\npairwise_joint_probabilities.statistic_dataframes[(0,1)].loc['A', 'B']\nclassmethod get(multilabel_insights_key)\nRetrieves pairwise joint probabilities\nYou might find it more convenient to use\nFeature.get_pairwise_joint_probabilities\ninstead.\nParameters:\nmultilabel_insights_key (string) – Key for multilabel insights, unique for a project, feature and EDA stage combination.\nThe multilabel_insights_key can be retrieved via\nFeature.multilabel_insights_key.\nReturns:\nThe pairwise joint probabilities\nReturn type:\nPairwiseJointProbabilities\nas_dataframe(relevance_configuration)\nJoint probabilities of label pairs as a (num_labels x num_labels) DataFrame.\nParameters:\nrelevance_configuration (tuple of length 2) – Valid options are (0, 0), (0, 1), (1, 0) and (1, 1). Values of 0 indicate absence of\nlabels and 1 indicates presence of labels. The first value describes the\npresence for the labels in axis=0 and the second value describes the presence for the\nlabels in axis=1.\nFor example the matrix values for a relevance configuration of (0, 1) describe the\nprobabilities of absent labels in the index axis and present labels in the column\naxis.\nE.g. The probability P(A=0,B=1) can be retrieved via:\npairwise_joint_probabilities.as_dataframe((0,1)).loc['A', 'B']\nReturns:\nThe joint probabilities for the requested relevance_configuration. Index and column\nnames allow the interpretation of the values.\nReturn type:\npandas.DataFrame\nclass datarobot.models.PairwiseConditionalProbabilities\nConditional probabilities of label pairs for multicategorical feature.\nAdded in version v2.24.\nNotes\nProbabilityValues contain:\nvalues.[].label_configuration : list of length 2 - Configuration of the label pair\nvalues.[].label_configuration.[].relevance : int – 0 for absence of the labels,\n1 for the presence of labels\nvalues.[].label_configuration.[].label : str – Label name\nvalues.[].statistic_value : float – Statistic value\nVariables:\nfeature_name (str) – Name of the feature\nvalues (list(dict)) – List of conditional probability values with a schema described as ProbabilityValues\nstatistic_dataframes (dict(pandas.DataFrame)) – Conditional Probability values as DataFrames for different relevance combinations.\nThe label names in the columns are the events, on which we condition. The label names in the\nindex are the events whose conditional probability given the indexes is in the dataframe.\nE.g. The probability P(A=0|B=1) can be retrieved via:\npairwise_conditional_probabilities.statistic_dataframes[(0,1)].loc['A', 'B']\nclassmethod get(multilabel_insights_key)\nRetrieves pairwise conditional probabilities\nYou might find it more convenient to use\nFeature.get_pairwise_conditional_probabilities\ninstead.\nParameters:\nmultilabel_insights_key (string) – Key for multilabel insights, unique for a project, feature and EDA stage combination.\nThe multilabel_insights_key can be retrieved via\nFeature.multilabel_insights_key.\nReturns:\nThe pairwise conditional probabilities\nReturn type:\nPairwiseConditionalProbabilities\nas_dataframe(relevance_configuration)\nConditional probabilities of label pairs as a (num_labels x num_labels) DataFrame.\nThe label names in the columns are the events, on which we condition. The label names in the\nindex are the events whose conditional probability given the indexes is in the dataframe.\nE.g. The probability P(A=0|B=1) can be retrieved via:\npairwise_conditional_probabilities.as_dataframe((0, 1)).loc['A', 'B']\nParameters:\nrelevance_configuration (tuple of length 2) – Valid options are (0, 0), (0, 1), (1, 0) and (1, 1). Values of 0 indicate absence of\nlabels and 1 indicates presence of labels. The first value describes the\npresence for the labels in axis=0 and the second value describes the presence for the\nlabels in axis=1.\nFor example the matrix values for a relevance configuration of (0, 1) describe the\nprobabilities of absent labels in the index axis given the\npresence of labels in the column axis.\nReturns:\nThe conditional probabilities for the requested relevance_configuration.\nIndex and column names allow the interpretation of the values.\nReturn type:\npandas.DataFrame\nRestoring Discarded Features\nclass datarobot.models.restore_discarded_features.DiscardedFeaturesInfo\nAn object containing information about time series features which were reduced\nduring time series feature generation process. These features can be restored back to the\nproject. They will be included into All Time Series Features and can be used to create new\nfeature lists.\nAdded in version v2.27.\nVariables:\ntotal_restore_limit (int) – The total limit indicating how many features can be restored in this project.\nremaining_restore_limit (int) – The remaining available number of the features which can be restored in this project.\nfeatures (list of strings) – Discarded features which can be restored.\ncount (int) – Discarded features count.\nclassmethod restore(project_id, features_to_restore, max_wait=600)\nRestore discarded during time series feature generation process features back to the\nproject. After restoration features will be included into All Time Series Features.\nAdded in version v2.27.\nParameters:\nproject_id (string)\nfeatures_to_restore (list of strings) – List of the feature names to restore\nmax_wait (Optional[int]) – max time to wait for features to be restored.\nDefaults to 10 min\nReturns:\nstatus – information about features which were restored and which were not.\nReturn type:\nFeatureRestorationStatus\nclassmethod retrieve(project_id)\nRetrieve the discarded features information for a given project.\nAdded in version v2.27.\nParameters:\nproject_id (string)\nReturns:\ninfo – information about features which were discarded during feature generation process and\nlimits how many features can be restored.\nReturn type:\nDiscardedFeaturesInfo\nclass datarobot.models.restore_discarded_features.FeatureRestorationStatus\nStatus of the feature restoration process.\nAdded in version v2.27.\nVariables:\nwarnings (list of strings) – Warnings generated for those features which failed to restore\nremaining_restore_limit (int) – The remaining available number of the features which can be restored in this project.\nrestored_features (list of strings) – Features which were restored\nFeature lists\nclass datarobot.DatasetFeaturelist\nA set of features attached to a dataset in the AI Catalog\nVariables:\nid (str) – the id of the dataset featurelist\ndataset_id (str) – the id of the dataset the featurelist belongs to\ndataset_version_id (Optional[str]) – the version id of the dataset this featurelist belongs to\nname (str) – the name of the dataset featurelist\nfeatures (List[str]) – a list of the names of features included in this dataset featurelist\ncreation_date (datetime.datetime) – when the featurelist was created\ncreated_by (str) – the user name of the user who created this featurelist\nuser_created (bool) – whether the featurelist was created by a user or by DataRobot automation\ndescription (Optional[str]) – the description of the featurelist. Only present on DataRobot-created featurelists.\nclassmethod get(dataset_id, featurelist_id)\nRetrieve a dataset featurelist\nParameters:\ndataset_id (str) – the id of the dataset the featurelist belongs to\nfeaturelist_id (str) – the id of the dataset featurelist to retrieve\nReturns:\nfeaturelist – the specified featurelist\nReturn type:\nDatasetFeatureList\ndelete()\nDelete a dataset featurelist\nFeaturelists configured into the dataset as a default featurelist cannot be deleted.\nReturn type:\nNone\nupdate(name=None)\nUpdate the name of an existing featurelist\nNote that only user-created featurelists can be renamed, and that names must not\nconflict with names used by other featurelists.\nParameters:\nname (Optional[str]) – the new name for the featurelist\nReturn type:\nNone\nclass datarobot.models.Featurelist\nA set of features used in modeling\nVariables:\nid (str) – the id of the featurelist\nname (str) – the name of the featurelist\nfeatures (List[str]) – the names of all the Features in the featurelist\nproject_id (str) – the project the featurelist belongs to\ncreated (datetime.datetime) – (New in version v2.13) when the featurelist was created\nis_user_created (bool) – (New in version v2.13) whether the featurelist was created by a user or by DataRobot\nautomation\nnum_models (int) – (New in version v2.13) the number of models currently using this featurelist.  A model is\nconsidered to use a featurelist if it is used to train the model or as a monotonic\nconstraint featurelist, or if the model is a blender with at least one component model\nusing the featurelist.\ndescription (str) – (New in version v2.13) the description of the featurelist.  Can be updated by the user\nand may be supplied by default for DataRobot-created featurelists.\nclassmethod from_data(data)\nOverrides the parent method to ensure description is always populated\nParameters:\ndata (dict) – the data from the server, having gone through processing\nReturn type:\nTypeVar(TFeaturelist, bound= Featurelist)\nclassmethod get(project_id, featurelist_id)\nRetrieve a known feature list\nParameters:\nproject_id (str) – The id of the project the featurelist is associated with\nfeaturelist_id (str) – The ID of the featurelist to retrieve\nReturns:\nfeaturelist – The queried instance\nReturn type:\nFeaturelist\nRaises:\nValueError – passed project_id parameter value is of not supported type\ndelete(dry_run=False, delete_dependencies=False)\nDelete a featurelist, and any models and jobs using it\nAll models using a featurelist, whether as the training featurelist or as a monotonic\nconstraint featurelist, will also be deleted when the deletion is executed and any queued or\nrunning jobs using it will be cancelled. Similarly, predictions made on these models will\nalso be deleted. All the entities that are to be deleted with a featurelist are described\nas “dependencies” of it.  To preview the results of deleting a featurelist, call delete\nwith dry_run=True\nWhen deleting a featurelist with dependencies, users must specify delete_dependencies=True\nto confirm they want to delete the featurelist and all its dependencies. Without that\noption, only featurelists with no dependencies may be successfully deleted and others will\nerror.\nFeaturelists configured into the project as a default featurelist or as a default monotonic\nconstraint featurelist cannot be deleted.\nFeaturelists used in a model deployment cannot be deleted until the model deployment is\ndeleted.\nParameters:\ndry_run (Optional[bool]) – specify True to preview the result of deleting the featurelist, instead of actually\ndeleting it.\ndelete_dependencies (Optional[bool]) – specify True to successfully delete featurelists with dependencies; if left\nFalse by default, featurelists without dependencies can be successfully deleted and\nthose with dependencies will error upon attempting to delete them.\nReturns:\nresult –\nA dictionary describing the result of deleting the featurelist, with the following keys\ndry_run : bool, whether the deletion was a dry run or an actual deletion\ncan_delete : bool, whether the featurelist can actually be deleted\ndeletion_blocked_reason : str, why the featurelist can’t be deleted (if it can’t)\nnum_affected_models : int, the number of models using this featurelist\nnum_affected_jobs : int, the number of jobs using this featurelist\nReturn type:\ndict\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nupdate(name=None, description=None)\nUpdate the name or description of an existing featurelist\nNote that only user-created featurelists can be renamed, and that names must not\nconflict with names used by other featurelists.\nParameters:\nname (Optional[str]) – the new name for the featurelist\ndescription (Optional[str]) – the new description for the featurelist\nReturn type:\nNone\nclass datarobot.models.ModelingFeaturelist\nA set of features that can be used to build a model\nIn time series projects, a new set of modeling features is created after setting the\npartitioning options.  These features are automatically derived from those in the project’s\ndataset and are the features used for modeling.  Modeling features are only accessible once\nthe target and partitioning options have been set.  In projects that don’t use time series\nmodeling, once the target has been set, ModelingFeaturelists and Featurelists will behave\nthe same.\nFor more information about input and modeling features, see the\ntime series documentation.\nVariables:\nid (str) – the id of the modeling featurelist\nproject_id (str) – the id of the project the modeling featurelist belongs to\nname (str) – the name of the modeling featurelist\nfeatures (List[str]) – a list of the names of features included in this modeling featurelist\ncreated (datetime.datetime) – (New in version v2.13) when the featurelist was created\nis_user_created (bool) – (New in version v2.13) whether the featurelist was created by a user or by DataRobot\nautomation\nnum_models (int) – (New in version v2.13) the number of models currently using this featurelist.  A model is\nconsidered to use a featurelist if it is used to train the model or as a monotonic\nconstraint featurelist, or if the model is a blender with at least one component model\nusing the featurelist.\ndescription (str) – (New in version v2.13) the description of the featurelist.  Can be updated by the user\nand may be supplied by default for DataRobot-created featurelists.\nclassmethod get(project_id, featurelist_id)\nRetrieve a modeling featurelist\nModeling featurelists can only be retrieved once the target and partitioning options have\nbeen set.\nParameters:\nproject_id (str) – the id of the project the modeling featurelist belongs to\nfeaturelist_id (str) – the id of the modeling featurelist to retrieve\nReturns:\nfeaturelist – the specified featurelist\nReturn type:\nModelingFeaturelist\nupdate(name=None, description=None)\nUpdate the name or description of an existing featurelist\nNote that only user-created featurelists can be renamed, and that names must not\nconflict with names used by other featurelists.\nParameters:\nname (Optional[str]) – the new name for the featurelist\ndescription (Optional[str]) – the new description for the featurelist\nReturn type:\nNone\ndelete(dry_run=False, delete_dependencies=False)\nDelete a featurelist, and any models and jobs using it\nAll models using a featurelist, whether as the training featurelist or as a monotonic\nconstraint featurelist, will also be deleted when the deletion is executed and any queued or\nrunning jobs using it will be cancelled. Similarly, predictions made on these models will\nalso be deleted. All the entities that are to be deleted with a featurelist are described\nas “dependencies” of it.  To preview the results of deleting a featurelist, call delete\nwith dry_run=True\nWhen deleting a featurelist with dependencies, users must specify delete_dependencies=True\nto confirm they want to delete the featurelist and all its dependencies. Without that\noption, only featurelists with no dependencies may be successfully deleted and others will\nerror.\nFeaturelists configured into the project as a default featurelist or as a default monotonic\nconstraint featurelist cannot be deleted.\nFeaturelists used in a model deployment cannot be deleted until the model deployment is\ndeleted.\nParameters:\ndry_run (Optional[bool]) – specify True to preview the result of deleting the featurelist, instead of actually\ndeleting it.\ndelete_dependencies (Optional[bool]) – specify True to successfully delete featurelists with dependencies; if left\nFalse by default, featurelists without dependencies can be successfully deleted and\nthose with dependencies will error upon attempting to delete them.\nReturns:\nresult –\nA dictionary describing the result of deleting the featurelist, with the following keys\ndry_run : bool, whether the deletion was a dry run or an actual deletion\ncan_delete : bool, whether the featurelist can actually be deleted\ndeletion_blocked_reason : str, why the featurelist can’t be deleted (if it can’t)\nnum_affected_models : int, the number of models using this featurelist\nnum_affected_jobs : int, the number of jobs using this featurelist\nReturn type:\ndict\nclass datarobot.models.featurelist.DeleteFeatureListResult\nDataset definition\nclass datarobot.helpers.feature_discovery.DatasetDefinition\nDataset definition for the Feature Discovery\nAdded in version v2.25.\nVariables:\nidentifier (str) – Alias of the dataset (used directly as part of the generated feature names)\ncatalog_id (Optional[str]) – Identifier of the catalog item\ncatalog_version_id (str) – Identifier of the catalog item version\nprimary_temporal_key (Optional[str]) – Name of the column indicating time of record creation\nfeature_list_id (Optional[str]) – Identifier of the feature list. This decides which columns in the dataset are\nused for feature generation\nsnapshot_policy (Optional[str]) – Policy to use  when creating a project or making predictions.\nIf omitted, by default endpoint will use ‘latest’.\nMust be one of the following values:\n‘specified’: Use specific snapshot specified by catalogVersionId\n‘latest’: Use latest snapshot from the same catalog item\n‘dynamic’: Get data from the source (only applicable for JDBC datasets)\nExamples\nimport datarobot as dr\ndataset_definition = dr.DatasetDefinition(\nidentifier='profile',\ncatalog_id='5ec4aec1f072bc028e3471ae',\ncatalog_version_id='5ec4aec2f072bc028e3471b1',\n)\ndataset_definition = dr.DatasetDefinition(\nidentifier='transaction',\ncatalog_id='5ec4aec1f072bc028e3471ae',\ncatalog_version_id='5ec4aec2f072bc028e3471b1',\nprimary_temporal_key='Date'\n)\nRelationships\nclass datarobot.helpers.feature_discovery.Relationship\nRelationship between dataset defined in DatasetDefinition\nAdded in version v2.25.\nVariables:\ndataset1_identifier (Optional[str]) – Identifier of the first dataset in this relationship.\nThis is specified in the identifier field of dataset_definition structure.\nIf None, then the relationship is with the primary dataset.\ndataset2_identifier (str) – Identifier of the second dataset in this relationship.\nThis is specified in the identifier field of dataset_definition schema.\ndataset1_keys (List[str]) – (max length: 10 min length: 1)\nColumn(s) from the first dataset which are used to join to the second dataset\ndataset2_keys (List[str]) – (max length: 10 min length: 1)\nColumn(s) from the second dataset that are used to join to the first dataset\nfeature_derivation_window_start (int, or None) – How many time_units of each dataset’s primary temporal key into the past relative\nto the datetimePartitionColumn the feature derivation window should begin.\nWill be a negative integer,\nIf present, the feature engineering Graph will perform time-aware joins.\nfeature_derivation_window_end (Optional[int]) – How many timeUnits of each dataset’s record\nprimary temporal key into the past relative to the datetimePartitionColumn the\nfeature derivation window should end.  Will be a non-positive integer, if present.\nIf present, the feature engineering Graph will perform time-aware joins.\nfeature_derivation_window_time_unit (Optional[int]) – Time unit of the feature derivation window.\nOne of datarobot.enums.AllowedTimeUnitsSAFER\nIf present, time-aware joins will be used.\nOnly applicable when dataset1_identifier is not provided.\nfeature_derivation_windows (list of dict, or None) – List of feature derivation windows settings. If present, time-aware joins will be used.\nOnly allowed when feature_derivation_window_start,\nfeature_derivation_window_end and feature_derivation_window_time_unit are not provided.\nprediction_point_rounding (Optional[int]) – Closest value of prediction_point_rounding_time_unit to round the prediction point\ninto the past when applying the feature derivation window. Will be a positive integer,\nif present.Only applicable when dataset1_identifier is not provided.\nprediction_point_rounding_time_unit (Optional[str]) – Time unit of the prediction point rounding.\nOne of datarobot.enums.AllowedTimeUnitsSAFER\nOnly applicable when dataset1_identifier is not provided.\nschema (The feature_derivation_windows is a list of dictionary with) –\nstart: intHow many time_units of each dataset’s primary temporal key into the past relative\nto the datetimePartitionColumn the feature derivation window should begin.\nend: intHow many timeUnits of each dataset’s record\nprimary temporal key into the past relative to the datetimePartitionColumn the\nfeature derivation window should end.\nunit: strTime unit of the feature derivation window.\nOne of datarobot.enums.AllowedTimeUnitsSAFER.\nExamples\nimport datarobot as dr\nrelationship = dr.Relationship(\ndataset1_identifier='profile',\ndataset2_identifier='transaction',\ndataset1_keys=['CustomerID'],\ndataset2_keys=['CustomerID']\n)\nrelationship = dr.Relationship(\ndataset2_identifier='profile',\ndataset1_keys=['CustomerID'],\ndataset2_keys=['CustomerID'],\nfeature_derivation_window_start=-14,\nfeature_derivation_window_end=-1,\nfeature_derivation_window_time_unit='DAY',\nprediction_point_rounding=1,\nprediction_point_rounding_time_unit='DAY'\n)\nRelationships configuration\nclass datarobot.models.RelationshipsConfiguration\nA Relationships configuration specifies a set of secondary datasets as well as\nthe relationships among them. It is used to configure Feature Discovery for a project\nto generate features automatically from these datasets.\nVariables:\nid (str) – Id of the created relationships configuration\ndataset_definitions (list) – Each element is a dataset_definitions for a dataset.\nrelationships (list) – Each element is a relationship between two datasets\nfeature_discovery_mode (str) – Mode of feature discovery. Supported values are ‘default’ and ‘manual’\nfeature_discovery_settings (list) – List of feature discovery settings used to customize the feature discovery process\nis (The feature_discovery_settings structure)\nidentifier (str) – Alias of the dataset (used directly as part of the generated feature names)\ncatalog_id (str, or None) – Identifier of the catalog item\ncatalog_version_id (str) – Identifier of the catalog item version\nprimary_temporal_key (Optional[str]) – Name of the column indicating time of record creation\nfeature_list_id (Optional[str]) – Identifier of the feature list. This decides which columns in the dataset are\nused for feature generation\nsnapshot_policy (str) – Policy to use  when creating a project or making predictions.\nMust be one of the following values:\n‘specified’: Use specific snapshot specified by catalogVersionId\n‘latest’: Use latest snapshot from the same catalog item\n‘dynamic’: Get data from the source (only applicable for JDBC datasets)\nfeature_lists (list) – List of feature list info\ndata_source (dict) – Data source info if the dataset is from data source\ndata_sources (list) – List of Data source details for a JDBC datasets\nis_deleted (Optional[bool]) – Whether the dataset is deleted or not\nis\ndata_store_id (str) – Id of the data store.\ndata_store_name (str) – User-friendly name of the data store.\nurl (str) – Url used to connect to the data store.\ndbtable (str) – Name of table from the data store.\nschema (The feature_derivation_windows is a list of dictionary with) – Schema definition of the table from the data store\ncatalog (str) – Catalog name of the data source.\nis\nid – Id of the featurelist\nname (str) – Name of the featurelist\nfeatures (List[str]) – Names of all the Features in the featurelist\ndataset_id (str) – Project the featurelist belongs to\ncreation_date (datetime.datetime) – When the featurelist was created\nuser_created (bool) – Whether the featurelist was created by a user or by DataRobot automation\ncreated_by (str) – Name of user who created it\ndescription (str) – Description of the featurelist.  Can be updated by the user\nand may be supplied by default for DataRobot-created featurelists.\ndataset_id – Dataset which is associated with the feature list\ndataset_version_id (str or None) – Version of the dataset which is associated with feature list.\nOnly relevant for Informative features\nis\ndataset1_identifier (str or None) – Identifier of the first dataset in this relationship.\nThis is specified in the identifier field of dataset_definition structure.\nIf None, then the relationship is with the primary dataset.\ndataset2_identifier (str) – Identifier of the second dataset in this relationship.\nThis is specified in the identifier field of dataset_definition schema.\ndataset1_keys (List[str] (max length: 10 min length: 1)) – Column(s) from the first dataset which are used to join to the second dataset\ndataset2_keys (List[str]) – (max length: 10 min length: 1)\nColumn(s) from the second dataset that are used to join to the first dataset\ntime_unit (str, or None) – Time unit of the feature derivation window. Supported\nvalues are MILLISECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, YEAR.\nIf present, the feature engineering Graph will perform time-aware joins.\nfeature_derivation_window_start (int, or None) – How many time_units of each dataset’s primary temporal key into the past relative\nto the datetimePartitionColumn the feature derivation window should begin.\nWill be a negative integer,\nIf present, the feature engineering Graph will perform time-aware joins.\nfeature_derivation_window_end (int or None) – How many timeUnits of each dataset’s record\nprimary temporal key into the past relative to the datetimePartitionColumn the\nfeature derivation window should end.  Will be a non-positive integer, if present.\nIf present, the feature engineering Graph will perform time-aware joins.\nfeature_derivation_window_time_unit (int or None) – Time unit of the feature derivation window. Supported values are\nMILLISECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, YEAR\nIf present, time-aware joins will be used.\nOnly applicable when dataset1Identifier is not provided.\nfeature_derivation_windows (list of dict, or None) – List of feature derivation windows settings. If present, time-aware joins will be used.\nOnly allowed when feature_derivation_window_start,\nfeature_derivation_window_end and feature_derivation_window_time_unit are not provided.\nprediction_point_rounding (int, or None) – Closest value of prediction_point_rounding_time_unit to round the prediction point\ninto the past when applying the feature derivation window. Will be a positive integer,\nif present.Only applicable when dataset1_identifier is not provided.\nprediction_point_rounding_time_unit (str, or None) – time unit of the prediction point rounding. Supported values are\nMILLISECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, YEAR\nOnly applicable when dataset1_identifier is not provided.\nschema –\nstart: intHow many time_units of each dataset’s primary temporal key into the past relative\nto the datetimePartitionColumn the feature derivation window should begin.\nend: intHow many timeUnits of each dataset’s record\nprimary temporal key into the past relative to the datetimePartitionColumn the\nfeature derivation window should end.\nunit: strTime unit of the feature derivation window.\nOne of datarobot.enums.AllowedTimeUnitsSAFER.\nis\nname – Name of the feature discovery setting\nvalue (bool) – Value of the feature discovery setting\nspecifying (To see the list of possible settings, create a RelationshipConfiguration without)\npossible (settings and check its feature_discovery_settings attribute, which is a list of)\nvalues. (settings with their default)\nclassmethod create(dataset_definitions, relationships, feature_discovery_settings=None)\nCreate a Relationships Configuration\nParameters:\ndataset_definitions (list of DatasetDefinition) – Each element is a datarobot.helpers.feature_discovery.DatasetDefinition\nrelationships (list of Relationship) – Each element is a datarobot.helpers.feature_discovery.Relationship\nfeature_discovery_settings (Optional[List[FeatureDiscoverySetting]]) – Each element is a dictionary or a\ndatarobot.helpers.feature_discovery.FeatureDiscoverySetting. If not provided,\ndefault settings will be used.\nReturns:\nrelationships_configuration – Created relationships configuration\nReturn type:\nRelationshipsConfiguration\nExamples\ndataset_definition = dr.DatasetDefinition(\nidentifier='profile',\ncatalog_id='5fd06b4af24c641b68e4d88f',\ncatalog_version_id='5fd06b4af24c641b68e4d88f'\n)\nrelationship = dr.Relationship(\ndataset2_identifier='profile',\ndataset1_keys=['CustomerID'],\ndataset2_keys=['CustomerID'],\nfeature_derivation_window_start=-14,\nfeature_derivation_window_end=-1,\nfeature_derivation_window_time_unit='DAY',\nprediction_point_rounding=1,\nprediction_point_rounding_time_unit='DAY'\n)\ndataset_definitions = [dataset_definition]\nrelationships = [relationship]\nrelationship_config = dr.RelationshipsConfiguration.create(\ndataset_definitions=dataset_definitions,\nrelationships=relationships,\nfeature_discovery_settings = [\n{'name': 'enable_categorical_statistics', 'value': True},\n{'name': 'enable_numeric_skewness', 'value': True},\n]\n)\n>>> relationship_config.id\n'5c88a37770fc42a2fcc62759'\nget()\nRetrieve the Relationships configuration for a given id\nReturns:\nrelationships_configuration – The requested relationships configuration\nReturn type:\nRelationshipsConfiguration\nRaises:\nClientError – Raised if an invalid relationships config id is provided.\nExamples\nrelationships_config = dr.RelationshipsConfiguration(valid_config_id)\nresult = relationships_config.get()\n>>> result.id\n'5c88a37770fc42a2fcc62759'\nreplace(dataset_definitions, relationships, feature_discovery_settings=None)\nUpdate the Relationships Configuration which is not used in\nthe feature discovery Project\nParameters:\ndataset_definitions (List[DatasetDefinition]) – Each element is a datarobot.helpers.feature_discovery.DatasetDefinition\nrelationships (List[Relationship]) – Each element is a datarobot.helpers.feature_discovery.Relationship\nfeature_discovery_settings (Optional[List[FeatureDiscoverySetting]]) – Each element is a dictionary or a\ndatarobot.helpers.feature_discovery.FeatureDiscoverySetting. If not provided,\ndefault settings will be used.\nReturns:\nrelationships_configuration – the updated relationships configuration\nReturn type:\nRelationshipsConfiguration\ndelete()\nDelete the Relationships configuration\nRaises:\nClientError – Raised if an invalid relationships config id is provided.\nExamples\n# Deleting with a valid id\nrelationships_config = dr.RelationshipsConfiguration(valid_config_id)\nstatus_code = relationships_config.delete()\nstatus_code\n>>> 204\nrelationships_config.get()\n>>> ClientError: Relationships Configuration not found\nFeature lineage\nclass datarobot.models.FeatureLineage\nLineage of an automatically engineered feature.\nVariables:\nsteps (list) – list of steps which were applied to build the feature.\nsteps structure is:\nid - (int)step id starting with 0.\nstep_type: (str)one of the data/action/json/generatedData.\nname: (str)name of the step.\ndescription: (str)description of the step.\nparents: (list[int])references to other steps id.\nis_time_aware: (bool)indicator of step being time aware. Mandatory only for action and join steps.\naction step provides additional information about feature derivation window\nin the timeInfo field.\ncatalog_id: (str)id of the catalog for a data step.\ncatalog_version_id: (str)id of the catalog version for a data step.\ngroup_by: (list[str])list of columns which this action step aggregated by.\ncolumns: (list)names of columns involved into the feature generation. Available only for data steps.\ntime_info: (dict)description of the feature derivation window which was applied to this action step.\njoin_info: (list[dict])join step details.\ncolumns structure is\ndata_type: (str)the type of the feature, e.g. ‘Categorical’, ‘Text’\nis_input: (bool)indicates features which provided data to transform in this lineage.\nname: (str)feature name.\nis_cutoff: (bool)indicates a cutoff column.\ntime_info structure is:\nlatest: (dict)end of the feature derivation window applied.\nduration: (dict)size of the feature derivation window applied.\nlatest and duration structure is:\ntime_unit: (str)time unit name like ‘MINUTE’, ‘DAY’, ‘MONTH’ etc.\nduration: (int)value/size of this duration object.\njoin_info structure is:\njoin_type - (str)kind of join, left/right.\nleft_table - (dict)information about a dataset which was considered as left.\nright_table - (str)information about a dataset which was considered as right.\nleft_table and right_table structure is:\ncolumns - (list[str])list of columns which datasets were joined by.\ndatasteps - (list[int])list of data steps id which brought the columns into the current step dataset.\nclassmethod get(project_id, id)\nRetrieve a single FeatureLineage.\nParameters:\nproject_id (str) – The id of the project the feature belongs to\nid (str) – id of a feature lineage to retrieve\nReturns:\nlineage – The queried instance\nReturn type:\nFeatureLineage\nOCR job resources\nclass datarobot.models.ocr_job_resource.OCRJobResource\nAn OCR job resource container. It is used to:\n- Get an existing OCR  job resource.\n- List available OCR job resources.\n- Start an OCR job.\n- Check the status of a started OCR job.\n- Download the error report of a started OCR job.\nAdded in version v3.6.0b0.\nVariables:\nid (str) – The identifier of an OCR job resource.\ninput_catalog_id (str) – The identifier of an AI catalog item used as the OCR job input.\noutput_catalog_id (str) – The identifier of an AI catalog item used as the OCR job output.\nuser_id (str) – The identifier of a user.\njob_started (bool) – Determines if a job associated with the OCRJobResource has started.\nlanguage (str) – String representation of OCRJobDatasetLanguage.\nclassmethod get(job_resource_id)\nGet an OCR job resource.\nParameters:\njob_resource_id (str) – identifier of OCR job resource\nReturns:\nreturned OCR job resource\nReturn type:\nOCRJobResource\nclassmethod list(offset=0, limit=10)\nGet a list of OCR job resources.\nParameters:\noffset (int) – The offset of the query.\nlimit (int) – The limit of returned OCR job resources.\nReturns:\nA list of OCR job resources.\nReturn type:\nList[OCRJobResource]\nclassmethod create(input_catalog_id, language)\nCreate a new OCR job resource and return it.\nParameters:\ninput_catalog_id (str) – The identifier of an AI catalog item used as the OCR job input.\nlanguage (OCRJobDatasetLanguage) – The OCR job dataset language.\nReturns:\nThe created OCR job resource.\nReturn type:\nOCRJobResource\nstart_job()\nStart an OCR job with this OCR job resource.\nReturns:\nThe response of starting an OCR job.\nReturn type:\nStartOCRJobResponse\nget_job_status()\nGet status of the OCR job associated with this OCR job resource.\nReturns:\nOCR job status enum\nReturn type:\nOCRJobStatusEnum\ndownload_error_report(download_file_path)\nDownload the error report of the OCR job associated with this OCR job resource.\nParameters:\ndownload_file_path (Path) – path to download error report\nReturn type:\nNone\nclass datarobot.models.ocr_job_resource.OCRJobDatasetLanguage\nSupported OCR language\nclass datarobot.models.ocr_job_resource.OCRJobStatusEnum\nOCR Job status enum\nclass datarobot.models.ocr_job_resource.StartOCRJobResponse\nContainer of Start OCR Job API response\nDocument text extraction\nclass datarobot.models.documentai.document.FeaturesWithSamples\nFeaturesWithSamples(model_id, feature_name, document_task)\ndocument_task\nAlias for field number 2\nfeature_name\nAlias for field number 1\nmodel_id\nAlias for field number 0\nclass datarobot.models.documentai.document.DocumentPageFile\nPage of a document as an image file.\nVariables:\nproject_id (str) – The identifier of the project which the document page belongs to.\ndocument_page_id (str) – The unique identifier for the document page.\nheight (int) – The height of the document thumbnail in pixels.\nwidth (int) – The width of the document thumbnail in pixels.\nthumbnail_bytes (bytes) – The number of bytes of the document thumbnail image. Accessing this may\nrequire a server request and an associated delay in fetching the resource.\nmime_type (str) – The mime image type of the document thumbnail. Example: ‘image/png’\nproperty thumbnail_bytes: bytes\nDocument thumbnail as bytes.\nReturns:\nDocument thumbnail.\nReturn type:\nbytes\nproperty mime_type: str\n‘image/png’\nReturns:\nMime image type of the document thumbnail.\nReturn type:\nstr\nType:\nMime image type of the document thumbnail. Example\nclass datarobot.models.documentai.document.DocumentThumbnail\nThumbnail of document from the project’s dataset.\nIf Project.stage is datarobot.enums.PROJECT_STAGE.EDA2\nand it is a supervised project then the target_* attributes\nof this class will have values, otherwise the values will all be None.\nVariables:\ndocument (Document) – The document object.\nproject_id (str) – The identifier of the project which the document thumbnail belongs to.\ntarget_value (str) – The target value used for filtering thumbnails.\nclassmethod list(project_id, feature_name, target_value=None, offset=None, limit=None)\nGet document thumbnails from a project.\nParameters:\nproject_id (str) – The identifier of the project which the document thumbnail belongs to.\nfeature_name (str) – The name of feature that specifies the document type.\ntarget_value (Optional[str], default None) – The target value to filter thumbnails.\noffset (Optional[int], default None) – The number of documents to be skipped.\nlimit (Optional[int], default None) – The number of document thumbnails to return.\nReturns:\ndocuments – A list of DocumentThumbnail objects, each representing a single document.\nReturn type:\nList[DocumentThumbnail]\nNotes\nActual document thumbnails are not fetched from the server by this method.\nInstead the data gets loaded lazily when DocumentPageFile object attributes\nare accessed.\nExamples\nFetch document thumbnails for the given project_id and feature_name.\nfrom datarobot._experimental.models.documentai.document import DocumentThumbnail\n# Fetch five documents from the EDA SAMPLE for the specified project and specific feature\ndocument_thumbs = DocumentThumbnail.list(project_id, feature_name, limit=5)\n# Fetch five documents for the specified project with target value filtering\n# This option is only available after selecting the project target and starting modeling\ntarget1_thumbs = DocumentThumbnail.list(project_id, feature_name, target_value='target1', limit=5)\nPreview the document thumbnail.\nfrom datarobot._experimental.models.documentai.document import DocumentThumbnail\nfrom datarobot.helpers.image_utils import get_image_from_bytes\n# Fetch 3 documents\ndocument_thumbs = DocumentThumbnail.list(project_id, feature_name, limit=3)\nfor doc_thumb in document_thumbs:\nthumbnail = get_image_from_bytes(doc_thumb.document.thumbnail_bytes)\nthumbnail.show()\nclass datarobot.models.documentai.document.DocumentTextExtractionSample\nStateless class for computing and retrieving Document Text Extraction Samples.\nNotes\nActual document text extraction samples are not fetched from the server in the moment of\na function call. Detailed information on the documents, the pages and the rendered images of them\nare fetched when accessed on demand (lazy loading).\nExamples\n1) Compute text extraction samples for a specific model, and fetch all existing document text\nextraction samples for a specific project.\nfrom datarobot._experimental.models.documentai.document import DocumentTextExtractionSample\nSPECIFIC_MODEL_ID1 = \"model_id1\"\nSPECIFIC_MODEL_ID2 = \"model_id2\"\nSPECIFIC_PROJECT_ID = \"project_id\"\n# Order computation of document text extraction sample for specific model.\n# By default `compute` method will await for computation to end before returning\nDocumentTextExtractionSample.compute(SPECIFIC_MODEL_ID1, await_completion=False)\nDocumentTextExtractionSample.compute(SPECIFIC_MODEL_ID2)\nsamples = DocumentTextExtractionSample.list_features_with_samples(SPECIFIC_PROJECT_ID)\n2) Fetch document text extraction samples for a specific model_id and feature_name, and\ndisplay all document sample pages.\nfrom datarobot._experimental.models.documentai.document import DocumentTextExtractionSample\nfrom datarobot.helpers.image_utils import get_image_from_bytes\nSPECIFIC_MODEL_ID = \"model_id\"\nSPECIFIC_FEATURE_NAME = \"feature_name\"\nsamples = DocumentTextExtractionSample.list_pages(\nmodel_id=SPECIFIC_MODEL_ID,\nfeature_name=SPECIFIC_FEATURE_NAME\n)\nfor sample in samples:\nthumbnail = sample.document_page.thumbnail\nimage = get_image_from_bytes(thumbnail.thumbnail_bytes)\nimage.show()\n3) Fetch document text extraction samples for specific model_id and feature_name and\ndisplay text extraction details for the first page. This example displays the image of the document\nwith bounding boxes of detected text lines. It also returns a list of all text\nlines extracted from page along with their coordinates.\nfrom datarobot._experimental.models.documentai.document import DocumentTextExtractionSample\nSPECIFIC_MODEL_ID = \"model_id\"\nSPECIFIC_FEATURE_NAME = \"feature_name\"\nsamples = DocumentTextExtractionSample.list_pages(SPECIFIC_MODEL_ID, SPECIFIC_FEATURE_NAME)\n# Draw bounding boxes for first document page sample and display related text data.\nimage = samples[0].get_document_page_with_text_locations()\nimage.show()\n# For each text block represented as bounding box object drawn on original image\n# display its coordinates (top, left, bottom, right) and extracted text value\nfor text_line in samples[0].text_lines:\nprint(text_line)\nclassmethod compute(model_id, await_completion=True, max_wait=600)\nStarts computation of document text extraction samples for the model and, if successful,\nreturns computed text samples for it. This method allows calculation to continue for\na specified time and, if not complete, cancels the request.\nParameters:\nmodel_id (str) – The identifier of the project’s model that start the creation of the cluster insights.\nawait_completion (bool) – Determines whether the method should wait for completion before exiting or not.\nmax_wait (int (default=600)) – The maximum number of seconds to wait for the request to finish before raising an\nAsyncTimeoutError.\nRaises:\nClientError – Server rejected creation due to client error.\nOften, a bad model_id is causing these errors.\nAsyncFailureError – Indicates whether any of the responses from the server are unexpected.\nAsyncProcessUnsuccessfulError – Indicates whether the cluster insights computation failed or was cancelled.\nAsyncTimeoutError – Indicates whether the cluster insights computation did not resolve within the specified\ntime limit (max_wait).\nReturn type:\nNone\nclassmethod list_features_with_samples(project_id)\nReturns a list of features, model_id pairs with computed document text extraction samples.\nParameters:\nproject_id (str) – The project ID to retrieve the list of computed samples for.\nReturn type:\nList[FeaturesWithSamples]\nclassmethod list_pages(model_id, feature_name, document_index=None, document_task=None)\nReturns a list of document text extraction sample pages.\nParameters:\nmodel_id (str) – The model identifier.\nfeature_name (str) – The specific feature name to retrieve.\ndocument_index (Optional[int]) – The specific document index to retrieve. Defaults to None.\ndocument_task (Optional[str]) – The document blueprint task.\nReturn type:\nList[DocumentTextExtractionSamplePage]\nclassmethod list_documents(model_id, feature_name)\nReturns a list of documents used for text extraction.\nParameters:\nmodel_id (str) – The model identifier.\nfeature_name (str) – The feature name.\nReturn type:\nList[DocumentTextExtractionSampleDocument]\nclass datarobot.models.documentai.document.DocumentTextExtractionSampleDocument\nDocument text extraction source.\nHolds data that contains feature and model prediction values, as well as the thumbnail of the document.\nVariables:\ndocument_index (int) – The index of the document page sample.\nfeature_name (str) – The name of the feature that the document text extraction sample is related to.\nthumbnail_id (str) – The document page ID.\nthumbnail_width (int) – The thumbnail image width.\nthumbnail_height (int) – The thumbnail image height.\nthumbnail_link (str) – The thumbnail image download link.\ndocument_task (str) – The document blueprint task that the document belongs to.\nactual_target_value (Optional[Union[str, int, List[str]]]) – The actual target value.\nprediction (Optional[PredictionType]) – Prediction values and labels.\nclassmethod list(model_id, feature_name, document_task=None)\nList available documents with document text extraction samples.\nParameters:\nmodel_id (str) – The identifier for the model.\nfeature_name (str) – The name of the feature,\ndocument_task (Optional[str]) – The document blueprint task.\nReturn type:\nList[DocumentTextExtractionSampleDocument]\nclass datarobot.models.documentai.document.DocumentTextExtractionSamplePage\nDocument text extraction sample covering one document page.\nHolds data about the document page, the recognized text, and the location of the text in the document page.\nVariables:\npage_index (int) – Index of the page inside the document\ndocument_index (int) – Index of the document inside the dataset\nfeature_name (str) – The name of the feature that the document text extraction sample belongs to.\ndocument_page_id (str) – The document page ID.\ndocument_page_width (int) – Document page width.\ndocument_page_height (int) – Document page height.\ndocument_page_link (str) – Document page link to download the document page image.\ntext_lines (List[Dict[str, Union[int, str]]]) – A list of text lines and their coordinates.\ndocument_task (str) – The document blueprint task that the page belongs to.\nactual_target_value (Optional[Union[str, int, List[str]]) – Actual target value.\nprediction (Optional[PredictionType]) – Prediction values and labels.\nclassmethod list(model_id, feature_name, document_index=None, document_task=None)\nReturns a list of document text extraction sample pages.\nParameters:\nmodel_id (str) – The model identifier, used to retrieve document text extraction page samples.\nfeature_name (str) – The feature name, used to retrieve document text extraction page samples.\ndocument_index (Optional[int]) – The specific document index to retrieve. Defaults to None.\ndocument_task (Optional[str]) – Document blueprint task.\nReturn type:\nList[DocumentTextExtractionSamplePage]\nget_document_page_with_text_locations(line_color='blue', line_width=3, padding=3)\nReturns the document page with bounding boxes drawn around the text lines as a PIL.Image.\nParameters:\nline_color (str) – The color used to draw a bounding box on the image page. Defaults to blue.\nline_width (int) – The line width of the bounding boxes that will be drawn. Defaults to 3.\npadding (int) – The additional space left between the text and the bounding box, measured in pixels. Defaults to 3.\nReturns:\nReturns a PIL.Image with drawn text-bounding boxes.\nReturn type:\nImage",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/features.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/features.html",
        "content_length": 68945
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.documentai",
        "datarobot.models.datasetfeature",
        "datarobot.models.restore_discarded_features",
        "datarobot.helpers.image_utils",
        "datarobot.errors.invalidusageerror",
        "datarobot._experimental.models",
        "project.stage",
        "datarobot.models.interactionfeature",
        "datarobot.models.multicategoricalhistogram",
        "datarobot.models.pairwiseconditionalprobabilities",
        "dr.relationshipsconfiguration.create",
        "datarobot.models.ocr_job_resource",
        "datarobot.models.featurelineage",
        "datarobot.models.pairwisejointprobabilities",
        "datarobot.helpers.feature_discovery",
        "datarobot.models.datasetfeaturehistogram",
        "datarobot.models.modelingfeaturelist",
        "datarobot.models.feature",
        "datarobot.models.pairwisecorrelations",
        "datarobot.models.relationshipsconfiguration",
        "datarobot.models.modelingfeature",
        "datarobot.models.featurehistogram",
        "datarobot.models.featurelist",
        "datarobot.enums.project_stage",
        "datarobot.enums.allowedtimeunitssafer"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_8521075107793463374",
      "title": "Deployment Management",
      "content": "Deployment Management\nDeployments\nclass datarobot.models.Deployment\nA deployment created from a DataRobot model.\nVariables:\nid (str) – the id of the deployment\nlabel (str) – the label of the deployment\ndescription (str) – the description of the deployment\nstatus (str) – (New in version v2.29) deployment status\ndefault_prediction_server (dict) – Information about the default prediction server for the deployment.\nAccepts the following values\nid str\nPrediction server ID.\nurl Optional[str]\nPrediction server URL.\ndatarobot-key str\nCorresponds the to the PredictionServer “snake_cased”\ndatarobot_key parameter that allows you to verify and access the prediction server.\nimportance (Optional[str]) – deployment importance\nmodel (dict) – information on the model of the deployment\nmodel_package (dict) – (New in version v3.4) information on the model package of the deployment\nprediction_usage (dict) – information on the prediction usage of the deployment\npermissions (list) – (New in version v2.18) user’s permissions on the deployment\nservice_health (dict) – information on the service health of the deployment\nmodel_health (dict) – information on the model health of the deployment\naccuracy_health (dict) – information on the accuracy health of the deployment\nfairness_health (dict) – information on the fairness health of a deployment\ngovernance (dict) – information on approval and change requests of a deployment\nowners (dict) – information on the owners of a deployment\nprediction_environment (dict) – information on the prediction environment of a deployment\ncreator (dict) – information about the creator of a deployment\nclassmethod create_from_learning_model(cls, model_id, label, description=None, default_prediction_server_id=None, importance=None, prediction_threshold=None, status=None, max_wait=600)\nCreate a deployment from a DataRobot model.\nAdded in version v2.17.\nParameters:\nmodel_id (str) – id of the DataRobot model to deploy\nlabel (str) – a human-readable label of the deployment\ndescription (Optional[str]) – a human-readable description of the deployment\ndefault_prediction_server_id (Optional[str]) – an identifier of a prediction server to be used as the default prediction server\nimportance (Optional[str]) – deployment importance\nprediction_threshold (Optional[float]) – threshold used for binary classification in predictions\nstatus (Optional[str]) – deployment status\nmax_wait (Optional[int]) – Seconds to wait for successful resolution of a deployment creation job.\nDeployment supports making predictions only after a deployment creating job\nhas successfully finished.\nReturns:\ndeployment – The created deployment\nReturn type:\nDeployment\nExamples\nfrom datarobot import Project, Deployment\nproject = Project.get('5506fcd38bd88f5953219da0')\nmodel = project.get_models()[0]\ndeployment = Deployment.create_from_learning_model(model.id, 'New Deployment')\ndeployment\n>>> Deployment('New Deployment')\nclassmethod create_from_leaderboard(model_id, label, description=None, default_prediction_server_id=None, importance=None, prediction_threshold=None, status=None, max_wait=600)\nCreate a deployment from a Leaderboard.\nAdded in version v2.17.\nParameters:\nmodel_id (str) – id of the Leaderboard to deploy\nlabel (str) – a human-readable label of the deployment\ndescription (Optional[str]) – a human-readable description of the deployment\ndefault_prediction_server_id (Optional[str]) – an identifier of a prediction server to be used as the default prediction server\nimportance (Optional[str]) – deployment importance\nprediction_threshold (Optional[float]) – threshold used for binary classification in predictions\nstatus (Optional[str]) – deployment status\nmax_wait (Optional[int]) – The amount of seconds to wait for successful resolution of a deployment creation job.\nDeployment supports making predictions only after a deployment creating job\nhas successfully finished.\nReturns:\ndeployment – The created deployment\nReturn type:\nDeployment\nExamples\nfrom datarobot import Project, Deployment\nproject = Project.get('5506fcd38bd88f5953219da0')\nmodel = project.get_models()[0]\ndeployment = Deployment.create_from_leaderboard(model.id, 'New Deployment')\ndeployment\n>>> Deployment('New Deployment')\nclassmethod create_from_custom_model_version(custom_model_version_id, label, description=None, default_prediction_server_id=None, max_wait=600, importance=None)\nCreate a deployment from a DataRobot custom model image.\nParameters:\ncustom_model_version_id (str) – The ID of the DataRobot custom model version to deploy.\nThe version must have a base_environment_id.\nlabel (str) – A label of the deployment.\ndescription (Optional[str]) – A description of the deployment.\ndefault_prediction_server_id (str) – An identifier of a prediction server to be used as the default\nprediction server. Required for SaaS users and optional for\nSelf-Managed users.\nmax_wait (Optional[int]) – Seconds to wait for successful resolution of a deployment creation job.\nDeployment supports making predictions only after a deployment creating job\nhas successfully finished.\nimportance (Optional[str]) – Deployment importance level.\nReturns:\ndeployment – The created deployment\nReturn type:\nDeployment\nclassmethod create_from_registered_model_version(model_package_id, label, description=None, default_prediction_server_id=None, prediction_environment_id=None, importance=None, user_provided_id=None, additional_metadata=None, max_wait=600)\nCreate a deployment from a DataRobot model package (version).\nParameters:\nmodel_package_id (str) – The ID of the DataRobot model package (also known as a registered model version) to deploy.\nlabel (str) – A human readable label of the deployment.\ndescription (Optional[str]) – A human readable description of the deployment.\ndefault_prediction_server_id (Optional[str]) – an identifier of a prediction server to be used as the default prediction server\nWhen working with prediction environments, default prediction server Id should not be provided\nprediction_environment_id (Optional[str]) – An identifier of a prediction environment to be used for model deployment.\nimportance (Optional[str]) – Deployment importance level.\nuser_provided_id (Optional[str]) – A user-provided unique ID associated with a deployment definition in a remote git repository.\nadditional_metadata (dict, optional) – ‘Key/Value pair dict, with additional metadata’\nmax_wait (Optional[int]) – The amount of seconds to wait for successful resolution of a deployment creation job.\nDeployment supports making predictions only after a deployment creating job\nhas successfully finished.\nReturns:\ndeployment – The created deployment\nReturn type:\nDeployment\nclassmethod list(order_by=None, search=None, filters=None, offset=0, limit=0)\nList all deployments a user can view.\nAdded in version v2.17.\nParameters:\norder_by (Optional[str]) – (New in version v2.18) the order to sort the deployment list by, defaults to label\nAllowed attributes to sort by are:\nlabel\nserviceHealth\nmodelHealth\naccuracyHealth\nrecentPredictions\nlastPredictionTimestamp\nIf the sort attribute is preceded by a hyphen, deployments will be sorted in descending\norder, otherwise in ascending order.\nFor health related sorting, ascending means failing, warning, passing, unknown.\nsearch (Optional[str]) – (New in version v2.18) case insensitive search against deployment’s\nlabel and description.\nfilters (Optional[datarobot.models.deployment.DeploymentListFilters]) – (New in version v2.20) an object containing all filters that you’d like to apply to the\nresulting list of deployments. See\nDeploymentListFilters for details on usage.\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 0 to maintain previous behavior.\nThe default on the server is 20, with a maximum of 100.\nReturns:\ndeployments – a list of deployments the user can view\nReturn type:\nlist\nExamples\nfrom datarobot import Deployment\ndeployments = Deployment.list()\ndeployments\n>>> [Deployment('New Deployment'), Deployment('Previous Deployment')]\nfrom datarobot import Deployment\nfrom datarobot.enums import DEPLOYMENT_SERVICE_HEALTH_STATUS\nfilters = DeploymentListFilters(\nrole='OWNER',\nservice_health=[DEPLOYMENT_SERVICE_HEALTH.FAILING]\n)\nfiltered_deployments = Deployment.list(filters=filters)\nfiltered_deployments\n>>> [Deployment('Deployment I Own w/ Failing Service Health')]\nclassmethod get(deployment_id)\nGet information about a deployment.\nAdded in version v2.17.\nParameters:\ndeployment_id (str) – the id of the deployment\nReturns:\ndeployment – the queried deployment\nReturn type:\nDeployment\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.id\n>>>'5c939e08962d741e34f609f0'\ndeployment.label\n>>>'New Deployment'\npredict_batch(source, passthrough_columns=None, download_timeout=None, download_read_timeout=None, upload_read_timeout=None)\nA convenience method for making predictions with csv file or pandas DataFrame\nusing a batch prediction job.\nFor advanced usage, use datarobot.models.BatchPredictionJob directly.\nAdded in version v3.0.\nParameters:\nsource (str, pd.DataFrame or file object) – Pass a filepath, file, or DataFrame for making batch predictions.\npassthrough_columns (Optional[List[str]]) – Keep these columns from the scoring dataset in the scored dataset.\nThis is useful for correlating predictions with source data.\ndownload_timeout (Optional[int]) – Wait this many seconds for the download to become available.\nSee datarobot.models.BatchPredictionJob.score().\ndownload_read_timeout (Optional[int]) – Wait this many seconds for the server to respond between chunks.\nSee datarobot.models.BatchPredictionJob.score().\nupload_read_timeout (Optional[int]) – Wait this many seconds for the server to respond after a whole dataset upload.\nSee datarobot.models.BatchPredictionJob.score().\nReturns:\nPrediction results in a pandas DataFrame.\nReturn type:\npd.DataFrame\nRaises:\nInvalidUsageError – If the source parameter cannot be determined to be a filepath, file, or DataFrame.\nExamples\nfrom datarobot.models.deployment import Deployment\ndeployment = Deployment.get(\"<MY_DEPLOYMENT_ID>\")\nprediction_results_as_dataframe = deployment.predict_batch(\nsource=\"./my_local_file.csv\",\n)\nget_uri()\nReturns:\nurl – Deployment’s overview URI\nReturn type:\nstr\nupdate(label=None, description=None, importance=None)\nUpdate the label and description of this deployment.\n:rtype: None\nAdded in version v2.19.\ndelete()\nDelete this deployment.\n:rtype: None\nAdded in version v2.17.\nactivate(max_wait=600)\nActivates this deployment. When succeeded, deployment status become active.\nAdded in version v2.29.\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for deployment activation to complete before erroring\nReturn type:\nNone\ndeactivate(max_wait=600)\nDeactivates this deployment. When succeeded, deployment status become inactive.\nAdded in version v2.29.\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for deployment deactivation to complete before erroring\nReturn type:\nNone\nreplace_model(new_model_id, reason, max_wait=600, new_registered_model_version_id=None)\nReplace the model used in this deployment. To confirm model replacement eligibility, usevalidate_replacement_model() beforehand.\nAdded in version v2.17.\nModel replacement is an asynchronous process, which means some preparatory work may\nbe performed after the initial request is completed. This function will not return until all\npreparatory work is fully finished.\nPredictions made against this deployment will start using the new model as soon as the\nrequest is completed. There will be no interruption for predictions throughout\nthe process.\nParameters:\nnew_model_id (Optional[str]) – The id of the new model to use. If replacing the deployment’s model with a\nCustomInferenceModel, a specific CustomModelVersion ID must be used.\nIf None, new_registered_model_version_id must be specified.\nreason (MODEL_REPLACEMENT_REASON) – The reason for the model replacement. Must be one of ‘ACCURACY’, ‘DATA_DRIFT’, ‘ERRORS’,\n‘SCHEDULED_REFRESH’, ‘SCORING_SPEED’, or ‘OTHER’. This value will be stored in the model\nhistory to keep track of why a model was replaced\nmax_wait (Optional[int]) – (new in version 2.22) The maximum time to wait for\nmodel replacement job to complete before erroring\nnew_registered_model_version_id (Optional[str]) – (new in version 3.4) The registered model version (model package) ID of the new model to use. Must be\npassed if new_model_id is None.\nReturn type:\nNone\nExamples\nfrom datarobot import Deployment\nfrom datarobot.enums import MODEL_REPLACEMENT_REASON\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.model['id'], deployment.model['type']\n>>>('5c0a979859b00004ba52e431', 'Decision Tree Classifier (Gini)')\ndeployment.replace_model('5c0a969859b00004ba52e41b', MODEL_REPLACEMENT_REASON.ACCURACY)\ndeployment.model['id'], deployment.model['type']\n>>>('5c0a969859b00004ba52e41b', 'Support Vector Classifier (Linear Kernel)')\nperform_model_replace(new_registered_model_version_id, reason, max_wait=600)\nReplace the model used in this deployment. To confirm model replacement eligibility, usevalidate_replacement_model() beforehand.\nAdded in version v3.4.\nModel replacement is an asynchronous process, which means some preparatory work may\nbe performed after the initial request is completed. This function will not return until all\npreparatory work is fully finished.\nPredictions made against this deployment will start using the new model as soon as the\nrequest is completed. There will be no interruption for predictions throughout\nthe process.\nParameters:\nnew_registered_model_version_id (str) – The registered model version (model package) ID of the new model to use.\nreason (MODEL_REPLACEMENT_REASON) – The reason for the model replacement. Must be one of ‘ACCURACY’, ‘DATA_DRIFT’, ‘ERRORS’,\n‘SCHEDULED_REFRESH’, ‘SCORING_SPEED’, or ‘OTHER’. This value will be stored in the model\nhistory to keep track of why a model was replaced\nmax_wait (Optional[int]) – The maximum time to wait for\nmodel replacement job to complete before erroring\nReturn type:\nNone\nExamples\nfrom datarobot import Deployment\nfrom datarobot.enums import MODEL_REPLACEMENT_REASON\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.model_package['id']\n>>>'5c0a979859b00004ba52e431'\ndeployment.perform_model_replace('5c0a969859b00004ba52e41b', MODEL_REPLACEMENT_REASON.ACCURACY)\ndeployment.model_package['id']\n>>>'5c0a969859b00004ba52e41b'\nvalidate_replacement_model(new_model_id=None, new_registered_model_version_id=None)\nValidate a model can be used as the replacement model of the deployment.\nAdded in version v2.17.\nParameters:\nnew_model_id (Optional[str]) – the id of the new model to validate\nnew_registered_model_version_id (Optional[str]) – (new in version 3.4) The registered model version (model package) ID of the new model to use.\nReturn type:\nTuple[str, str, Dict[str, Any]]\nReturns:\nstatus (str) – status of the validation, will be one of ‘passing’, ‘warning’ or ‘failing’.\nIf the status is passing or warning, use replace_model() to\nperform a model replacement. If the status is failing, refer to checks for more\ndetail on why the new model cannot be used as a replacement.\nmessage (str) – message for the validation result\nchecks (dict) – explain why the new model can or cannot replace the deployment’s current model\nget_features()\nRetrieve the list of features needed to make predictions on this deployment.\nNotes\nEach feature dict contains the following structure:\nname : str, feature name\nfeature_type : str, feature type\nimportance : float, numeric measure of the relationship strength between\nthe feature and target (independent of model or other features)\ndate_format : str or None, the date format string for how this feature was\ninterpreted, null if not a date feature, compatible with\nhttps://docs.python.org/2/library/time.html#time.strftime.\nknown_in_advance : bool, whether the feature was selected as known in advance in\na time series model, false for non-time series models.\nReturns:\nfeatures – a list of feature dict\nReturn type:\nlist\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nfeatures = deployment.get_features()\nfeatures[0]['feature_type']\n>>>'Categorical'\nfeatures[0]['importance']\n>>>0.133\nsubmit_actuals(data, batch_size=10000)\nSubmit actuals for processing.\nThe actuals submitted will be used to calculate accuracy metrics.\nParameters:\ndata (list or pandas.DataFrame)\nbatch_size (the max number of actuals in each request)\nlist (If data is a)\nand (each item should be a dict-like object with the following keys)\npandas.DataFrame (values; if data is a)\ncolumns (it should contain the following)\nassociation_id (-) – max length 128 characters\nactual_value (-) – should be numeric for deployments with regression models or\nstring for deployments with classification model\nwas_acted_on (-) – could have affected the actual outcome\ntimestamp (-) – does not have a timezone, we assume it is UTC.\nRaises:\nValueError – if input data is not a list of dict-like objects or a pandas.DataFrame\nif input data is empty\nReturn type:\nNone\nExamples\nfrom datarobot import Deployment, AccuracyOverTime\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndata = [{\n'association_id': '439917',\n'actual_value': 'True',\n'was_acted_on': True\n}]\ndeployment.submit_actuals(data)\nsubmit_actuals_from_catalog_async(dataset_id, actual_value_column, association_id_column, dataset_version_id=None, timestamp_column=None, was_acted_on_column=None)\nSubmit actuals from AI Catalog for processing.\nThe actuals submitted will be used to calculate accuracy metrics.\nParameters:\ndataset_id (str,) – The ID of the source dataset.\ndataset_version_id (Optional[str]) – The ID of the dataset version to apply the query to. If not specified, the\nlatest version associated with dataset_id is used.\nassociation_id_column (str,) – The name of the column that contains a unique identifier used with a prediction.\nactual_value_column (str,) – The name of the column that contains the actual value of a prediction.\nwas_acted_on_column (Optional[str],) – The name of the column that indicates if the prediction was acted on in a way that\ncould have affected the actual outcome.\ntimestamp_column (Optional[str],) – The name of the column that contains datetime or string in RFC3339 format.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nRaises:\nValueError – if dataset_id not provided\nif actual_value_column not provided\nif association_id_column not provided\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nstatus_check_job = deployment.submit_actuals_from_catalog_async(data)\nget_predictions_by_forecast_date_settings()\nRetrieve predictions by forecast date settings of this deployment.\nAdded in version v2.27.\nReturns:\nsettings – Predictions by forecast date settings of the deployment.\nReturn type:\nForecastDateSettings\nupdate_predictions_by_forecast_date_settings(enable_predictions_by_forecast_date, forecast_date_column_name=None, forecast_date_format=None, max_wait=600)\nUpdate predictions by forecast date settings of this deployment.\nAdded in version v2.27.\nUpdating predictions by forecast date setting is an asynchronous process,\nwhich means some preparatory work may be performed after the initial request is completed.\nThis function will not return until all preparatory work is fully finished.\nExamples\n# To set predictions by forecast date settings to the same default settings you see when using\n# the DataRobot web application, you use your 'Deployment' object like this:\ndeployment.update_predictions_by_forecast_date_settings(\nenable_predictions_by_forecast_date=True,\nforecast_date_column_name=\"date (actual)\",\nforecast_date_format=\"%Y-%m-%d\",\n)\nParameters:\nenable_predictions_by_forecast_date (bool) – set to True if predictions by forecast date is to be turned on or set to ‘’False’’\nif predictions by forecast date is to be turned off.\nforecast_date_column_name (string, optional) – The column name in prediction datasets to be used as forecast date.\nIf ‘’enable_predictions_by_forecast_date’’ is set to ‘’False’’,\nthen the parameter will be ignored.\nforecast_date_format (string, optional) – The datetime format of the forecast date column in prediction datasets.\nIf ‘’enable_predictions_by_forecast_date’’ is set to ‘’False’’,\nthen the parameter will be ignored.\nmax_wait (Optional[int]) – seconds to wait for successful\nReturn type:\nNone\nget_challenger_models_settings()\nRetrieve challenger models settings of this deployment.\nAdded in version v2.27.\nReturns:\nsettings\nReturn type:\nChallengerModelsSettings\nupdate_challenger_models_settings(challenger_models_enabled, max_wait=600)\nUpdate challenger models settings of this deployment.\nAdded in version v2.27.\nUpdating challenger models setting is an asynchronous process, which means some preparatory\nwork may be performed after the initial request is completed. This function will not return\nuntil all preparatory work is fully finished.\nParameters:\nchallenger_models_enabled (bool) – set to True if challenger models is to be turned on or set to ‘’False’’ if\nchallenger models is to be turned off\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nget_segment_analysis_settings()\nRetrieve segment analysis settings of this deployment.\nAdded in version v2.27.\nReturns:\nsettings\nReturn type:\nSegmentAnalysisSettings\nupdate_segment_analysis_settings(segment_analysis_enabled, segment_analysis_attributes=None, max_wait=600)\nUpdate segment analysis settings of this deployment.\nAdded in version v2.27.\nUpdating segment analysis setting is an asynchronous process, which means some preparatory\nwork may be performed after the initial request is completed. This function will not return\nuntil all preparatory work is fully finished.\nParameters:\nsegment_analysis_enabled (bool) – set to True if segment analysis is to be turned on or set to ‘’False’’ if\nsegment analysis is to be turned off\nsegment_analysis_attributes (Optional[List]) – A list of strings that gives the segment attributes selected for tracking.\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nget_bias_and_fairness_settings()\nRetrieve bias and fairness settings of this deployment.\n..versionadded:: v3.2.0\nReturns:\nsettings\nReturn type:\nBiasAndFairnessSettings\nupdate_bias_and_fairness_settings(protected_features, fairness_metric_set, fairness_threshold, preferable_target_value, max_wait=600)\nUpdate bias and fairness settings of this deployment.\n..versionadded:: v3.2.0\nUpdating bias and fairness setting is an asynchronous process, which means some preparatory\nwork may be performed after the initial request is completed. This function will not return\nuntil all preparatory work is fully finished.\nParameters:\nprotected_features (List[str]) – A list of features to mark as protected.\npreferable_target_value (bool) – A target value that should be treated as a positive outcome for the prediction.\nfairness_metric_set (str) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\nfairness_threshold (float) – Threshold value of the fairness metric. Cannot be less than 0 or greater than 1.\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nget_challenger_replay_settings()\nRetrieve challenger replay settings of this deployment.\nAdded in version v3.4.\nReturns:\nsettings\nReturn type:\nChallengerReplaySettings\nupdate_challenger_replay_settings(enabled, schedule=None)\nUpdate challenger replay settings of this deployment.\nAdded in version v3.4.\nParameters:\nenabled (bool) – If challenger replay is enabled.\nschedule (Optional[Schedule]) – The recurring schedule for the challenger replay job.\nReturn type:\nNone\nget_drift_tracking_settings()\nRetrieve drift tracking settings of this deployment.\nAdded in version v2.17.\nReturns:\nsettings\nReturn type:\nDriftTrackingSettings\nupdate_drift_tracking_settings(target_drift_enabled=None, feature_drift_enabled=None, max_wait=600)\nUpdate drift tracking settings of this deployment.\nAdded in version v2.17.\nUpdating drift tracking setting is an asynchronous process, which means some preparatory\nwork may be performed after the initial request is completed. This function will not return\nuntil all preparatory work is fully finished.\nParameters:\ntarget_drift_enabled (Optional[bool]) – if target drift tracking is to be turned on\nfeature_drift_enabled (Optional[bool]) – if feature drift tracking is to be turned on\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nget_association_id_settings()\nRetrieve association ID setting for this deployment.\nAdded in version v2.19.\nReturns:\nassociation_id_settings\nReturn type:\nstr\nupdate_association_id_settings(column_names=None, required_in_prediction_requests=None, max_wait=600)\nUpdate association ID setting for this deployment.\nAdded in version v2.19.\nParameters:\ncolumn_names (list[string], optional) – name of the columns to be used as association ID,\ncurrently only support a list of one string\nrequired_in_prediction_requests (Optional[bool]) – whether the association ID column is required in prediction requests\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nget_predictions_data_collection_settings()\nRetrieve predictions data collection settings of this deployment.\nAdded in version v2.21.\nReturns:\npredictions_data_collection_settings –\nenabled (bool)If predictions data collection is enabled for this deployment. To update\nexisting ‘’predictions_data_collection’’ settings, see\nupdate_predictions_data_collection_settings()\nReturn type:\ndict in the following format:\nSee also\ndatarobot.models.Deployment.update_predictions_data_collection_settingsMethod to update existing predictions data collection settings.\nupdate_predictions_data_collection_settings(enabled, max_wait=600)\nUpdate predictions data collection settings of this deployment.\nAdded in version v2.21.\nUpdating predictions data collection setting is an asynchronous process, which means some\npreparatory work may be performed after the initial request is completed.\nThis function will not return until all preparatory work is fully finished.\nParameters:\nenabled (bool) – if predictions data collection is to be turned on\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nget_prediction_warning_settings()\nRetrieve prediction warning settings of this deployment.\nAdded in version v2.19.\nReturns:\nsettings\nReturn type:\nPredictionWarningSettings\nupdate_prediction_warning_settings(prediction_warning_enabled, use_default_boundaries=None, lower_boundary=None, upper_boundary=None, max_wait=600)\nUpdate prediction warning settings of this deployment.\nAdded in version v2.19.\nParameters:\nprediction_warning_enabled (bool) – If prediction warnings should be turned on.\nuse_default_boundaries (Optional[bool]) – If default boundaries of the model should be used for the deployment.\nupper_boundary (Optional[float]) – All predictions greater than provided value will be considered anomalous\nlower_boundary (Optional[float]) – All predictions less than provided value will be considered anomalous\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nget_prediction_intervals_settings()\nRetrieve prediction intervals settings for this deployment.\nAdded in version v2.19.\nNotes\nNote that prediction intervals are only supported for time series deployments.\nReturns:\nsettings\nReturn type:\nPredictionIntervalsSettings\nupdate_prediction_intervals_settings(percentiles, enabled=True, max_wait=600)\nUpdate prediction intervals settings for this deployment.\nAdded in version v2.19.\nNotes\nUpdating prediction intervals settings is an asynchronous process, which means some\npreparatory work may be performed before the settings request is completed. This function\nwill not return until all work is fully finished.\nNote that prediction intervals are only supported for time series deployments.\nParameters:\npercentiles (list[int]) – The prediction intervals percentiles to enable for this deployment. Currently we only\nsupport setting one percentile at a time.\nenabled (Optional[bool] (defaults to True)) – Whether to enable showing prediction intervals in the results of predictions requested\nusing this deployment.\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nRaises:\nAssertionError – If percentiles is in an invalid format\nAsyncFailureError – If any of the responses from the server are unexpected\nAsyncProcessUnsuccessfulError – If the prediction intervals calculation job has failed or has been cancelled.\nAsyncTimeoutError – If the prediction intervals calculation job did not resolve in time\nReturn type:\nNone\nget_health_settings()\nRetrieve health settings of this deployment.\nAdded in version v3.4.\nReturns:\nsettings\nReturn type:\nHealthSettings\nupdate_health_settings(service=None, data_drift=None, accuracy=None, fairness=None, custom_metrics=None, predictions_timeliness=None, actuals_timeliness=None)\nUpdate health settings of this deployment.\nAdded in version v3.4.\nParameters:\nservice (dict) – Service health settings.\ndata_drift (dict) – Data drift health settings.\naccuracy (dict) – Accuracy health settings.\nfairness (dict) – Fairness health settings.\ncustom_metrics (dict) – Custom metrics health settings.\npredictions_timeliness (dict) – Predictions timeliness health settings.\nactuals_timeliness (dict) – Actuals timeliness health settings.\nReturn type:\nHealthSettings\nget_default_health_settings()\nRetrieve default health settings of this deployment.\nAdded in version v3.4.\nReturns:\nsettings\nReturn type:\nHealthSettings\nget_service_stats(model_id=None, start_time=None, end_time=None, execution_time_quantile=None, response_time_quantile=None, slow_requests_threshold=None, segment_attribute=None, segment_value=None)\nRetrieves values of many service stat metrics aggregated over a time period.\nAdded in version v2.18.\nParameters:\nmodel_id (Optional[str]) – the id of the model\nstart_time (datetime, optional) – start of the time period\nend_time (datetime, optional) – end of the time period\nexecution_time_quantile (Optional[float]) – quantile for executionTime, defaults to 0.5\nresponse_time_quantile (Optional[float]) – quantile for responseTime, defaults to 0.5\nslow_requests_threshold (Optional[float]) – threshold for slowRequests, defaults to 1000\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\nservice_stats – the queried service stats metrics information\nReturn type:\nServiceStats\nget_service_stats_over_time(metric=None, model_id=None, start_time=None, end_time=None, bucket_size=None, quantile=None, threshold=None, segment_attribute=None, segment_value=None)\nRetrieves values of a single service stat metric over a time period.\nAdded in version v2.18.\nParameters:\nmetric (SERVICE_STAT_METRIC, optional) – the service stat metric to retrieve\nmodel_id (Optional[str]) – the id of the model\nstart_time (datetime, optional) – start of the time period\nend_time (datetime, optional) – end of the time period\nbucket_size (Optional[str]) – time duration of a bucket, in ISO 8601 time duration format\nquantile (Optional[float]) – quantile for ‘executionTime’ or ‘responseTime’, ignored when querying other metrics\nthreshold (Optional[int]) – threshold for ‘slowQueries’, ignored when querying other metrics\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\nservice_stats_over_time – the queried service stats metric over time information\nReturn type:\nServiceStatsOverTime\nget_target_drift(model_id=None, start_time=None, end_time=None, metric=None, segment_attribute=None, segment_value=None)\nRetrieve target drift information over a certain time period.\nAdded in version v2.21.\nParameters:\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nmetric (str) – (New in version v2.22) metric used to calculate the drift score\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\ntarget_drift – the queried target drift information\nReturn type:\nTargetDrift\nget_feature_drift(model_id=None, start_time=None, end_time=None, metric=None, segment_attribute=None, segment_value=None)\nRetrieve drift information for deployment’s features over a certain time period.\nAdded in version v2.21.\nParameters:\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nmetric (str) – (New in version v2.22) The metric used to calculate the drift score. Allowed\nvalues include psi, kl_divergence, dissimilarity, hellinger, and\njs_divergence.\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\nfeature_drift_data – the queried feature drift information\nReturn type:\n[FeatureDrift]\nget_predictions_over_time(model_ids=None, start_time=None, end_time=None, bucket_size=None, target_classes=None, include_percentiles=False, segment_attribute=None, segment_value=None)\nRetrieve stats of deployment’s prediction response over a certain time period.\nAdded in version v3.2.\nParameters:\nmodel_ids (list[str]) – ID of models to retrieve prediction stats\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nbucket_size (BUCKET_SIZE) – time duration of each bucket\ntarget_classes (list[str]) – class names of target, only for deployments with multiclass target\ninclude_percentiles (bool) – if the returned data includes percentiles,\nonly for a deployment with a binary and regression target\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\npredictions_over_time – the queried predictions over time information\nReturn type:\nPredictionsOverTime\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\npredictions_over_time = deployment.get_predictions_over_time()\npredictions_over_time.buckets[0]['mean_predicted_value']\n>>>0.3772\npredictions_over_time.buckets[0]['row_count']\n>>>2000\nget_accuracy(model_id=None, start_time=None, end_time=None, start=None, end=None, target_classes=None, segment_attribute=None, segment_value=None)\nRetrieves values of many accuracy metrics aggregated over a time period.\nAdded in version v2.18.\nParameters:\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\ntarget_classes (list[str], optional) – Optional list of target class strings\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\naccuracy – the queried accuracy metrics information\nReturn type:\nAccuracy\nget_accuracy_over_time(metric=None, model_id=None, start_time=None, end_time=None, bucket_size=None, target_classes=None, segment_attribute=None, segment_value=None)\nRetrieves values of a single accuracy metric over a time period.\nAdded in version v2.18.\nParameters:\nmetric (ACCURACY_METRIC) – the accuracy metric to retrieve\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nbucket_size (str) – time duration of a bucket, in ISO 8601 time duration format\ntarget_classes (list[str], optional) – Optional list of target class strings\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\naccuracy_over_time – the queried accuracy metric over time information\nReturn type:\nAccuracyOverTime\nget_predictions_vs_actuals_over_time(model_ids=None, start_time=None, end_time=None, bucket_size=None, target_classes=None, segment_attribute=None, segment_value=None)\nRetrieve information for deployment’s predictions vs actuals over a certain time period.\nAdded in version v3.3.\nParameters:\nmodel_ids (list[str]) – The ID of models to retrieve predictions vs actuals stats for.\nstart_time (datetime) – Start of the time period.\nend_time (datetime) – End of the time period.\nbucket_size (BUCKET_SIZE) – Time duration of each bucket.\ntarget_classes (list[str]) – Class names of target, only for deployments with a multiclass target.\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\npredictions_vs_actuals_over_time – The queried predictions vs actuals over time information.\nReturn type:\nPredictionsVsActualsOverTime\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\npredictions_over_time = deployment.get_predictions_vs_actuals_over_time()\npredictions_over_time.buckets[0]['mean_actual_value']\n>>>0.6673\npredictions_over_time.buckets[0]['row_count_with_actual']\n>>>500\nget_fairness_scores_over_time(start_time=None, end_time=None, bucket_size=None, model_id=None, protected_feature=None, fairness_metric=None)\nRetrieves values of a single fairness score over a time period.\nAdded in version v3.2.\nParameters:\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nbucket_size (str) – time duration of a bucket, in ISO 8601 time duration format\nprotected_feature (str) – name of protected feature\nfairness_metric (str) – A consolidation of the fairness metrics by the use case.\nReturns:\nfairness_scores_over_time – the queried fairness score over time information\nReturn type:\nFairnessScoresOverTime\nupdate_secondary_dataset_config(secondary_dataset_config_id, credential_ids=None)\nUpdate the secondary dataset config used by Feature discovery model for a\ngiven deployment.\nAdded in version v2.23.\nParameters:\nsecondary_dataset_config_id (str) – Id of the secondary dataset config\ncredential_ids (list or None) – List of DatasetsCredentials used by the secondary datasets\nReturn type:\nstr\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment(deployment_id='5c939e08962d741e34f609f0')\nconfig = deployment.update_secondary_dataset_config('5df109112ca582033ff44084')\nconfig\n>>> '5df109112ca582033ff44084'\nget_secondary_dataset_config()\nGet the secondary dataset config used by Feature discovery model for a\ngiven deployment.\nAdded in version v2.23.\nReturns:\nsecondary_dataset_config – Id of the secondary dataset config\nReturn type:\nSecondaryDatasetConfigurations\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_secondary_dataset_config('5df109112ca582033ff44084')\nconfig = deployment.get_secondary_dataset_config()\nconfig\n>>> '5df109112ca582033ff44084'\nget_prediction_results(model_id=None, start_time=None, end_time=None, actuals_present=None, offset=None, limit=None)\nRetrieve a list of prediction results of the deployment.\nAdded in version v2.24.\nParameters:\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nactuals_present (bool) – filters predictions results to only those\nwho have actuals present or with missing actuals\noffset (int) – this many results will be skipped\nlimit (int) – at most this many results are returned\nReturns:\nprediction_results – a list of prediction results\nReturn type:\nlist[dict]\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nresults = deployment.get_prediction_results()\ndownload_prediction_results(filepath, model_id=None, start_time=None, end_time=None, actuals_present=None, offset=None, limit=None)\nDownload prediction results of the deployment as a CSV file.\nAdded in version v2.24.\nParameters:\nfilepath (str) – path of the csv file\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nactuals_present (bool) – filters predictions results to only those\nwho have actuals present or with missing actuals\noffset (int) – this many results will be skipped\nlimit (int) – at most this many results are returned\nReturn type:\nNone\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nresults = deployment.download_prediction_results('path_to_prediction_results.csv')\ndownload_scoring_code(filepath, source_code=False, include_agent=False, include_prediction_explanations=False, include_prediction_intervals=False, max_wait=600)\nRetrieve scoring code of the current deployed model.\nAdded in version v2.24.\nNotes\nWhen setting include_agent or include_predictions_explanations or\ninclude_prediction_intervals to True,\nit can take a considerably longer time to download the scoring code.\nParameters:\nfilepath (str) – path of the scoring code file\nsource_code (bool) – whether source code or binary of the scoring code will be retrieved\ninclude_agent (bool) – whether the scoring code retrieved will include tracking agent\ninclude_prediction_explanations (bool) – whether the scoring code retrieved will include prediction explanations\ninclude_prediction_intervals (bool) – whether the scoring code retrieved will support prediction intervals\nmax_wait (Optional[int]) – Seconds to wait for successful resolution of a deployment creation job.\nDeployment supports making predictions only after a deployment creating job\nhas successfully finished\nReturn type:\nNone\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nresults = deployment.download_scoring_code('path_to_scoring_code.jar')\ndownload_model_package_file(filepath, compute_all_ts_intervals=False)\nRetrieve model package file (mlpkg) of the current deployed model.\nAdded in version v3.3.\nParameters:\nfilepath (str) – The file path of the model package file.\ncompute_all_ts_intervals (bool) – Includes all time series intervals into the built Model Package (.mlpkg) if set to True.\nReturn type:\nNone\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.download_model_package_file('path_to_model_package.mlpkg')\ndelete_monitoring_data(model_id, start_time=None, end_time=None, max_wait=600)\nDelete deployment monitoring data.\nParameters:\nmodel_id (str) – id of the model to delete monitoring data\nstart_time (datetime, optional) – start of the time period to delete monitoring data\nend_time (datetime, optional) – end of the time period to delete monitoring data\nmax_wait (Optional[int]) – seconds to wait for successful resolution\nReturn type:\nNone\nlist_shared_roles(id=None, name=None, share_recipient_type=None, limit=100, offset=0)\nGet a list of users, groups and organizations that have an access to this user blueprint\nParameters:\nid (Optional[str]) – Only return the access control information for a organization, group or user with this\nID.\nname (string, Optional) – Only return the access control information for a organization, group or user with this\nname.\nshare_recipient_type (enum(``’user’, ``'group', 'organization'), Optional) – Only returns results with the given recipient type.\nlimit (int (Default=0)) – At most this many results are returned.\noffset (int (Default=0)) – This many results will be skipped.\nReturn type:\nList[DeploymentSharedRole]\nupdate_shared_roles(roles)\nShare a deployment with a user, group, or organization\nParameters:\nroles (list(or(GrantAccessControlWithUsernameValidator, GrantAccessControlWithIdValidator, SharingRole))) – Array of GrantAccessControl objects, up to maximum 100 objects.\nReturn type:\nNone\nshare(roles)\nShare a deployment with a user, group, or organization\nParameters:\nroles (list(SharingRole)) – Array of SharingRole objects.\nReturn type:\nNone\nlist_challengers()\nGet a list of challengers for this deployment.\nAdded in version v3.4.\nReturn type:\nlist(Challenger)\nget_champion_model_package()\nGet a champion model package for this deployment.\nReturns:\nchampion_model_package – A champion model package object.\nReturn type:\nChampionModelPackage\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nchampion_model_package = deployment.get_champion_model_package()\nlist_prediction_data_exports(model_id=None, status=None, batch=None, offset=0, limit=100)\nRetrieve a list of asynchronous prediction data exports.\nParameters:\nmodel_id (Optional[str]) – The ID of the model used for prediction data export.\nstatus (Optional[str]) – A prediction data export processing state.\nbatch (Optional[bool]) – If true, only return batch exports.\nIf false, only return real-time exports.\nIf not provided, return both real-time and batch exports.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 100 (0 means no limit).\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nReturns:\nprediction_data_exports – A list of prediction data exports.\nReturn type:\nList[PredictionDataExport]\nlist_actuals_data_exports(status=None, offset=0, limit=100)\nRetrieve a list of asynchronous actuals data exports.\nParameters:\nstatus (Optional[str]) – Actuals data export processing state.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 100 (0 means no limit).\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nReturns:\nactuals_data_exports – A list of actuals data exports.\nReturn type:\nList[ActualsDataExport]\nlist_training_data_exports()\nRetrieve a list of successful training data exports.\nReturns:\ntraining_data_export – A list of training data exports.\nReturn type:\nList[TrainingDataExport]\nlist_data_quality_exports(start, end, model_id=None, prediction_pattern=None, prompt_pattern=None, actual_pattern=None, order_by=None, order_metric=None, filter_metric=None, filter_value=None, offset=0, limit=100)\nRetrieve a list of data-quality export records for a given deployment.\nAdded in version v3.6.\nParameters:\nstart (Union[str, datetime]) – The earliest time of the objects to return.\nend (Union[str, datetime]) – The latest time of the objects to return.\nmodel_id (Optional[str]) – The ID of the model.\nprediction_pattern (Optional[str]) – The keywords to search in a predicted value for a text generation target.\nprompt_pattern (Optional[str]) – The keywords to search in a prompt value for a text generation target.\nactual_pattern (Optional[str]) – The keywords to search in an actual value for a text generation target.\norder_by (Optional[str]) – The field to sort by (e.g. associationId, timestamp, or customMetrics). Use a leading ‘-’\nto indicate descending order. When ordering by a custom-metric, must also specify ‘order_metric’.\nThe default is None, which equates to ‘-timestamp’.\norder_metric (Optional[str]) – When ‘order_by’ is a custom-metric, this specifies the custom-metric name or ID to use for ordering.\nThe default is None.\nfilter_metric (Optional[str]) – Specifies the metric name or ID to use for matching. Must also use ‘filter_value’ to specify\nthe value that must be matched. The default is None.\nfilter_value (Optional[str]) – Specifies the value associated with ‘filter_metric’ that must be matched. The default\nis None.\noffset (Optional[int]) – The starting offset of the results. The default is 0.\nlimit (Optional[int]) – The maximum number of objects to return. The default is 100 (which is maximum).\nReturns:\ndata_quality_exports – A list of DataQualityExport objects.\nReturn type:\nlist\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndata_quality_exports = deployment.list_data_quality_exports(start_time='2024-07-01', end_time='2024-08-01')\nget_capabilities()\nGet a list capabilities for this deployment.\nAdded in version v3.5.\nReturn type:\nlist(Capability)\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ncapabilities = deployment.get_capabilities()\nget_segment_attributes(monitoringType='serviceHealth')\nGet a list of segment attributes for this deployment.\nAdded in version v3.6.\nParameters:\nmonitoringType (Optional[str]) – The monitoring type for which segment attributes are being retrieved.\nReturn type:\nlist(str)\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsegment_attributes = deployment.get_segment_attributes(DEPLOYMENT_MONITORING_TYPE.SERVICE_HEALTH)\nget_segment_values(segment_attribute=None, limit=100, offset=0, search=None)\nGet a list of segment values for this deployment.\nAdded in version v3.6.\nParameters:\nsegment_attribute (Optional[str]) – Represents the different ways that prediction requests can be viewed.\nlimit (int, Optional) – The maximum number of values to return.\noffset (int, Optional) – The starting point of the values to be returned.\nsearch (Optional[str]) – A string to filter the values.\nReturn type:\nlist(str)\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsegment_values = deployment.get_segment_values(segment_attribute=ReservedSegmentAttributes.CONSUMER)\nget_moderation_events(limit=100, offset=0)\nGet a list of moderation events for this deployment\nParameters:\nlimit (int (Default=100)) – The maximum number of values to return.\noffset (int (Default=0)) – The starting point of the values to be returned.\nReturns:\nevents\nReturn type:\nList[MLOpsEvent]\nget_accuracy_metrics_settings()\nGet accuracy metrics settings for this deployment.\nReturns:\naccuracy_metrics – A list of deployment accuracy metric names.\nReturn type:\nlist(str)\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\naccuracy_metrics = deployment.get_accuracy_metrics_settings()\nupdate_accuracy_metrics_settings(accuracy_metrics)\nUpdate accuracy metrics settings for this deployment.\nParameters:\naccuracy_metrics (list(str)) – A list of accuracy metric names.\nReturns:\naccuracy_metrics – A list of deployment accuracy metric names.\nReturn type:\nlist(str)\nExamples\nfrom datarobot import Deployment\nfrom datarobot.enums import ACCURACY_METRIC\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\npayload = [ACCURACY_METRIC.AUC, ACCURACY_METRIC.LOGLOSS]\naccuracy_metrics = deployment.update_accuracy_metrics_settings(payload)\nget_retraining_settings()\nRetrieve retraining settings of this deployment.\nAdded in version v2.29.\nReturns:\nsettings\nReturn type:\nRetrainingSettings\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nretraining_settings = deployment.get_retraining_settings()\nupdate_retraining_settings(retraining_user_id=<object object>, dataset_id=<object object>, prediction_environment_id=<object object>)\nUpdate retraining settings of this deployment.\nAdded in version v2.29.\nParameters:\nretraining_user_id (Optional[str]) – The retraining user ID.\ndataset_id (Optional[str]) – The retraining dataset ID.\nprediction_environment_id (Optional[str]) – The retraining prediction environment ID.\nReturn type:\nNone\nExamples\nfrom datarobot import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_retraining_settings(retaining_user_id='5c939e08962d741e34f609f0')\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nclass datarobot.models.deployment.DeploymentListFilters\nConstruct a set of filters to pass to Deployment.list()\nAdded in version v2.20.\nParameters:\nrole (str) – A user role. If specified, then take those deployments that the user can view, then\nfilter them down to those that the user has the specified role for, and return only\nthem. Allowed options are OWNER and USER.\nservice_health (List[str]) – A list of service health status values. If specified, then only deployments whose\nservice health status is one of these will be returned. See\ndatarobot.enums.DEPLOYMENT_SERVICE_HEALTH_STATUS for allowed values.\nSupports comma-separated lists.\nmodel_health (List[str]) – A list of model health status values. If specified, then only deployments whose model\nhealth status is one of these will be returned. See\ndatarobot.enums.DEPLOYMENT_MODEL_HEALTH_STATUS for allowed values.\nSupports comma-separated lists.\naccuracy_health (List[str]) – A list of accuracy health status values. If specified, then only deployments whose\naccuracy health status is one of these will be returned. See\ndatarobot.enums.DEPLOYMENT_ACCURACY_HEALTH_STATUS for allowed values.\nSupports comma-separated lists.\nexecution_environment_type (List[str]) – A list of strings representing the type of the deployments’ execution environment.\nIf provided, then only return those deployments whose execution environment type is\none of those provided. See datarobot.enums.DEPLOYMENT_EXECUTION_ENVIRONMENT_TYPE\nfor allowed values. Supports comma-separated lists.\nimportance (List[str]) – A list of strings representing the deployments’ “importance”.\nIf provided, then only return those deployments whose importance\nis one of those provided. See datarobot.enums.DEPLOYMENT_IMPORTANCE\nfor allowed values. Supports comma-separated lists. Note that Approval Workflows must\nbe enabled for your account to use this filter, otherwise the API will return a 403.\nExamples\nMultiple filters can be combined in interesting ways to return very specific subsets of\ndeployments.\nPerforming AND logic\nProviding multiple different parameters will result in AND logic between them.\nFor example, the following will return all deployments that I own whose service health\nstatus is failing.\nfrom datarobot import Deployment\nfrom datarobot.models.deployment import DeploymentListFilters\nfrom datarobot.enums import DEPLOYMENT_SERVICE_HEALTH_STATUS\nfilters = DeploymentListFilters(\nrole='OWNER',\nservice_health=[DEPLOYMENT_SERVICE_HEALTH.FAILING]\n)\ndeployments = Deployment.list(filters=filters)\nPerforming OR logic\nSome filters support comma-separated lists (and will say so if they do). Providing a\ncomma-separated list of values to a single filter performs OR logic between those\nvalues. For example, the following will return all deployments whose service health\nis either warning OR failing.\nfrom datarobot import Deployment\nfrom datarobot.models.deployment import DeploymentListFilters\nfrom datarobot.enums import DEPLOYMENT_SERVICE_HEALTH_STATUS\nfilters = DeploymentListFilters(\nservice_health=[\nDEPLOYMENT_SERVICE_HEALTH.WARNING,\nDEPLOYMENT_SERVICE_HEALTH.FAILING,\n]\n)\ndeployments = Deployment.list(filters=filters)\nPerforming OR logic across different filter types is not supported.\nNotes\nIn all cases, you may only retrieve deployments for which you have at least\nthe USER role for. Deployments for which you are a CONSUMER of will not be returned,\nregardless of the filters applied.\nclass datarobot.models.deployment.ServiceStats\nDeployment service stats information.\nVariables:\nmodel_id (str) – the model used to retrieve service stats metrics\nperiod (dict) – the time period used to retrieve service stats metrics\nmetrics (dict) – the service stats metrics\nclassmethod get(deployment_id, model_id=None, start_time=None, end_time=None, execution_time_quantile=None, response_time_quantile=None, segment_attribute=None, segment_value=None, slow_requests_threshold=None)\nRetrieve value of service stat metrics over a certain time period.\nAdded in version v2.18.\nParameters:\ndeployment_id (str) – the id of the deployment\nmodel_id (Optional[str]) – the id of the model\nstart_time (datetime, optional) – start of the time period\nend_time (datetime, optional) – end of the time period\nexecution_time_quantile (Optional[float]) – quantile for executionTime, defaults to 0.5\nresponse_time_quantile (Optional[float]) – quantile for responseTime, defaults to 0.5\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nslow_requests_threshold (Optional[float]) – threshold for slowRequests, defaults to 1000\nReturns:\nservice_stats – the queried service stats metrics\nReturn type:\nServiceStats\nclass datarobot.models.deployment.ServiceStatsOverTime\nDeployment service stats over time information.\nVariables:\nmodel_id (str) – the model used to retrieve accuracy metric\nmetric (str) – the service stat metric being retrieved\nbuckets (dict) – how the service stat metric changes over time\nsummary (dict) – summary for the service stat metric\nclassmethod get(deployment_id, metric=None, model_id=None, start_time=None, end_time=None, bucket_size=None, quantile=None, threshold=None, segment_attribute=None, segment_value=None)\nRetrieve information about how a service stat metric changes over a certain time period.\nAdded in version v2.18.\nParameters:\ndeployment_id (str) – the id of the deployment\nmetric (SERVICE_STAT_METRIC, optional) – the service stat metric to retrieve\nmodel_id (Optional[str]) – the id of the model\nstart_time (datetime, optional) – start of the time period\nend_time (datetime, optional) – end of the time period\nbucket_size (Optional[str]) – time duration of a bucket, in ISO 8601 time duration format\nquantile (Optional[float]) – quantile for ‘executionTime’ or ‘responseTime’, ignored when querying other metrics\nthreshold (Optional[int]) – threshold for ‘slowQueries’, ignored when querying other metrics\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\nservice_stats_over_time – the queried service stat over time information\nReturn type:\nServiceStatsOverTime\nproperty bucket_values: OrderedDict[str, int | float | None]\nThe metric value for all time buckets, keyed by start time of the bucket.\nReturns:\nbucket_values\nReturn type:\nOrderedDict\nclass datarobot.models.deployment.TargetDrift\nDeployment target drift information.\nVariables:\nmodel_id (str) – the model used to retrieve target drift metric\nperiod (dict) – the time period used to retrieve target drift metric\nmetric (str) – the data drift metric\ntarget_name (str) – name of the target\ndrift_score (float) – target drift score\nsample_size (int) – count of data points for comparison\nbaseline_sample_size (int) – count of data points for baseline\nclassmethod get(deployment_id, model_id=None, start_time=None, end_time=None, metric=None, segment_attribute=None, segment_value=None)\nRetrieve target drift information over a certain time period.\nAdded in version v2.21.\nParameters:\ndeployment_id (str) – the id of the deployment\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nmetric (str) – (New in version v2.22) metric used to calculate the drift score\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\ntarget_drift – the queried target drift information\nReturn type:\nTargetDrift\nExamples\nfrom datarobot import Deployment, TargetDrift\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ntarget_drift = TargetDrift.get(deployment.id)\ntarget_drift.period['end']\n>>>'2019-08-01 00:00:00+00:00'\ntarget_drift.drift_score\n>>>0.03423\naccuracy.target_name\n>>>'readmitted'\nclass datarobot.models.deployment.FeatureDrift\nDeployment feature drift information.\nVariables:\nmodel_id (str) – the model used to retrieve feature drift metric\nperiod (dict) – the time period used to retrieve feature drift metric\nmetric (str) – the data drift metric\nname (str) – name of the feature\ndrift_score (float) – feature drift score\nsample_size (int) – count of data points for comparison\nbaseline_sample_size (int) – count of data points for baseline\nclassmethod list(deployment_id, model_id=None, start_time=None, end_time=None, metric=None, segment_attribute=None, segment_value=None)\nRetrieve drift information for deployment’s features over a certain time period.\nAdded in version v2.21.\nParameters:\ndeployment_id (str) – the id of the deployment\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nmetric (str) – (New in version v2.22) metric used to calculate the drift score\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\nfeature_drift_data – the queried feature drift information\nReturn type:\n[FeatureDrift]\nExamples\nfrom datarobot import Deployment, TargetDrift\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nfeature_drift = FeatureDrift.list(deployment.id)[0]\nfeature_drift.period\n>>>'2019-08-01 00:00:00+00:00'\nfeature_drift.drift_score\n>>>0.252\nfeature_drift.name\n>>>'age'\nclass datarobot.models.deployment.PredictionsOverTime\nDeployment predictions over time information.\nVariables:\nbaselines (List) – target baseline for each model queried\nbuckets (List) – predictions over time bucket for each model and bucket queried\nclassmethod get(deployment_id, model_ids=None, start_time=None, end_time=None, bucket_size=None, target_classes=None, include_percentiles=False, segment_attribute=None, segment_value=None)\nRetrieve information for deployment’s prediction response over a certain time period.\nAdded in version v3.2.\nParameters:\ndeployment_id (str) – the id of the deployment\nmodel_ids (list[str]) – ID of models to retrieve prediction stats\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nbucket_size (BUCKET_SIZE) – time duration of each bucket\ntarget_classes (list[str]) – class names of target, only for deployments with multiclass target\ninclude_percentiles (bool) – if the returned data includes percentiles,\nonly for a deployment with a binary and regression target\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\npredictions_over_time – the queried predictions over time information\nReturn type:\nPredictionsOverTime\nclass datarobot.models.deployment.Accuracy\nDeployment accuracy information.\nVariables:\nmodel_id (str) – the model used to retrieve accuracy metrics\nperiod (dict) – the time period used to retrieve accuracy metrics\nmetrics (dict) – the accuracy metrics\nclassmethod get(deployment_id, model_id=None, start_time=None, end_time=None, target_classes=None, segment_attribute=None, segment_value=None)\nRetrieve values of accuracy metrics over a certain time period.\nAdded in version v2.18.\nParameters:\ndeployment_id (str) – the id of the deployment\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\ntarget_classes (list[str], optional) – Optional list of target class strings\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\naccuracy – the queried accuracy metrics information\nReturn type:\nAccuracy\nExamples\nfrom datarobot import Deployment, Accuracy\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\naccuracy = Accuracy.get(deployment.id)\naccuracy.period['end']\n>>>'2019-08-01 00:00:00+00:00'\naccuracy.metric['LogLoss']['value']\n>>>0.7533\naccuracy.metric_values['LogLoss']\n>>>0.7533\nproperty metric_values: Dict[str, int | None]\nThe value for all metrics, keyed by metric name.\nReturns:\nmetric_values\nReturn type:\nDict\nproperty metric_baselines: Dict[str, int | None]\nThe baseline value for all metrics, keyed by metric name.\nReturns:\nmetric_baselines\nReturn type:\nDict\nproperty percent_changes: Dict[str, int | None]\nThe percent change of value over baseline for all metrics, keyed by metric name.\nReturns:\npercent_changes\nReturn type:\nDict\nclass datarobot.models.deployment.AccuracyOverTime\nDeployment accuracy over time information.\nVariables:\nmodel_id (str) – the model used to retrieve accuracy metric\nmetric (str) – the accuracy metric being retrieved\nbuckets (dict) – how the accuracy metric changes over time\nsummary (dict) – summary for the accuracy metric\nbaseline (dict) – baseline for the accuracy metric\nclassmethod get(deployment_id, metric=None, model_id=None, start_time=None, end_time=None, bucket_size=None, target_classes=None, segment_attribute=None, segment_value=None)\nRetrieve information about how an accuracy metric changes over a certain time period.\nAdded in version v2.18.\nParameters:\ndeployment_id (str) – the id of the deployment\nmetric (ACCURACY_METRIC) – the accuracy metric to retrieve\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nbucket_size (str) – time duration of a bucket, in ISO 8601 time duration format\ntarget_classes (list[str], optional) – Optional list of target class strings\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\naccuracy_over_time – the queried accuracy metric over time information\nReturn type:\nAccuracyOverTime\nExamples\nfrom datarobot import Deployment, AccuracyOverTime\nfrom datarobot.enums import ACCURACY_METRICS\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\naccuracy_over_time = AccuracyOverTime.get(deployment.id, metric=ACCURACY_METRIC.LOGLOSS)\naccuracy_over_time.metric\n>>>'LogLoss'\naccuracy_over_time.metric_values\n>>>{datetime.datetime(2019, 8, 1): 0.73, datetime.datetime(2019, 8, 2): 0.55}\nclassmethod get_as_dataframe(deployment_id, metrics=None, model_id=None, start_time=None, end_time=None, bucket_size=None)\nRetrieve information about how a list of accuracy metrics change over\na certain time period as pandas DataFrame.\nIn the returned DataFrame, the columns corresponds to the metrics being retrieved;\nthe rows are labeled with the start time of each bucket.\nParameters:\ndeployment_id (str) – the id of the deployment\nmetrics ([ACCURACY_METRIC]) – the accuracy metrics to retrieve\nmodel_id (str) – the id of the model\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nbucket_size (str) – time duration of a bucket, in ISO 8601 time duration format\nReturns:\naccuracy_over_time\nReturn type:\npd.DataFrame\nproperty bucket_values: Dict[datetime, int]\nThe metric value for all time buckets, keyed by start time of the bucket.\nReturns:\nbucket_values\nReturn type:\nDict\nproperty bucket_sample_sizes: Dict[datetime, int]\nThe sample size for all time buckets, keyed by start time of the bucket.\nReturns:\nbucket_sample_sizes\nReturn type:\nDict\nclass datarobot.models.deployment.PredictionsVsActualsOverTime\nDeployment predictions vs actuals over time information.\nVariables:\nsummary (dict) – predictions vs actuals over time summary for all models and buckets queried\nbaselines (List) – target baseline for each model queried\nbuckets (List) – predictions vs actuals over time bucket for each model and bucket queried\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nclassmethod get(deployment_id, model_ids=None, start_time=None, end_time=None, bucket_size=None, target_classes=None, segment_attribute=None, segment_value=None)\nRetrieve information for deployment’s predictions vs actuals over a certain time period.\nAdded in version v3.3.\nParameters:\ndeployment_id (str) – the id of the deployment\nmodel_ids (list[str]) – ID of models to retrieve predictions vs actuals stats\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nbucket_size (BUCKET_SIZE) – time duration of each bucket\ntarget_classes (list[str]) – class names of target, only for deployments with multiclass target\nsegment_attribute (Optional[str]) – (New in Version v3.6) the segment attribute\nsegment_value (Optional[str]) – (New in Version v3.6) the segment value\nReturns:\npredictions_vs_actuals_over_time – the queried predictions vs actuals over time information\nReturn type:\nPredictionsVsActualsOverTime\nclass datarobot.models.deployment.bias_and_fairness.FairnessScoresOverTime\nDeployment fairness over time information.\nVariables:\nbuckets (List) – fairness over time bucket for each model and bucket queried\nsummary (dict) – summary for the fairness score\nprotected_feature (str) – name of protected feature\nfairnessThreshold (float) – threshold used to compute fairness results\nmodelId (str) – model id for which fairness is computed\nmodelPackageId (str) – model package (version) id for which fairness is computed\nfavorableTargetOutcome (bool) – preferable class of the target\nclassmethod get(deployment_id, model_id=None, start_time=None, end_time=None, bucket_size=None, fairness_metric=None, protected_feature=None)\nRetrieve information for deployment’s fairness score response over a certain time period.\nAdded in version FUTURE.\nParameters:\ndeployment_id (str) – the id of the deployment\nmodel_id (str) – id of models to retrieve fairness score stats\nstart_time (datetime) – start of the time period\nend_time (datetime) – end of the time period\nprotected_feature (str) – name of the protected feature\nfairness_metric (str) – A consolidation of the fairness metrics by the use case.\nbucket_size (BUCKET_SIZE) – time duration of each bucket\nReturns:\nfairness_scores_over_time – the queried fairness score over time information\nReturn type:\nFairnessScoresOverTime\nclass datarobot.models.deployment.DeploymentSharedRole\nParameters:\nshare_recipient_type ({'user', 'group', 'organization'}) – Describes the recipient type, either user, group, or organization.\nrole ({'CONSUMER', 'USER', 'OWNER'}) – The role of the org/group/user on this deployment.\nid (str) – The ID of the recipient organization, group, or user.\nname (str) – The name of the recipient organization, group, or user.\nclass datarobot.models.deployment.DeploymentGrantSharedRoleWithId\nParameters:\nshare_recipient_type ({'user', 'group', 'organization'}) – Describes the recipient type, either user, group, or organization.\nrole ({'OWNER', 'USER', 'OBSERVER', 'NO_ROLE'}) – The role of the recipient on this entity. One of OWNER, USER, OBSERVER, NO_ROLE.\nIf NO_ROLE is specified, any existing role for the recipient will be removed.\nid (str) – The ID of the recipient.\nclass datarobot.models.deployment.DeploymentGrantSharedRoleWithUsername\nParameters:\nrole (string) – The role of the recipient on this entity. One of OWNER, USER, CONSUMER, NO_ROLE.\nIf NO_ROLE is specified, any existing role for the user will be removed.\nusername (string) – Username of the user to update the access role for.\nclass datarobot.models.deployment.deployment.FeatureDict\nclass datarobot.models.deployment.deployment.ForecastDateSettings\nForecast date settings of the deployment\nVariables:\nenabled (bool) – Is True if predictions by forecast date is enabled for this deployment.\nTo update this setting, see\nupdate_predictions_by_forecast_date_settings()\ncolumn_name (str) – The column name in prediction datasets to be used as forecast date.\ndatetime_format (str) – The datetime format of the forecast date column in prediction datasets.\nclass datarobot.models.deployment.deployment.ChallengerModelsSettings\nChallenger models settings of the deployment is a dict with the following format:\nVariables:\nenabled (bool) – Is True if challenger models is enabled for this deployment. To update\nexisting ‘’challenger_models’’ settings, see\nupdate_challenger_models_settings()\nclass datarobot.models.deployment.deployment.SegmentAnalysisSettings\nSegment analysis settings of the deployment containing two items with keysenabled and attributes, which are further described below.\nVariables:\nenabled (bool) – Set to True if segment analysis is enabled for this deployment. To update\nexisting setting, see\nupdate_segment_analysis_settings()\nattributes (list) – To create or update existing segment analysis attributes, see\nupdate_segment_analysis_settings()\nclass datarobot.models.deployment.deployment.BiasAndFairnessSettings\nBias and fairness settings of this deployment\nVariables:\nprotected_features (List[str]) – A list of features to mark as protected.\npreferable_target_value (bool) – A target value that should be treated as a positive outcome for the prediction.\nfairness_metric_set (str) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nA set of fairness metrics to use for calculating fairness.\nfairness_threshold (float) – Threshold value of the fairness metric. Cannot be less than 0 or greater than 1.\nclass datarobot.models.deployment.deployment.ChallengerReplaySettings\nChallenger replay settings of the deployment is a dict with the following format:\nVariables:\nenabled (bool) – If challenger replay is enabled. To update\nexisting challenger_replay settings, see\nupdate_challenger_replay_settings()\nschedule (Schedule) – The recurring schedule for the challenger replay job.\nclass datarobot.models.deployment.deployment.HealthSettings\nHealth settings of the deployment containing seven nested dicts with keys\nVariables:\nservice (dict) – Service health settings.\ndata_drift (dict) – Data drift health settings.\naccuracy (dict) – Accuracy health settings.\nfairness (dict) – Fairness health settings.\ncustom_metrics (dict) – Custom metrics health settings.\npredictions_timeliness (dict) – Predictions timeliness health settings.\nactuals_timeliness (dict) – Actuals timeliness health settings.\nclass datarobot.models.deployment.deployment.DriftTrackingSettings\nDrift tracking settings of the deployment containing two nested dicts with keytarget_drift and feature_drift, which are further described below.\nVariables:\ntarget_drift (Settings) – If target drift tracking is enabled for this deployment. To create or update\nexisting target_drift settings, see\nupdate_drift_tracking_settings()\nfeature_drift (Settings) – If feature drift tracking is enabled for this deployment. To create or update\nexisting feature_drift settings, see\nupdate_drift_tracking_settings()\nclass datarobot.models.deployment.deployment.PredictionWarningSettings\nPrediction warning settings of the deployment\nVariables:\nenabled (bool) – If target prediction_warning is enabled for this deployment. To create or update\nexisting ‘’prediction_warning’’ settings, see\nupdate_prediction_warning_settings()\ncustom_boundaries (dict or None) – If None default boundaries for a model are used. Otherwise has following keys:\nupper (float): All predictions greater than provided value are considered anomalous\nlower (float): All predictions less than provided value are considered anomalous\nclass datarobot.models.deployment.deployment.PredictionIntervalsSettings\nPrediction intervals settings of the deployment is a dict with the following format:\nVariables:\nenabled (bool) – Whether prediction intervals are enabled for this deployment\npercentiles (list[int]) – List of enabled prediction intervals’ sizes for this deployment. Currently we only\nsupport one percentile at a time.\nclass datarobot.models.deployment.deployment.Capability\nclass datarobot.enums.ACCURACY_METRIC\nPredictions\nclass datarobot.models.Predictions\nRepresents predictions metadata and provides access to prediction results.\nVariables:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model\nprediction_id (str) – id of generated predictions\nincludes_prediction_intervals (Optional[bool]) – (New in v2.16) For time series projects only.\nIndicates if prediction intervals will be part of the response. Defaults to False.\nprediction_intervals_size (Optional[int]) – (New in v2.16) For time series projects only.\nIndicates the percentile used for prediction intervals calculation. Will be present only\nif includes_prediction_intervals is True.\nforecast_point (datetime.datetime, optional) – (New in v2.20) For time series projects only. This is the default point\nrelative to which predictions will be generated, based on the forecast window of the\nproject. See the time series prediction documentation for more\ninformation.\npredictions_start_date (datetime.datetime or None, optional) – (New in v2.20) For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in v2.20) For time series projects only. The end date for bulk\npredictions, exclusive. Note that this parameter is for generating historical predictions\nusing the training data. This parameter should be provided in conjunction with\npredictions_start_date. Can’t be provided with the forecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column which was used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nexplanation_algorithm (datarobot.enums.EXPLANATIONS_ALGORITHM, optional) – (New in version v2.21) If set to ‘shap’, the response will include prediction\nexplanations based on the SHAP explainer (SHapley Additive exPlanations). Defaults to null\n(no prediction explanations).\nmax_explanations (Optional[int]) – (New in version v2.21) The maximum number of explanation values that should be returned\nfor each row, ordered by absolute value, greatest to least. If null, no limit. In the case\nof ‘shap’: if the number of features is greater than the limit, the sum of remaining values\nwill also be returned as shapRemainingTotal. Defaults to null. Cannot be set if\nexplanation_algorithm is omitted.\nshap_warnings (dict, optional) – (New in version v2.21) Will be present if explanation_algorithm was set to\ndatarobot.enums.EXPLANATIONS_ALGORITHM.SHAP and there were additivity failures during SHAP\nvalues calculation.\nExamples\nList all predictions for a project\nimport datarobot as dr\n# Fetch all predictions for a project\nall_predictions = dr.Predictions.list(project_id)\n# Inspect all calculated predictions\nfor predictions in all_predictions:\nprint(predictions)  # repr includes project_id, model_id, and dataset_id\nRetrieve predictions by id\nimport datarobot as dr\n# Getting predictions by id\npredictions = dr.Predictions.get(project_id, prediction_id)\n# Dump actual predictions\ndf = predictions.get_all_as_dataframe()\nprint(df)\nclassmethod list(project_id, model_id=None, dataset_id=None)\nFetch all the computed predictions metadata for a project.\nParameters:\nproject_id (str) – id of the project\nmodel_id (Optional[str]) – if specified, only predictions metadata for this model will be retrieved\ndataset_id (Optional[str]) – if specified, only predictions metadata for this dataset will be retrieved\nReturn type:\nA list of Predictions objects\nclassmethod get(project_id, prediction_id)\nRetrieve the specific predictions metadata\nParameters:\nproject_id (str) – id of the project the model belongs to\nprediction_id (str) – id of the prediction set\nReturn type:\nPredictions\nReturns:\nPredictions object representing specified\npredictions\nget_all_as_dataframe(class_prefix='class_', serializer='json')\nRetrieve all prediction rows and return them as a pandas.DataFrame.\nParameters:\nclass_prefix (Optional[str]) – The prefix to append to labels in the final dataframe. Default is class_\n(e.g., apple -> class_apple)\nserializer (Optional[str]) – Serializer to use for the download. Options: json (default) or csv.\nReturns:\ndataframe\nReturn type:\npandas.DataFrame\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\ndownload_to_csv(filename, encoding='utf-8', serializer='json')\nSave prediction rows into CSV file.\nParameters:\nfilename (str or file object) – path or file object to save prediction rows\nencoding (string, optional) – A string representing the encoding to use in the output file, defaults to\n‘utf-8’\nserializer (Optional[str]) – Serializer to use for the download. Options: json (default) or csv.\nReturn type:\nNone\nPredictionServer\nclass datarobot.PredictionServer\nA prediction server can be used to make predictions.\nVariables:\nid (Optional[str]) – The id of the prediction server.\nurl (str) – The url of the prediction server.\ndatarobot_key (Optional[str]) – The Datarobot-Key HTTP header used in requests to this prediction server. Note that in the\ndatarobot.models.Deployment instance there is the default_prediction_server\nproperty which has this value as a “kebab-cased” key as opposed to “snake_cased”.\nclassmethod list()\nReturns a list of prediction servers a user can use to make predictions.\nAdded in version v2.17.\nReturns:\nprediction_servers – Contains a list of prediction servers that can be used to make predictions.\nReturn type:\nlist of PredictionServer instances\nExamples\nprediction_servers = PredictionServer.list()\nprediction_servers\n>>> [PredictionServer('https://example.com')]\nPrediction environment\nclass datarobot.models.PredictionEnvironment\nA prediction environment entity.\nAdded in version v3.3.0.\nVariables:\nid (str) – The ID of the prediction environment.\nname (str) – The name of the prediction environment.\ndescription (Optional[str]) – The description of the prediction environment.\nplatform (Optional[str]) – Indicates which platform is in use (AWS, GCP, DataRobot, etc.).\npermissions (Optional[List]) – A set of permissions for the prediction environment.\nis_deleted (boolean, optional) – The flag that shows if this prediction environment deleted.\nsupported_model_formats (list[PredictionEnvironmentModelFormats], optional) – The list of supported model formats.\nis_managed_by_management_agent (boolean, optional) – Determines if the prediction environment should be managed by the management agent. False by default.\ndatastore_id (Optional[str]) – The ID of the data store connection configuration.\nOnly applicable for external prediction environments managed by DataRobot.\ncredential_id (Optional[str]) – The ID of the credential associated with the data connection.\nOnly applicable for external prediction environments managed by DataRobot.\nclassmethod list()\nReturns list of available external prediction environments.\nReturns:\nprediction_environments – contains a list of available prediction environments.\nReturn type:\nlist of PredictionEnvironment instances\nExamples\n>>> import datarobot as dr\n>>> prediction_environments = dr.PredictionEnvironment.list()\n>>> prediction_environments\n[\nPredictionEnvironment('5e429d6ecf8a5f36c5693e03', 'demo_pe', 'aws', 'env for demo testing'),\nPredictionEnvironment('5e42cc4dcf8a5f3256865840', 'azure_pe', 'azure', 'env for azure demo testing'),\n]\nclassmethod get(pe_id)\nGets the PredictionEnvironment by id.\nParameters:\npe_id (str) – the identifier of the PredictionEnvironment.\nReturns:\nprediction_environment – the requested prediction environment object.\nReturn type:\nPredictionEnvironment\nExamples\n>>> import datarobot as dr\n>>> pe = dr.PredictionEnvironment.get('5a8ac9ab07a57a1231be501f')\n>>> pe\nPredictionEnvironment('5a8ac9ab07a57a1231be501f', 'my_predict_env', 'aws', 'demo env'),\ndelete()\nDeletes the prediction environment.\n:rtype: None\nExamples\n>>> import datarobot as dr\n>>> pe = dr.PredictionEnvironment.get('5a8ac9ab07a57a1231be501f')\n>>> pe.delete()\nclassmethod create(name, platform, description=None, plugin=None, supported_model_formats=None, is_managed_by_management_agent=False, datastore=None, credential=None)\nCreate a prediction environment.\nParameters:\nname (str) – The name of the prediction environment.\ndescription (Optional[str]) – The description of the prediction environment.\nplatform (str) – Indicates which platform is in use (AWS, GCP, DataRobot, etc.).\nplugin (str) – Optional. The plugin name to use.\nsupported_model_formats (list[PredictionEnvironmentModelFormats], optional) – The list of supported model formats.\nWhen not provided, the default value is inferred based on platform, (DataRobot platform: DataRobot,\nCustom Models; All other platforms: DataRobot, Custom Models, External Models).\nis_managed_by_management_agent (boolean, optional) – Determines if this prediction environment should be managed by the management agent. default: False\ndatastore (DataStore|Optional[str]]) – The datastore object or ID of the data store connection configuration.\nOnly applicable for external Prediction Environments managed by DataRobot.\ncredential (Credential|Optional[str]]) – The credential object or ID of the credential associated with the data connection.\nOnly applicable for external Prediction Environments managed by DataRobot.\nReturns:\nprediction_environment – the prediction environment was created\nReturn type:\nPredictionEnvironment\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nExamples\n>>> import datarobot as dr\n>>> pe = dr.PredictionEnvironment.create(\n...     name='my_predict_env',\n...     platform=PredictionEnvironmentPlatform.AWS,\n...     description='demo prediction env',\n... )\n>>> pe\nPredictionEnvironment('5e429d6ecf8a5f36c5693e99', 'my_predict_env', 'aws', 'demo prediction env'),",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/deployment-management.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/deployment-management.html",
        "content_length": 87207
      },
      "code_examples": [],
      "api_methods": [
        "deployment.servicestatsovertime",
        "dr.predictions.get",
        "deployment.get",
        "deployment.get_predictions_over_time",
        "deployment.bias_and_fairness",
        "deployment.create_from_learning_model",
        "deployment.list",
        "deployment.get_secondary_dataset_config",
        "deployment.deployment",
        "datarobot.enums.deployment_service_health_status",
        "deployment.download_model_package_file",
        "deployment.predictionsvsactualsovertime",
        "datarobot.enums.fairnessmetricsset",
        "deployment.update_accuracy_metrics_settings",
        "datarobot.enums.deployment_model_health_status",
        "deployment.targetdrift",
        "deployment.label",
        "deployment.replace_model",
        "deployment.get_retraining_settings",
        "deployment.update_predictions_data_collection_settingsmethod",
        "model.id",
        "deployment.predict_batch",
        "deployment.model",
        "deployment.submit_actuals",
        "dr.predictions.list",
        "deployment.deploymentlistfilters",
        "deployment.get_segment_attributes",
        "datarobot.enums.deployment_accuracy_health_status",
        "datarobot.enums.deployment_importance",
        "project.get",
        "deployment.deploymentgrantsharedrolewithusername",
        "deployment.submit_actuals_from_catalog_async",
        "deployment.download_scoring_code",
        "datarobot.models.predictionenvironment",
        "deployment.id",
        "deployment.download_prediction_results",
        "deployment.accuracyovertime",
        "deployment.deploymentsharedrole",
        "deployment.list_data_quality_exports",
        "deployment.get_predictions_vs_actuals_over_time",
        "deployment.predictionsovertime",
        "datarobot.enums.explanations_algorithm",
        "deployment.update_predictions_by_forecast_date_settings",
        "deployment.perform_model_replace",
        "deployment.get_accuracy_metrics_settings",
        "datarobot.errors.servererror",
        "dr.predictionenvironment.create",
        "deployment.update_retraining_settings",
        "deployment.get_features",
        "deployment.get_segment_values",
        "deployment.create_from_leaderboard",
        "datarobot.models.batchpredictionjob",
        "deployment.deploymentgrantsharedrolewithid",
        "datarobot.enums.deployment_execution_environment_type",
        "deployment.servicestats",
        "deployment.get_champion_model_package",
        "datarobot.enums.accuracy_metric",
        "deployment.update_secondary_dataset_config",
        "deployment.get_capabilities",
        "deployment.get_prediction_results",
        "deployment.model_package",
        "datarobot.models.predictions",
        "deployment.featuredrift",
        "datarobot.models.deployment",
        "deployment.accuracy",
        "project.get_models",
        "dr.predictionenvironment.list",
        "dr.predictionenvironment.get",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_7860752953297460056",
      "title": "LLM Generation",
      "content": "LLM Generation\nclass datarobot.models.genai.custom_model_validation.CustomModelValidation\nThe validation record checking the ability of the deployment to serve\nas a custom model LLM, custom model vector database, or custom model embedding.\nVariables:\nid (str) – The ID of the validation.\nprompt_column_name (str) – The name of the column the deployed model uses for prompt text input.\ntarget_column_name (str) – The name of the column the deployed model uses for prediction output.\ndeployment_id (str) – The ID of the deployment.\nmodel_id (str) – The ID of the underlying deployed model, which can be found using Deployment.model[“id”].\nvalidation_status (str) – Can be TESTING, FAILED, or PASSED. Only PASSED is allowed for use.\ndeployment_access_data (dict, optional) – The data that will be used for accessing the deployment prediction server.\nThis field is only available for deployments that pass validation.\nDict fields are as follows:\n- prediction_api_url - The URL for the deployment prediction server.\n- datarobot_key - The first of two auth headers for the prediction server.\n- authorization_header - The second of two auth headers for the prediction server.\n- input_type - The input type the model expects, either JSON or CSV.\n- model_type - The target type of the deployed custom model.\ntenant_id (str) – The creating user’s tenant ID.\nname (str) – The display name of the validated custom model.\ncreation_date (str) – The creation date of the validation (ISO 8601 formatted).\nuser_id (str) – The ID of the creating user.\nerror_message (Optional[str]) – Additional information for the errored validation.\ndeployment_name (Optional[str]) – The name of the validated deployment.\nuser_name (Optional[str]) – The name of the creating user.\nuse_case_id (Optional[str]) – The ID of the Use Case associated with the validation.\nprediction_timeout (int) – The timeout, in seconds, for the prediction API used in this custom model validation.\nclassmethod get(validation_id)\nGet the validation record by id.\nParameters:\nvalidation_id (Union[CustomModelValidation, str]) – The CustomModelValidation to retrieve, either CustomModelValidation or validation ID.\nReturn type:\nCustomModelValidation\nclassmethod get_by_values(prompt_column_name, target_column_name, deployment_id, model_id)\nGet the validation record by field values.\nParameters:\nprompt_column_name (str) – The name of the column the deployed model uses for prompt text input.\ntarget_column_name (str) – The name of the column the deployed model uses for prediction output.\ndeployment_id (str) – The ID of the deployment.\nmodel_id (str) – The ID of the underlying deployed model.\nReturn type:\nCustomModelValidation\nclassmethod list(prompt_column_name=None, target_column_name=None, deployment=None, model=None, use_cases=None, playground=None, completed_only=False, search=None, sort=None)\nList the validation records by field values.\nParameters:\nprompt_column_name (Optional[str], optional) – The column name the deployed model expects as the input.\ntarget_column_name (Optional[str], optional) – The target name that the deployed model will output.\ndeployment (Optional[Union[Deployment, str]], optional) – The returned validations are filtered to those associated with a specific deployment\nif specified, either Deployment or the deployment ID.\nmodel_id (Optional[Union[Model, str]], optional) – The returned validations are filtered to those associated with a specific model\nif specified, either Model or model ID.\nuse_cases (Optional[list[Union[UseCase, str]]], optional) – The returned validations are filtered to those associated with specific Use Cases\nif specified, either UseCase objects or the Use Case IDs.\nplayground_id (Optional[Union[Playground, str]], optional) – The returned validations are filtered to those used in a specific playground\nif specified, either Playground or playground ID.\ncompleted_only (Optional[bool]) – Whether to retrieve only completed validations.\nsearch (Optional[str], optional) – String for filtering validations.\nValidations that contain the string in name will be returned.\nsort (Optional[str], optional) – Property to sort validations by.\nPrefix the attribute name with a dash to sort in descending order,\ne.g. sort=’-name’.\nCurrently supported options are listed in ListCustomModelValidationsSortQueryParams\nbut the values can differ with different platform versions.\nBy default, the sort parameter is None which will result in\nvalidations being returned in order of creation time descending.\nReturn type:\nList[CustomModelValidation]\nclassmethod revalidate(validation_id)\nRevalidate an unlinked custom model vector database or LLM.\nThis method is helpful when a deployment used as vector database or LLM is accidentally\nreplaced with another model that stopped complying with the response schema requirements.\nReplace the deployment’s model with a complying model and call this method instead of\ncreating a new custom model validation from scratch.\nAnother application is if the API token used to create a validation record got revoked and\nno longer can be used to call the deployment.\nCalling revalidate will update the validation record with the token currently in use.\nParameters:\nvalidation_id (str) – The ID of the CustomModelValidation for revalidation.\nReturn type:\nCustomModelValidation\ndelete()\nDelete the custom model validation.\nReturn type:\nNone\nclass datarobot.models.genai.custom_model_llm_validation.CustomModelLLMValidation\nValidation record checking the ability of the deployment to serve\nas a custom model LLM.\nVariables:\nid (str) – The ID of the validation.\nprompt_column_name (str) – The name of the column the deployed model uses for prompt text input.\ntarget_column_name (str) – The name of the column the deployed model uses for prediction output.\nchat_model_id (Optional[str]) – The model ID to specify when calling the chat completion API of the deployment.\ndeployment_id (str) – The ID of the deployment.\nmodel_id (str) – The ID of the underlying deployed model, which can be found using Deployment.model[“id”].\nvalidation_status (str) – Can be TESTING, FAILED, or PASSED. Only PASSED is allowed for use.\ndeployment_access_data (dict, optional) – The data that will be used for accessing the deployment prediction server.\nThis field is only available for deployments that pass validation.\nDict fields are as follows:\n- prediction_api_url - The URL for the deployment prediction server.\n- datarobot_key - The first of two auth headers for the prediction server.\n- authorization_header - The second of two auth headers for the prediction server.\n- input_type - The input type the model expects, either JSON or CSV.\n- model_type - The target type of the deployed custom model.\ntenant_id (str) – The creating user’s tenant ID.\nname (str) – The display name of the validated custom model.\ncreation_date (str) – The creation date of the validation (ISO 8601 formatted).\nuser_id (str) – The ID of the creating user.\nerror_message (Optional[str]) – Additional information for the errored validation.\ndeployment_name (Optional[str]) – The name of the validated deployment.\nuser_name (Optional[str]) – The name of the creating user.\nuse_case_id (Optional[str]) – The ID of the Use Case associated with the validation.\nprediction_timeout (int) – The timeout in seconds for the prediction API used in this custom model validation.\nclassmethod create(deployment_id, model=None, use_case=None, name=None, wait_for_completion=False, prediction_timeout=None, prompt_column_name=None, target_column_name=None, chat_model_id=None)\nStart the validation of the deployment that will serve as an LLM.\nParameters:\ndeployment_id (Union[Deployment, str]) – The deployment to validate, either Deployment or the deployment ID.\nmodel (Optional[Union[Model, str]], optional) – The specific model within the deployment, either Model or the model ID.\nIf not specified, the underlying model ID will be automatically derived from the\ndeployment information.\nuse_case (Optional[Union[UseCase, str]], optional) – The Use Case to link the validation to, either UseCase or the Use Case ID.\nname (Optional[str], optional) – The name of the validation.\nwait_for_completion (bool) – If set to True, the code will wait for the validation job to complete before returning\nresults. If the job does not finish in 10 minutes, this method call raises a timeout\nerror.\nIf set to False, the code does not wait for the job to complete. Instead,\nCustomModelLLMValidation.get can be used to poll for the status of the job using\nthe validation ID returned by the method.\nprediction_timeout (Optional[int], optional) – The timeout, in seconds, for the prediction API used in this custom model validation.\nprompt_column_name (Optional[str], optional) – The name of the column the deployed model uses for prompt text input.\nThis value is used to call the Prediction API of the deployment.\nFor LLM deployments that support the chat completion API, it is recommended to\nspecify chatModelId instead.\ntarget_column_name (Optional[str], optional) – The name of the column the deployed model uses for prediction output.\nThis value is used to call the Prediction API of the deployment.\nFor LLM deployments that support the chat completion API, it is recommended to\nspecify chatModelId instead.\nchat_model_id (Optional[str], optional) – The model ID to specify when calling the chat completion API of the deployment.\nIf this parameter is specified, the deployment must support the chat completion API.\nReturn type:\nCustomModelLLMValidation\nupdate(name=None, prompt_column_name=None, target_column_name=None, chat_model_id=None, deployment=None, model=None, prediction_timeout=None)\nUpdate a custom model validation.\nParameters:\nname (Optional[str], optional) – The new name of the custom model validation.\nprompt_column_name (Optional[str], optional) – The new name of the prompt column.\ntarget_column_name (Optional[str], optional) – The new name of the target column.\nchat_model_id (Optional[str], optional) – The new model ID to specify when calling the chat completion API of the deployment.\ndeployment (Optional[Union[Deployment, str]]) – The new deployment to validate.\nmodel (Optional[Union[Model, str]], optional) – The new model within the deployment to validate.\nprediction_timeout (Optional[int], optional) – The new timeout, in seconds, for the prediction API used in this custom model validation.\nReturn type:\nCustomModelLLMValidation\nclass datarobot.models.genai.llm_blueprint.LLMBlueprint\nMetadata for a DataRobot GenAI LLM blueprint.\nVariables:\nid (str) – The LLM blueprint ID.\nname (str) – The LLM blueprint name.\ndescription (str) – A description of the LLM blueprint.\nis_saved (bool) – Whether the LLM blueprint is saved (meaning the settings are locked\nand blueprint is eligible for use with ComparisonPrompts).\nis_starred (bool) – Whether the LLM blueprint is starred.\nplayground_id (str) – The ID of the playground associated with the LLM blueprint.\nllm_id (str or None) – The ID of the LLM type. If not None, this must be one of the IDs returned by LLMDefinition.list\nfor this user.\nllm_name (str or None) – The name of the LLM.\nllm_settings (dict or None) – The LLM settings for the LLM blueprint. The specific keys allowed and the\nconstraints on the values are defined in the response from LLMDefinition.list\nbut this typically has dict fields:\n- system_prompt - The system prompt that tells the LLM how to behave.\n- max_completion_length - The maximum number of tokens in the completion.\n- temperature - Controls the variability in the LLM response.\n- top_p - Whether the model considers next tokens with top_p probability mass.\nOr\n- system_prompt - The system prompt that tells the LLM how to behave.\n- validation_id - The ID of the external model LLM validation.\n- external_llm_context_size - The external LLM’s context size, in tokens,\nfor external model LLM blueprints.\ncreation_date (str) – The date the playground was created.\ncreation_user_id (str) – The ID of the user creating the playground.\ncreation_user_name (str) – The name of the user creating the playground.\nlast_update_date (str) – The date the playground was last updated.\nlast_update_user_id (str) – The ID of the user who most recently updated the playground.\nprompt_type (PromptType) – The prompting strategy for the LLM blueprint.\nCurrently supported options are listed in PromptType.\nvector_database_id (str or None) – The ID of the vector database, if any, associated with the LLM blueprint.\nvector_database_settings (VectorDatabaseSettings or None) – The settings for the vector database, if any, associated with the LLM blueprint.\nvector_database_name (str or None) – The name of the vector database associated with the LLM blueprint, if any.\nvector_database_status (str or None) – The status of the vector database, if any, associated with the LLM blueprint.\nvector_database_error_message (str or None) – The error message for the vector database, if any, associated with the LLM blueprint.\nvector_database_error_resolution (str or None) – The resolution for the vector database error, if any, associated with the LLM blueprint.\ncustom_model_llm_validation_status (str or None) – The status of the custom model LLM validation if the llm_id is ‘custom-model’.\ncustom_model_llm_error_message (str or None) – The error message for the custom model LLM, if any.\ncustom_model_llm_error_resolution (str or None) – The resolution for the custom model LLM error, if any.\nclassmethod create(playground, name, prompt_type=PromptType.CHAT_HISTORY_AWARE, description='', llm=None, llm_settings=None, vector_database=None, vector_database_settings=None)\nCreate a new LLM blueprint.\nParameters:\nplayground (Playground or str) – The playground associated with the created LLM blueprint.\nAccepts playground or playground ID.\nname (str) – The LLM blueprint name.\nprompt_type (PromptType, optional) – Prompting type of the LLM blueprint, by default PromptType.CHAT_HISTORY_AWARE.\ndescription (Optional[str]) – An optional description for the LLM blueprint, otherwise null.\nllm (LLMDefinition, str, or None, optional) – The LLM to use for the blueprint, either LLMDefinition or LLM ID.\nllm_settings (dict or None) – The LLM settings for the LLM blueprint. The specific keys allowed and the\nconstraints on the values are defined in the response from LLMDefinition.list\nbut this typically has dict fields:\n- system_prompt - The system prompt that tells the LLM how to behave.\n- max_completion_length - The maximum number of tokens in the completion.\n- temperature - Controls the variability in the LLM response.\n- top_p - Whether the model considers next tokens with top_p probability mass.\nOr\n- system_prompt - The system prompt that tells the LLM how to behave.\n- validation_id - The ID of the custom model LLM validation\nfor custom model LLM blueprints.\nvector_database (VectorDatabase, str, or None, optional) – The vector database to use with this LLM blueprint, either\nVectorDatabase or vector database ID.\nvector_database_settings (VectorDatabaseSettings or None, optional) – The settings for the vector database, if any.\nReturns:\nllm_blueprint – The created LLM blueprint.\nReturn type:\nLLMBlueprint\nclassmethod create_from_llm_blueprint(llm_blueprint, name, description='')\nCreate a new LLM blueprint from an existing LLM blueprint.\nParameters:\nllm_blueprint (LLMBlueprint or str) – The LLM blueprint to use to create the new LLM blueprint.\nAccepts LLM blueprint or LLM blueprint ID.\nname (str) – LLM blueprint name.\ndescription (Optional[str]) – Description of the LLM blueprint, by default “”.\nReturns:\nllm_blueprint – The created LLM blueprint.\nReturn type:\nLLMBlueprint\nclassmethod get(llm_blueprint_id)\nRetrieve a single LLM blueprint.\nParameters:\nllm_blueprint_id (str) – The ID of the LLM blueprint you want to retrieve.\nReturns:\nllm_blueprint – The requested LLM blueprint.\nReturn type:\nLLMBlueprint\nclassmethod list(playground=None, llms=None, vector_databases=None, is_saved=None, is_starred=None, sort=None)\nLists all LLM blueprints available to the user. If the playground is specified, then the\nresults are restricted to the LLM blueprints associated with the playground. If the\nLLMs are specified, then the results are restricted to the LLM blueprints using those\nLLM types. If vector_databases are specified, then the results are restricted to the\nLLM blueprints using those vector databases.\nParameters:\nplayground (Optional[Union[Playground, str]], optional) – The returned LLM blueprints are filtered to those associated with a specific playground\nif it is specified. Accepts either the entity or the ID.\nllms (Optional[list[Union[LLMDefinition, str]]], optional) – The returned LLM blueprints are filtered to those associated with the LLM types\nspecified. Accepts either the entity or the ID.\nvector_databases (Optional[list[Union[VectorDatabase, str]]], optional) – The returned LLM blueprints are filtered to those associated with the vector databases\nspecified. Accepts either the entity or the ID.\nis_saved (Optional[bool], optional) – The returned LLM blueprints are filtered to those matching is_saved.\nis_starred (Optional[bool], optional) – The returned LLM blueprints are filtered to those matching is_starred.\nsort (Optional[str]) – Property to sort LLM blueprints by.\nPrefix the attribute name with a dash to sort in descending order,\ne.g. sort=’-creationDate’.\nCurrently supported options are listed in ListLLMBlueprintsSortQueryParams\nbut the values can differ with different platform versions.\nBy default, the sort parameter is None which will result in\nLLM blueprints being returned in order of creation time descending.\nReturns:\nplaygrounds – A list of playgrounds available to the user.\nReturn type:\nlist[Playground]\nupdate(name=None, description=None, llm=None, llm_settings=None, vector_database=None, vector_database_settings=None, is_saved=None, is_starred=None, prompt_type=None, remove_vector_database=False)\nUpdate the LLM blueprint.\nParameters:\nname (str or None, optional) – The new name for the LLM blueprint.\ndescription (str or None, optional) – The new description for the LLM blueprint.\nllm (Optional[Union[LLMDefinition, str]], optional) – The new LLM type for the LLM blueprint.\nllm_settings (Optional[dict], optional) – The new LLM settings for the LLM blueprint. These must match the LLMSettings\nreturned from the LLMDefinition.list method for the LLM type used for this\nLLM blueprint but this typically has dict fields:\n- system_prompt - The system prompt that tells the LLM how to behave.\n- max_completion_length - The maximum number of tokens in the completion.\n- temperature - Controls the variability in the LLM response.\n- top_p - Whether the model considers next tokens with top_p probability mass.\nOr\n- system_prompt - The system prompt that tells the LLM how to behave.\n- validation_id - The ID of the custom model LLM validation\nfor custom model LLM blueprints.\nvector_database (Optional[Union[VectorDatabase, str]], optional) – The new vector database for the LLM blueprint.\nvector_database_settings (Optional[VectorDatabaseSettings], optional) – The new vector database settings for the LLM blueprint.\nis_saved (Optional[bool], optional) – The new is_saved attribute for the LLM blueprint (meaning the settings\nare locked and blueprint is eligible for use with ComparisonPrompts).\nis_starred (Optional[bool], optional) – The new setting for whether the LLM blueprint is starred.\nprompt_type (PromptType, optional) – The new prompting type of the LLM blueprint.\nremove_vector_database (Optional[bool], optional) – Whether to remove the vector database from the LLM blueprint.\nReturns:\nllm_blueprint – The updated LLM blueprint.\nReturn type:\nLLMBlueprint\ndelete()\nDelete the single LLM blueprint.\nReturn type:\nNone\nregister_custom_model(prompt_column_name=None, target_column_name=None, llm_test_configuration_ids=None, vector_database_default_prediction_server_id=None, vector_database_prediction_environment_id=None, vector_database_maximum_memory=None, vector_database_resource_bundle_id=None, vector_database_replicas=None, vector_database_network_egress_policy=None)\nCreate a new CustomModelVersion. This registers a custom model from the LLM blueprint.\nIf this LLM Blueprint uses a vector database and that vector database is not yet deployed,\nthis will also deploy that vector database.\nParameters:\nprompt_column_name (Optional[str]) – The column name of the prompt text.\ntarget_column_name (Optional[str]) – The column name of the response text.\nllm_test_configuration_ids (List[str], optional) – The IDs of the LLM test configurations to execute.\nvector_database_default_prediction_server_id (Optional[str]) – Only used if a new vector database is deployed. An identifier of a prediction server\nto be used as the default prediction server by the vector database. When working with\nprediction environments, default_prediction_server_id should not be provided.\nvector_database_prediction_environment_id (Optional[str]) – Only used if a new vector database is deployed. The identifier of the prediction\nenvironment to be used by the vector database.\nvector_database_maximum_memory (Optional[int]) – Only used if a new vector database is deployed. The maximum memory that will be\nallocated to the vector database.\nvector_database_resource_bundle_id (Optional[str]) – Only used if a new vector database is deployed. The ID of a\ndatarobot.models.resource_bundle.ResourceBundle that will be used by the vector\ndatabase.\nvector_database_replicas (Optional[int]) – Only used if a new vector database is deployed. The number of replicas to be deployed\nfor the vector database.\nvector_database_network_egress_policy (Optional[str]) – Only used if a new vector database is deployed. Determines whether the new vector\ndatabase deployment is isolated or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE,\ndatarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nReturns:\ncustom_model – The registered custom model.\nReturn type:\nCustomModelVersion\nclass datarobot.models.genai.llm.LLMDefinition\nMetadata for a DataRobot GenAI LLM.\nVariables:\nid (str) – Language model type ID.\nname (str) – Language model name.\ndescription (str) – Description of the language model.\nvendor (str) – Name of the vendor for this model.\nlicense (str) – License for this model.\nsupported_languages (str) – Languages supported by this model.\nsettings (list of LLMSettingDefinition) – Settings for this model\ncontext_size (int) – The context size for this model\nretirement_date (datetime.date, optional) – When the LLM is expected to be retired and no longer\navailable for submitting new prompts.\nis_deprecated (bool, optional) – Whether the LLM is deprecated and will be removed when\nit is past the retirement date.\nis_active (bool, optional) – Whether the LLM is active.\nclassmethod list(use_case=None, as_dict=True)\nList all large language models (LLMs) available to the user.\nParameters:\nuse_case (Optional[UseCase or str], optional) – The returned LLMs, including external LLMs, available\nfor the specified Use Case.\nAccepts either the entity or the Use CaseID.\nReturns:\nllms – A list of large language models (LLMs) available to the user.\nReturn type:\nlist[LLMDefinition] or list[LLMDefinitionDict]\nclass datarobot.models.genai.llm.LLMDefinitionDict\nDict representation of LLMDefinition.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-llm-generation.html",
      "tags": [
        "advanced",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-llm-generation.html",
        "content_length": 23322
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.resource_bundle",
        "deployment.model",
        "datarobot.network_egress_policy.none",
        "datarobot.models.genai",
        "datarobot.network_egress_policy.public"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_3085176038744805352",
      "title": "Generative AI Moderation",
      "content": "Generative AI Moderation\nclass datarobot.models.moderation.configuration.ModerationConfiguration\nDetails of overall moderation configuration for a model.\nclassmethod get(config_id)\nGet a guard configuration by ID.\nAdded in version v3.6.\nParameters:\nconfig_id (str) – ID of the configuration\nReturns:\nretrieved configuration\nReturn type:\nModerationConfiguration\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nclassmethod list(entity_id, entity_type)\nList Guard Configurations.\nAdded in version v3.6.\nParameters:\nentity_id (str) – ID of the entity\nentity_type (ModerationGuardEntityType) – Type of the entity\nReturns:\na list of configurations\nReturn type:\nList[ModerationConfiguration]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod create(template_id, name, description, stages, entity_id, entity_type, intervention=None, llm_type=None, deployment_id=None, model_info=None, nemo_info=None, openai_api_key='', openai_api_base='', openai_deployment_id='', openai_credential='')\nCreate a configuration. This is not a full create from scratch; it’s based on a template.\nAdded in version v3.6.\nParameters:\ntemplate_id (str) – ID of the template to base this configuration on.\nname (str) – name of the configuration.\ndescription (str) – description of the configuration.\nstages (List[ModerationGuardStage]) – the stages of moderation where this guard is active\nentity_id (str) – ID of the custom model version or playground this configuration applies to.\nentity_type (ModerationGuardEntityType) – Type of the associated entity_id\nllm_type (Optional[ModerationGuardLlmType]) – the backing LLM this guard uses.\nnemo_info (Optional[GuardNemoInfo]) – additional configuration for NeMo Guardrails guards.\nmodel_info (Optional[GuardModelInfo]) – additional configuration for guards using a deployed model.\nintervention (Optional[GuardInterventionForConfiguration]) – the assessment conditions, and action the guard should take if conditions are met.\nopenai_api_key (str) – Token to use for OpenAI. Deprecated; use openai_credential instead.\nopenai_api_base (Optional[str]) – Base of the OpenAI connection\nopenai_deployment_id (Optional[str]) – ID of the OpenAI deployment\nopenai_credential (Optional[str]) – ID of the credential defined in DataRobot for OpenAI.\nReturns:\ncreated ModerationConfiguration\nReturn type:\nModerationConfiguration\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nupdate(name=None, description=None, intervention=None, llm_type=None, deployment_id=None, model_info=None, nemo_info=None, openai_api_key='', openai_api_base='', openai_deployment_id='', openai_credential='')\nUpdate configuration. All fields are optional, and omitted fields are left unchanged.\nentity_id, entity_type, and stages cannot be modified for a guard configuration,.\nAdded in version v3.6.\nParameters:\nname (str) – name of the configuration.\ndescription (str) – description of the configuration.\nllm_type (Optional[ModerationGuardLlmType]) – the backing LLM this guard uses.\nnemo_info (Optional[GuardNemoInfo]) – additional configuration for NeMo Guardrails guards.\nmodel_info (Optional[GuardModelInfo]) – additional configuration for guards using a deployed model.\nintervention (Optional[GuardInterventionForConfiguration]) – the assessment conditions, and action the guard should take if conditions are met.\nopenai_api_key (str) – Token to use for OpenAI. Deprecated; use openai_credential instead.\nopenai_api_base (Optional[str]) – Base of the OpenAI connection\nopenai_deployment_id (Optional[str]) – ID of the OpenAI deployment\nopenai_credential (Optional[str]) – ID of the credential defined in DataRobot for OpenAI.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nNone\nrefresh()\nUpdate OverallModerationConfig with the latest data from server.\n:rtype: None\nAdded in version v3.6.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndelete()\nDelete configuration.\n:rtype: None\nAdded in version v3.6.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nclass datarobot.models.moderation.intervention.GuardInterventionCondition\nDefines a condition for intervention.\nclass datarobot.models.moderation.intervention.GuardInterventionForTemplate\nDefines the intervention conditions and actions a guard can take.\nConfiguration schema differs slightly from template because changes were requested after\ntemplates were baked in.\nclassmethod ensure_object(maybe_dict)\nintervention may arrive as an object, or as a dict. Return an object.\nReturn type:\nGuardInterventionForTemplate\nclass datarobot.models.moderation.intervention.GuardInterventionForConfiguration\nDefines the intervention conditions and actions a guard can take.\nConfiguration schema differs slightly from template because changes were requested after\ntemplates were baked in.\nclassmethod ensure_object(maybe_dict)\nintervention may arrive as an object, or as a dict. Return an object.\nReturn type:\nGuardInterventionForConfiguration\nclass datarobot.models.moderation.model_info.GuardModelInfo\nModel information for moderation templates and configurations.\nOmitted optional values are stored and presented as:\n* []  (for class names)\n* None  (all others)\nto_dict()\nConvert the model information object to a dictionary.\nReturn type:\nDict[str, Union[str, List[str], None]]\nclassmethod ensure_object(maybe_dict)\nintervention may arrive as an object, or as a dict. Return an object.\nReturn type:\nGuardModelInfo\nclass datarobot.models.moderation.model_version_update.ModelVersionUpdate\nImplements the operation provided by “Save Configuration” in moderation UI.\nAll guard configurations and overall config is saved to a new custom model version.\nclassmethod new_custom_model_version_with_config(custom_model_id, overall_config, configs)\nCreate a new custom model version with the provided moderation configuration\n:type custom_model_id: str\n:param custom_model_id:\n:type overall_config: OverallModerationConfig\n:param overall_config:\n:type configs: List[ModerationConfiguration]\n:param configs:\nReturn type:\nID of the new custom model version.\nclass datarobot.models.moderation.nemo_info.GuardNemoInfo\nDetails of a NeMo Guardrails moderation guard.\nclass datarobot.models.moderation.overall.OverallModerationConfig\nDetails of overall moderation configuration for a model.\nclassmethod find(entity_id, entity_type)\nFind overall configuration by entity ID and entity type.\nEach entity (such as a customModelVersion) may have at most 1 overall moderation configuration.\nAdded in version v3.6.\nParameters:\nentity_id (str) – ID of the entity\nentity_type (str) – Type of the entity\nReturns:\nan OverallModerationConfig or None\nReturn type:\nOptional[OverallModerationConfig]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod locate(entity_id, entity_type)\nFind overall configuration by entity ID and entity type.\nThis version of find() expects the object to exist. Its return type is not optional.\nAdded in version v3.6.\nParameters:\nentity_id (str) – ID of the entity\nentity_type (str) – Type of the entity\nReturns:\na list of OverallModerationConfig\nReturn type:\nList[OverallModerationConfig]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod create(timeout_sec, timeout_action, entity_id, entity_type)\nCreate an OverallModerationConfig.\nAdded in version v3.6.\nParameters:\ntimeout_sec (int) – how long to wait for all moderation tasks in a phase to complete.\ntimeout_action (ModerationTimeoutActionType) – what to do if moderation times out.\nentity_id (str) – entity, such as customModelVersion, that this configuration applies to.\nentity_type (ModerationGuardEntityType) – type of the entity defined by entity_id\nReturns:\ncreated OverallModerationConfig\nReturn type:\nOverallModerationConfig\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nupdate(timeout_sec, timeout_action, entity_id, entity_type)\nUpdate an OverallModerationConfig.\nAdded in version v3.6.\nParameters:\ntimeout_sec (int) – how long to wait for all moderation tasks in a phase to complete.\ntimeout_action (ModerationTimeoutActionType) – what to do if moderation times out.\nentity_id (str) – entity, such as customModelVersion, that this configuration applies to.\nentity_type (ModerationGuardEntityType) – type of the entity defined by entity_id\nReturns:\ncreated OverallModerationConfig\nReturn type:\nOverallModerationConfig\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nrefresh()\nUpdate OverallModerationConfig with the latest data from server.\n:rtype: None\nAdded in version v3.6.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclass datarobot.models.moderation.template.ModerationTemplate\nA DataRobot Moderation Template.\nAdded in version v3.6.\nVariables:\nid (str) – ID of the Template\nname (str) – Template name\nclassmethod get(template_id)\nGet Template by id.\nAdded in version v3.6.\nParameters:\ntemplate_id (str) – ID of the Template\nReturns:\nretrieved Template\nReturn type:\nModerationTemplate\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nclassmethod list()\nList Templates.\nAdded in version v3.6.\nParameters:\nyet (none)\nReturns:\na list of Templates\nReturn type:\nList[ModerationTemplate]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod find(name)\nFind Template by name.\nAdded in version v3.6.\nParameters:\nname (str) – name of the Template\nReturns:\na list of Templates\nReturn type:\nList[ModerationTemplate]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod create(name, description, type, allowed_stages, intervention=None, ootb_type=None, llm_type=None, model_info=None, nemo_info=None)\nCreate a Template.\nAdded in version v3.6.\nParameters:\nname (str) – name of the template.\ndescription (str) – description of the template.\ntype (GuardType) – type of the template.\nallowed_stages (List[ModerationGuardStage]) – the stages of moderation this guard is allowed to be used\nootb_type (ModerationGuardOotbType) – for guards of type “ootb”, the specific “Out of the Box” metric type.\nllm_type (Optional[ModerationGuardLlmType]) – the backing LLM this guard uses.\nnemo_info (Optional[GuardNemoInfo]) – additional configuration for NeMo Guardrails guards.\nmodel_info (Optional[GuardModelInfo]) – additional configuration for guards using a deployed model.\nintervention (Optional[GuardInterventionForTemplate]) – the assessment conditions, and action the guard should take if conditions are met.\nReturns:\ncreated Template\nReturn type:\nModerationTemplate\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nupdate(name=None, description=None, type=None, allowed_stages=None, intervention=None, ootb_type=None, llm_type=None, model_info=None, nemo_info=None)\nUpdate Template. All fields are optional, and omitted fields are left unchanged.\nAdded in version v3.6.\nParameters:\nname (str) – name of the template.\ndescription (str) – description of the template.\ntype (GuardType) – type of the template.\nallowed_stages (List[ModerationGuardStage]) – the stages of moderation this guard is allowed to be used\nootb_type (ModerationGuardOotbType) – for guards of type “ootb”, the specific “Out of the Box” metric type.\nllm_type (Optional[ModerationGuardLlmType]) – the backing LLM this guard uses.\nnemo_info (Optional[GuardNemoInfo]) – additional configuration for NeMo Guardrails guards.\nmodel_info (Optional[GuardModelInfo]) – additional configuration for guards using a deployed model.\nintervention (Optional[GuardInterventionForTemplate]) – the assessment conditions, and action the guard should take if conditions are met.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nNone\nrefresh()\nUpdate Template with the latest data from server.\n:rtype: None\nAdded in version v3.6.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndelete()\nDelete Template.\n:rtype: None\nAdded in version v3.6.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-moderation.html",
      "tags": [
        "advanced",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-moderation.html",
        "content_length": 13576
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.errors.clienterror",
        "datarobot.models.moderation",
        "datarobot.errors.servererror"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-4190972416499874375",
      "title": "DataRobot Models",
      "content": "DataRobot Models\nGeneric models\nclass datarobot.models.GenericModel\nGenericModel [ModelRecord] is the object which is returned from /modelRecords list route.\nContains most generic model information.\nModels\nclass datarobot.models.Model\nA model trained on a project’s dataset capable of making predictions.\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nSee datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nid (str) – ID of the model.\nproject_id (str) – ID of the project the model belongs to.\nprocesses (List[str]) – Processes used by the model.\nfeaturelist_name (str) – Name of the featurelist used by the model.\nfeaturelist_id (str) – ID of the featurelist used by the model.\nsample_pct (float or None) – Percentage of the project dataset used in model training. If the project uses\ndatetime partitioning, the sample_pct will be None.  See training_row_count,\ntraining_duration, and training_start_date / training_end_date instead.\ntraining_row_count (int or None) – Number of rows of the project dataset used in model training.  In a datetime\npartitioned project, if specified, defines the number of rows used to train the model and\nevaluate backtest scores; if unspecified, either training_duration or\ntraining_start_date and training_end_date is used for training_row_count.\ntraining_duration (str or None) – For datetime partitioned projects only. If specified, defines the duration spanned by the data used to train\nthe model and evaluate backtest scores.\ntraining_start_date (datetime or None) – For frozen models in datetime partitioned projects only. If specified, the start\ndate of the data used to train the model.\ntraining_end_date (datetime or None) – For frozen models in datetime partitioned projects only. If specified, the end\ndate of the data used to train the model.\nmodel_type (str) – Type of model, for example ‘Nystroem Kernel SVM Regressor’.\nmodel_category (str) – Category of model, for example ‘prime’ for DataRobot Prime models, ‘blend’ for blender models, and\n‘model’ for other models.\nis_frozen (bool) – Whether this model is a frozen model.\nis_n_clusters_dynamically_determined (bool) – (New in version v2.27) Optional. Whether this model determines the number of clusters dynamically.\nblueprint_id (str) – ID of the blueprint used to build this model.\nmetrics (dict) – Mapping from each metric to the model’s score for that metric.\nmonotonic_increasing_featurelist_id (str) – Optional. ID of the featurelist that defines the set of features with\na monotonically increasing relationship to the target.\nIf None, no such constraints are enforced.\nmonotonic_decreasing_featurelist_id (str) – Optional. ID of the featurelist that defines the set of features with\na monotonically decreasing relationship to the target.\nIf None, no such constraints are enforced.\nn_clusters (int) – (New in version v2.27) Optional. Number of data clusters discovered by model.\nhas_empty_clusters (bool) – (New in version v2.27) Optional. Whether clustering model produces empty clusters.\nsupports_monotonic_constraints (bool) – Optional. Whether this model supports enforcing monotonic constraints.\nis_starred (bool) – Whether this model is marked as a starred model.\nprediction_threshold (float) – Binary classification projects only. Threshold used for predictions.\nprediction_threshold_read_only (bool) – Whether modification of the prediction threshold is forbidden. Threshold\nmodification is forbidden once a model has had a deployment created or predictions made via\nthe dedicated prediction API.\nmodel_number (integer) – Model number assigned to the model.\nparent_model_id (str or None) – (New in version v2.20) ID of the model that tuning parameters are derived from.\nsupports_composable_ml (bool or None) – (New in version v2.26)\nWhether this model is supported Composable ML.\nclassmethod get(project, model_id)\nRetrieve a specific model.\nParameters:\nproject (str) – Project ID.\nmodel_id (str) – ID of the model to retrieve.\nReturns:\nmodel – Queried instance.\nReturn type:\nModel\nRaises:\nValueError – passed project parameter value is of not supported type\nadvanced_tune(params, description=None)\nGenerate a new model with the specified advanced-tuning parameters\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nParameters:\nparams (dict) – Mapping of parameter ID to parameter value.\nThe list of valid parameter IDs for a model can be found by calling\nget_advanced_tuning_parameters().\nThis endpoint does not need to include values for all parameters.  If a parameter\nis omitted, its current_value will be used.\ndescription (str) – Human-readable string describing the newly advanced-tuned model\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ncontinue_incremental_learning_from_incremental_model(chunk_definition_id, early_stopping_rounds=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nchunk_definition_id (str) – The Mongo ID for the chunking service.\nearly_stopping_rounds (Optional[int]) – The number of chunks that, when no improvement has been shown, triggers the early stopping mechanism.\nReturns:\njob – The model retraining job that is created.\nReturn type:\nModelJob\ncross_validate()\nRun cross validation on the model.\nNotes\nTo perform Cross Validation on a new model with new parameters, use train instead.\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ndelete()\nDelete a model from the project’s leaderboard.\nReturn type:\nNone\ndownload_scoring_code(file_name, source_code=False)\nDownload the Scoring Code JAR.\nParameters:\nfile_name (str) – File path where scoring code will be saved.\nsource_code (Optional[bool]) – Set to True to download source code archive.\nIt will not be executable.\nReturn type:\nNone\ndownload_training_artifact(file_name)\nRetrieve trained artifact(s) from a model containing one or more custom tasks.\nArtifact(s) will be downloaded to the specified local filepath.\nParameters:\nfile_name (str) – File path where trained model artifact(s) will be saved.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nOverrides the inherited method since the model must _not_ recursively change casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (list) – List of attribute namespaces like: [‘top.middle.bottom’], that should be kept\neven if their values are None\nget_advanced_tuning_parameters()\nGet the advanced-tuning parameters available for this model.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nA dictionary describing the advanced-tuning parameters for the current model.\nThere are two top-level keys, tuning_description and tuning_parameters.\ntuning_description an optional value. If not None, then it indicates the\nuser-specified description of this set of tuning parameter.\ntuning_parameters is a list of a dicts, each has the following keys\nparameter_name : (str) name of the parameter (unique per task, see below)\nparameter_id : (str) opaque ID string uniquely identifying parameter\ndefault_value : (*) the actual value used to train the model; either\nthe single value of the parameter specified before training, or the best\nvalue from the list of grid-searched values (based on current_value)\ncurrent_value : (*) the single value or list of values of the\nparameter that were grid searched. Depending on the grid search\nspecification, could be a single fixed value (no grid search),\na list of discrete values, or a range.\ntask_name : (str) name of the task that this parameter belongs to\nconstraints: (dict) see the notes below\nvertex_id: (str) ID of vertex that this parameter belongs to\nReturn type:\ndict\nNotes\nThe type of default_value and current_value is defined by the constraints structure.\nIt will be a string or numeric Python type.\nconstraints is a dict with at least one, possibly more, of the following keys.\nThe presence of a key indicates that the parameter may take on the specified type.\n(If a key is absent, this means that the parameter may not take on the specified type.)\nIf a key on constraints is present, its value will be a dict containing\nall of the fields described below for that key.\n\"constraints\": {\n\"select\": {\n\"values\": [<list(basestring or number) : possible values>]\n},\n\"ascii\": {},\n\"unicode\": {},\n\"int\": {\n\"min\": <int : minimum valid value>,\n\"max\": <int : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"float\": {\n\"min\": <float : minimum valid value>,\n\"max\": <float : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"intList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <int : minimum valid value>,\n\"max_val\": <int : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"floatList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <float : minimum valid value>,\n\"max_val\": <float : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n}\n}\nThe keys have meaning as follows:\nselect:\nRather than specifying a specific data type, if present, it indicates that the parameter\nis permitted to take on any of the specified values.  Listed values may be of any string\nor real (non-complex) numeric type.\nascii:\nThe parameter may be a unicode object that encodes simple ASCII characters.\n(A-Z, a-z, 0-9, whitespace, and certain common symbols.)  In addition to listed\nconstraints, ASCII keys currently may not contain either newlines or semicolons.\nunicode:\nThe parameter may be any Python unicode object.\nint:\nThe value may be an object of type int within the specified range (inclusive).\nPlease note that the value will be passed around using the JSON format, and\nsome JSON parsers have undefined behavior with integers outside of the range\n[-(2**53)+1, (2**53)-1].\nfloat:\nThe value may be an object of type float within the specified range (inclusive).\nintList, floatList:\nThe value may be a list of int or float objects, respectively, following constraints\nas specified respectively by the int and float types (above).\nMany parameters only specify one key under constraints.  If a parameter specifies multiple\nkeys, the parameter may take on any value permitted by any key.\nget_all_confusion_charts(fallback_to_parent_insights=False)\nRetrieve a list of all confusion matrices available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent for any source that is not available for this model and if this\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\nReturns:\nData for all available confusion charts for model.\nReturn type:\nlist of ConfusionChart\nget_all_feature_impacts(data_slice_filter=None)\nRetrieve a list of all feature impact results available for the model.\nParameters:\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen no data_slice filtering will be applied when requesting the roc_curve.\nReturns:\nData for all available model feature impacts. Or an empty list if not data found.\nReturn type:\nlist of dicts\nExamples\nmodel = datarobot.Model(id='model-id', project_id='project-id')\n# Get feature impact insights for sliced data\ndata_slice = datarobot.DataSlice(id='data-slice-id')\nsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get feature impact insights for unsliced data\ndata_slice = datarobot.DataSlice()\nunsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get all feature impact insights\nall_fi = model.get_all_feature_impacts()\nget_all_lift_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (Optional[bool]) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned lift chart by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model lift charts. Or an empty list if no data found.\nReturn type:\nlist of LiftChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get lift chart insights for sliced data\nsliced_lift_charts = model.get_all_lift_charts(data_slice_id='data-slice-id')\n# Get lift chart insights for unsliced data\nunsliced_lift_charts = model.get_all_lift_charts(unsliced_only=True)\n# Get all lift chart insights\nall_lift_charts = model.get_all_lift_charts()\nget_all_multiclass_lift_charts(fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nData for all available model lift charts.\nReturn type:\nlist of LiftChart\nget_all_residuals_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all residuals charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent\nfor any source that is not available for this model and if this model has a\ndefined parent model. If omitted or False, or this model has no parent, this will\nnot attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned residuals charts by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model residuals charts.\nReturn type:\nlist of ResidualsChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get residuals chart insights for sliced data\nsliced_residuals_charts = model.get_all_residuals_charts(data_slice_id='data-slice-id')\n# Get residuals chart insights for unsliced data\nunsliced_residuals_charts = model.get_all_residuals_charts(unsliced_only=True)\n# Get all residuals chart insights\nall_residuals_charts = model.get_all_residuals_charts()\nget_all_roc_curves(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all ROC curves available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – filters the returned roc_curve by data_slice_filter.id.  If None (the default) applies no filter based on\ndata_slice_id.\nReturns:\nData for all available model ROC curves. Or an empty list if no RocCurves are found.\nReturn type:\nlist of RocCurve\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\nds_filter=DataSlice(id='data-slice-id')\n# Get roc curve insights for sliced data\nsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get roc curve insights for unsliced data\ndata_slice_filter=DataSlice(id=None)\nunsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get all roc curve insights\nall_roc_curves = model.get_all_roc_curves()\nget_confusion_chart(source, fallback_to_parent_insights=False)\nRetrieve a multiclass model’s confusion matrix for the specified source.\nParameters:\nsource (str) – Confusion chart source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent if the confusion chart is not available for this model and the\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel ConfusionChart data\nReturn type:\nConfusionChart\nRaises:\nClientError – If the insight is not available for this model\nget_cross_class_accuracy_scores()\nRetrieves a list of Cross Class Accuracy scores for the model.\nReturn type:\njson\nget_cross_validation_scores(partition=None, metric=None)\nReturn a dictionary, keyed by metric, showing cross validation\nscores per partition.\nCross Validation should already have been performed using\ncross_validate or\ntrain.\nNotes\nModels that computed cross validation before this feature was added will need\nto be deleted and retrained before this method can be used.\nParameters:\npartition (float) – optional, the id of the partition (1,2,3.0,4.0,etc…) to filter results by\ncan be a whole number positive integer or float value. 0 corresponds to the\nvalidation partition.\nmetric (unicode) – optional name of the metric to filter to resulting cross validation scores by\nReturns:\ncross_validation_scores – A dictionary keyed by metric showing cross validation scores per\npartition.\nReturn type:\ndict\nget_data_disparity_insights(feature, class_name1, class_name2)\nRetrieve a list of Cross Class Data Disparity insights for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\nclass_name1 (str) – One of the compared classes\nclass_name2 (str) – Another compared class\nReturn type:\njson\nget_fairness_insights(fairness_metrics_set=None, offset=0, limit=100)\nRetrieve a list of Per Class Bias insights for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\noffset (Optional[int]) – Number of items to skip.\nlimit (Optional[int]) – Number of items to return.\nReturn type:\njson\nget_feature_effect(source, data_slice_id=None)\nRetrieve Feature Effects for the model.\nFeature Effects provides partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, retrieve unsliced insight data.\nReturns:\nfeature_effects – The feature effects data.\nReturn type:\nFeatureEffects\nRaises:\nClientError – If the feature effects have not been computed or source is not valid value.\nget_feature_effect_metadata()\nRetrieve Feature Effects metadata. Response contains status and available model sources.\nFeature Effect for the training partition is always available, with the exception of older\nprojects that only supported Feature Effect for validation.\nWhen a model is trained into validation or holdout without stacked predictions\n(i.e., no out-of-sample predictions in those partitions),\nFeature Effects is not available for validation or holdout.\nFeature Effects for holdout is not available when holdout was not unlocked for\nthe project.\nUse source to retrieve Feature Effects, selecting one of the provided sources.\nReturns:\nfeature_effect_metadata\nReturn type:\nFeatureEffectMetadata\nget_feature_effects_multiclass(source='training', class_=None)\nRetrieve Feature Effects for the multiclass model.\nFeature Effects provide partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (str) – The source Feature Effects are retrieved for.\nclass (str or None) – The class name Feature Effects are retrieved for.\nReturns:\nThe list of multiclass feature effects.\nReturn type:\nlist\nRaises:\nClientError – If Feature Effects have not been computed or source is not valid value.\nget_feature_impact(with_metadata=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the computed Feature Impact results, a measure of the relevance of each\nfeature in the model.\nFeature Impact is computed for each column by creating new data with that column randomly\npermuted (but the others left unchanged), and seeing how the error metric score for the\npredictions is affected. The ‘impactUnnormalized’ is how much worse the error metric score\nis when making predictions on this modified data. The ‘impactNormalized’ is normalized so\nthat the largest value is 1. In both cases, larger values indicate more important features.\nIf a feature is a redundant feature, i.e. once other features are considered it doesn’t\ncontribute much in addition, the ‘redundantWith’ value is the name of feature that has the\nhighest correlation with this feature. Note that redundancy detection is only available for\njobs run after the addition of this feature. When retrieving data that predates this\nfunctionality, a NoRedundancyImpactAvailable warning will be used.\nElsewhere this technique is sometimes called ‘Permutation Importance’.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nParameters:\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_feature_impact will raise a ValueError.\nReturns:\nThe feature impact data response depends on the with_metadata parameter. The response is\neither a dict with metadata and a list with actual data or just a list with that data.\nEach List item is a dict with the keys featureName, impactNormalized, and\nimpactUnnormalized, redundantWith and count.\nFor dict response available keys are:\nfeatureImpacts - Feature Impact data as a dictionary. Each item is a dict withkeys: featureName, impactNormalized, and impactUnnormalized, and\nredundantWith.\nshapBased - A boolean that indicates whether Feature Impact was calculated usingShapley values.\nranRedundancyDetection - A boolean that indicates whether redundant featureidentification was run while calculating this Feature Impact.\nrowCount - An integer or None that indicates the number of rows that was used tocalculate Feature Impact. For the Feature Impact calculated with the default\nlogic, without specifying the rowCount, we return None here.\ncount - An integer with the number of features under the featureImpacts.\nReturn type:\nlist or dict\nRaises:\nClientError – If the feature impacts have not been computed.\nValueError – If data_slice_filter passed as None\nget_features_used()\nQuery the server to determine which features were used.\nNote that the data returned by this method is possibly different\nthan the names of the features in the featurelist used by this model.\nThis method will return the raw features that must be supplied in order\nfor predictions to be generated on a new set of data. The featurelist,\nin contrast, would also include the names of derived features.\nReturns:\nfeatures – The names of the features used in the model.\nReturn type:\nList[str]\nget_frozen_child_models()\nRetrieve the IDs for all models that are frozen from this model.\nReturn type:\nA list of Models\nget_labelwise_roc_curves(source, fallback_to_parent_insights=False)\nRetrieve a list of LabelwiseRocCurve instances for a multilabel model for the given source and all labels.\nThis method is valid only for multilabel projects. For binary projects, use Model.get_roc_curve API .\nAdded in version v2.24.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\nReturns:\nLabelwise ROC Curve instances for source and all labels\nReturn type:\nlist of LabelwiseRocCurve\nRaises:\nClientError – If the insight is not available for this model\nget_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\nReturns:\nModel lift chart data\nReturn type:\nLiftChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_missing_report_info()\nRetrieve a report on missing training data that can be used to understand missing\nvalues treatment in the model. The report consists of missing values resolutions for\nfeatures numeric or categorical features that were part of building the model.\nReturns:\nThe queried model missing report, sorted by missing count (DESCENDING order).\nReturn type:\nAn iterable of MissingReportPerFeature\nget_model_blueprint_chart()\nRetrieve a diagram that can be used to understand\ndata flow in the blueprint.\nReturns:\nThe queried model blueprint chart.\nReturn type:\nModelBlueprintChart\nget_model_blueprint_documents()\nGet documentation for tasks used in this model.\nReturns:\nAll documents available for the model.\nReturn type:\nlist of BlueprintTaskDocument\nget_model_blueprint_json()\nGet the blueprint json representation used by this model.\nReturns:\nJson representation of the blueprint stages.\nReturn type:\nBlueprintJson\nget_multiclass_feature_impact()\nFor multiclass it’s possible to calculate feature impact separately for each target class.\nThe method for calculation is exactly the same, calculated in one-vs-all style for each\ntarget class.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. Each item is a dict with the keys ‘featureImpacts’ (list),\n‘class’ (str). Each item in ‘featureImpacts’ is a dict with the keys ‘featureName’,\n‘impactNormalized’, and ‘impactUnnormalized’, and ‘redundantWith’.\nReturn type:\nlist of dict\nRaises:\nClientError – If the multiclass feature impacts have not been computed.\nget_multiclass_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_multilabel_lift_charts(source, fallback_to_parent_insights=False)\nRetrieve model Lift charts for the specified source.\nAdded in version v2.24.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_num_iterations_trained()\nRetrieves the number of estimators trained by early-stopping tree-based models.\n– versionadded:: v2.22\nReturns:\nprojectId (str) – id of project containing the model\nmodelId (str) – id of the model\ndata (array) – list of numEstimatorsItem objects, one for each modeling stage.\nnumEstimatorsItem will be of the form\nstage (str) – indicates the modeling stage (for multi-stage models); None of single-stage models\nnumIterations (int) – the number of estimators or iterations trained by the model\nget_or_request_feature_effect(source, max_wait=600, row_count=None, data_slice_id=None)\nRetrieve Feature Effects for the model, requesting a new job if it hasn’t been run previously.\nSee get_feature_effect_metadata\nfor retrieving information of source.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effect job to complete before erroring.\nrow_count (Optional[int]) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nfeature_effects – The Feature Effects data.\nReturn type:\nFeatureEffects\nget_or_request_feature_effects_multiclass(source, top_n_features=None, features=None, row_count=None, class_=None, max_wait=600)\nRetrieve Feature Effects for the multiclass model, requesting a job if it hasn’t been run\npreviously.\nParameters:\nsource (string) – The source Feature Effects retrieve for.\nclass (str or None) – The class name Feature Effects retrieve for.\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by Feature Impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effects job to complete before\nerroring.\nReturns:\nfeature_effects – The list of multiclass feature effects data.\nReturn type:\nlist of FeatureEffectsMulticlass\nget_or_request_feature_impact(max_wait=600, **kwargs)\nRetrieve feature impact for the model, requesting a job if it hasn’t been run previously\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for a requested feature impact job to complete before erroring\n**kwargs – Arbitrary keyword arguments passed to\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. See\nget_feature_impact for the exact\nschema.\nReturn type:\nlist or dict\nget_parameters()\nRetrieve model parameters.\nReturns:\nModel parameters for this model.\nReturn type:\nModelParameters\nget_pareto_front()\nRetrieve the Pareto Front for a Eureqa model.\nThis method is only supported for Eureqa models.\nReturns:\nModel ParetoFront data\nReturn type:\nParetoFront\nget_prime_eligibility()\nCheck if this model can be approximated with DataRobot Prime\nReturns:\nprime_eligibility – a dict indicating whether a model can be approximated with DataRobot Prime\n(key can_make_prime) and why it may be ineligible (key message)\nReturn type:\ndict\nget_residuals_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible\nvalues.\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent if\nthe residuals chart is not available for this model and the model has a defined parent\nmodel. If omitted or False, or there is no parent model, will not attempt to return\nresiduals data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_residuals_chart will raise a ValueError.\nReturns:\nModel residuals chart data\nReturn type:\nResidualsChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_roc_curve(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the ROC curve for a binary model for the specified source.\nThis method is valid only for binary projects. For multilabel projects, use\nModel.get_labelwise_roc_curves.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_roc_curve will raise a ValueError.\nReturns:\nModel ROC curve data\nReturn type:\nRocCurve\nRaises:\nClientError – If the insight is not available for this model\n(New in version v3.0) TypeError – If the underlying project type is multilabel\nValueError – If data_slice_filter passed as None\nget_rulesets()\nList the rulesets approximating this model generated by DataRobot Prime\nIf this model hasn’t been approximated yet, will return an empty list.  Note that these\nare rulesets approximating this model, not rulesets used to construct this model.\nReturns:\nrulesets\nReturn type:\nlist of Ruleset\nget_supported_capabilities()\nRetrieves a summary of the capabilities supported by a model.\nAdded in version v2.14.\nReturns:\nsupportsBlending (bool) – whether the model supports blending\nsupportsMonotonicConstraints (bool) – whether the model supports monotonic constraints\nhasWordCloud (bool) – whether the model has word cloud data available\neligibleForPrime (bool) – (Deprecated in version v3.6)\nwhether the model is eligible for Prime\nhasParameters (bool) – whether the model has parameters that can be retrieved\nsupportsCodeGeneration (bool) – (New in version v2.18) whether the model supports code generation\nsupportsShap (bool) –\n(New in version v2.18) True if the model supports Shapley package. i.e. Shapley basedfeature Importance\nsupportsEarlyStopping (bool) – (New in version v2.22) True if this is an early stopping\ntree-based model and number of trained iterations can be retrieved.\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this model at leaderboard.\nReturn type:\nstr\nget_word_cloud(exclude_stop_words=False)\nRetrieve word cloud data for the model.\nParameters:\nexclude_stop_words (Optional[bool]) – Set to True if you want stopwords filtered out of response.\nReturns:\nWord cloud data for the model.\nReturn type:\nWordCloud\nincremental_train(data_stage_id, training_data_name=None)\nSubmit a job to the queue to perform incremental training on an existing model.\nSee train_incremental documentation.\nReturn type:\nModelJob\nclassmethod list(project_id, sort_by_partition='validation', sort_by_metric=None, with_metric=None, search_term=None, featurelists=None, families=None, blueprints=None, labels=None, characteristics=None, training_filters=None, number_of_clusters=None, limit=100, offset=0)\nRetrieve paginated model records, sorted by scores, with optional filtering.\nParameters:\nsort_by_partition (str, one of validation, backtesting, crossValidation or holdout) – Set the partition to use for sorted (by score) list of models. validation is the default.\nsort_by_metric (str) – Set the project metric to use for model sorting. DataRobot-selected project optimization metric\nis the default.\nwith_metric (str) – For a single-metric list of results, specify that project metric.\nsearch_term (str) – If specified, only models containing the term in their name or processes are returned.\nfeaturelists (List[str]) – If specified, only models trained on selected featurelists are returned.\nfamilies (List[str]) – If specified, only models belonging to selected families are returned.\nblueprints (List[str]) – If specified, only models trained on specified blueprint IDs are returned.\nlabels (List[str], starred or prepared for deployment) – If specified, only models tagged with all listed labels are returned.\ncharacteristics (List[str]) – If specified, only models matching all listed characteristics are returned.\ntraining_filters (List[str]) – If specified, only models matching at least one of the listed training conditions are returned.\nThe following formats are supported for autoML and datetime partitioned projects:\n- number of rows in training subset\nFor datetime partitioned projects:\n- <training duration>, example P6Y0M0D\n- <training_duration>-<time_window_sample_percent>-<sampling_method> Example: P6Y0M0D-78-Random,\n(returns models trained on 6 years of data, sampling rate 78%, random sampling).\n- Start/end date\n- Project settings\nnumber_of_clusters (list of int) – Filter models by number of clusters. Applicable only in unsupervised clustering projects.\nlimit (int)\noffset (int)\nReturns:\ngeneric_models\nReturn type:\nlist of GenericModel\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nrequest_approximation()\nRequest an approximation of this model using DataRobot Prime\nThis will create several rulesets that could be used to approximate this model.  After\ncomparing their scores and rule counts, the code used in the approximation can be downloaded\nand run locally.\nReturns:\njob – the job generating the rulesets\nReturn type:\nJob\nrequest_cross_class_accuracy_scores()\nRequest data disparity insights to be computed for the model.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_data_disparity_insights(feature, compared_class_names)\nRequest data disparity insights to be computed for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\ncompared_class_names (list(str)) – List of two classes to compare\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_external_test(dataset_id, actual_value_column=None)\nRequest external test to compute scores and insights on an external test dataset\nParameters:\ndataset_id (string) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nReturns:\njob – a Job representing external dataset insights computation\nReturn type:\nJob\nrequest_fairness_insights(fairness_metrics_set=None)\nRequest fairness insights to be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_feature_effect(row_count=None, data_slice_id=None)\nSubmit request to compute Feature Effects for the model.\nSee get_feature_effect for more\ninformation on the result of the job.\nParameters:\nrow_count (int) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – A Job representing the feature effect computation. To get the completed feature effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nRaises:\nJobAlreadyRequested – If the feature effect have already been requested.\nrequest_feature_effects_multiclass(row_count=None, top_n_features=None, features=None)\nRequest Feature Effects computation for the multiclass model.\nSee get_feature_effect for\nmore information on the result of the job.\nParameters:\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by feature impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nReturns:\njob – A Job representing Feature Effect computation. To get the completed Feature Effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nrequest_feature_impact(row_count=None, with_metadata=False, data_slice_id=None)\nRequest feature impacts to be computed for the model.\nSee get_feature_impact for more\ninformation on the result of the job.\nParameters:\nrow_count (Optional[int]) – The sample size (specified in rows) to use for Feature Impact computation. This is not\nsupported for unsupervised, multiclass (which has a separate method), and time series\nprojects.\nwith_metadata (Optional[bool]) – Flag indicating whether the result should include the metadata.\nIf true, metadata is included.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – Job representing the Feature Impact computation. To retrieve the completed Feature Impact\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob or status_id\nRaises:\nJobAlreadyRequested – If the feature impacts have already been requested.\nrequest_frozen_datetime_model(training_row_count=None, training_duration=None, training_start_date=None, training_end_date=None, time_window_sample_pct=None, sampling_method=None)\nTrain a new frozen model with parameters from this model.\nRequires that this model belongs to a datetime partitioned project.  If it does not, an\nerror will occur when submitting the job.\nFrozen models use the same tuning parameters as their parent model instead of independently\noptimizing them to allow efficiently retraining models on larger amounts of the training\ndata.\nIn addition of training_row_count and training_duration, frozen datetime models may be\ntrained on an exact date range.  Only one of training_row_count, training_duration, or\ntraining_start_date and training_end_date should be specified.\nModels specified using training_start_date and training_end_date are the only ones that can\nbe trained into the holdout data (once the holdout is unlocked).\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\ntraining_duration may not be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, training_row_count may not be specified.\ntraining_start_date (datetime.datetime, optional) – the start date of the data to train to model on.  Only rows occurring at or after\nthis datetime will be used.  If training_start_date is specified, training_end_date\nmust also be specified.\ntraining_end_date (datetime.datetime, optional) – the end date of the data to train the model on.  Only rows occurring strictly before\nthis datetime will be used.  If training_end_date is specified, training_start_date\nmust also be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates).  An integer between 1 and 99 indicating the percentage to sample by\nwithin the window.  The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nReturns:\nmodel_job – the modeling job training a frozen model\nReturn type:\nModelJob\nrequest_frozen_model(sample_pct=None, training_row_count=None)\nTrain a new frozen model with parameters from this model\nNotes\nThis method only works if project the model belongs to is not datetime\npartitioned.  If it is, use request_frozen_datetime_model instead.\nFrozen models use the same tuning parameters as their parent model instead of independently\noptimizing them to allow efficiently retraining models on larger amounts of the training\ndata.\nParameters:\nsample_pct (float) – optional, the percentage of the dataset to use with the model.  If not provided, will\nuse the value from this model.\ntraining_row_count (int) – (New in version v2.9) optional, the integer number of rows of the dataset to use with\nthe model. Only one of sample_pct and training_row_count should be specified.\nReturns:\nmodel_job – the modeling job training a frozen model\nReturn type:\nModelJob\nrequest_lift_chart(source, data_slice_id=None)\nRequest the model Lift Chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_per_class_fairness_insights(fairness_metrics_set=None)\nRequest per-class fairness insights be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – The fairness metric used to calculate the fairness scores.\nValue can be any one of <datarobot.enums.FairnessMetricsSet>.\nReturns:\nstatus_check_job – The returned object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_predictions(dataset_id=None, dataset=None, dataframe=None, file_path=None, file=None, include_prediction_intervals=None, prediction_intervals_size=None, forecast_point=None, predictions_start_date=None, predictions_end_date=None, actual_value_column=None, explanation_algorithm=None, max_explanations=None, max_ngram_explanations=None)\nRequests predictions against a previously uploaded dataset.\nParameters:\ndataset_id (string, optional) – The ID of the dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataset (Dataset, optional) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataframe (pd.DataFrame, optional) – (New in v3.0)\nThe dataframe to make predictions against\nfile_path (Optional[str]) – (New in v3.0)\nPath to file to make predictions against\nfile (IOBase, optional) – (New in v3.0)\nFile to make predictions against\ninclude_prediction_intervals (Optional[bool]) – (New in v2.16) For time series projects only.\nSpecifies whether prediction intervals should be calculated for this request. Defaults\nto True if prediction_intervals_size is specified, otherwise defaults to False.\nprediction_intervals_size (Optional[int]) – (New in v2.16) For time series projects only.\nRepresents the percentile to use for the size of the prediction intervals. Defaults to\n80 if include_prediction_intervals is True. Prediction intervals size must be\nbetween 1 and 100 (inclusive).\nforecast_point (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. This is the default point relative\nto which predictions will be generated, based on the forecast window of the project. See\nthe time series prediction documentation for more\ninformation.\npredictions_start_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The end date for bulk\npredictions, exclusive. Note that this parameter is for generating historical\npredictions using the training data. This parameter should be provided in conjunction\nwith predictions_start_date. Can’t be provided with the\nforecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nexplanation_algorithm ((New in version v2.21) optional; If set to 'shap', the) – response will include prediction explanations based on the SHAP explainer (SHapley\nAdditive exPlanations). Defaults to null (no prediction explanations).\nmax_explanations ((New in version v2.21) int optional; specifies the maximum number of) – explanation values that should be returned for each row, ordered by absolute value,\ngreatest to least. If null, no limit. In the case of ‘shap’: if the number of features\nis greater than the limit, the sum of remaining values will also be returned as\nshapRemainingTotal. Defaults to null. Cannot be set if explanation_algorithm is\nomitted.\nmax_ngram_explanations (optional;  int or str) – (New in version v2.29) Specifies the maximum number of text explanation values that\nshould be returned. If set to all, text explanations will be computed and all the\nngram explanations will be returned. If set to a non zero positive integer value, text\nexplanations will be computed and this amount of descendingly sorted ngram explanations\nwill be returned. By default text explanation won’t be triggered to be computed.\nReturns:\njob – The job computing the predictions\nReturn type:\nPredictJob\nrequest_residuals_chart(source, data_slice_id=None)\nRequest the model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_roc_curve(source, data_slice_id=None)\nRequest the model Roc Curve for the specified source.\nParameters:\nsource (str) – Roc Curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_training_predictions(data_subset, explanation_algorithm=None, max_explanations=None)\nStart a job to build training predictions\nParameters:\ndata_subset (str) – data set definition to build predictions on.\nChoices are:\ndr.enums.DATA_SUBSET.ALL or string all for all data available. Not valid formodels in datetime partitioned projects\ndr.enums.DATA_SUBSET.VALIDATION_AND_HOLDOUT or string validationAndHoldout forall data except training set. Not valid for models in datetime partitioned\nprojects\ndr.enums.DATA_SUBSET.HOLDOUT or string holdout for holdout data set only\ndr.enums.DATA_SUBSET.ALL_BACKTESTS or string allBacktests for downloadingthe predictions for all backtest validation folds. Requires the model to have\nsuccessfully scored all backtests. Datetime partitioned projects only.\nexplanation_algorithm (dr.enums.EXPLANATIONS_ALGORITHM) – (New in v2.21) Optional. If set to dr.enums.EXPLANATIONS_ALGORITHM.SHAP, the response\nwill include prediction explanations based on the SHAP explainer (SHapley Additive\nexPlanations). Defaults to None (no prediction explanations).\nmax_explanations (int) – (New in v2.21) Optional. Specifies the maximum number of explanation values that should\nbe returned for each row, ordered by absolute value, greatest to least. In the case of\ndr.enums.EXPLANATIONS_ALGORITHM.SHAP:  If not set, explanations are returned for all\nfeatures. If the number of features is greater than the max_explanations, the sum of\nremaining values will also be returned as shap_remaining_total. Max 100. Defaults to\nnull for datasets narrower than 100 columns, defaults to 100 for datasets wider than 100\ncolumns. Is ignored if explanation_algorithm is not set.\nReturns:\nan instance of created async job\nReturn type:\nJob\nretrain(sample_pct=None, featurelist_id=None, training_row_count=None, n_clusters=None)\nSubmit a job to the queue to train a blender model.\nParameters:\nsample_pct (Optional[float]) – The sample size in percents (1 to 100) to use in training. If this parameter is used\nthen training_row_count should not be given.\nfeaturelist_id (Optional[str]) – The featurelist id\ntraining_row_count (Optional[int]) – The number of rows used to train the model. If this parameter is used, then sample_pct\nshould not be given.\nn_clusters (Optional[int]) – (new in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that do not determine\nthe number of clusters automatically.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nset_prediction_threshold(threshold)\nSet a custom prediction threshold for the model.\nMay not be used once prediction_threshold_read_only is True for this model.\nParameters:\nthreshold (float) – only used for binary classification projects. The threshold to when deciding between\nthe positive and negative classes when making predictions.  Should be between 0.0 and\n1.0 (inclusive).\nstar_model()\nMark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nstart_advanced_tuning_session()\nStart an Advanced Tuning session.  Returns an object that helps\nset up arguments for an Advanced Tuning model execution.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nSession for setting up and running Advanced Tuning on a model\nReturn type:\nAdvancedTuningSession\nstart_incremental_learning_from_sample(early_stopping_rounds=None, first_iteration_only=False, chunk_definition_id=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nearly_stopping_rounds (Optional[int]) – The number of chunks in which no improvement is observed that triggers the early stopping mechanism.\nfirst_iteration_only (bool) – Specifies whether incremental learning training should be limited to the first\niteration. If set to True, the training process will be performed only for the first\niteration. If set to False, training will continue until early stopping conditions\nare met or the maximum number of iterations is reached. The default value is False.\nchunk_definition_id (str) – The id of the chunk definition to be use for incremental training.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\ntrain(sample_pct=None, featurelist_id=None, scoring_type=None, training_row_count=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>)\nTrain the blueprint used in model on a particular featurelist or amount of data.\nThis method creates a new training job for worker and appends it to\nthe end of the queue for this project.\nAfter the job has finished you can get the newly trained model by retrieving\nit from the project leaderboard, or by retrieving the result of the job.\nEither sample_pct or training_row_count can be used to specify the amount of data to\nuse, but not both.  If neither are specified, a default of the maximum amount of data that\ncan safely be used to train any blueprint without going into the validation data will be\nselected.\nIn smart-sampled projects, sample_pct and training_row_count are assumed to be in terms\nof rows of the minority class.\nNotes\nFor datetime partitioned projects, see train_datetime instead.\nParameters:\nsample_pct (Optional[float]) – The amount of data to use for training, as a percentage of the project dataset from\n0 to 100.\nfeaturelist_id (Optional[str]) – The identifier of the featurelist to use. If not defined, the\nfeaturelist of this model is used.\nscoring_type (Optional[str]) – Either validation or crossValidation (also dr.SCORING_TYPE.validation\nor dr.SCORING_TYPE.cross_validation). validation is available for every\npartitioning type, and indicates that the default model validation should be\nused for the project.\nIf the project uses a form of cross-validation partitioning,\ncrossValidation can also be used to indicate\nthat all of the available training/validation combinations\nshould be used to evaluate the model.\ntraining_row_count (Optional[int]) – The number of rows to use to train the requested model.\nmonotonic_increasing_featurelist_id (str) – (new in version 2.11) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (str) – (new in version 2.11) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nReturns:\nmodel_job_id – id of created job, can be used as parameter to ModelJob.get\nmethod or wait_for_async_model_creation function\nReturn type:\nstr\nExamples\nproject = Project.get('project-id')\nmodel = Model.get('project-id', 'model-id')\nmodel_job_id = model.train(training_row_count=project.max_train_rows)\ntrain_datetime(featurelist_id=None, training_row_count=None, training_duration=None, time_window_sample_pct=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>, use_project_settings=False, sampling_method=None, n_clusters=None)\nTrains this model on a different featurelist or sample size.\nRequires that this model is part of a datetime partitioned project; otherwise, an error will\noccur.\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\nfeaturelist_id (Optional[str]) – the featurelist to use to train the model.  If not specified, the featurelist of this\nmodel is used.\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\nneither training_duration nor use_project_settings may be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, neither training_row_count nor use_project_settings may be\nspecified.\nuse_project_settings (Optional[bool]) – (New in version v2.20) defaults to False. If True, indicates that the custom\nbacktest partitioning settings specified by the user will be used to train the model and\nevaluate backtest scores. If specified, neither training_row_count nor\ntraining_duration may be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates). An integer between 1 and 99 indicating the percentage to sample by\nwithin the window. The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nmonotonic_increasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nn_clusters (Optional[int]) – (New in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that don’t automatically\ndetermine the number of clusters.\nReturns:\njob – the created job to build the model\nReturn type:\nModelJob\ntrain_incremental(data_stage_id, training_data_name=None, data_stage_encoding=None, data_stage_delimiter=None, data_stage_compression=None)\nSubmit a job to the queue to perform incremental training on an existing model using\nadditional data. The id of the additional data to use for training is specified with the data_stage_id.\nOptionally a name for the iteration can be supplied by the user to help identify the contents of data in\nthe iteration.\nThis functionality requires the INCREMENTAL_LEARNING feature flag to be enabled.\nParameters:\ndata_stage_id (str) – The id of the data stage to use for training.\ntraining_data_name (Optional[str]) – The name of the iteration or data stage to indicate what the incremental learning was performed on.\ndata_stage_encoding (Optional[str]) – The encoding type of the data in the data stage (default: UTF-8).\nSupported formats: UTF-8, ASCII, WINDOWS1252\ndata_stage_encoding – The delimiter used by the data in the data stage (default: ‘,’).\ndata_stage_compression (Optional[str]) – The compression type of the data stage file, e.g. ‘zip’ (default: None).\nSupported formats: zip\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nunstar_model()\nUnmark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nclass datarobot.models.model.AdvancedTuningParamsType\nclass datarobot.models.model.BiasMitigationFeatureInfo\nPrime models\nclass datarobot.models.PrimeModel\nRepresents a DataRobot Prime model approximating a parent model with downloadable code.\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nid (str) – the id of the model\nproject_id (str) – the id of the project the model belongs to\nprocesses (List[str]) – the processes used by the model\nfeaturelist_name (str) – the name of the featurelist used by the model\nfeaturelist_id (str) – the id of the featurelist used by the model\nsample_pct (float) – the percentage of the project dataset used in training the model\ntraining_row_count (int or None) – the number of rows of the project dataset used in training the model.  In a datetime\npartitioned project, if specified, defines the number of rows used to train the model and\nevaluate backtest scores; if unspecified, either training_duration or\ntraining_start_date and training_end_date was used to determine that instead.\ntraining_duration (str or None) – only present for models in datetime partitioned projects.  If specified, a duration string\nspecifying the duration spanned by the data used to train the model and evaluate backtest\nscores.\ntraining_start_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the start\ndate of the data used to train the model.\ntraining_end_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the end\ndate of the data used to train the model.\nmodel_type (str) – what model this is, e.g. ‘DataRobot Prime’\nmodel_category (str) – what kind of model this is - always ‘prime’ for DataRobot Prime models\nis_frozen (bool) – whether this model is a frozen model\nblueprint_id (str) – the id of the blueprint used in this model\nmetrics (dict) – a mapping from each metric to the model’s scores for that metric\nruleset (Ruleset) – the ruleset used in the Prime model\nparent_model_id (str) – the id of the model that this Prime model approximates\nmonotonic_increasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically increasing relationship to the target.\nIf None, no such constraints are enforced.\nmonotonic_decreasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically decreasing relationship to the target.\nIf None, no such constraints are enforced.\nsupports_monotonic_constraints (bool) – optional, whether this model supports enforcing monotonic constraints\nis_starred (bool) – whether this model is marked as starred\nprediction_threshold (float) – for binary classification projects, the threshold used for predictions\nprediction_threshold_read_only (bool) – indicated whether modification of the prediction threshold is forbidden. Threshold\nmodification is forbidden once a model has had a deployment created or predictions made via\nthe dedicated prediction API.\nsupports_composable_ml (bool or None) – (New in version v2.26)\nwhether this model is supported in the Composable ML.\nclassmethod get(project_id, model_id)\nRetrieve a specific prime model.\nParameters:\nproject_id (str) – The id of the project the prime model belongs to\nmodel_id (str) – The model_id of the prime model to retrieve.\nReturns:\nmodel – The queried instance.\nReturn type:\nPrimeModel\nrequest_download_validation(language)\nPrep and validate the downloadable code for the ruleset associated with this model.\nParameters:\nlanguage (str) – the language the code should be downloaded in - see datarobot.enums.PRIME_LANGUAGE\nfor available languages\nReturns:\njob – A job tracking the code preparation and validation\nReturn type:\nJob\nadvanced_tune(params, description=None)\nGenerate a new model with the specified advanced-tuning parameters\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nParameters:\nparams (dict) – Mapping of parameter ID to parameter value.\nThe list of valid parameter IDs for a model can be found by calling\nget_advanced_tuning_parameters().\nThis endpoint does not need to include values for all parameters.  If a parameter\nis omitted, its current_value will be used.\ndescription (str) – Human-readable string describing the newly advanced-tuned model\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ncontinue_incremental_learning_from_incremental_model(chunk_definition_id, early_stopping_rounds=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nchunk_definition_id (str) – The Mongo ID for the chunking service.\nearly_stopping_rounds (Optional[int]) – The number of chunks that, when no improvement has been shown, triggers the early stopping mechanism.\nReturns:\njob – The model retraining job that is created.\nReturn type:\nModelJob\ncross_validate()\nRun cross validation on the model.\nNotes\nTo perform Cross Validation on a new model with new parameters, use train instead.\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ndelete()\nDelete a model from the project’s leaderboard.\nReturn type:\nNone\ndownload_scoring_code(file_name, source_code=False)\nDownload the Scoring Code JAR.\nParameters:\nfile_name (str) – File path where scoring code will be saved.\nsource_code (Optional[bool]) – Set to True to download source code archive.\nIt will not be executable.\nReturn type:\nNone\ndownload_training_artifact(file_name)\nRetrieve trained artifact(s) from a model containing one or more custom tasks.\nArtifact(s) will be downloaded to the specified local filepath.\nParameters:\nfile_name (str) – File path where trained model artifact(s) will be saved.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nget_advanced_tuning_parameters()\nGet the advanced-tuning parameters available for this model.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nA dictionary describing the advanced-tuning parameters for the current model.\nThere are two top-level keys, tuning_description and tuning_parameters.\ntuning_description an optional value. If not None, then it indicates the\nuser-specified description of this set of tuning parameter.\ntuning_parameters is a list of a dicts, each has the following keys\nparameter_name : (str) name of the parameter (unique per task, see below)\nparameter_id : (str) opaque ID string uniquely identifying parameter\ndefault_value : (*) the actual value used to train the model; either\nthe single value of the parameter specified before training, or the best\nvalue from the list of grid-searched values (based on current_value)\ncurrent_value : (*) the single value or list of values of the\nparameter that were grid searched. Depending on the grid search\nspecification, could be a single fixed value (no grid search),\na list of discrete values, or a range.\ntask_name : (str) name of the task that this parameter belongs to\nconstraints: (dict) see the notes below\nvertex_id: (str) ID of vertex that this parameter belongs to\nReturn type:\ndict\nNotes\nThe type of default_value and current_value is defined by the constraints structure.\nIt will be a string or numeric Python type.\nconstraints is a dict with at least one, possibly more, of the following keys.\nThe presence of a key indicates that the parameter may take on the specified type.\n(If a key is absent, this means that the parameter may not take on the specified type.)\nIf a key on constraints is present, its value will be a dict containing\nall of the fields described below for that key.\n\"constraints\": {\n\"select\": {\n\"values\": [<list(basestring or number) : possible values>]\n},\n\"ascii\": {},\n\"unicode\": {},\n\"int\": {\n\"min\": <int : minimum valid value>,\n\"max\": <int : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"float\": {\n\"min\": <float : minimum valid value>,\n\"max\": <float : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"intList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <int : minimum valid value>,\n\"max_val\": <int : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"floatList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <float : minimum valid value>,\n\"max_val\": <float : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n}\n}\nThe keys have meaning as follows:\nselect:\nRather than specifying a specific data type, if present, it indicates that the parameter\nis permitted to take on any of the specified values.  Listed values may be of any string\nor real (non-complex) numeric type.\nascii:\nThe parameter may be a unicode object that encodes simple ASCII characters.\n(A-Z, a-z, 0-9, whitespace, and certain common symbols.)  In addition to listed\nconstraints, ASCII keys currently may not contain either newlines or semicolons.\nunicode:\nThe parameter may be any Python unicode object.\nint:\nThe value may be an object of type int within the specified range (inclusive).\nPlease note that the value will be passed around using the JSON format, and\nsome JSON parsers have undefined behavior with integers outside of the range\n[-(2**53)+1, (2**53)-1].\nfloat:\nThe value may be an object of type float within the specified range (inclusive).\nintList, floatList:\nThe value may be a list of int or float objects, respectively, following constraints\nas specified respectively by the int and float types (above).\nMany parameters only specify one key under constraints.  If a parameter specifies multiple\nkeys, the parameter may take on any value permitted by any key.\nget_all_confusion_charts(fallback_to_parent_insights=False)\nRetrieve a list of all confusion matrices available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent for any source that is not available for this model and if this\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\nReturns:\nData for all available confusion charts for model.\nReturn type:\nlist of ConfusionChart\nget_all_feature_impacts(data_slice_filter=None)\nRetrieve a list of all feature impact results available for the model.\nParameters:\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen no data_slice filtering will be applied when requesting the roc_curve.\nReturns:\nData for all available model feature impacts. Or an empty list if not data found.\nReturn type:\nlist of dicts\nExamples\nmodel = datarobot.Model(id='model-id', project_id='project-id')\n# Get feature impact insights for sliced data\ndata_slice = datarobot.DataSlice(id='data-slice-id')\nsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get feature impact insights for unsliced data\ndata_slice = datarobot.DataSlice()\nunsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get all feature impact insights\nall_fi = model.get_all_feature_impacts()\nget_all_lift_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (Optional[bool]) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned lift chart by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model lift charts. Or an empty list if no data found.\nReturn type:\nlist of LiftChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get lift chart insights for sliced data\nsliced_lift_charts = model.get_all_lift_charts(data_slice_id='data-slice-id')\n# Get lift chart insights for unsliced data\nunsliced_lift_charts = model.get_all_lift_charts(unsliced_only=True)\n# Get all lift chart insights\nall_lift_charts = model.get_all_lift_charts()\nget_all_multiclass_lift_charts(fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nData for all available model lift charts.\nReturn type:\nlist of LiftChart\nget_all_residuals_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all residuals charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent\nfor any source that is not available for this model and if this model has a\ndefined parent model. If omitted or False, or this model has no parent, this will\nnot attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned residuals charts by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model residuals charts.\nReturn type:\nlist of ResidualsChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get residuals chart insights for sliced data\nsliced_residuals_charts = model.get_all_residuals_charts(data_slice_id='data-slice-id')\n# Get residuals chart insights for unsliced data\nunsliced_residuals_charts = model.get_all_residuals_charts(unsliced_only=True)\n# Get all residuals chart insights\nall_residuals_charts = model.get_all_residuals_charts()\nget_all_roc_curves(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all ROC curves available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – filters the returned roc_curve by data_slice_filter.id.  If None (the default) applies no filter based on\ndata_slice_id.\nReturns:\nData for all available model ROC curves. Or an empty list if no RocCurves are found.\nReturn type:\nlist of RocCurve\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\nds_filter=DataSlice(id='data-slice-id')\n# Get roc curve insights for sliced data\nsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get roc curve insights for unsliced data\ndata_slice_filter=DataSlice(id=None)\nunsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get all roc curve insights\nall_roc_curves = model.get_all_roc_curves()\nget_confusion_chart(source, fallback_to_parent_insights=False)\nRetrieve a multiclass model’s confusion matrix for the specified source.\nParameters:\nsource (str) – Confusion chart source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent if the confusion chart is not available for this model and the\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel ConfusionChart data\nReturn type:\nConfusionChart\nRaises:\nClientError – If the insight is not available for this model\nget_cross_class_accuracy_scores()\nRetrieves a list of Cross Class Accuracy scores for the model.\nReturn type:\njson\nget_cross_validation_scores(partition=None, metric=None)\nReturn a dictionary, keyed by metric, showing cross validation\nscores per partition.\nCross Validation should already have been performed using\ncross_validate or\ntrain.\nNotes\nModels that computed cross validation before this feature was added will need\nto be deleted and retrained before this method can be used.\nParameters:\npartition (float) – optional, the id of the partition (1,2,3.0,4.0,etc…) to filter results by\ncan be a whole number positive integer or float value. 0 corresponds to the\nvalidation partition.\nmetric (unicode) – optional name of the metric to filter to resulting cross validation scores by\nReturns:\ncross_validation_scores – A dictionary keyed by metric showing cross validation scores per\npartition.\nReturn type:\ndict\nget_data_disparity_insights(feature, class_name1, class_name2)\nRetrieve a list of Cross Class Data Disparity insights for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\nclass_name1 (str) – One of the compared classes\nclass_name2 (str) – Another compared class\nReturn type:\njson\nget_fairness_insights(fairness_metrics_set=None, offset=0, limit=100)\nRetrieve a list of Per Class Bias insights for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\noffset (Optional[int]) – Number of items to skip.\nlimit (Optional[int]) – Number of items to return.\nReturn type:\njson\nget_feature_effect(source, data_slice_id=None)\nRetrieve Feature Effects for the model.\nFeature Effects provides partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, retrieve unsliced insight data.\nReturns:\nfeature_effects – The feature effects data.\nReturn type:\nFeatureEffects\nRaises:\nClientError – If the feature effects have not been computed or source is not valid value.\nget_feature_effect_metadata()\nRetrieve Feature Effects metadata. Response contains status and available model sources.\nFeature Effect for the training partition is always available, with the exception of older\nprojects that only supported Feature Effect for validation.\nWhen a model is trained into validation or holdout without stacked predictions\n(i.e., no out-of-sample predictions in those partitions),\nFeature Effects is not available for validation or holdout.\nFeature Effects for holdout is not available when holdout was not unlocked for\nthe project.\nUse source to retrieve Feature Effects, selecting one of the provided sources.\nReturns:\nfeature_effect_metadata\nReturn type:\nFeatureEffectMetadata\nget_feature_effects_multiclass(source='training', class_=None)\nRetrieve Feature Effects for the multiclass model.\nFeature Effects provide partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (str) – The source Feature Effects are retrieved for.\nclass (str or None) – The class name Feature Effects are retrieved for.\nReturns:\nThe list of multiclass feature effects.\nReturn type:\nlist\nRaises:\nClientError – If Feature Effects have not been computed or source is not valid value.\nget_feature_impact(with_metadata=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the computed Feature Impact results, a measure of the relevance of each\nfeature in the model.\nFeature Impact is computed for each column by creating new data with that column randomly\npermuted (but the others left unchanged), and seeing how the error metric score for the\npredictions is affected. The ‘impactUnnormalized’ is how much worse the error metric score\nis when making predictions on this modified data. The ‘impactNormalized’ is normalized so\nthat the largest value is 1. In both cases, larger values indicate more important features.\nIf a feature is a redundant feature, i.e. once other features are considered it doesn’t\ncontribute much in addition, the ‘redundantWith’ value is the name of feature that has the\nhighest correlation with this feature. Note that redundancy detection is only available for\njobs run after the addition of this feature. When retrieving data that predates this\nfunctionality, a NoRedundancyImpactAvailable warning will be used.\nElsewhere this technique is sometimes called ‘Permutation Importance’.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nParameters:\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_feature_impact will raise a ValueError.\nReturns:\nThe feature impact data response depends on the with_metadata parameter. The response is\neither a dict with metadata and a list with actual data or just a list with that data.\nEach List item is a dict with the keys featureName, impactNormalized, and\nimpactUnnormalized, redundantWith and count.\nFor dict response available keys are:\nfeatureImpacts - Feature Impact data as a dictionary. Each item is a dict withkeys: featureName, impactNormalized, and impactUnnormalized, and\nredundantWith.\nshapBased - A boolean that indicates whether Feature Impact was calculated usingShapley values.\nranRedundancyDetection - A boolean that indicates whether redundant featureidentification was run while calculating this Feature Impact.\nrowCount - An integer or None that indicates the number of rows that was used tocalculate Feature Impact. For the Feature Impact calculated with the default\nlogic, without specifying the rowCount, we return None here.\ncount - An integer with the number of features under the featureImpacts.\nReturn type:\nlist or dict\nRaises:\nClientError – If the feature impacts have not been computed.\nValueError – If data_slice_filter passed as None\nget_features_used()\nQuery the server to determine which features were used.\nNote that the data returned by this method is possibly different\nthan the names of the features in the featurelist used by this model.\nThis method will return the raw features that must be supplied in order\nfor predictions to be generated on a new set of data. The featurelist,\nin contrast, would also include the names of derived features.\nReturns:\nfeatures – The names of the features used in the model.\nReturn type:\nList[str]\nget_frozen_child_models()\nRetrieve the IDs for all models that are frozen from this model.\nReturn type:\nA list of Models\nget_labelwise_roc_curves(source, fallback_to_parent_insights=False)\nRetrieve a list of LabelwiseRocCurve instances for a multilabel model for the given source and all labels.\nThis method is valid only for multilabel projects. For binary projects, use Model.get_roc_curve API .\nAdded in version v2.24.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\nReturns:\nLabelwise ROC Curve instances for source and all labels\nReturn type:\nlist of LabelwiseRocCurve\nRaises:\nClientError – If the insight is not available for this model\nget_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\nReturns:\nModel lift chart data\nReturn type:\nLiftChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_missing_report_info()\nRetrieve a report on missing training data that can be used to understand missing\nvalues treatment in the model. The report consists of missing values resolutions for\nfeatures numeric or categorical features that were part of building the model.\nReturns:\nThe queried model missing report, sorted by missing count (DESCENDING order).\nReturn type:\nAn iterable of MissingReportPerFeature\nget_model_blueprint_chart()\nRetrieve a diagram that can be used to understand\ndata flow in the blueprint.\nReturns:\nThe queried model blueprint chart.\nReturn type:\nModelBlueprintChart\nget_model_blueprint_documents()\nGet documentation for tasks used in this model.\nReturns:\nAll documents available for the model.\nReturn type:\nlist of BlueprintTaskDocument\nget_model_blueprint_json()\nGet the blueprint json representation used by this model.\nReturns:\nJson representation of the blueprint stages.\nReturn type:\nBlueprintJson\nget_multiclass_feature_impact()\nFor multiclass it’s possible to calculate feature impact separately for each target class.\nThe method for calculation is exactly the same, calculated in one-vs-all style for each\ntarget class.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. Each item is a dict with the keys ‘featureImpacts’ (list),\n‘class’ (str). Each item in ‘featureImpacts’ is a dict with the keys ‘featureName’,\n‘impactNormalized’, and ‘impactUnnormalized’, and ‘redundantWith’.\nReturn type:\nlist of dict\nRaises:\nClientError – If the multiclass feature impacts have not been computed.\nget_multiclass_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_multilabel_lift_charts(source, fallback_to_parent_insights=False)\nRetrieve model Lift charts for the specified source.\nAdded in version v2.24.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_num_iterations_trained()\nRetrieves the number of estimators trained by early-stopping tree-based models.\n– versionadded:: v2.22\nReturns:\nprojectId (str) – id of project containing the model\nmodelId (str) – id of the model\ndata (array) – list of numEstimatorsItem objects, one for each modeling stage.\nnumEstimatorsItem will be of the form\nstage (str) – indicates the modeling stage (for multi-stage models); None of single-stage models\nnumIterations (int) – the number of estimators or iterations trained by the model\nget_or_request_feature_effect(source, max_wait=600, row_count=None, data_slice_id=None)\nRetrieve Feature Effects for the model, requesting a new job if it hasn’t been run previously.\nSee get_feature_effect_metadata\nfor retrieving information of source.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effect job to complete before erroring.\nrow_count (Optional[int]) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nfeature_effects – The Feature Effects data.\nReturn type:\nFeatureEffects\nget_or_request_feature_effects_multiclass(source, top_n_features=None, features=None, row_count=None, class_=None, max_wait=600)\nRetrieve Feature Effects for the multiclass model, requesting a job if it hasn’t been run\npreviously.\nParameters:\nsource (string) – The source Feature Effects retrieve for.\nclass (str or None) – The class name Feature Effects retrieve for.\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by Feature Impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effects job to complete before\nerroring.\nReturns:\nfeature_effects – The list of multiclass feature effects data.\nReturn type:\nlist of FeatureEffectsMulticlass\nget_or_request_feature_impact(max_wait=600, **kwargs)\nRetrieve feature impact for the model, requesting a job if it hasn’t been run previously\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for a requested feature impact job to complete before erroring\n**kwargs – Arbitrary keyword arguments passed to\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. See\nget_feature_impact for the exact\nschema.\nReturn type:\nlist or dict\nget_parameters()\nRetrieve model parameters.\nReturns:\nModel parameters for this model.\nReturn type:\nModelParameters\nget_pareto_front()\nRetrieve the Pareto Front for a Eureqa model.\nThis method is only supported for Eureqa models.\nReturns:\nModel ParetoFront data\nReturn type:\nParetoFront\nget_prime_eligibility()\nCheck if this model can be approximated with DataRobot Prime\nReturns:\nprime_eligibility – a dict indicating whether a model can be approximated with DataRobot Prime\n(key can_make_prime) and why it may be ineligible (key message)\nReturn type:\ndict\nget_residuals_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible\nvalues.\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent if\nthe residuals chart is not available for this model and the model has a defined parent\nmodel. If omitted or False, or there is no parent model, will not attempt to return\nresiduals data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_residuals_chart will raise a ValueError.\nReturns:\nModel residuals chart data\nReturn type:\nResidualsChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_roc_curve(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the ROC curve for a binary model for the specified source.\nThis method is valid only for binary projects. For multilabel projects, use\nModel.get_labelwise_roc_curves.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_roc_curve will raise a ValueError.\nReturns:\nModel ROC curve data\nReturn type:\nRocCurve\nRaises:\nClientError – If the insight is not available for this model\n(New in version v3.0) TypeError – If the underlying project type is multilabel\nValueError – If data_slice_filter passed as None\nget_rulesets()\nList the rulesets approximating this model generated by DataRobot Prime\nIf this model hasn’t been approximated yet, will return an empty list.  Note that these\nare rulesets approximating this model, not rulesets used to construct this model.\nReturns:\nrulesets\nReturn type:\nlist of Ruleset\nget_supported_capabilities()\nRetrieves a summary of the capabilities supported by a model.\nAdded in version v2.14.\nReturns:\nsupportsBlending (bool) – whether the model supports blending\nsupportsMonotonicConstraints (bool) – whether the model supports monotonic constraints\nhasWordCloud (bool) – whether the model has word cloud data available\neligibleForPrime (bool) – (Deprecated in version v3.6)\nwhether the model is eligible for Prime\nhasParameters (bool) – whether the model has parameters that can be retrieved\nsupportsCodeGeneration (bool) – (New in version v2.18) whether the model supports code generation\nsupportsShap (bool) –\n(New in version v2.18) True if the model supports Shapley package. i.e. Shapley basedfeature Importance\nsupportsEarlyStopping (bool) – (New in version v2.22) True if this is an early stopping\ntree-based model and number of trained iterations can be retrieved.\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this model at leaderboard.\nReturn type:\nstr\nget_word_cloud(exclude_stop_words=False)\nRetrieve word cloud data for the model.\nParameters:\nexclude_stop_words (Optional[bool]) – Set to True if you want stopwords filtered out of response.\nReturns:\nWord cloud data for the model.\nReturn type:\nWordCloud\nincremental_train(data_stage_id, training_data_name=None)\nSubmit a job to the queue to perform incremental training on an existing model.\nSee train_incremental documentation.\nReturn type:\nModelJob\nclassmethod list(project_id, sort_by_partition='validation', sort_by_metric=None, with_metric=None, search_term=None, featurelists=None, families=None, blueprints=None, labels=None, characteristics=None, training_filters=None, number_of_clusters=None, limit=100, offset=0)\nRetrieve paginated model records, sorted by scores, with optional filtering.\nParameters:\nsort_by_partition (str, one of validation, backtesting, crossValidation or holdout) – Set the partition to use for sorted (by score) list of models. validation is the default.\nsort_by_metric (str) – Set the project metric to use for model sorting. DataRobot-selected project optimization metric\nis the default.\nwith_metric (str) – For a single-metric list of results, specify that project metric.\nsearch_term (str) – If specified, only models containing the term in their name or processes are returned.\nfeaturelists (List[str]) – If specified, only models trained on selected featurelists are returned.\nfamilies (List[str]) – If specified, only models belonging to selected families are returned.\nblueprints (List[str]) – If specified, only models trained on specified blueprint IDs are returned.\nlabels (List[str], starred or prepared for deployment) – If specified, only models tagged with all listed labels are returned.\ncharacteristics (List[str]) – If specified, only models matching all listed characteristics are returned.\ntraining_filters (List[str]) – If specified, only models matching at least one of the listed training conditions are returned.\nThe following formats are supported for autoML and datetime partitioned projects:\n- number of rows in training subset\nFor datetime partitioned projects:\n- <training duration>, example P6Y0M0D\n- <training_duration>-<time_window_sample_percent>-<sampling_method> Example: P6Y0M0D-78-Random,\n(returns models trained on 6 years of data, sampling rate 78%, random sampling).\n- Start/end date\n- Project settings\nnumber_of_clusters (list of int) – Filter models by number of clusters. Applicable only in unsupervised clustering projects.\nlimit (int)\noffset (int)\nReturns:\ngeneric_models\nReturn type:\nlist of GenericModel\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nrequest_cross_class_accuracy_scores()\nRequest data disparity insights to be computed for the model.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_data_disparity_insights(feature, compared_class_names)\nRequest data disparity insights to be computed for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\ncompared_class_names (list(str)) – List of two classes to compare\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_external_test(dataset_id, actual_value_column=None)\nRequest external test to compute scores and insights on an external test dataset\nParameters:\ndataset_id (string) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nReturns:\njob – a Job representing external dataset insights computation\nReturn type:\nJob\nrequest_fairness_insights(fairness_metrics_set=None)\nRequest fairness insights to be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_feature_effect(row_count=None, data_slice_id=None)\nSubmit request to compute Feature Effects for the model.\nSee get_feature_effect for more\ninformation on the result of the job.\nParameters:\nrow_count (int) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – A Job representing the feature effect computation. To get the completed feature effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nRaises:\nJobAlreadyRequested – If the feature effect have already been requested.\nrequest_feature_effects_multiclass(row_count=None, top_n_features=None, features=None)\nRequest Feature Effects computation for the multiclass model.\nSee get_feature_effect for\nmore information on the result of the job.\nParameters:\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by feature impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nReturns:\njob – A Job representing Feature Effect computation. To get the completed Feature Effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nrequest_feature_impact(row_count=None, with_metadata=False, data_slice_id=None)\nRequest feature impacts to be computed for the model.\nSee get_feature_impact for more\ninformation on the result of the job.\nParameters:\nrow_count (Optional[int]) – The sample size (specified in rows) to use for Feature Impact computation. This is not\nsupported for unsupervised, multiclass (which has a separate method), and time series\nprojects.\nwith_metadata (Optional[bool]) – Flag indicating whether the result should include the metadata.\nIf true, metadata is included.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – Job representing the Feature Impact computation. To retrieve the completed Feature Impact\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob or status_id\nRaises:\nJobAlreadyRequested – If the feature impacts have already been requested.\nrequest_lift_chart(source, data_slice_id=None)\nRequest the model Lift Chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_per_class_fairness_insights(fairness_metrics_set=None)\nRequest per-class fairness insights be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – The fairness metric used to calculate the fairness scores.\nValue can be any one of <datarobot.enums.FairnessMetricsSet>.\nReturns:\nstatus_check_job – The returned object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_predictions(dataset_id=None, dataset=None, dataframe=None, file_path=None, file=None, include_prediction_intervals=None, prediction_intervals_size=None, forecast_point=None, predictions_start_date=None, predictions_end_date=None, actual_value_column=None, explanation_algorithm=None, max_explanations=None, max_ngram_explanations=None)\nRequests predictions against a previously uploaded dataset.\nParameters:\ndataset_id (string, optional) – The ID of the dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataset (Dataset, optional) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataframe (pd.DataFrame, optional) – (New in v3.0)\nThe dataframe to make predictions against\nfile_path (Optional[str]) – (New in v3.0)\nPath to file to make predictions against\nfile (IOBase, optional) – (New in v3.0)\nFile to make predictions against\ninclude_prediction_intervals (Optional[bool]) – (New in v2.16) For time series projects only.\nSpecifies whether prediction intervals should be calculated for this request. Defaults\nto True if prediction_intervals_size is specified, otherwise defaults to False.\nprediction_intervals_size (Optional[int]) – (New in v2.16) For time series projects only.\nRepresents the percentile to use for the size of the prediction intervals. Defaults to\n80 if include_prediction_intervals is True. Prediction intervals size must be\nbetween 1 and 100 (inclusive).\nforecast_point (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. This is the default point relative\nto which predictions will be generated, based on the forecast window of the project. See\nthe time series prediction documentation for more\ninformation.\npredictions_start_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The end date for bulk\npredictions, exclusive. Note that this parameter is for generating historical\npredictions using the training data. This parameter should be provided in conjunction\nwith predictions_start_date. Can’t be provided with the\nforecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nexplanation_algorithm ((New in version v2.21) optional; If set to 'shap', the) – response will include prediction explanations based on the SHAP explainer (SHapley\nAdditive exPlanations). Defaults to null (no prediction explanations).\nmax_explanations ((New in version v2.21) int optional; specifies the maximum number of) – explanation values that should be returned for each row, ordered by absolute value,\ngreatest to least. If null, no limit. In the case of ‘shap’: if the number of features\nis greater than the limit, the sum of remaining values will also be returned as\nshapRemainingTotal. Defaults to null. Cannot be set if explanation_algorithm is\nomitted.\nmax_ngram_explanations (optional;  int or str) – (New in version v2.29) Specifies the maximum number of text explanation values that\nshould be returned. If set to all, text explanations will be computed and all the\nngram explanations will be returned. If set to a non zero positive integer value, text\nexplanations will be computed and this amount of descendingly sorted ngram explanations\nwill be returned. By default text explanation won’t be triggered to be computed.\nReturns:\njob – The job computing the predictions\nReturn type:\nPredictJob\nrequest_residuals_chart(source, data_slice_id=None)\nRequest the model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_roc_curve(source, data_slice_id=None)\nRequest the model Roc Curve for the specified source.\nParameters:\nsource (str) – Roc Curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_training_predictions(data_subset, explanation_algorithm=None, max_explanations=None)\nStart a job to build training predictions\nParameters:\ndata_subset (str) – data set definition to build predictions on.\nChoices are:\ndr.enums.DATA_SUBSET.ALL or string all for all data available. Not valid formodels in datetime partitioned projects\ndr.enums.DATA_SUBSET.VALIDATION_AND_HOLDOUT or string validationAndHoldout forall data except training set. Not valid for models in datetime partitioned\nprojects\ndr.enums.DATA_SUBSET.HOLDOUT or string holdout for holdout data set only\ndr.enums.DATA_SUBSET.ALL_BACKTESTS or string allBacktests for downloadingthe predictions for all backtest validation folds. Requires the model to have\nsuccessfully scored all backtests. Datetime partitioned projects only.\nexplanation_algorithm (dr.enums.EXPLANATIONS_ALGORITHM) – (New in v2.21) Optional. If set to dr.enums.EXPLANATIONS_ALGORITHM.SHAP, the response\nwill include prediction explanations based on the SHAP explainer (SHapley Additive\nexPlanations). Defaults to None (no prediction explanations).\nmax_explanations (int) – (New in v2.21) Optional. Specifies the maximum number of explanation values that should\nbe returned for each row, ordered by absolute value, greatest to least. In the case of\ndr.enums.EXPLANATIONS_ALGORITHM.SHAP:  If not set, explanations are returned for all\nfeatures. If the number of features is greater than the max_explanations, the sum of\nremaining values will also be returned as shap_remaining_total. Max 100. Defaults to\nnull for datasets narrower than 100 columns, defaults to 100 for datasets wider than 100\ncolumns. Is ignored if explanation_algorithm is not set.\nReturns:\nan instance of created async job\nReturn type:\nJob\nretrain(sample_pct=None, featurelist_id=None, training_row_count=None, n_clusters=None)\nSubmit a job to the queue to train a blender model.\nParameters:\nsample_pct (Optional[float]) – The sample size in percents (1 to 100) to use in training. If this parameter is used\nthen training_row_count should not be given.\nfeaturelist_id (Optional[str]) – The featurelist id\ntraining_row_count (Optional[int]) – The number of rows used to train the model. If this parameter is used, then sample_pct\nshould not be given.\nn_clusters (Optional[int]) – (new in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that do not determine\nthe number of clusters automatically.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nset_prediction_threshold(threshold)\nSet a custom prediction threshold for the model.\nMay not be used once prediction_threshold_read_only is True for this model.\nParameters:\nthreshold (float) – only used for binary classification projects. The threshold to when deciding between\nthe positive and negative classes when making predictions.  Should be between 0.0 and\n1.0 (inclusive).\nstar_model()\nMark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nstart_advanced_tuning_session()\nStart an Advanced Tuning session.  Returns an object that helps\nset up arguments for an Advanced Tuning model execution.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nSession for setting up and running Advanced Tuning on a model\nReturn type:\nAdvancedTuningSession\nstart_incremental_learning_from_sample(early_stopping_rounds=None, first_iteration_only=False, chunk_definition_id=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nearly_stopping_rounds (Optional[int]) – The number of chunks in which no improvement is observed that triggers the early stopping mechanism.\nfirst_iteration_only (bool) – Specifies whether incremental learning training should be limited to the first\niteration. If set to True, the training process will be performed only for the first\niteration. If set to False, training will continue until early stopping conditions\nare met or the maximum number of iterations is reached. The default value is False.\nchunk_definition_id (str) – The id of the chunk definition to be use for incremental training.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\ntrain_incremental(data_stage_id, training_data_name=None, data_stage_encoding=None, data_stage_delimiter=None, data_stage_compression=None)\nSubmit a job to the queue to perform incremental training on an existing model using\nadditional data. The id of the additional data to use for training is specified with the data_stage_id.\nOptionally a name for the iteration can be supplied by the user to help identify the contents of data in\nthe iteration.\nThis functionality requires the INCREMENTAL_LEARNING feature flag to be enabled.\nParameters:\ndata_stage_id (str) – The id of the data stage to use for training.\ntraining_data_name (Optional[str]) – The name of the iteration or data stage to indicate what the incremental learning was performed on.\ndata_stage_encoding (Optional[str]) – The encoding type of the data in the data stage (default: UTF-8).\nSupported formats: UTF-8, ASCII, WINDOWS1252\ndata_stage_encoding – The delimiter used by the data in the data stage (default: ‘,’).\ndata_stage_compression (Optional[str]) – The compression type of the data stage file, e.g. ‘zip’ (default: None).\nSupported formats: zip\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nunstar_model()\nUnmark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nPrime files\nclass datarobot.models.PrimeFile\nRepresents an executable file available for download of the code for a DataRobot Prime model\nVariables:\nid (str) – the id of the PrimeFile\nproject_id (str) – the id of the project this PrimeFile belongs to\nparent_model_id (str) – the model being approximated by this PrimeFile\nmodel_id (str) – the prime model this file represents\nruleset_id (int) – the ruleset being used in this PrimeFile\nlanguage (str) – the language of the code in this file - see enums.LANGUAGE for possibilities\nis_valid (bool) – whether the code passed basic validation\ndownload(filepath)\nDownload the code and save it to a file\nParameters:\nfilepath (string) – the location to save the file to\nReturn type:\nNone\nBlender models\nclass datarobot.models.BlenderModel\nRepresents blender model that combines prediction results from other models.\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nid (str) – the id of the model\nproject_id (str) – the id of the project the model belongs to\nprocesses (List[str]) – the processes used by the model\nfeaturelist_name (str) – the name of the featurelist used by the model\nfeaturelist_id (str) – the id of the featurelist used by the model\nsample_pct (float) – the percentage of the project dataset used in training the model\ntraining_row_count (int or None) – the number of rows of the project dataset used in training the model.  In a datetime\npartitioned project, if specified, defines the number of rows used to train the model and\nevaluate backtest scores; if unspecified, either training_duration or\ntraining_start_date and training_end_date was used to determine that instead.\ntraining_duration (str or None) – only present for models in datetime partitioned projects.  If specified, a duration string\nspecifying the duration spanned by the data used to train the model and evaluate backtest\nscores.\ntraining_start_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the start\ndate of the data used to train the model.\ntraining_end_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the end\ndate of the data used to train the model.\nmodel_type (str) – what model this is, e.g. ‘DataRobot Prime’\nmodel_category (str) – what kind of model this is - always ‘prime’ for DataRobot Prime models\nis_frozen (bool) – whether this model is a frozen model\nblueprint_id (str) – the id of the blueprint used in this model\nmetrics (dict) – a mapping from each metric to the model’s scores for that metric\nmodel_ids (List[str]) – List of model ids used in blender\nblender_method (str) – Method used to blend results from underlying models\nmonotonic_increasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically increasing relationship to the target.\nIf None, no such constraints are enforced.\nmonotonic_decreasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically decreasing relationship to the target.\nIf None, no such constraints are enforced.\nsupports_monotonic_constraints (bool) – optional, whether this model supports enforcing monotonic constraints\nis_starred (bool) – whether this model marked as starred\nprediction_threshold (float) – for binary classification projects, the threshold used for predictions\nprediction_threshold_read_only (bool) – indicated whether modification of the prediction threshold is forbidden. Threshold\nmodification is forbidden once a model has had a deployment created or predictions made via\nthe dedicated prediction API.\nmodel_number (integer) – model number assigned to a model\nparent_model_id (str or None) – (New in version v2.20) the id of the model that tuning parameters are derived from\nsupports_composable_ml (bool or None) – (New in version v2.26)\nwhether this model is supported in the Composable ML.\nclassmethod get(project_id, model_id)\nRetrieve a specific blender.\nParameters:\nproject_id (str) – The project’s id.\nmodel_id (str) – The model_id of the leaderboard item to retrieve.\nReturns:\nmodel – The queried instance.\nReturn type:\nBlenderModel\nadvanced_tune(params, description=None)\nGenerate a new model with the specified advanced-tuning parameters\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nParameters:\nparams (dict) – Mapping of parameter ID to parameter value.\nThe list of valid parameter IDs for a model can be found by calling\nget_advanced_tuning_parameters().\nThis endpoint does not need to include values for all parameters.  If a parameter\nis omitted, its current_value will be used.\ndescription (str) – Human-readable string describing the newly advanced-tuned model\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ncontinue_incremental_learning_from_incremental_model(chunk_definition_id, early_stopping_rounds=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nchunk_definition_id (str) – The Mongo ID for the chunking service.\nearly_stopping_rounds (Optional[int]) – The number of chunks that, when no improvement has been shown, triggers the early stopping mechanism.\nReturns:\njob – The model retraining job that is created.\nReturn type:\nModelJob\ncross_validate()\nRun cross validation on the model.\nNotes\nTo perform Cross Validation on a new model with new parameters, use train instead.\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ndelete()\nDelete a model from the project’s leaderboard.\nReturn type:\nNone\ndownload_scoring_code(file_name, source_code=False)\nDownload the Scoring Code JAR.\nParameters:\nfile_name (str) – File path where scoring code will be saved.\nsource_code (Optional[bool]) – Set to True to download source code archive.\nIt will not be executable.\nReturn type:\nNone\ndownload_training_artifact(file_name)\nRetrieve trained artifact(s) from a model containing one or more custom tasks.\nArtifact(s) will be downloaded to the specified local filepath.\nParameters:\nfile_name (str) – File path where trained model artifact(s) will be saved.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nOverrides the inherited method since the model must _not_ recursively change casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (list) – List of attribute namespaces like: [‘top.middle.bottom’], that should be kept\neven if their values are None\nget_advanced_tuning_parameters()\nGet the advanced-tuning parameters available for this model.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nA dictionary describing the advanced-tuning parameters for the current model.\nThere are two top-level keys, tuning_description and tuning_parameters.\ntuning_description an optional value. If not None, then it indicates the\nuser-specified description of this set of tuning parameter.\ntuning_parameters is a list of a dicts, each has the following keys\nparameter_name : (str) name of the parameter (unique per task, see below)\nparameter_id : (str) opaque ID string uniquely identifying parameter\ndefault_value : (*) the actual value used to train the model; either\nthe single value of the parameter specified before training, or the best\nvalue from the list of grid-searched values (based on current_value)\ncurrent_value : (*) the single value or list of values of the\nparameter that were grid searched. Depending on the grid search\nspecification, could be a single fixed value (no grid search),\na list of discrete values, or a range.\ntask_name : (str) name of the task that this parameter belongs to\nconstraints: (dict) see the notes below\nvertex_id: (str) ID of vertex that this parameter belongs to\nReturn type:\ndict\nNotes\nThe type of default_value and current_value is defined by the constraints structure.\nIt will be a string or numeric Python type.\nconstraints is a dict with at least one, possibly more, of the following keys.\nThe presence of a key indicates that the parameter may take on the specified type.\n(If a key is absent, this means that the parameter may not take on the specified type.)\nIf a key on constraints is present, its value will be a dict containing\nall of the fields described below for that key.\n\"constraints\": {\n\"select\": {\n\"values\": [<list(basestring or number) : possible values>]\n},\n\"ascii\": {},\n\"unicode\": {},\n\"int\": {\n\"min\": <int : minimum valid value>,\n\"max\": <int : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"float\": {\n\"min\": <float : minimum valid value>,\n\"max\": <float : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"intList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <int : minimum valid value>,\n\"max_val\": <int : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"floatList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <float : minimum valid value>,\n\"max_val\": <float : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n}\n}\nThe keys have meaning as follows:\nselect:\nRather than specifying a specific data type, if present, it indicates that the parameter\nis permitted to take on any of the specified values.  Listed values may be of any string\nor real (non-complex) numeric type.\nascii:\nThe parameter may be a unicode object that encodes simple ASCII characters.\n(A-Z, a-z, 0-9, whitespace, and certain common symbols.)  In addition to listed\nconstraints, ASCII keys currently may not contain either newlines or semicolons.\nunicode:\nThe parameter may be any Python unicode object.\nint:\nThe value may be an object of type int within the specified range (inclusive).\nPlease note that the value will be passed around using the JSON format, and\nsome JSON parsers have undefined behavior with integers outside of the range\n[-(2**53)+1, (2**53)-1].\nfloat:\nThe value may be an object of type float within the specified range (inclusive).\nintList, floatList:\nThe value may be a list of int or float objects, respectively, following constraints\nas specified respectively by the int and float types (above).\nMany parameters only specify one key under constraints.  If a parameter specifies multiple\nkeys, the parameter may take on any value permitted by any key.\nget_all_confusion_charts(fallback_to_parent_insights=False)\nRetrieve a list of all confusion matrices available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent for any source that is not available for this model and if this\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\nReturns:\nData for all available confusion charts for model.\nReturn type:\nlist of ConfusionChart\nget_all_feature_impacts(data_slice_filter=None)\nRetrieve a list of all feature impact results available for the model.\nParameters:\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen no data_slice filtering will be applied when requesting the roc_curve.\nReturns:\nData for all available model feature impacts. Or an empty list if not data found.\nReturn type:\nlist of dicts\nExamples\nmodel = datarobot.Model(id='model-id', project_id='project-id')\n# Get feature impact insights for sliced data\ndata_slice = datarobot.DataSlice(id='data-slice-id')\nsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get feature impact insights for unsliced data\ndata_slice = datarobot.DataSlice()\nunsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get all feature impact insights\nall_fi = model.get_all_feature_impacts()\nget_all_lift_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (Optional[bool]) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned lift chart by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model lift charts. Or an empty list if no data found.\nReturn type:\nlist of LiftChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get lift chart insights for sliced data\nsliced_lift_charts = model.get_all_lift_charts(data_slice_id='data-slice-id')\n# Get lift chart insights for unsliced data\nunsliced_lift_charts = model.get_all_lift_charts(unsliced_only=True)\n# Get all lift chart insights\nall_lift_charts = model.get_all_lift_charts()\nget_all_multiclass_lift_charts(fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nData for all available model lift charts.\nReturn type:\nlist of LiftChart\nget_all_residuals_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all residuals charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent\nfor any source that is not available for this model and if this model has a\ndefined parent model. If omitted or False, or this model has no parent, this will\nnot attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned residuals charts by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model residuals charts.\nReturn type:\nlist of ResidualsChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get residuals chart insights for sliced data\nsliced_residuals_charts = model.get_all_residuals_charts(data_slice_id='data-slice-id')\n# Get residuals chart insights for unsliced data\nunsliced_residuals_charts = model.get_all_residuals_charts(unsliced_only=True)\n# Get all residuals chart insights\nall_residuals_charts = model.get_all_residuals_charts()\nget_all_roc_curves(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all ROC curves available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – filters the returned roc_curve by data_slice_filter.id.  If None (the default) applies no filter based on\ndata_slice_id.\nReturns:\nData for all available model ROC curves. Or an empty list if no RocCurves are found.\nReturn type:\nlist of RocCurve\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\nds_filter=DataSlice(id='data-slice-id')\n# Get roc curve insights for sliced data\nsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get roc curve insights for unsliced data\ndata_slice_filter=DataSlice(id=None)\nunsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get all roc curve insights\nall_roc_curves = model.get_all_roc_curves()\nget_confusion_chart(source, fallback_to_parent_insights=False)\nRetrieve a multiclass model’s confusion matrix for the specified source.\nParameters:\nsource (str) – Confusion chart source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent if the confusion chart is not available for this model and the\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel ConfusionChart data\nReturn type:\nConfusionChart\nRaises:\nClientError – If the insight is not available for this model\nget_cross_class_accuracy_scores()\nRetrieves a list of Cross Class Accuracy scores for the model.\nReturn type:\njson\nget_cross_validation_scores(partition=None, metric=None)\nReturn a dictionary, keyed by metric, showing cross validation\nscores per partition.\nCross Validation should already have been performed using\ncross_validate or\ntrain.\nNotes\nModels that computed cross validation before this feature was added will need\nto be deleted and retrained before this method can be used.\nParameters:\npartition (float) – optional, the id of the partition (1,2,3.0,4.0,etc…) to filter results by\ncan be a whole number positive integer or float value. 0 corresponds to the\nvalidation partition.\nmetric (unicode) – optional name of the metric to filter to resulting cross validation scores by\nReturns:\ncross_validation_scores – A dictionary keyed by metric showing cross validation scores per\npartition.\nReturn type:\ndict\nget_data_disparity_insights(feature, class_name1, class_name2)\nRetrieve a list of Cross Class Data Disparity insights for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\nclass_name1 (str) – One of the compared classes\nclass_name2 (str) – Another compared class\nReturn type:\njson\nget_fairness_insights(fairness_metrics_set=None, offset=0, limit=100)\nRetrieve a list of Per Class Bias insights for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\noffset (Optional[int]) – Number of items to skip.\nlimit (Optional[int]) – Number of items to return.\nReturn type:\njson\nget_feature_effect(source, data_slice_id=None)\nRetrieve Feature Effects for the model.\nFeature Effects provides partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, retrieve unsliced insight data.\nReturns:\nfeature_effects – The feature effects data.\nReturn type:\nFeatureEffects\nRaises:\nClientError – If the feature effects have not been computed or source is not valid value.\nget_feature_effect_metadata()\nRetrieve Feature Effects metadata. Response contains status and available model sources.\nFeature Effect for the training partition is always available, with the exception of older\nprojects that only supported Feature Effect for validation.\nWhen a model is trained into validation or holdout without stacked predictions\n(i.e., no out-of-sample predictions in those partitions),\nFeature Effects is not available for validation or holdout.\nFeature Effects for holdout is not available when holdout was not unlocked for\nthe project.\nUse source to retrieve Feature Effects, selecting one of the provided sources.\nReturns:\nfeature_effect_metadata\nReturn type:\nFeatureEffectMetadata\nget_feature_effects_multiclass(source='training', class_=None)\nRetrieve Feature Effects for the multiclass model.\nFeature Effects provide partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (str) – The source Feature Effects are retrieved for.\nclass (str or None) – The class name Feature Effects are retrieved for.\nReturns:\nThe list of multiclass feature effects.\nReturn type:\nlist\nRaises:\nClientError – If Feature Effects have not been computed or source is not valid value.\nget_feature_impact(with_metadata=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the computed Feature Impact results, a measure of the relevance of each\nfeature in the model.\nFeature Impact is computed for each column by creating new data with that column randomly\npermuted (but the others left unchanged), and seeing how the error metric score for the\npredictions is affected. The ‘impactUnnormalized’ is how much worse the error metric score\nis when making predictions on this modified data. The ‘impactNormalized’ is normalized so\nthat the largest value is 1. In both cases, larger values indicate more important features.\nIf a feature is a redundant feature, i.e. once other features are considered it doesn’t\ncontribute much in addition, the ‘redundantWith’ value is the name of feature that has the\nhighest correlation with this feature. Note that redundancy detection is only available for\njobs run after the addition of this feature. When retrieving data that predates this\nfunctionality, a NoRedundancyImpactAvailable warning will be used.\nElsewhere this technique is sometimes called ‘Permutation Importance’.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nParameters:\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_feature_impact will raise a ValueError.\nReturns:\nThe feature impact data response depends on the with_metadata parameter. The response is\neither a dict with metadata and a list with actual data or just a list with that data.\nEach List item is a dict with the keys featureName, impactNormalized, and\nimpactUnnormalized, redundantWith and count.\nFor dict response available keys are:\nfeatureImpacts - Feature Impact data as a dictionary. Each item is a dict withkeys: featureName, impactNormalized, and impactUnnormalized, and\nredundantWith.\nshapBased - A boolean that indicates whether Feature Impact was calculated usingShapley values.\nranRedundancyDetection - A boolean that indicates whether redundant featureidentification was run while calculating this Feature Impact.\nrowCount - An integer or None that indicates the number of rows that was used tocalculate Feature Impact. For the Feature Impact calculated with the default\nlogic, without specifying the rowCount, we return None here.\ncount - An integer with the number of features under the featureImpacts.\nReturn type:\nlist or dict\nRaises:\nClientError – If the feature impacts have not been computed.\nValueError – If data_slice_filter passed as None\nget_features_used()\nQuery the server to determine which features were used.\nNote that the data returned by this method is possibly different\nthan the names of the features in the featurelist used by this model.\nThis method will return the raw features that must be supplied in order\nfor predictions to be generated on a new set of data. The featurelist,\nin contrast, would also include the names of derived features.\nReturns:\nfeatures – The names of the features used in the model.\nReturn type:\nList[str]\nget_frozen_child_models()\nRetrieve the IDs for all models that are frozen from this model.\nReturn type:\nA list of Models\nget_labelwise_roc_curves(source, fallback_to_parent_insights=False)\nRetrieve a list of LabelwiseRocCurve instances for a multilabel model for the given source and all labels.\nThis method is valid only for multilabel projects. For binary projects, use Model.get_roc_curve API .\nAdded in version v2.24.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\nReturns:\nLabelwise ROC Curve instances for source and all labels\nReturn type:\nlist of LabelwiseRocCurve\nRaises:\nClientError – If the insight is not available for this model\nget_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\nReturns:\nModel lift chart data\nReturn type:\nLiftChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_missing_report_info()\nRetrieve a report on missing training data that can be used to understand missing\nvalues treatment in the model. The report consists of missing values resolutions for\nfeatures numeric or categorical features that were part of building the model.\nReturns:\nThe queried model missing report, sorted by missing count (DESCENDING order).\nReturn type:\nAn iterable of MissingReportPerFeature\nget_model_blueprint_chart()\nRetrieve a diagram that can be used to understand\ndata flow in the blueprint.\nReturns:\nThe queried model blueprint chart.\nReturn type:\nModelBlueprintChart\nget_model_blueprint_documents()\nGet documentation for tasks used in this model.\nReturns:\nAll documents available for the model.\nReturn type:\nlist of BlueprintTaskDocument\nget_model_blueprint_json()\nGet the blueprint json representation used by this model.\nReturns:\nJson representation of the blueprint stages.\nReturn type:\nBlueprintJson\nget_multiclass_feature_impact()\nFor multiclass it’s possible to calculate feature impact separately for each target class.\nThe method for calculation is exactly the same, calculated in one-vs-all style for each\ntarget class.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. Each item is a dict with the keys ‘featureImpacts’ (list),\n‘class’ (str). Each item in ‘featureImpacts’ is a dict with the keys ‘featureName’,\n‘impactNormalized’, and ‘impactUnnormalized’, and ‘redundantWith’.\nReturn type:\nlist of dict\nRaises:\nClientError – If the multiclass feature impacts have not been computed.\nget_multiclass_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_multilabel_lift_charts(source, fallback_to_parent_insights=False)\nRetrieve model Lift charts for the specified source.\nAdded in version v2.24.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_num_iterations_trained()\nRetrieves the number of estimators trained by early-stopping tree-based models.\n– versionadded:: v2.22\nReturns:\nprojectId (str) – id of project containing the model\nmodelId (str) – id of the model\ndata (array) – list of numEstimatorsItem objects, one for each modeling stage.\nnumEstimatorsItem will be of the form\nstage (str) – indicates the modeling stage (for multi-stage models); None of single-stage models\nnumIterations (int) – the number of estimators or iterations trained by the model\nget_or_request_feature_effect(source, max_wait=600, row_count=None, data_slice_id=None)\nRetrieve Feature Effects for the model, requesting a new job if it hasn’t been run previously.\nSee get_feature_effect_metadata\nfor retrieving information of source.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effect job to complete before erroring.\nrow_count (Optional[int]) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nfeature_effects – The Feature Effects data.\nReturn type:\nFeatureEffects\nget_or_request_feature_effects_multiclass(source, top_n_features=None, features=None, row_count=None, class_=None, max_wait=600)\nRetrieve Feature Effects for the multiclass model, requesting a job if it hasn’t been run\npreviously.\nParameters:\nsource (string) – The source Feature Effects retrieve for.\nclass (str or None) – The class name Feature Effects retrieve for.\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by Feature Impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effects job to complete before\nerroring.\nReturns:\nfeature_effects – The list of multiclass feature effects data.\nReturn type:\nlist of FeatureEffectsMulticlass\nget_or_request_feature_impact(max_wait=600, **kwargs)\nRetrieve feature impact for the model, requesting a job if it hasn’t been run previously\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for a requested feature impact job to complete before erroring\n**kwargs – Arbitrary keyword arguments passed to\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. See\nget_feature_impact for the exact\nschema.\nReturn type:\nlist or dict\nget_parameters()\nRetrieve model parameters.\nReturns:\nModel parameters for this model.\nReturn type:\nModelParameters\nget_pareto_front()\nRetrieve the Pareto Front for a Eureqa model.\nThis method is only supported for Eureqa models.\nReturns:\nModel ParetoFront data\nReturn type:\nParetoFront\nget_prime_eligibility()\nCheck if this model can be approximated with DataRobot Prime\nReturns:\nprime_eligibility – a dict indicating whether a model can be approximated with DataRobot Prime\n(key can_make_prime) and why it may be ineligible (key message)\nReturn type:\ndict\nget_residuals_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible\nvalues.\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent if\nthe residuals chart is not available for this model and the model has a defined parent\nmodel. If omitted or False, or there is no parent model, will not attempt to return\nresiduals data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_residuals_chart will raise a ValueError.\nReturns:\nModel residuals chart data\nReturn type:\nResidualsChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_roc_curve(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the ROC curve for a binary model for the specified source.\nThis method is valid only for binary projects. For multilabel projects, use\nModel.get_labelwise_roc_curves.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_roc_curve will raise a ValueError.\nReturns:\nModel ROC curve data\nReturn type:\nRocCurve\nRaises:\nClientError – If the insight is not available for this model\n(New in version v3.0) TypeError – If the underlying project type is multilabel\nValueError – If data_slice_filter passed as None\nget_rulesets()\nList the rulesets approximating this model generated by DataRobot Prime\nIf this model hasn’t been approximated yet, will return an empty list.  Note that these\nare rulesets approximating this model, not rulesets used to construct this model.\nReturns:\nrulesets\nReturn type:\nlist of Ruleset\nget_supported_capabilities()\nRetrieves a summary of the capabilities supported by a model.\nAdded in version v2.14.\nReturns:\nsupportsBlending (bool) – whether the model supports blending\nsupportsMonotonicConstraints (bool) – whether the model supports monotonic constraints\nhasWordCloud (bool) – whether the model has word cloud data available\neligibleForPrime (bool) – (Deprecated in version v3.6)\nwhether the model is eligible for Prime\nhasParameters (bool) – whether the model has parameters that can be retrieved\nsupportsCodeGeneration (bool) – (New in version v2.18) whether the model supports code generation\nsupportsShap (bool) –\n(New in version v2.18) True if the model supports Shapley package. i.e. Shapley basedfeature Importance\nsupportsEarlyStopping (bool) – (New in version v2.22) True if this is an early stopping\ntree-based model and number of trained iterations can be retrieved.\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this model at leaderboard.\nReturn type:\nstr\nget_word_cloud(exclude_stop_words=False)\nRetrieve word cloud data for the model.\nParameters:\nexclude_stop_words (Optional[bool]) – Set to True if you want stopwords filtered out of response.\nReturns:\nWord cloud data for the model.\nReturn type:\nWordCloud\nincremental_train(data_stage_id, training_data_name=None)\nSubmit a job to the queue to perform incremental training on an existing model.\nSee train_incremental documentation.\nReturn type:\nModelJob\nclassmethod list(project_id, sort_by_partition='validation', sort_by_metric=None, with_metric=None, search_term=None, featurelists=None, families=None, blueprints=None, labels=None, characteristics=None, training_filters=None, number_of_clusters=None, limit=100, offset=0)\nRetrieve paginated model records, sorted by scores, with optional filtering.\nParameters:\nsort_by_partition (str, one of validation, backtesting, crossValidation or holdout) – Set the partition to use for sorted (by score) list of models. validation is the default.\nsort_by_metric (str) – Set the project metric to use for model sorting. DataRobot-selected project optimization metric\nis the default.\nwith_metric (str) – For a single-metric list of results, specify that project metric.\nsearch_term (str) – If specified, only models containing the term in their name or processes are returned.\nfeaturelists (List[str]) – If specified, only models trained on selected featurelists are returned.\nfamilies (List[str]) – If specified, only models belonging to selected families are returned.\nblueprints (List[str]) – If specified, only models trained on specified blueprint IDs are returned.\nlabels (List[str], starred or prepared for deployment) – If specified, only models tagged with all listed labels are returned.\ncharacteristics (List[str]) – If specified, only models matching all listed characteristics are returned.\ntraining_filters (List[str]) – If specified, only models matching at least one of the listed training conditions are returned.\nThe following formats are supported for autoML and datetime partitioned projects:\n- number of rows in training subset\nFor datetime partitioned projects:\n- <training duration>, example P6Y0M0D\n- <training_duration>-<time_window_sample_percent>-<sampling_method> Example: P6Y0M0D-78-Random,\n(returns models trained on 6 years of data, sampling rate 78%, random sampling).\n- Start/end date\n- Project settings\nnumber_of_clusters (list of int) – Filter models by number of clusters. Applicable only in unsupervised clustering projects.\nlimit (int)\noffset (int)\nReturns:\ngeneric_models\nReturn type:\nlist of GenericModel\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nrequest_approximation()\nRequest an approximation of this model using DataRobot Prime\nThis will create several rulesets that could be used to approximate this model.  After\ncomparing their scores and rule counts, the code used in the approximation can be downloaded\nand run locally.\nReturns:\njob – the job generating the rulesets\nReturn type:\nJob\nrequest_cross_class_accuracy_scores()\nRequest data disparity insights to be computed for the model.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_data_disparity_insights(feature, compared_class_names)\nRequest data disparity insights to be computed for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\ncompared_class_names (list(str)) – List of two classes to compare\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_external_test(dataset_id, actual_value_column=None)\nRequest external test to compute scores and insights on an external test dataset\nParameters:\ndataset_id (string) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nReturns:\njob – a Job representing external dataset insights computation\nReturn type:\nJob\nrequest_fairness_insights(fairness_metrics_set=None)\nRequest fairness insights to be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_feature_effect(row_count=None, data_slice_id=None)\nSubmit request to compute Feature Effects for the model.\nSee get_feature_effect for more\ninformation on the result of the job.\nParameters:\nrow_count (int) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – A Job representing the feature effect computation. To get the completed feature effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nRaises:\nJobAlreadyRequested – If the feature effect have already been requested.\nrequest_feature_effects_multiclass(row_count=None, top_n_features=None, features=None)\nRequest Feature Effects computation for the multiclass model.\nSee get_feature_effect for\nmore information on the result of the job.\nParameters:\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by feature impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nReturns:\njob – A Job representing Feature Effect computation. To get the completed Feature Effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nrequest_feature_impact(row_count=None, with_metadata=False, data_slice_id=None)\nRequest feature impacts to be computed for the model.\nSee get_feature_impact for more\ninformation on the result of the job.\nParameters:\nrow_count (Optional[int]) – The sample size (specified in rows) to use for Feature Impact computation. This is not\nsupported for unsupervised, multiclass (which has a separate method), and time series\nprojects.\nwith_metadata (Optional[bool]) – Flag indicating whether the result should include the metadata.\nIf true, metadata is included.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – Job representing the Feature Impact computation. To retrieve the completed Feature Impact\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob or status_id\nRaises:\nJobAlreadyRequested – If the feature impacts have already been requested.\nrequest_frozen_datetime_model(training_row_count=None, training_duration=None, training_start_date=None, training_end_date=None, time_window_sample_pct=None, sampling_method=None)\nTrain a new frozen model with parameters from this model.\nRequires that this model belongs to a datetime partitioned project.  If it does not, an\nerror will occur when submitting the job.\nFrozen models use the same tuning parameters as their parent model instead of independently\noptimizing them to allow efficiently retraining models on larger amounts of the training\ndata.\nIn addition of training_row_count and training_duration, frozen datetime models may be\ntrained on an exact date range.  Only one of training_row_count, training_duration, or\ntraining_start_date and training_end_date should be specified.\nModels specified using training_start_date and training_end_date are the only ones that can\nbe trained into the holdout data (once the holdout is unlocked).\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\ntraining_duration may not be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, training_row_count may not be specified.\ntraining_start_date (datetime.datetime, optional) – the start date of the data to train to model on.  Only rows occurring at or after\nthis datetime will be used.  If training_start_date is specified, training_end_date\nmust also be specified.\ntraining_end_date (datetime.datetime, optional) – the end date of the data to train the model on.  Only rows occurring strictly before\nthis datetime will be used.  If training_end_date is specified, training_start_date\nmust also be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates).  An integer between 1 and 99 indicating the percentage to sample by\nwithin the window.  The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nReturns:\nmodel_job – the modeling job training a frozen model\nReturn type:\nModelJob\nrequest_frozen_model(sample_pct=None, training_row_count=None)\nTrain a new frozen model with parameters from this model\nNotes\nThis method only works if project the model belongs to is not datetime\npartitioned.  If it is, use request_frozen_datetime_model instead.\nFrozen models use the same tuning parameters as their parent model instead of independently\noptimizing them to allow efficiently retraining models on larger amounts of the training\ndata.\nParameters:\nsample_pct (float) – optional, the percentage of the dataset to use with the model.  If not provided, will\nuse the value from this model.\ntraining_row_count (int) – (New in version v2.9) optional, the integer number of rows of the dataset to use with\nthe model. Only one of sample_pct and training_row_count should be specified.\nReturns:\nmodel_job – the modeling job training a frozen model\nReturn type:\nModelJob\nrequest_lift_chart(source, data_slice_id=None)\nRequest the model Lift Chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_per_class_fairness_insights(fairness_metrics_set=None)\nRequest per-class fairness insights be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – The fairness metric used to calculate the fairness scores.\nValue can be any one of <datarobot.enums.FairnessMetricsSet>.\nReturns:\nstatus_check_job – The returned object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_predictions(dataset_id=None, dataset=None, dataframe=None, file_path=None, file=None, include_prediction_intervals=None, prediction_intervals_size=None, forecast_point=None, predictions_start_date=None, predictions_end_date=None, actual_value_column=None, explanation_algorithm=None, max_explanations=None, max_ngram_explanations=None)\nRequests predictions against a previously uploaded dataset.\nParameters:\ndataset_id (string, optional) – The ID of the dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataset (Dataset, optional) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataframe (pd.DataFrame, optional) – (New in v3.0)\nThe dataframe to make predictions against\nfile_path (Optional[str]) – (New in v3.0)\nPath to file to make predictions against\nfile (IOBase, optional) – (New in v3.0)\nFile to make predictions against\ninclude_prediction_intervals (Optional[bool]) – (New in v2.16) For time series projects only.\nSpecifies whether prediction intervals should be calculated for this request. Defaults\nto True if prediction_intervals_size is specified, otherwise defaults to False.\nprediction_intervals_size (Optional[int]) – (New in v2.16) For time series projects only.\nRepresents the percentile to use for the size of the prediction intervals. Defaults to\n80 if include_prediction_intervals is True. Prediction intervals size must be\nbetween 1 and 100 (inclusive).\nforecast_point (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. This is the default point relative\nto which predictions will be generated, based on the forecast window of the project. See\nthe time series prediction documentation for more\ninformation.\npredictions_start_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The end date for bulk\npredictions, exclusive. Note that this parameter is for generating historical\npredictions using the training data. This parameter should be provided in conjunction\nwith predictions_start_date. Can’t be provided with the\nforecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nexplanation_algorithm ((New in version v2.21) optional; If set to 'shap', the) – response will include prediction explanations based on the SHAP explainer (SHapley\nAdditive exPlanations). Defaults to null (no prediction explanations).\nmax_explanations ((New in version v2.21) int optional; specifies the maximum number of) – explanation values that should be returned for each row, ordered by absolute value,\ngreatest to least. If null, no limit. In the case of ‘shap’: if the number of features\nis greater than the limit, the sum of remaining values will also be returned as\nshapRemainingTotal. Defaults to null. Cannot be set if explanation_algorithm is\nomitted.\nmax_ngram_explanations (optional;  int or str) – (New in version v2.29) Specifies the maximum number of text explanation values that\nshould be returned. If set to all, text explanations will be computed and all the\nngram explanations will be returned. If set to a non zero positive integer value, text\nexplanations will be computed and this amount of descendingly sorted ngram explanations\nwill be returned. By default text explanation won’t be triggered to be computed.\nReturns:\njob – The job computing the predictions\nReturn type:\nPredictJob\nrequest_residuals_chart(source, data_slice_id=None)\nRequest the model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_roc_curve(source, data_slice_id=None)\nRequest the model Roc Curve for the specified source.\nParameters:\nsource (str) – Roc Curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_training_predictions(data_subset, explanation_algorithm=None, max_explanations=None)\nStart a job to build training predictions\nParameters:\ndata_subset (str) – data set definition to build predictions on.\nChoices are:\ndr.enums.DATA_SUBSET.ALL or string all for all data available. Not valid formodels in datetime partitioned projects\ndr.enums.DATA_SUBSET.VALIDATION_AND_HOLDOUT or string validationAndHoldout forall data except training set. Not valid for models in datetime partitioned\nprojects\ndr.enums.DATA_SUBSET.HOLDOUT or string holdout for holdout data set only\ndr.enums.DATA_SUBSET.ALL_BACKTESTS or string allBacktests for downloadingthe predictions for all backtest validation folds. Requires the model to have\nsuccessfully scored all backtests. Datetime partitioned projects only.\nexplanation_algorithm (dr.enums.EXPLANATIONS_ALGORITHM) – (New in v2.21) Optional. If set to dr.enums.EXPLANATIONS_ALGORITHM.SHAP, the response\nwill include prediction explanations based on the SHAP explainer (SHapley Additive\nexPlanations). Defaults to None (no prediction explanations).\nmax_explanations (int) – (New in v2.21) Optional. Specifies the maximum number of explanation values that should\nbe returned for each row, ordered by absolute value, greatest to least. In the case of\ndr.enums.EXPLANATIONS_ALGORITHM.SHAP:  If not set, explanations are returned for all\nfeatures. If the number of features is greater than the max_explanations, the sum of\nremaining values will also be returned as shap_remaining_total. Max 100. Defaults to\nnull for datasets narrower than 100 columns, defaults to 100 for datasets wider than 100\ncolumns. Is ignored if explanation_algorithm is not set.\nReturns:\nan instance of created async job\nReturn type:\nJob\nretrain(sample_pct=None, featurelist_id=None, training_row_count=None, n_clusters=None)\nSubmit a job to the queue to train a blender model.\nParameters:\nsample_pct (Optional[float]) – The sample size in percents (1 to 100) to use in training. If this parameter is used\nthen training_row_count should not be given.\nfeaturelist_id (Optional[str]) – The featurelist id\ntraining_row_count (Optional[int]) – The number of rows used to train the model. If this parameter is used, then sample_pct\nshould not be given.\nn_clusters (Optional[int]) – (new in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that do not determine\nthe number of clusters automatically.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nset_prediction_threshold(threshold)\nSet a custom prediction threshold for the model.\nMay not be used once prediction_threshold_read_only is True for this model.\nParameters:\nthreshold (float) – only used for binary classification projects. The threshold to when deciding between\nthe positive and negative classes when making predictions.  Should be between 0.0 and\n1.0 (inclusive).\nstar_model()\nMark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nstart_advanced_tuning_session()\nStart an Advanced Tuning session.  Returns an object that helps\nset up arguments for an Advanced Tuning model execution.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nSession for setting up and running Advanced Tuning on a model\nReturn type:\nAdvancedTuningSession\nstart_incremental_learning_from_sample(early_stopping_rounds=None, first_iteration_only=False, chunk_definition_id=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nearly_stopping_rounds (Optional[int]) – The number of chunks in which no improvement is observed that triggers the early stopping mechanism.\nfirst_iteration_only (bool) – Specifies whether incremental learning training should be limited to the first\niteration. If set to True, the training process will be performed only for the first\niteration. If set to False, training will continue until early stopping conditions\nare met or the maximum number of iterations is reached. The default value is False.\nchunk_definition_id (str) – The id of the chunk definition to be use for incremental training.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\ntrain(sample_pct=None, featurelist_id=None, scoring_type=None, training_row_count=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>)\nTrain the blueprint used in model on a particular featurelist or amount of data.\nThis method creates a new training job for worker and appends it to\nthe end of the queue for this project.\nAfter the job has finished you can get the newly trained model by retrieving\nit from the project leaderboard, or by retrieving the result of the job.\nEither sample_pct or training_row_count can be used to specify the amount of data to\nuse, but not both.  If neither are specified, a default of the maximum amount of data that\ncan safely be used to train any blueprint without going into the validation data will be\nselected.\nIn smart-sampled projects, sample_pct and training_row_count are assumed to be in terms\nof rows of the minority class.\nNotes\nFor datetime partitioned projects, see train_datetime instead.\nParameters:\nsample_pct (Optional[float]) – The amount of data to use for training, as a percentage of the project dataset from\n0 to 100.\nfeaturelist_id (Optional[str]) – The identifier of the featurelist to use. If not defined, the\nfeaturelist of this model is used.\nscoring_type (Optional[str]) – Either validation or crossValidation (also dr.SCORING_TYPE.validation\nor dr.SCORING_TYPE.cross_validation). validation is available for every\npartitioning type, and indicates that the default model validation should be\nused for the project.\nIf the project uses a form of cross-validation partitioning,\ncrossValidation can also be used to indicate\nthat all of the available training/validation combinations\nshould be used to evaluate the model.\ntraining_row_count (Optional[int]) – The number of rows to use to train the requested model.\nmonotonic_increasing_featurelist_id (str) – (new in version 2.11) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (str) – (new in version 2.11) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nReturns:\nmodel_job_id – id of created job, can be used as parameter to ModelJob.get\nmethod or wait_for_async_model_creation function\nReturn type:\nstr\nExamples\nproject = Project.get('project-id')\nmodel = Model.get('project-id', 'model-id')\nmodel_job_id = model.train(training_row_count=project.max_train_rows)\ntrain_datetime(featurelist_id=None, training_row_count=None, training_duration=None, time_window_sample_pct=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>, use_project_settings=False, sampling_method=None, n_clusters=None)\nTrains this model on a different featurelist or sample size.\nRequires that this model is part of a datetime partitioned project; otherwise, an error will\noccur.\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\nfeaturelist_id (Optional[str]) – the featurelist to use to train the model.  If not specified, the featurelist of this\nmodel is used.\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\nneither training_duration nor use_project_settings may be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, neither training_row_count nor use_project_settings may be\nspecified.\nuse_project_settings (Optional[bool]) – (New in version v2.20) defaults to False. If True, indicates that the custom\nbacktest partitioning settings specified by the user will be used to train the model and\nevaluate backtest scores. If specified, neither training_row_count nor\ntraining_duration may be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates). An integer between 1 and 99 indicating the percentage to sample by\nwithin the window. The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nmonotonic_increasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nn_clusters (Optional[int]) – (New in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that don’t automatically\ndetermine the number of clusters.\nReturns:\njob – the created job to build the model\nReturn type:\nModelJob\ntrain_incremental(data_stage_id, training_data_name=None, data_stage_encoding=None, data_stage_delimiter=None, data_stage_compression=None)\nSubmit a job to the queue to perform incremental training on an existing model using\nadditional data. The id of the additional data to use for training is specified with the data_stage_id.\nOptionally a name for the iteration can be supplied by the user to help identify the contents of data in\nthe iteration.\nThis functionality requires the INCREMENTAL_LEARNING feature flag to be enabled.\nParameters:\ndata_stage_id (str) – The id of the data stage to use for training.\ntraining_data_name (Optional[str]) – The name of the iteration or data stage to indicate what the incremental learning was performed on.\ndata_stage_encoding (Optional[str]) – The encoding type of the data in the data stage (default: UTF-8).\nSupported formats: UTF-8, ASCII, WINDOWS1252\ndata_stage_encoding – The delimiter used by the data in the data stage (default: ‘,’).\ndata_stage_compression (Optional[str]) – The compression type of the data stage file, e.g. ‘zip’ (default: None).\nSupported formats: zip\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nunstar_model()\nUnmark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nDatetime models\nclass datarobot.models.DatetimeModel\nRepresents a model from a datetime partitioned project\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nNote that only one of training_row_count, training_duration, and\ntraining_start_date and training_end_date will be specified, depending on the\ndata_selection_method of the model.  Whichever method was selected determines the amount of\ndata used to train on when making predictions and scoring the backtests and the holdout.\nVariables:\nid (str) – the id of the model\nproject_id (str) – the id of the project the model belongs to\nprocesses (List[str]) – the processes used by the model\nfeaturelist_name (str) – the name of the featurelist used by the model\nfeaturelist_id (str) – the id of the featurelist used by the model\nsample_pct (float) – the percentage of the project dataset used in training the model\ntraining_row_count (int or None) – If specified, an int specifying the number of rows used to train the model and evaluate\nbacktest scores.\ntraining_duration (str or None) – If specified, a duration string specifying the duration spanned by the data used to train\nthe model and evaluate backtest scores.\ntraining_start_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the start\ndate of the data used to train the model.\ntraining_end_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the end\ndate of the data used to train the model.\ntime_window_sample_pct (int or None) – An integer between 1 and 99 indicating the percentage of sampling within the training\nwindow.  The points kept are determined by a random uniform sample.  If not specified, no\nsampling was done.\nsampling_method (str or None) – (New in v2.23) indicates the way training data has been selected (either how rows have been\nselected within backtest or how time_window_sample_pct has been applied).\nmodel_type (str) – what model this is, e.g. ‘Nystroem Kernel SVM Regressor’\nmodel_category (str) – what kind of model this is - ‘prime’ for DataRobot Prime models, ‘blend’ for blender models,\nand ‘model’ for other models\nis_frozen (bool) – whether this model is a frozen model\nblueprint_id (str) – the id of the blueprint used in this model\nmetrics (dict) – a mapping from each metric to the model’s scores for that metric.  The keys in metrics are\nthe different metrics used to evaluate the model, and the values are the results.  The\ndictionaries inside of metrics will be as described here: ‘validation’, the score\nfor a single backtest; ‘crossValidation’, always None; ‘backtesting’, the average score for\nall backtests if all are available and computed, or None otherwise; ‘backtestingScores’, a\nlist of scores for all backtests where the score is None if that backtest does not have a\nscore available; and ‘holdout’, the score for the holdout or None if the holdout is locked\nor the score is unavailable.\nbacktests (list of dict) – describes what data was used to fit each backtest, the score for the project metric, and\nwhy the backtest score is unavailable if it is not provided.\ndata_selection_method (str) – which of training_row_count, training_duration, or training_start_data and training_end_date\nwere used to determine the data used to fit the model.  One of ‘rowCount’,\n‘duration’, or ‘selectedDateRange’.\ntraining_info (dict) – describes which data was used to train on when scoring the holdout and making predictions.\ntraining_info` will have the following keys: holdout_training_start_date,\nholdout_training_duration, holdout_training_row_count, holdout_training_end_date,\nprediction_training_start_date, prediction_training_duration,\nprediction_training_row_count, prediction_training_end_date. Start and end dates will\nbe datetimes, durations will be duration strings, and rows will be integers.\nholdout_score (float or None) – the score against the holdout, if available and the holdout is unlocked, according to the\nproject metric.\nholdout_status (string or None) – the status of the holdout score, e.g. “COMPLETED”, “HOLDOUT_BOUNDARIES_EXCEEDED”.\nUnavailable if the holdout fold was disabled in the partitioning configuration.\nmonotonic_increasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically increasing relationship to the target.\nIf None, no such constraints are enforced.\nmonotonic_decreasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically decreasing relationship to the target.\nIf None, no such constraints are enforced.\nsupports_monotonic_constraints (bool) – optional, whether this model supports enforcing monotonic constraints\nis_starred (bool) – whether this model marked as starred\nprediction_threshold (float) – for binary classification projects, the threshold used for predictions\nprediction_threshold_read_only (bool) – indicated whether modification of the prediction threshold is forbidden. Threshold\nmodification is forbidden once a model has had a deployment created or predictions made via\nthe dedicated prediction API.\neffective_feature_derivation_window_start (int or None) – (New in v2.16) For time series projects only.\nHow many units of the windows_basis_unit into the past relative to the forecast point\nthe user needs to provide history for at prediction time. This can differ from the\nfeature_derivation_window_start set on the project due to the differencing method and\nperiod selected, or if the model is a time series native model such as ARIMA. Will be a\nnegative integer in time series projects and None otherwise.\neffective_feature_derivation_window_end (int or None) – (New in v2.16) For time series projects only.\nHow many units of the windows_basis_unit into the past relative to the forecast point\nthe feature derivation window should end. Will be a non-positive integer in time series\nprojects and None otherwise.\nforecast_window_start (int or None) – (New in v2.16) For time series projects only.\nHow many units of the windows_basis_unit into the future relative to the forecast point\nthe forecast window should start. Note that this field will be the same as what is shown in\nthe project settings. Will be a non-negative integer in time series projects and None\notherwise.\nforecast_window_end (int or None) – (New in v2.16) For time series projects only.\nHow many units of the windows_basis_unit into the future relative to the forecast point\nthe forecast window should end. Note that this field will be the same as what is shown in\nthe project settings. Will be a non-negative integer in time series projects and None\notherwise.\nwindows_basis_unit (str or None) – (New in v2.16) For time series projects only.\nIndicates which unit is the basis for the feature derivation window and the forecast window.\nNote that this field will be the same as what is shown in the project settings. In time\nseries projects, will be either the detected time unit or “ROW”, and None otherwise.\nmodel_number (integer) – model number assigned to a model\nparent_model_id (str or None) – (New in version v2.20) the id of the model that tuning parameters are derived from\nsupports_composable_ml (bool or None) – (New in version v2.26)\nwhether this model is supported in the Composable ML.\nis_n_clusters_dynamically_determined (Optional[bool]) – (New in version 2.27) if True, indicates that model determines number of clusters\nautomatically.\nn_clusters (Optional[int]) – (New in version 2.27) Number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that don’t automatically\ndetermine the number of clusters.\nclassmethod get(project, model_id)\nRetrieve a specific datetime model.\nIf the project does not use datetime partitioning, a ClientError will occur.\nParameters:\nproject (str) – the id of the project the model belongs to\nmodel_id (str) – the id of the model to retrieve\nReturns:\nmodel – the model\nReturn type:\nDatetimeModel\nscore_backtests()\nCompute the scores for all available backtests.\nSome backtests may be unavailable if the model is trained into their validation data.\nReturns:\njob – a job tracking the backtest computation.  When it is complete, all available backtests\nwill have scores computed.\nReturn type:\nJob\ncross_validate()\nInherited from the model. DatetimeModels cannot request cross validation scores;\nuse backtests instead.\nReturn type:\nNoReturn\nget_cross_validation_scores(partition=None, metric=None)\nInherited from Model - DatetimeModels cannot request Cross Validation scores,\nUse backtests instead.\nReturn type:\nNoReturn\nrequest_training_predictions(data_subset, *args, **kwargs)\nStart a job that builds training predictions.\nParameters:\ndata_subset (str) – data set definition to build predictions on.\nChoices are:\ndr.enums.DATA_SUBSET.HOLDOUT for holdout data set only\ndr.enums.DATA_SUBSET.ALL_BACKTESTS for downloading the predictions for allbacktest validation folds. Requires the model to have successfully scored all\nbacktests.\nReturns:\nan instance of created async job\nReturn type:\nJob\nget_series_accuracy_as_dataframe(offset=0, limit=100, metric=None, multiseries_value=None, order_by=None, reverse=False)\nRetrieve series accuracy results for the specified model as a pandas.DataFrame.\nParameters:\noffset (Optional[int]) – The number of results to skip. Defaults to 0 if not specified.\nlimit (Optional[int]) – The maximum number of results to return. Defaults to 100 if not specified.\nmetric (Optional[str]) – The name of the metric to retrieve scores for. If omitted, the default project metric\nwill be used.\nmultiseries_value (Optional[str]) – If specified, only the series containing the given value in one of the series ID columns\nwill be returned.\norder_by (Optional[str]) – Used for sorting the series. Attribute must be one of\ndatarobot.enums.SERIES_ACCURACY_ORDER_BY.\nreverse (Optional[bool]) – Used for sorting the series. If True, will sort the series in descending order by\nthe attribute specified by order_by.\nReturns:\nA pandas.DataFrame with the Series Accuracy for the specified model.\nReturn type:\ndata\ndownload_series_accuracy_as_csv(filename, encoding='utf-8', offset=0, limit=100, metric=None, multiseries_value=None, order_by=None, reverse=False)\nSave series accuracy results for the specified model in a CSV file.\nParameters:\nfilename (str or file object) – The path or file object to save the data to.\nencoding (Optional[str]) – A string representing the encoding to use in the output csv file.\nDefaults to ‘utf-8’.\noffset (Optional[int]) – The number of results to skip. Defaults to 0 if not specified.\nlimit (Optional[int]) – The maximum number of results to return. Defaults to 100 if not specified.\nmetric (Optional[str]) – The name of the metric to retrieve scores for. If omitted, the default project metric\nwill be used.\nmultiseries_value (Optional[str]) – If specified, only the series containing the given value in one of the series ID columns\nwill be returned.\norder_by (Optional[str]) – Used for sorting the series. Attribute must be one of\ndatarobot.enums.SERIES_ACCURACY_ORDER_BY.\nreverse (Optional[bool]) – Used for sorting the series. If True, will sort the series in descending order by\nthe attribute specified by order_by.\nget_series_clusters(offset=0, limit=100, order_by=None, reverse=False)\nRetrieve a dictionary of series and the clusters assigned to each series. This\nis only usable for clustering projects.\nParameters:\noffset (Optional[int]) – The number of results to skip. Defaults to 0 if not specified.\nlimit (Optional[int]) – The maximum number of results to return. Defaults to 100 if not specified.\norder_by (Optional[str]) – Used for sorting the series. Attribute must be one of\ndatarobot.enums.SERIES_ACCURACY_ORDER_BY.\nreverse (Optional[bool]) – Used for sorting the series. If True, will sort the series in descending order by\nthe attribute specified by order_by.\nReturns:\nA dictionary of the series in the dataset with their associated cluster\nReturn type:\nDict\nRaises:\nValueError – If the model type returns an unsupported insight\nClientError – If the insight is not available for this model\ncompute_series_accuracy(compute_all_series=False)\nCompute series accuracy for the model.\nParameters:\ncompute_all_series (Optional[bool]) – Calculate accuracy for all series or only first 1000.\nReturns:\nan instance of the created async job\nReturn type:\nJob\nretrain(time_window_sample_pct=None, featurelist_id=None, training_row_count=None, training_duration=None, training_start_date=None, training_end_date=None, sampling_method=None, n_clusters=None)\nRetrain an existing datetime model using a new training period for the model’s training\nset (with optional time window sampling) or a different feature list.\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\nfeaturelist_id (Optional[str]) – The ID of the featurelist to use.\ntraining_row_count (Optional[int]) – The number of rows to train the model on. If this parameter is used then sample_pct\ncannot be specified.\ntime_window_sample_pct (Optional[int]) – An int between 1 and 99 indicating the percentage of\nsampling within the time window. The points kept are determined by a random uniform\nsample. If specified, training_row_count must not be specified and either\ntraining_duration or training_start_date and training_end_date must be specified.\ntraining_duration (Optional[str]) – A duration string representing the training duration for the submitted model. If\nspecified then training_row_count, training_start_date, and training_end_date\ncannot be specified.\ntraining_start_date (Optional[str]) – A datetime string representing the start date of\nthe data to use for training this model.  If specified, training_end_date must also be\nspecified, and training_duration cannot be specified. The value must be before the\ntraining_end_date value.\ntraining_end_date (Optional[str]) – A datetime string representing the end date of the\ndata to use for training this model.  If specified, training_start_date must also be\nspecified, and training_duration cannot be specified. The value must be after the\ntraining_start_date value.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nn_clusters (Optional[int]) – (New in version 2.27) Number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that don’t automatically\ndetermine the number of clusters.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nget_feature_effect_metadata()\nRetrieve Feature Effect metadata for each backtest. Response contains status and available\nsources for each backtest of the model.\nEach backtest is available for training and validation\nIf holdout is configured for the project it has holdout as backtestIndex. It has\ntraining and holdout sources available.\nStart/stop models contain a single response item with startstop value for backtestIndex.\nFeature Effect of training is always available\n(except for the old project which supports only Feature Effect for validation).\nWhen a model is trained into validation or holdout without stacked prediction\n(e.g. no out-of-sample prediction in validation or holdout),\nFeature Effect is not available for validation or holdout.\nFeature Effect for holdout is not available when there is no holdout configured for\nthe project.\nsource is expected parameter to retrieve Feature Effect. One of provided sources\nshall be used.\nbacktestIndex is expected parameter to submit compute request and retrieve Feature Effect.\nOne of provided backtest indexes shall be used.\nReturns:\nfeature_effect_metadata\nReturn type:\nFeatureEffectMetadataDatetime\nrequest_feature_effect(backtest_index, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRequest feature effects to be computed for the model.\nSee get_feature_effect for more\ninformation on the result of the job.\nSee get_feature_effect_metadata\nfor retrieving information of backtest_index.\nParameters:\nbacktest_index (string, FeatureEffectMetadataDatetime.backtest_index.) – The backtest index to retrieve Feature Effects for.\nReturns:\njob – A Job representing the feature effect computation. To get the completed feature effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nRaises:\nJobAlreadyRequested – If the feature effect have already been requested.\nget_feature_effect(source, backtest_index, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve Feature Effects for the model.\nFeature Effects provides partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information of source, backtest_index.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\nOne value of [FeatureEffectMetadataDatetime.sources]. To retrieve the available\nsources for feature effect.\nbacktest_index (string, FeatureEffectMetadataDatetime.backtest_index.) – The backtest index to retrieve Feature Effects for.\nReturns:\nfeature_effects – The feature effects data.\nReturn type:\nFeatureEffects\nRaises:\nClientError – If the feature effects have not been computed or source is not valid value.\nget_or_request_feature_effect(source, backtest_index, max_wait=600, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve Feature Effects computations for the model, requesting a new job if it hasn’t been run previously.\nSee get_feature_effect_metadata\nfor retrieving information of source, backtest_index.\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for a requested feature effect job to complete before erroring\nsource (string) – The source Feature Effects are retrieved for.\nOne value of [FeatureEffectMetadataDatetime.sources]. To retrieve the available sources\nfor feature effect.\nbacktest_index (string, FeatureEffectMetadataDatetime.backtest_index.) – The backtest index to retrieve Feature Effects for.\nReturns:\nfeature_effects – The feature effects data.\nReturn type:\nFeatureEffects\nrequest_feature_effects_multiclass(backtest_index, row_count=None, top_n_features=None, features=None)\nRequest feature effects to be computed for the multiclass datetime model.\nSee get_feature_effect for\nmore information on the result of the job.\nParameters:\nbacktest_index (str) – The backtest index to use for Feature Effects calculation.\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by Feature Impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features to use to calculate Feature Effects.\nReturns:\njob – A Job representing Feature Effects computation. To get the completed Feature Effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nget_feature_effects_multiclass(backtest_index, source='training', class_=None)\nRetrieve Feature Effects for the multiclass datetime model.\nFeature Effects provides partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nbacktest_index (str) – The backtest index to retrieve Feature Effects for.\nsource (str) – The source Feature Effects are retrieved for.\nclass (str or None) – The class name Feature Effects are retrieved for.\nReturns:\nThe list of multiclass Feature Effects.\nReturn type:\nlist\nRaises:\nClientError – If the Feature Effects have not been computed or source is not valid value.\nget_or_request_feature_effects_multiclass(backtest_index, source, top_n_features=None, features=None, row_count=None, class_=None, max_wait=600)\nRetrieve Feature Effects for a datetime multiclass model, and request a job if it hasn’t\nbeen run previously.\nParameters:\nbacktest_index (str) – The backtest index to retrieve Feature Effects for.\nsource (string) – The source from which Feature Effects are retrieved.\nclass (str or None) – The class name Feature Effects retrieve for.\nrow_count (int) – The number of rows used from the dataset for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by feature impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nmax_wait (Optional[int]) – The maximum time to wait for a requested feature effect job to complete before erroring.\nReturns:\nfeature_effects – The list of multiclass feature effects data.\nReturn type:\nlist of FeatureEffectsMulticlass\ncalculate_prediction_intervals(prediction_intervals_size)\nCalculate prediction intervals for this DatetimeModel for the specified size.\nAdded in version v2.19.\nParameters:\nprediction_intervals_size (int) – The prediction interval’s size to calculate for this model. See the\nprediction intervals documentation for more information.\nReturns:\njob – a Job tracking the prediction intervals computation\nReturn type:\nJob\nget_calculated_prediction_intervals(offset=None, limit=None)\nRetrieve a list of already-calculated prediction intervals for this model\nAdded in version v2.19.\nParameters:\noffset (Optional[int]) – If provided, this many results will be skipped\nlimit (Optional[int]) – If provided, at most this many results will be returned. If not provided, will return\nat most 100 results.\nReturns:\nA descending-ordered list of already-calculated prediction interval sizes\nReturn type:\nlist[int]\ncompute_datetime_trend_plots(backtest=0, source=SOURCE_TYPE.VALIDATION, forecast_distance_start=None, forecast_distance_end=None)\nComputes datetime trend plots\n(Accuracy over Time, Forecast vs Actual, Anomaly over Time) for this model\nAdded in version v2.25.\nParameters:\nbacktest (int or string, optional) – Compute plots for a specific backtest (use the backtest index starting from zero).\nTo compute plots for holdout, use dr.enums.DATA_SUBSET.HOLDOUT\nsource (string, optional) – The source of the data for the backtest/holdout.\nAttribute must be one of dr.enums.SOURCE_TYPE\nforecast_distance_start (Optional[int]:) – The start of forecast distance range (forecast window) to compute.\nIf not specified, the first forecast distance for this project will be used.\nOnly for time series supervised models\nforecast_distance_end (Optional[int]:) – The end of forecast distance range (forecast window) to compute.\nIf not specified, the last forecast distance for this project will be used.\nOnly for time series supervised models\nReturns:\njob – a Job tracking the datetime trend plots computation\nReturn type:\nJob\nNotes\nForecast distance specifies the number of time steps\nbetween the predicted point and the origin point.\nFor the multiseries models only first 1000 series in\nalphabetical order and an average plot for them will be computed.\nMaximum 100 forecast distances can be requested for\ncalculation in time series supervised projects.\nget_accuracy_over_time_plots_metadata(forecast_distance=None)\nRetrieve Accuracy over Time plots metadata for this model.\nAdded in version v2.25.\nParameters:\nforecast_distance (Optional[int]) – Forecast distance to retrieve the metadata for.\nIf not specified, the first forecast distance for this project will be used.\nOnly available for time series projects.\nReturns:\nmetadata – a AccuracyOverTimePlotsMetadata\nrepresenting Accuracy over Time plots metadata\nReturn type:\nAccuracyOverTimePlotsMetadata\nget_accuracy_over_time_plot(backtest=0, source=SOURCE_TYPE.VALIDATION, forecast_distance=None, series_id=None, resolution=None, max_bin_size=None, start_date=None, end_date=None, max_wait=600)\nRetrieve Accuracy over Time plots for this model.\nAdded in version v2.25.\nParameters:\nbacktest (int or string, optional) – Retrieve plots for a specific backtest (use the backtest index starting from zero).\nTo retrieve plots for holdout, use dr.enums.DATA_SUBSET.HOLDOUT\nsource (string, optional) – The source of the data for the backtest/holdout.\nAttribute must be one of dr.enums.SOURCE_TYPE\nforecast_distance (Optional[int]) – Forecast distance to retrieve the plots for.\nIf not specified, the first forecast distance for this project will be used.\nOnly available for time series projects.\nseries_id (string, optional) – The name of the series to retrieve for multiseries projects.\nIf not provided an average plot for the first 1000 series will be retrieved.\nresolution (string, optional) – Specifying at which resolution the data should be binned.\nIf not provided an optimal resolution will be used to\nbuild chart data with number of bins <= max_bin_size.\nOne of dr.enums.DATETIME_TREND_PLOTS_RESOLUTION.\nmax_bin_size (Optional[int]) – An int between 1 and 1000, which specifies\nthe maximum number of bins for the retrieval. Default is 500.\nstart_date (datetime.datetime, optional) – The start of the date range to return.\nIf not specified, start date for requested plot will be used.\nend_date (datetime.datetime, optional) – The end of the date range to return.\nIf not specified, end date for requested plot will be used.\nmax_wait (int or None, optional) – The maximum time to wait for a compute job to complete before retrieving the plots.\nDefault is dr.enums.DEFAULT_MAX_WAIT.\nIf 0 or None, the plots would be retrieved without attempting the computation.\nReturns:\nplot – a AccuracyOverTimePlot\nrepresenting Accuracy over Time plot\nReturn type:\nAccuracyOverTimePlot\nExamples\nimport datarobot as dr\nimport pandas as pd\nmodel = dr.DatetimeModel(project_id=project_id, id=model_id)\nplot = model.get_accuracy_over_time_plot()\ndf = pd.DataFrame.from_dict(plot.bins)\nfigure = df.plot(\"start_date\", [\"actual\", \"predicted\"]).get_figure()\nfigure.savefig(\"accuracy_over_time.png\")\nget_accuracy_over_time_plot_preview(backtest=0, source=SOURCE_TYPE.VALIDATION, forecast_distance=None, series_id=None, max_wait=600)\nRetrieve Accuracy over Time preview plots for this model.\nAdded in version v2.25.\nParameters:\nbacktest (int or string, optional) – Retrieve plots for a specific backtest (use the backtest index starting from zero).\nTo retrieve plots for holdout, use dr.enums.DATA_SUBSET.HOLDOUT\nsource (string, optional) – The source of the data for the backtest/holdout.\nAttribute must be one of dr.enums.SOURCE_TYPE\nforecast_distance (Optional[int]) – Forecast distance to retrieve the plots for.\nIf not specified, the first forecast distance for this project will be used.\nOnly available for time series projects.\nseries_id (string, optional) – The name of the series to retrieve for multiseries projects.\nIf not provided an average plot for the first 1000 series will be retrieved.\nmax_wait (int or None, optional) – The maximum time to wait for a compute job to complete before retrieving the plots.\nDefault is dr.enums.DEFAULT_MAX_WAIT.\nIf 0 or None, the plots would be retrieved without attempting the computation.\nReturns:\nplot – a AccuracyOverTimePlotPreview\nrepresenting Accuracy over Time plot preview\nReturn type:\nAccuracyOverTimePlotPreview\nExamples\nimport datarobot as dr\nimport pandas as pd\nmodel = dr.DatetimeModel(project_id=project_id, id=model_id)\nplot = model.get_accuracy_over_time_plot_preview()\ndf = pd.DataFrame.from_dict(plot.bins)\nfigure = df.plot(\"start_date\", [\"actual\", \"predicted\"]).get_figure()\nfigure.savefig(\"accuracy_over_time_preview.png\")\nget_forecast_vs_actual_plots_metadata()\nRetrieve Forecast vs Actual plots metadata for this model.\nAdded in version v2.25.\nReturns:\nmetadata – a ForecastVsActualPlotsMetadata\nrepresenting Forecast vs Actual plots metadata\nReturn type:\nForecastVsActualPlotsMetadata\nget_forecast_vs_actual_plot(backtest=0, source=SOURCE_TYPE.VALIDATION, forecast_distance_start=None, forecast_distance_end=None, series_id=None, resolution=None, max_bin_size=None, start_date=None, end_date=None, max_wait=600)\nRetrieve Forecast vs Actual plots for this model.\nAdded in version v2.25.\nParameters:\nbacktest (int or string, optional) – Retrieve plots for a specific backtest (use the backtest index starting from zero).\nTo retrieve plots for holdout, use dr.enums.DATA_SUBSET.HOLDOUT\nsource (string, optional) – The source of the data for the backtest/holdout.\nAttribute must be one of dr.enums.SOURCE_TYPE\nforecast_distance_start (Optional[int]:) – The start of forecast distance range (forecast window) to retrieve.\nIf not specified, the first forecast distance for this project will be used.\nforecast_distance_end (Optional[int]:) – The end of forecast distance range (forecast window) to retrieve.\nIf not specified, the last forecast distance for this project will be used.\nseries_id (string, optional) – The name of the series to retrieve for multiseries projects.\nIf not provided an average plot for the first 1000 series will be retrieved.\nresolution (string, optional) – Specifying at which resolution the data should be binned.\nIf not provided an optimal resolution will be used to\nbuild chart data with number of bins <= max_bin_size.\nOne of dr.enums.DATETIME_TREND_PLOTS_RESOLUTION.\nmax_bin_size (Optional[int]) – An int between 1 and 1000, which specifies\nthe maximum number of bins for the retrieval. Default is 500.\nstart_date (datetime.datetime, optional) – The start of the date range to return.\nIf not specified, start date for requested plot will be used.\nend_date (datetime.datetime, optional) – The end of the date range to return.\nIf not specified, end date for requested plot will be used.\nmax_wait (int or None, optional) – The maximum time to wait for a compute job to complete before retrieving the plots.\nDefault is dr.enums.DEFAULT_MAX_WAIT.\nIf 0 or None, the plots would be retrieved without attempting the computation.\nReturns:\nplot – a ForecastVsActualPlot\nrepresenting Forecast vs Actual plot\nReturn type:\nForecastVsActualPlot\nExamples\nimport datarobot as dr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nmodel = dr.DatetimeModel(project_id=project_id, id=model_id)\nplot = model.get_forecast_vs_actual_plot()\ndf = pd.DataFrame.from_dict(plot.bins)\n# As an example, get the forecasts for the 10th point\nforecast_point_index = 10\n# Pad the forecasts for plotting. The forecasts length must match the df length\nforecasts = [None] * forecast_point_index + df.forecasts[forecast_point_index]\nforecasts = forecasts + [None] * (len(df) - len(forecasts))\nplt.plot(df.start_date, df.actual, label=\"Actual\")\nplt.plot(df.start_date, forecasts, label=\"Forecast\")\nforecast_point = df.start_date[forecast_point_index]\nplt.title(\"Forecast vs Actual (Forecast Point {})\".format(forecast_point))\nplt.legend()\nplt.savefig(\"forecast_vs_actual.png\")\nget_forecast_vs_actual_plot_preview(backtest=0, source=SOURCE_TYPE.VALIDATION, series_id=None, max_wait=600)\nRetrieve Forecast vs Actual preview plots for this model.\nAdded in version v2.25.\nParameters:\nbacktest (int or string, optional) – Retrieve plots for a specific backtest (use the backtest index starting from zero).\nTo retrieve plots for holdout, use dr.enums.DATA_SUBSET.HOLDOUT\nsource (string, optional) – The source of the data for the backtest/holdout.\nAttribute must be one of dr.enums.SOURCE_TYPE\nseries_id (string, optional) – The name of the series to retrieve for multiseries projects.\nIf not provided an average plot for the first 1000 series will be retrieved.\nmax_wait (int or None, optional) – The maximum time to wait for a compute job to complete before retrieving the plots.\nDefault is dr.enums.DEFAULT_MAX_WAIT.\nIf 0 or None, the plots would be retrieved without attempting the computation.\nReturns:\nplot – a ForecastVsActualPlotPreview\nrepresenting Forecast vs Actual plot preview\nReturn type:\nForecastVsActualPlotPreview\nExamples\nimport datarobot as dr\nimport pandas as pd\nmodel = dr.DatetimeModel(project_id=project_id, id=model_id)\nplot = model.get_forecast_vs_actual_plot_preview()\ndf = pd.DataFrame.from_dict(plot.bins)\nfigure = df.plot(\"start_date\", [\"actual\", \"predicted\"]).get_figure()\nfigure.savefig(\"forecast_vs_actual_preview.png\")\nget_anomaly_over_time_plots_metadata()\nRetrieve Anomaly over Time plots metadata for this model.\nAdded in version v2.25.\nReturns:\nmetadata – a AnomalyOverTimePlotsMetadata\nrepresenting Anomaly over Time plots metadata\nReturn type:\nAnomalyOverTimePlotsMetadata\nget_anomaly_over_time_plot(backtest=0, source=SOURCE_TYPE.VALIDATION, series_id=None, resolution=None, max_bin_size=None, start_date=None, end_date=None, max_wait=600)\nRetrieve Anomaly over Time plots for this model.\nAdded in version v2.25.\nParameters:\nbacktest (int or string, optional) – Retrieve plots for a specific backtest (use the backtest index starting from zero).\nTo retrieve plots for holdout, use dr.enums.DATA_SUBSET.HOLDOUT\nsource (string, optional) – The source of the data for the backtest/holdout.\nAttribute must be one of dr.enums.SOURCE_TYPE\nseries_id (string, optional) – The name of the series to retrieve for multiseries projects.\nIf not provided an average plot for the first 1000 series will be retrieved.\nresolution (string, optional) – Specifying at which resolution the data should be binned.\nIf not provided an optimal resolution will be used to\nbuild chart data with number of bins <= max_bin_size.\nOne of dr.enums.DATETIME_TREND_PLOTS_RESOLUTION.\nmax_bin_size (Optional[int]) – An int between 1 and 1000, which specifies\nthe maximum number of bins for the retrieval. Default is 500.\nstart_date (datetime.datetime, optional) – The start of the date range to return.\nIf not specified, start date for requested plot will be used.\nend_date (datetime.datetime, optional) – The end of the date range to return.\nIf not specified, end date for requested plot will be used.\nmax_wait (int or None, optional) – The maximum time to wait for a compute job to complete before retrieving the plots.\nDefault is dr.enums.DEFAULT_MAX_WAIT.\nIf 0 or None, the plots would be retrieved without attempting the computation.\nReturns:\nplot – a AnomalyOverTimePlot\nrepresenting Anomaly over Time plot\nReturn type:\nAnomalyOverTimePlot\nExamples\nimport datarobot as dr\nimport pandas as pd\nmodel = dr.DatetimeModel(project_id=project_id, id=model_id)\nplot = model.get_anomaly_over_time_plot()\ndf = pd.DataFrame.from_dict(plot.bins)\nfigure = df.plot(\"start_date\", \"predicted\").get_figure()\nfigure.savefig(\"anomaly_over_time.png\")\nget_anomaly_over_time_plot_preview(prediction_threshold=0.5, backtest=0, source=SOURCE_TYPE.VALIDATION, series_id=None, max_wait=600)\nRetrieve Anomaly over Time preview plots for this model.\nAdded in version v2.25.\nParameters:\nprediction_threshold (Optional[float]) – Only bins with predictions exceeding this threshold will be returned in the response.\nbacktest (int or string, optional) – Retrieve plots for a specific backtest (use the backtest index starting from zero).\nTo retrieve plots for holdout, use dr.enums.DATA_SUBSET.HOLDOUT\nsource (string, optional) – The source of the data for the backtest/holdout.\nAttribute must be one of dr.enums.SOURCE_TYPE\nseries_id (string, optional) – The name of the series to retrieve for multiseries projects.\nIf not provided an average plot for the first 1000 series will be retrieved.\nmax_wait (int or None, optional) – The maximum time to wait for a compute job to complete before retrieving the plots.\nDefault is dr.enums.DEFAULT_MAX_WAIT.\nIf 0 or None, the plots would be retrieved without attempting the computation.\nReturns:\nplot – a AnomalyOverTimePlotPreview\nrepresenting Anomaly over Time plot preview\nReturn type:\nAnomalyOverTimePlotPreview\nExamples\nimport datarobot as dr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nmodel = dr.DatetimeModel(project_id=project_id, id=model_id)\nplot = model.get_anomaly_over_time_plot_preview(prediction_threshold=0.01)\ndf = pd.DataFrame.from_dict(plot.bins)\nx = pd.date_range(\nplot.start_date, plot.end_date, freq=df.end_date[0] - df.start_date[0]\n)\nplt.plot(x, [0] * len(x), label=\"Date range\")\nplt.plot(df.start_date, [0] * len(df.start_date), \"ro\", label=\"Anomaly\")\nplt.yticks([])\nplt.legend()\nplt.savefig(\"anomaly_over_time_preview.png\")\ninitialize_anomaly_assessment(backtest, source, series_id=None)\nInitialize the anomaly assessment insight and calculate\nShapley explanations for the most anomalous points in the subset.\nThe insight is available for anomaly detection models in time series unsupervised projects\nwhich also support calculation of Shapley values.\nParameters:\nbacktest (int starting with 0 or \"holdout\") – The backtest to compute insight for.\nsource (\"training\" or \"validation\") – The source to compute insight for.\nseries_id (string) – Required for multiseries projects. The series id to compute insight for.\nSay if there is a series column containing cities,\nthe example of the series name to pass would be “Boston”\nReturn type:\nAnomalyAssessmentRecord\nget_anomaly_assessment_records(backtest=None, source=None, series_id=None, limit=100, offset=0, with_data_only=False)\nRetrieve computed Anomaly Assessment records for this model. Model must be an anomaly\ndetection model in time series unsupervised project which also supports calculation of\nShapley values.\nRecords can be filtered by the data backtest, source and series_id.\nThe results can be limited.\nAdded in version v2.25.\nParameters:\nbacktest (int starting with 0 or \"holdout\") – The backtest of the data to filter records by.\nsource (\"training\" or \"validation\") – The source of the data to filter records by.\nseries_id (string) – The series id to filter records by.\nlimit (Optional[int])\noffset (Optional[int])\nwith_data_only (Optional[bool]) – Whether to return only records with preview and explanations available.\nFalse by default.\nReturns:\nrecords – a AnomalyAssessmentRecord\nrepresenting Anomaly Assessment Record\nReturn type:\nlist of AnomalyAssessmentRecord\nget_feature_impact(with_metadata=False, backtest=None, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the computed Feature Impact results, a measure of the relevance of each\nfeature in the model.\nFeature Impact is computed for each column by creating new data with that column randomly\npermuted (but the others left unchanged), and seeing how the error metric score for the\npredictions is affected. The ‘impactUnnormalized’ is how much worse the error metric score\nis when making predictions on this modified data. The ‘impactNormalized’ is normalized so\nthat the largest value is 1. In both cases, larger values indicate more important features.\nIf a feature is a redundant feature, i.e. once other features are considered it doesn’t\ncontribute much in addition, the ‘redundantWith’ value is the name of feature that has the\nhighest correlation with this feature. Note that redundancy detection is only available for\njobs run after the addition of this feature. When retrieving data that predates this\nfunctionality, a NoRedundancyImpactAvailable warning will be used.\nElse where this technique is sometimes called ‘Permutation Importance’.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nParameters:\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\nbacktest (int or string) – The index of the backtest unless it is holdout then it is string ‘holdout’. This is supported\nonly in DatetimeModels\ndata_slice_filter (DataSlice, optional) – (New in version v3.4) A data slice used to filter the return values based on the dataslice.id.\nBy default, this function will use data_slice_filter.id == None which returns an unsliced insight.\nIf data_slice_filter is None then get_roc_curve will raise a ValueError.\nReturns:\nThe feature impact data response depends on the with_metadata parameter. The response is\neither a dict with metadata and a list with actual data or just a list with that data.\nEach List item is a dict with the keys featureName, impactNormalized, and\nimpactUnnormalized, redundantWith and count.\nFor dict response available keys are:\nfeatureImpacts - Feature Impact data as a dictionary. Each item is a dict withkeys: featureName, impactNormalized, and impactUnnormalized, and\nredundantWith.\nshapBased - A boolean that indicates whether Feature Impact was calculated usingShapley values.\nranRedundancyDetection - A boolean that indicates whether redundant featureidentification was run while calculating this Feature Impact.\nrowCount - An integer or None that indicates the number of rows that was used tocalculate Feature Impact. For the Feature Impact calculated with the default\nlogic, without specifying the rowCount, we return None here.\ncount - An integer with the number of features under the featureImpacts.\nReturn type:\nlist or dict\nRaises:\nClientError – If the feature impacts have not been computed.\nrequest_feature_impact(row_count=None, with_metadata=False, backtest=None, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRequest feature impacts to be computed for the model.\nSee get_feature_impact for more\ninformation on the result of the job.\nParameters:\nrow_count (int) – The sample size (specified in rows) to use for Feature Impact computation. This is not\nsupported for unsupervised, multi-class (that has a separate method) and time series\nprojects.\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\nbacktest (int or string) – The index of the backtest unless it is holdout then it is string ‘holdout’. This is supported\nonly in DatetimeModels\ndata_slice_filter (DataSlice, optional) – (New in version v3.4) A data slice used to filter the return values based on the dataslice.id.\nBy default, this function will use data_slice_filter.id == None which returns an unsliced insight.\nIf data_slice_filter is None then get_roc_curve will raise a ValueError.\nReturns:\njob – A Job representing the feature impact computation. To get the completed feature impact\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nRaises:\nJobAlreadyRequested – If the feature impacts have already been requested.\nget_or_request_feature_impact(max_wait=600, row_count=None, with_metadata=False, backtest=None, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve feature impact for the model, requesting a job if it hasn’t been run previously\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for a requested feature impact job to complete before erroring\nrow_count (int) – The sample size (specified in rows) to use for Feature Impact computation. This is not\nsupported for unsupervised, multi-class (that has a separate method) and time series\nprojects.\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\nbacktest (str) – Feature Impact backtest. Can be ‘holdout’ or numbers from 0 up to max number of backtests in project.\ndata_slice_filter (DataSlice, optional) – (New in version v3.4) A data slice used to filter the return values based on the dataslice.id.\nBy default, this function will use data_slice_filter.id == None which returns an unsliced insight.\nIf data_slice_filter is None then get_roc_curve will raise a ValueError.\nReturns:\nfeature_impacts – The feature impact data. See\nget_feature_impact for the exact\nschema.\nReturn type:\nlist or dict\nrequest_lift_chart(source=None, backtest_index=None, data_slice_filter=<datarobot.models.model.Sentinel object>)\n(New in version v3.4)\nRequest the model Lift Chart for the specified backtest data slice.\nParameters:\nsource (str) – (Deprecated in version v3.4)\nLift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nIf backtest_index is present then this will be ignored.\nbacktest_index (str) – Lift chart data backtest. Can be ‘holdout’ or numbers from 0 up to max number of backtests in project.\ndata_slice_filter (DataSlice, optional) – A data slice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen request_lift_chart will raise a ValueError.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nget_lift_chart(source=None, backtest_index=None, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\n(New in version v3.4)\nRetrieve the model Lift chart for the specified backtest and data slice.\nParameters:\nsource (str) – (Deprecated in version v3.4)\nLift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nFor time series and OTV models, also accepts values backtest_2, backtest_3, …,\nup to the number of backtests in the model.\nIf backtest_index is present then this will be ignored.\nbacktest_index (str) – Lift chart data backtest. Can be ‘holdout’ or numbers from 0 up to max number of backtests in project.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A data slice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\nReturns:\nModel lift chart data\nReturn type:\nLiftChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nrequest_roc_curve(source=None, backtest_index=None, data_slice_filter=<datarobot.models.model.Sentinel object>)\n(New in version v3.4)\nRequest the binary model Roc Curve for the specified backtest and data slice.\nParameters:\nsource (str) – (Deprecated in version v3.4)\nRoc Curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nIf backtest_index is present then this will be ignored.\nbacktest_index (str) – ROC curve data backtest. Can be ‘holdout’ or numbers from 0 up to max number of backtests in project.\ndata_slice_filter (DataSlice, optional) – A data slice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen request_roc_curve will raise a ValueError.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nget_roc_curve(source=None, backtest_index=None, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\n(New in version v3.4)\nRetrieve the ROC curve for a binary model for the specified backtest and data slice.\nParameters:\nsource (str) – (Deprecated in version v3.4)\nROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nFor time series and OTV models, also accepts values backtest_2, backtest_3, …,\nup to the number of backtests in the model.\nIf backtest_index is present then this will be ignored.\nbacktest_index (str) – ROC curve data backtest. Can be ‘holdout’ or numbers from 0 up to max number of backtests in project.\nfallback_to_parent_insights (bool) – Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A data slice used to filter the return values based on the data slice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_roc_curve will raise a ValueError.\nReturns:\nModel ROC curve data\nReturn type:\nRocCurve\nRaises:\nClientError – If the insight is not available for this model\nTypeError – If the underlying project type is multilabel\nValueError – If data_slice_filter passed as None\nadvanced_tune(params, description=None)\nGenerate a new model with the specified advanced-tuning parameters\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nParameters:\nparams (dict) – Mapping of parameter ID to parameter value.\nThe list of valid parameter IDs for a model can be found by calling\nget_advanced_tuning_parameters().\nThis endpoint does not need to include values for all parameters.  If a parameter\nis omitted, its current_value will be used.\ndescription (str) – Human-readable string describing the newly advanced-tuned model\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ncontinue_incremental_learning_from_incremental_model(chunk_definition_id, early_stopping_rounds=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nchunk_definition_id (str) – The Mongo ID for the chunking service.\nearly_stopping_rounds (Optional[int]) – The number of chunks that, when no improvement has been shown, triggers the early stopping mechanism.\nReturns:\njob – The model retraining job that is created.\nReturn type:\nModelJob\ndelete()\nDelete a model from the project’s leaderboard.\nReturn type:\nNone\ndownload_scoring_code(file_name, source_code=False)\nDownload the Scoring Code JAR.\nParameters:\nfile_name (str) – File path where scoring code will be saved.\nsource_code (Optional[bool]) – Set to True to download source code archive.\nIt will not be executable.\nReturn type:\nNone\ndownload_training_artifact(file_name)\nRetrieve trained artifact(s) from a model containing one or more custom tasks.\nArtifact(s) will be downloaded to the specified local filepath.\nParameters:\nfile_name (str) – File path where trained model artifact(s) will be saved.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nget_advanced_tuning_parameters()\nGet the advanced-tuning parameters available for this model.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nA dictionary describing the advanced-tuning parameters for the current model.\nThere are two top-level keys, tuning_description and tuning_parameters.\ntuning_description an optional value. If not None, then it indicates the\nuser-specified description of this set of tuning parameter.\ntuning_parameters is a list of a dicts, each has the following keys\nparameter_name : (str) name of the parameter (unique per task, see below)\nparameter_id : (str) opaque ID string uniquely identifying parameter\ndefault_value : (*) the actual value used to train the model; either\nthe single value of the parameter specified before training, or the best\nvalue from the list of grid-searched values (based on current_value)\ncurrent_value : (*) the single value or list of values of the\nparameter that were grid searched. Depending on the grid search\nspecification, could be a single fixed value (no grid search),\na list of discrete values, or a range.\ntask_name : (str) name of the task that this parameter belongs to\nconstraints: (dict) see the notes below\nvertex_id: (str) ID of vertex that this parameter belongs to\nReturn type:\ndict\nNotes\nThe type of default_value and current_value is defined by the constraints structure.\nIt will be a string or numeric Python type.\nconstraints is a dict with at least one, possibly more, of the following keys.\nThe presence of a key indicates that the parameter may take on the specified type.\n(If a key is absent, this means that the parameter may not take on the specified type.)\nIf a key on constraints is present, its value will be a dict containing\nall of the fields described below for that key.\n\"constraints\": {\n\"select\": {\n\"values\": [<list(basestring or number) : possible values>]\n},\n\"ascii\": {},\n\"unicode\": {},\n\"int\": {\n\"min\": <int : minimum valid value>,\n\"max\": <int : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"float\": {\n\"min\": <float : minimum valid value>,\n\"max\": <float : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"intList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <int : minimum valid value>,\n\"max_val\": <int : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"floatList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <float : minimum valid value>,\n\"max_val\": <float : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n}\n}\nThe keys have meaning as follows:\nselect:\nRather than specifying a specific data type, if present, it indicates that the parameter\nis permitted to take on any of the specified values.  Listed values may be of any string\nor real (non-complex) numeric type.\nascii:\nThe parameter may be a unicode object that encodes simple ASCII characters.\n(A-Z, a-z, 0-9, whitespace, and certain common symbols.)  In addition to listed\nconstraints, ASCII keys currently may not contain either newlines or semicolons.\nunicode:\nThe parameter may be any Python unicode object.\nint:\nThe value may be an object of type int within the specified range (inclusive).\nPlease note that the value will be passed around using the JSON format, and\nsome JSON parsers have undefined behavior with integers outside of the range\n[-(2**53)+1, (2**53)-1].\nfloat:\nThe value may be an object of type float within the specified range (inclusive).\nintList, floatList:\nThe value may be a list of int or float objects, respectively, following constraints\nas specified respectively by the int and float types (above).\nMany parameters only specify one key under constraints.  If a parameter specifies multiple\nkeys, the parameter may take on any value permitted by any key.\nget_all_confusion_charts(fallback_to_parent_insights=False)\nRetrieve a list of all confusion matrices available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent for any source that is not available for this model and if this\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\nReturns:\nData for all available confusion charts for model.\nReturn type:\nlist of ConfusionChart\nget_all_feature_impacts(data_slice_filter=None)\nRetrieve a list of all feature impact results available for the model.\nParameters:\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen no data_slice filtering will be applied when requesting the roc_curve.\nReturns:\nData for all available model feature impacts. Or an empty list if not data found.\nReturn type:\nlist of dicts\nExamples\nmodel = datarobot.Model(id='model-id', project_id='project-id')\n# Get feature impact insights for sliced data\ndata_slice = datarobot.DataSlice(id='data-slice-id')\nsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get feature impact insights for unsliced data\ndata_slice = datarobot.DataSlice()\nunsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get all feature impact insights\nall_fi = model.get_all_feature_impacts()\nget_all_lift_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (Optional[bool]) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned lift chart by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model lift charts. Or an empty list if no data found.\nReturn type:\nlist of LiftChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get lift chart insights for sliced data\nsliced_lift_charts = model.get_all_lift_charts(data_slice_id='data-slice-id')\n# Get lift chart insights for unsliced data\nunsliced_lift_charts = model.get_all_lift_charts(unsliced_only=True)\n# Get all lift chart insights\nall_lift_charts = model.get_all_lift_charts()\nget_all_multiclass_lift_charts(fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nData for all available model lift charts.\nReturn type:\nlist of LiftChart\nget_all_residuals_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all residuals charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent\nfor any source that is not available for this model and if this model has a\ndefined parent model. If omitted or False, or this model has no parent, this will\nnot attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned residuals charts by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model residuals charts.\nReturn type:\nlist of ResidualsChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get residuals chart insights for sliced data\nsliced_residuals_charts = model.get_all_residuals_charts(data_slice_id='data-slice-id')\n# Get residuals chart insights for unsliced data\nunsliced_residuals_charts = model.get_all_residuals_charts(unsliced_only=True)\n# Get all residuals chart insights\nall_residuals_charts = model.get_all_residuals_charts()\nget_all_roc_curves(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all ROC curves available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – filters the returned roc_curve by data_slice_filter.id.  If None (the default) applies no filter based on\ndata_slice_id.\nReturns:\nData for all available model ROC curves. Or an empty list if no RocCurves are found.\nReturn type:\nlist of RocCurve\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\nds_filter=DataSlice(id='data-slice-id')\n# Get roc curve insights for sliced data\nsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get roc curve insights for unsliced data\ndata_slice_filter=DataSlice(id=None)\nunsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get all roc curve insights\nall_roc_curves = model.get_all_roc_curves()\nget_confusion_chart(source, fallback_to_parent_insights=False)\nRetrieve a multiclass model’s confusion matrix for the specified source.\nParameters:\nsource (str) – Confusion chart source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent if the confusion chart is not available for this model and the\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel ConfusionChart data\nReturn type:\nConfusionChart\nRaises:\nClientError – If the insight is not available for this model\nget_cross_class_accuracy_scores()\nRetrieves a list of Cross Class Accuracy scores for the model.\nReturn type:\njson\nget_data_disparity_insights(feature, class_name1, class_name2)\nRetrieve a list of Cross Class Data Disparity insights for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\nclass_name1 (str) – One of the compared classes\nclass_name2 (str) – Another compared class\nReturn type:\njson\nget_fairness_insights(fairness_metrics_set=None, offset=0, limit=100)\nRetrieve a list of Per Class Bias insights for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\noffset (Optional[int]) – Number of items to skip.\nlimit (Optional[int]) – Number of items to return.\nReturn type:\njson\nget_features_used()\nQuery the server to determine which features were used.\nNote that the data returned by this method is possibly different\nthan the names of the features in the featurelist used by this model.\nThis method will return the raw features that must be supplied in order\nfor predictions to be generated on a new set of data. The featurelist,\nin contrast, would also include the names of derived features.\nReturns:\nfeatures – The names of the features used in the model.\nReturn type:\nList[str]\nget_frozen_child_models()\nRetrieve the IDs for all models that are frozen from this model.\nReturn type:\nA list of Models\nget_labelwise_roc_curves(source, fallback_to_parent_insights=False)\nRetrieve a list of LabelwiseRocCurve instances for a multilabel model for the given source and all labels.\nThis method is valid only for multilabel projects. For binary projects, use Model.get_roc_curve API .\nAdded in version v2.24.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\nReturns:\nLabelwise ROC Curve instances for source and all labels\nReturn type:\nlist of LabelwiseRocCurve\nRaises:\nClientError – If the insight is not available for this model\nget_missing_report_info()\nRetrieve a report on missing training data that can be used to understand missing\nvalues treatment in the model. The report consists of missing values resolutions for\nfeatures numeric or categorical features that were part of building the model.\nReturns:\nThe queried model missing report, sorted by missing count (DESCENDING order).\nReturn type:\nAn iterable of MissingReportPerFeature\nget_model_blueprint_chart()\nRetrieve a diagram that can be used to understand\ndata flow in the blueprint.\nReturns:\nThe queried model blueprint chart.\nReturn type:\nModelBlueprintChart\nget_model_blueprint_documents()\nGet documentation for tasks used in this model.\nReturns:\nAll documents available for the model.\nReturn type:\nlist of BlueprintTaskDocument\nget_model_blueprint_json()\nGet the blueprint json representation used by this model.\nReturns:\nJson representation of the blueprint stages.\nReturn type:\nBlueprintJson\nget_multiclass_feature_impact()\nFor multiclass it’s possible to calculate feature impact separately for each target class.\nThe method for calculation is exactly the same, calculated in one-vs-all style for each\ntarget class.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. Each item is a dict with the keys ‘featureImpacts’ (list),\n‘class’ (str). Each item in ‘featureImpacts’ is a dict with the keys ‘featureName’,\n‘impactNormalized’, and ‘impactUnnormalized’, and ‘redundantWith’.\nReturn type:\nlist of dict\nRaises:\nClientError – If the multiclass feature impacts have not been computed.\nget_multiclass_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_multilabel_lift_charts(source, fallback_to_parent_insights=False)\nRetrieve model Lift charts for the specified source.\nAdded in version v2.24.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_num_iterations_trained()\nRetrieves the number of estimators trained by early-stopping tree-based models.\n– versionadded:: v2.22\nReturns:\nprojectId (str) – id of project containing the model\nmodelId (str) – id of the model\ndata (array) – list of numEstimatorsItem objects, one for each modeling stage.\nnumEstimatorsItem will be of the form\nstage (str) – indicates the modeling stage (for multi-stage models); None of single-stage models\nnumIterations (int) – the number of estimators or iterations trained by the model\nget_parameters()\nRetrieve model parameters.\nReturns:\nModel parameters for this model.\nReturn type:\nModelParameters\nget_pareto_front()\nRetrieve the Pareto Front for a Eureqa model.\nThis method is only supported for Eureqa models.\nReturns:\nModel ParetoFront data\nReturn type:\nParetoFront\nget_prime_eligibility()\nCheck if this model can be approximated with DataRobot Prime\nReturns:\nprime_eligibility – a dict indicating whether a model can be approximated with DataRobot Prime\n(key can_make_prime) and why it may be ineligible (key message)\nReturn type:\ndict\nget_residuals_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible\nvalues.\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent if\nthe residuals chart is not available for this model and the model has a defined parent\nmodel. If omitted or False, or there is no parent model, will not attempt to return\nresiduals data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_residuals_chart will raise a ValueError.\nReturns:\nModel residuals chart data\nReturn type:\nResidualsChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_rulesets()\nList the rulesets approximating this model generated by DataRobot Prime\nIf this model hasn’t been approximated yet, will return an empty list.  Note that these\nare rulesets approximating this model, not rulesets used to construct this model.\nReturns:\nrulesets\nReturn type:\nlist of Ruleset\nget_supported_capabilities()\nRetrieves a summary of the capabilities supported by a model.\nAdded in version v2.14.\nReturns:\nsupportsBlending (bool) – whether the model supports blending\nsupportsMonotonicConstraints (bool) – whether the model supports monotonic constraints\nhasWordCloud (bool) – whether the model has word cloud data available\neligibleForPrime (bool) – (Deprecated in version v3.6)\nwhether the model is eligible for Prime\nhasParameters (bool) – whether the model has parameters that can be retrieved\nsupportsCodeGeneration (bool) – (New in version v2.18) whether the model supports code generation\nsupportsShap (bool) –\n(New in version v2.18) True if the model supports Shapley package. i.e. Shapley basedfeature Importance\nsupportsEarlyStopping (bool) – (New in version v2.22) True if this is an early stopping\ntree-based model and number of trained iterations can be retrieved.\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this model at leaderboard.\nReturn type:\nstr\nget_word_cloud(exclude_stop_words=False)\nRetrieve word cloud data for the model.\nParameters:\nexclude_stop_words (Optional[bool]) – Set to True if you want stopwords filtered out of response.\nReturns:\nWord cloud data for the model.\nReturn type:\nWordCloud\nincremental_train(data_stage_id, training_data_name=None)\nSubmit a job to the queue to perform incremental training on an existing model.\nSee train_incremental documentation.\nReturn type:\nModelJob\nclassmethod list(project_id, sort_by_partition='validation', sort_by_metric=None, with_metric=None, search_term=None, featurelists=None, families=None, blueprints=None, labels=None, characteristics=None, training_filters=None, number_of_clusters=None, limit=100, offset=0)\nRetrieve paginated model records, sorted by scores, with optional filtering.\nParameters:\nsort_by_partition (str, one of validation, backtesting, crossValidation or holdout) – Set the partition to use for sorted (by score) list of models. validation is the default.\nsort_by_metric (str) – Set the project metric to use for model sorting. DataRobot-selected project optimization metric\nis the default.\nwith_metric (str) – For a single-metric list of results, specify that project metric.\nsearch_term (str) – If specified, only models containing the term in their name or processes are returned.\nfeaturelists (List[str]) – If specified, only models trained on selected featurelists are returned.\nfamilies (List[str]) – If specified, only models belonging to selected families are returned.\nblueprints (List[str]) – If specified, only models trained on specified blueprint IDs are returned.\nlabels (List[str], starred or prepared for deployment) – If specified, only models tagged with all listed labels are returned.\ncharacteristics (List[str]) – If specified, only models matching all listed characteristics are returned.\ntraining_filters (List[str]) – If specified, only models matching at least one of the listed training conditions are returned.\nThe following formats are supported for autoML and datetime partitioned projects:\n- number of rows in training subset\nFor datetime partitioned projects:\n- <training duration>, example P6Y0M0D\n- <training_duration>-<time_window_sample_percent>-<sampling_method> Example: P6Y0M0D-78-Random,\n(returns models trained on 6 years of data, sampling rate 78%, random sampling).\n- Start/end date\n- Project settings\nnumber_of_clusters (list of int) – Filter models by number of clusters. Applicable only in unsupervised clustering projects.\nlimit (int)\noffset (int)\nReturns:\ngeneric_models\nReturn type:\nlist of GenericModel\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nrequest_approximation()\nRequest an approximation of this model using DataRobot Prime\nThis will create several rulesets that could be used to approximate this model.  After\ncomparing their scores and rule counts, the code used in the approximation can be downloaded\nand run locally.\nReturns:\njob – the job generating the rulesets\nReturn type:\nJob\nrequest_cross_class_accuracy_scores()\nRequest data disparity insights to be computed for the model.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_data_disparity_insights(feature, compared_class_names)\nRequest data disparity insights to be computed for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\ncompared_class_names (list(str)) – List of two classes to compare\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_external_test(dataset_id, actual_value_column=None)\nRequest external test to compute scores and insights on an external test dataset\nParameters:\ndataset_id (string) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nReturns:\njob – a Job representing external dataset insights computation\nReturn type:\nJob\nrequest_fairness_insights(fairness_metrics_set=None)\nRequest fairness insights to be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_frozen_datetime_model(training_row_count=None, training_duration=None, training_start_date=None, training_end_date=None, time_window_sample_pct=None, sampling_method=None)\nTrain a new frozen model with parameters from this model.\nRequires that this model belongs to a datetime partitioned project.  If it does not, an\nerror will occur when submitting the job.\nFrozen models use the same tuning parameters as their parent model instead of independently\noptimizing them to allow efficiently retraining models on larger amounts of the training\ndata.\nIn addition of training_row_count and training_duration, frozen datetime models may be\ntrained on an exact date range.  Only one of training_row_count, training_duration, or\ntraining_start_date and training_end_date should be specified.\nModels specified using training_start_date and training_end_date are the only ones that can\nbe trained into the holdout data (once the holdout is unlocked).\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\ntraining_duration may not be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, training_row_count may not be specified.\ntraining_start_date (datetime.datetime, optional) – the start date of the data to train to model on.  Only rows occurring at or after\nthis datetime will be used.  If training_start_date is specified, training_end_date\nmust also be specified.\ntraining_end_date (datetime.datetime, optional) – the end date of the data to train the model on.  Only rows occurring strictly before\nthis datetime will be used.  If training_end_date is specified, training_start_date\nmust also be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates).  An integer between 1 and 99 indicating the percentage to sample by\nwithin the window.  The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nReturns:\nmodel_job – the modeling job training a frozen model\nReturn type:\nModelJob\nrequest_per_class_fairness_insights(fairness_metrics_set=None)\nRequest per-class fairness insights be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – The fairness metric used to calculate the fairness scores.\nValue can be any one of <datarobot.enums.FairnessMetricsSet>.\nReturns:\nstatus_check_job – The returned object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_predictions(dataset_id=None, dataset=None, dataframe=None, file_path=None, file=None, include_prediction_intervals=None, prediction_intervals_size=None, forecast_point=None, predictions_start_date=None, predictions_end_date=None, actual_value_column=None, explanation_algorithm=None, max_explanations=None, max_ngram_explanations=None)\nRequests predictions against a previously uploaded dataset.\nParameters:\ndataset_id (string, optional) – The ID of the dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataset (Dataset, optional) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataframe (pd.DataFrame, optional) – (New in v3.0)\nThe dataframe to make predictions against\nfile_path (Optional[str]) – (New in v3.0)\nPath to file to make predictions against\nfile (IOBase, optional) – (New in v3.0)\nFile to make predictions against\ninclude_prediction_intervals (Optional[bool]) – (New in v2.16) For time series projects only.\nSpecifies whether prediction intervals should be calculated for this request. Defaults\nto True if prediction_intervals_size is specified, otherwise defaults to False.\nprediction_intervals_size (Optional[int]) – (New in v2.16) For time series projects only.\nRepresents the percentile to use for the size of the prediction intervals. Defaults to\n80 if include_prediction_intervals is True. Prediction intervals size must be\nbetween 1 and 100 (inclusive).\nforecast_point (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. This is the default point relative\nto which predictions will be generated, based on the forecast window of the project. See\nthe time series prediction documentation for more\ninformation.\npredictions_start_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The end date for bulk\npredictions, exclusive. Note that this parameter is for generating historical\npredictions using the training data. This parameter should be provided in conjunction\nwith predictions_start_date. Can’t be provided with the\nforecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nexplanation_algorithm ((New in version v2.21) optional; If set to 'shap', the) – response will include prediction explanations based on the SHAP explainer (SHapley\nAdditive exPlanations). Defaults to null (no prediction explanations).\nmax_explanations ((New in version v2.21) int optional; specifies the maximum number of) – explanation values that should be returned for each row, ordered by absolute value,\ngreatest to least. If null, no limit. In the case of ‘shap’: if the number of features\nis greater than the limit, the sum of remaining values will also be returned as\nshapRemainingTotal. Defaults to null. Cannot be set if explanation_algorithm is\nomitted.\nmax_ngram_explanations (optional;  int or str) – (New in version v2.29) Specifies the maximum number of text explanation values that\nshould be returned. If set to all, text explanations will be computed and all the\nngram explanations will be returned. If set to a non zero positive integer value, text\nexplanations will be computed and this amount of descendingly sorted ngram explanations\nwill be returned. By default text explanation won’t be triggered to be computed.\nReturns:\njob – The job computing the predictions\nReturn type:\nPredictJob\nrequest_residuals_chart(source, data_slice_id=None)\nRequest the model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nset_prediction_threshold(threshold)\nSet a custom prediction threshold for the model.\nMay not be used once prediction_threshold_read_only is True for this model.\nParameters:\nthreshold (float) – only used for binary classification projects. The threshold to when deciding between\nthe positive and negative classes when making predictions.  Should be between 0.0 and\n1.0 (inclusive).\nstar_model()\nMark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nstart_advanced_tuning_session()\nStart an Advanced Tuning session.  Returns an object that helps\nset up arguments for an Advanced Tuning model execution.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nSession for setting up and running Advanced Tuning on a model\nReturn type:\nAdvancedTuningSession\nstart_incremental_learning_from_sample(early_stopping_rounds=None, first_iteration_only=False, chunk_definition_id=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nearly_stopping_rounds (Optional[int]) – The number of chunks in which no improvement is observed that triggers the early stopping mechanism.\nfirst_iteration_only (bool) – Specifies whether incremental learning training should be limited to the first\niteration. If set to True, the training process will be performed only for the first\niteration. If set to False, training will continue until early stopping conditions\nare met or the maximum number of iterations is reached. The default value is False.\nchunk_definition_id (str) – The id of the chunk definition to be use for incremental training.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\ntrain_datetime(featurelist_id=None, training_row_count=None, training_duration=None, time_window_sample_pct=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>, use_project_settings=False, sampling_method=None, n_clusters=None)\nTrains this model on a different featurelist or sample size.\nRequires that this model is part of a datetime partitioned project; otherwise, an error will\noccur.\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\nfeaturelist_id (Optional[str]) – the featurelist to use to train the model.  If not specified, the featurelist of this\nmodel is used.\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\nneither training_duration nor use_project_settings may be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, neither training_row_count nor use_project_settings may be\nspecified.\nuse_project_settings (Optional[bool]) – (New in version v2.20) defaults to False. If True, indicates that the custom\nbacktest partitioning settings specified by the user will be used to train the model and\nevaluate backtest scores. If specified, neither training_row_count nor\ntraining_duration may be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates). An integer between 1 and 99 indicating the percentage to sample by\nwithin the window. The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nmonotonic_increasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nn_clusters (Optional[int]) – (New in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that don’t automatically\ndetermine the number of clusters.\nReturns:\njob – the created job to build the model\nReturn type:\nModelJob\ntrain_incremental(data_stage_id, training_data_name=None, data_stage_encoding=None, data_stage_delimiter=None, data_stage_compression=None)\nSubmit a job to the queue to perform incremental training on an existing model using\nadditional data. The id of the additional data to use for training is specified with the data_stage_id.\nOptionally a name for the iteration can be supplied by the user to help identify the contents of data in\nthe iteration.\nThis functionality requires the INCREMENTAL_LEARNING feature flag to be enabled.\nParameters:\ndata_stage_id (str) – The id of the data stage to use for training.\ntraining_data_name (Optional[str]) – The name of the iteration or data stage to indicate what the incremental learning was performed on.\ndata_stage_encoding (Optional[str]) – The encoding type of the data in the data stage (default: UTF-8).\nSupported formats: UTF-8, ASCII, WINDOWS1252\ndata_stage_encoding – The delimiter used by the data in the data stage (default: ‘,’).\ndata_stage_compression (Optional[str]) – The compression type of the data stage file, e.g. ‘zip’ (default: None).\nSupported formats: zip\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nunstar_model()\nUnmark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nFrozen models\nclass datarobot.models.FrozenModel\nRepresents a model tuned with parameters which are derived from another model\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nid (str) – the id of the model\nproject_id (str) – the id of the project the model belongs to\nprocesses (List[str]) – the processes used by the model\nfeaturelist_name (str) – the name of the featurelist used by the model\nfeaturelist_id (str) – the id of the featurelist used by the model\nsample_pct (float) – the percentage of the project dataset used in training the model\ntraining_row_count (int or None) – the number of rows of the project dataset used in training the model.  In a datetime\npartitioned project, if specified, defines the number of rows used to train the model and\nevaluate backtest scores; if unspecified, either training_duration or\ntraining_start_date and training_end_date was used to determine that instead.\ntraining_duration (str or None) – only present for models in datetime partitioned projects.  If specified, a duration string\nspecifying the duration spanned by the data used to train the model and evaluate backtest\nscores.\ntraining_start_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the start\ndate of the data used to train the model.\ntraining_end_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the end\ndate of the data used to train the model.\nmodel_type (str) – what model this is, e.g. ‘Nystroem Kernel SVM Regressor’\nmodel_category (str) – what kind of model this is - ‘prime’ for DataRobot Prime models, ‘blend’ for blender models,\nand ‘model’ for other models\nis_frozen (bool) – whether this model is a frozen model\nparent_model_id (str) – the id of the model that tuning parameters are derived from\nblueprint_id (str) – the id of the blueprint used in this model\nmetrics (dict) – a mapping from each metric to the model’s scores for that metric\nmonotonic_increasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically increasing relationship to the target.\nIf None, no such constraints are enforced.\nmonotonic_decreasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically decreasing relationship to the target.\nIf None, no such constraints are enforced.\nsupports_monotonic_constraints (bool) – optional, whether this model supports enforcing monotonic constraints\nis_starred (bool) – whether this model marked as starred\nprediction_threshold (float) – for binary classification projects, the threshold used for predictions\nprediction_threshold_read_only (bool) – indicated whether modification of the prediction threshold is forbidden. Threshold\nmodification is forbidden once a model has had a deployment created or predictions made via\nthe dedicated prediction API.\nmodel_number (integer) – model number assigned to a model\nsupports_composable_ml (bool or None) – (New in version v2.26)\nwhether this model is supported in the Composable ML.\nclassmethod get(project_id, model_id)\nRetrieve a specific frozen model.\nParameters:\nproject_id (str) – The project’s id.\nmodel_id (str) – The model_id of the leaderboard item to retrieve.\nReturns:\nmodel – The queried instance.\nReturn type:\nFrozenModel\nRating table models\nclass datarobot.models.RatingTableModel\nA model that has a rating table.\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nid (str) – the id of the model\nproject_id (str) – the id of the project the model belongs to\nprocesses (List[str]) – the processes used by the model\nfeaturelist_name (str) – the name of the featurelist used by the model\nfeaturelist_id (str) – the id of the featurelist used by the model\nsample_pct (float or None) – the percentage of the project dataset used in training the model.  If the project uses\ndatetime partitioning, the sample_pct will be None.  See training_row_count,\ntraining_duration, and training_start_date and training_end_date instead.\ntraining_row_count (int or None) – the number of rows of the project dataset used in training the model.  In a datetime\npartitioned project, if specified, defines the number of rows used to train the model and\nevaluate backtest scores; if unspecified, either training_duration or\ntraining_start_date and training_end_date was used to determine that instead.\ntraining_duration (str or None) – only present for models in datetime partitioned projects.  If specified, a duration string\nspecifying the duration spanned by the data used to train the model and evaluate backtest\nscores.\ntraining_start_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the start\ndate of the data used to train the model.\ntraining_end_date (datetime or None) – only present for frozen models in datetime partitioned projects.  If specified, the end\ndate of the data used to train the model.\nmodel_type (str) – what model this is, e.g. ‘Nystroem Kernel SVM Regressor’\nmodel_category (str) – what kind of model this is - ‘prime’ for DataRobot Prime models, ‘blend’ for blender models,\nand ‘model’ for other models\nis_frozen (bool) – whether this model is a frozen model\nblueprint_id (str) – the id of the blueprint used in this model\nmetrics (dict) – a mapping from each metric to the model’s scores for that metric\nrating_table_id (str) – the id of the rating table that belongs to this model\nmonotonic_increasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically increasing relationship to the target.\nIf None, no such constraints are enforced.\nmonotonic_decreasing_featurelist_id (str) – optional, the id of the featurelist that defines the set of features with\na monotonically decreasing relationship to the target.\nIf None, no such constraints are enforced.\nsupports_monotonic_constraints (bool) – optional, whether this model supports enforcing monotonic constraints\nis_starred (bool) – whether this model marked as starred\nprediction_threshold (float) – for binary classification projects, the threshold used for predictions\nprediction_threshold_read_only (bool) – indicated whether modification of the prediction threshold is forbidden. Threshold\nmodification is forbidden once a model has had a deployment created or predictions made via\nthe dedicated prediction API.\nmodel_number (integer) – model number assigned to a model\nsupports_composable_ml (bool or None) – (New in version v2.26)\nwhether this model is supported in the Composable ML.\nclassmethod get(project_id, model_id)\nRetrieve a specific rating table model\nIf the project does not have a rating table, a ClientError will occur.\nParameters:\nproject_id (str) – the id of the project the model belongs to\nmodel_id (str) – the id of the model to retrieve\nReturns:\nmodel – the model\nReturn type:\nRatingTableModel\nclassmethod create_from_rating_table(project_id, rating_table_id)\nCreates a new model from a validated rating table record. The\nRatingTable must not be associated with an existing model.\nParameters:\nproject_id (str) – the id of the project the rating table belongs to\nrating_table_id (str) – the id of the rating table to create this model from\nReturns:\njob – an instance of created async job\nReturn type:\nJob\nRaises:\nClientError – Raised if creating model from a RatingTable that failed validation\nJobAlreadyRequested – Raised if creating model from a RatingTable that is already\nassociated with a RatingTableModel\nadvanced_tune(params, description=None)\nGenerate a new model with the specified advanced-tuning parameters\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nParameters:\nparams (dict) – Mapping of parameter ID to parameter value.\nThe list of valid parameter IDs for a model can be found by calling\nget_advanced_tuning_parameters().\nThis endpoint does not need to include values for all parameters.  If a parameter\nis omitted, its current_value will be used.\ndescription (str) – Human-readable string describing the newly advanced-tuned model\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ncontinue_incremental_learning_from_incremental_model(chunk_definition_id, early_stopping_rounds=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nchunk_definition_id (str) – The Mongo ID for the chunking service.\nearly_stopping_rounds (Optional[int]) – The number of chunks that, when no improvement has been shown, triggers the early stopping mechanism.\nReturns:\njob – The model retraining job that is created.\nReturn type:\nModelJob\ncross_validate()\nRun cross validation on the model.\nNotes\nTo perform Cross Validation on a new model with new parameters, use train instead.\nReturns:\nThe created job to build the model\nReturn type:\nModelJob\ndelete()\nDelete a model from the project’s leaderboard.\nReturn type:\nNone\ndownload_scoring_code(file_name, source_code=False)\nDownload the Scoring Code JAR.\nParameters:\nfile_name (str) – File path where scoring code will be saved.\nsource_code (Optional[bool]) – Set to True to download source code archive.\nIt will not be executable.\nReturn type:\nNone\ndownload_training_artifact(file_name)\nRetrieve trained artifact(s) from a model containing one or more custom tasks.\nArtifact(s) will be downloaded to the specified local filepath.\nParameters:\nfile_name (str) – File path where trained model artifact(s) will be saved.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nOverrides the inherited method since the model must _not_ recursively change casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (list) – List of attribute namespaces like: [‘top.middle.bottom’], that should be kept\neven if their values are None\nget_advanced_tuning_parameters()\nGet the advanced-tuning parameters available for this model.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nA dictionary describing the advanced-tuning parameters for the current model.\nThere are two top-level keys, tuning_description and tuning_parameters.\ntuning_description an optional value. If not None, then it indicates the\nuser-specified description of this set of tuning parameter.\ntuning_parameters is a list of a dicts, each has the following keys\nparameter_name : (str) name of the parameter (unique per task, see below)\nparameter_id : (str) opaque ID string uniquely identifying parameter\ndefault_value : (*) the actual value used to train the model; either\nthe single value of the parameter specified before training, or the best\nvalue from the list of grid-searched values (based on current_value)\ncurrent_value : (*) the single value or list of values of the\nparameter that were grid searched. Depending on the grid search\nspecification, could be a single fixed value (no grid search),\na list of discrete values, or a range.\ntask_name : (str) name of the task that this parameter belongs to\nconstraints: (dict) see the notes below\nvertex_id: (str) ID of vertex that this parameter belongs to\nReturn type:\ndict\nNotes\nThe type of default_value and current_value is defined by the constraints structure.\nIt will be a string or numeric Python type.\nconstraints is a dict with at least one, possibly more, of the following keys.\nThe presence of a key indicates that the parameter may take on the specified type.\n(If a key is absent, this means that the parameter may not take on the specified type.)\nIf a key on constraints is present, its value will be a dict containing\nall of the fields described below for that key.\n\"constraints\": {\n\"select\": {\n\"values\": [<list(basestring or number) : possible values>]\n},\n\"ascii\": {},\n\"unicode\": {},\n\"int\": {\n\"min\": <int : minimum valid value>,\n\"max\": <int : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"float\": {\n\"min\": <float : minimum valid value>,\n\"max\": <float : maximum valid value>,\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"intList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <int : minimum valid value>,\n\"max_val\": <int : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n},\n\"floatList\": {\n\"min_length\": <int : minimum valid length>,\n\"max_length\": <int : maximum valid length>\n\"min_val\": <float : minimum valid value>,\n\"max_val\": <float : maximum valid value>\n\"supports_grid_search\": <bool : True if Grid Search may be\nrequested for this param>\n}\n}\nThe keys have meaning as follows:\nselect:\nRather than specifying a specific data type, if present, it indicates that the parameter\nis permitted to take on any of the specified values.  Listed values may be of any string\nor real (non-complex) numeric type.\nascii:\nThe parameter may be a unicode object that encodes simple ASCII characters.\n(A-Z, a-z, 0-9, whitespace, and certain common symbols.)  In addition to listed\nconstraints, ASCII keys currently may not contain either newlines or semicolons.\nunicode:\nThe parameter may be any Python unicode object.\nint:\nThe value may be an object of type int within the specified range (inclusive).\nPlease note that the value will be passed around using the JSON format, and\nsome JSON parsers have undefined behavior with integers outside of the range\n[-(2**53)+1, (2**53)-1].\nfloat:\nThe value may be an object of type float within the specified range (inclusive).\nintList, floatList:\nThe value may be a list of int or float objects, respectively, following constraints\nas specified respectively by the int and float types (above).\nMany parameters only specify one key under constraints.  If a parameter specifies multiple\nkeys, the parameter may take on any value permitted by any key.\nget_all_confusion_charts(fallback_to_parent_insights=False)\nRetrieve a list of all confusion matrices available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent for any source that is not available for this model and if this\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\nReturns:\nData for all available confusion charts for model.\nReturn type:\nlist of ConfusionChart\nget_all_feature_impacts(data_slice_filter=None)\nRetrieve a list of all feature impact results available for the model.\nParameters:\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen no data_slice filtering will be applied when requesting the roc_curve.\nReturns:\nData for all available model feature impacts. Or an empty list if not data found.\nReturn type:\nlist of dicts\nExamples\nmodel = datarobot.Model(id='model-id', project_id='project-id')\n# Get feature impact insights for sliced data\ndata_slice = datarobot.DataSlice(id='data-slice-id')\nsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get feature impact insights for unsliced data\ndata_slice = datarobot.DataSlice()\nunsliced_fi = model.get_all_feature_impacts(data_slice_filter=data_slice)\n# Get all feature impact insights\nall_fi = model.get_all_feature_impacts()\nget_all_lift_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (Optional[bool]) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned lift chart by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model lift charts. Or an empty list if no data found.\nReturn type:\nlist of LiftChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get lift chart insights for sliced data\nsliced_lift_charts = model.get_all_lift_charts(data_slice_id='data-slice-id')\n# Get lift chart insights for unsliced data\nunsliced_lift_charts = model.get_all_lift_charts(unsliced_only=True)\n# Get all lift chart insights\nall_lift_charts = model.get_all_lift_charts()\nget_all_multiclass_lift_charts(fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve a list of all Lift charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nData for all available model lift charts.\nReturn type:\nlist of LiftChart\nget_all_residuals_charts(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all residuals charts available for the model.\nParameters:\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent\nfor any source that is not available for this model and if this model has a\ndefined parent model. If omitted or False, or this model has no parent, this will\nnot attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – Filters the returned residuals charts by data_slice_filter.id.\nIf None (the default) applies no filter based on data_slice_id.\nReturns:\nData for all available model residuals charts.\nReturn type:\nlist of ResidualsChart\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\n# Get residuals chart insights for sliced data\nsliced_residuals_charts = model.get_all_residuals_charts(data_slice_id='data-slice-id')\n# Get residuals chart insights for unsliced data\nunsliced_residuals_charts = model.get_all_residuals_charts(unsliced_only=True)\n# Get all residuals chart insights\nall_residuals_charts = model.get_all_residuals_charts()\nget_all_roc_curves(fallback_to_parent_insights=False, data_slice_filter=None)\nRetrieve a list of all ROC curves available for the model.\nParameters:\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent for any source that is not available for this model and if this model\nhas a defined parent model. If omitted or False, or this model has no parent,\nthis will not attempt to retrieve any data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – filters the returned roc_curve by data_slice_filter.id.  If None (the default) applies no filter based on\ndata_slice_id.\nReturns:\nData for all available model ROC curves. Or an empty list if no RocCurves are found.\nReturn type:\nlist of RocCurve\nExamples\nmodel = datarobot.Model.get('project-id', 'model-id')\nds_filter=DataSlice(id='data-slice-id')\n# Get roc curve insights for sliced data\nsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get roc curve insights for unsliced data\ndata_slice_filter=DataSlice(id=None)\nunsliced_roc = model.get_all_roc_curves(data_slice_filter=ds_filter)\n# Get all roc curve insights\nall_roc_curves = model.get_all_roc_curves()\nget_confusion_chart(source, fallback_to_parent_insights=False)\nRetrieve a multiclass model’s confusion matrix for the specified source.\nParameters:\nsource (str) – Confusion chart source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return confusion chart data for\nthis model’s parent if the confusion chart is not available for this model and the\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel ConfusionChart data\nReturn type:\nConfusionChart\nRaises:\nClientError – If the insight is not available for this model\nget_cross_class_accuracy_scores()\nRetrieves a list of Cross Class Accuracy scores for the model.\nReturn type:\njson\nget_cross_validation_scores(partition=None, metric=None)\nReturn a dictionary, keyed by metric, showing cross validation\nscores per partition.\nCross Validation should already have been performed using\ncross_validate or\ntrain.\nNotes\nModels that computed cross validation before this feature was added will need\nto be deleted and retrained before this method can be used.\nParameters:\npartition (float) – optional, the id of the partition (1,2,3.0,4.0,etc…) to filter results by\ncan be a whole number positive integer or float value. 0 corresponds to the\nvalidation partition.\nmetric (unicode) – optional name of the metric to filter to resulting cross validation scores by\nReturns:\ncross_validation_scores – A dictionary keyed by metric showing cross validation scores per\npartition.\nReturn type:\ndict\nget_data_disparity_insights(feature, class_name1, class_name2)\nRetrieve a list of Cross Class Data Disparity insights for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\nclass_name1 (str) – One of the compared classes\nclass_name2 (str) – Another compared class\nReturn type:\njson\nget_fairness_insights(fairness_metrics_set=None, offset=0, limit=100)\nRetrieve a list of Per Class Bias insights for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\noffset (Optional[int]) – Number of items to skip.\nlimit (Optional[int]) – Number of items to return.\nReturn type:\njson\nget_feature_effect(source, data_slice_id=None)\nRetrieve Feature Effects for the model.\nFeature Effects provides partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, retrieve unsliced insight data.\nReturns:\nfeature_effects – The feature effects data.\nReturn type:\nFeatureEffects\nRaises:\nClientError – If the feature effects have not been computed or source is not valid value.\nget_feature_effect_metadata()\nRetrieve Feature Effects metadata. Response contains status and available model sources.\nFeature Effect for the training partition is always available, with the exception of older\nprojects that only supported Feature Effect for validation.\nWhen a model is trained into validation or holdout without stacked predictions\n(i.e., no out-of-sample predictions in those partitions),\nFeature Effects is not available for validation or holdout.\nFeature Effects for holdout is not available when holdout was not unlocked for\nthe project.\nUse source to retrieve Feature Effects, selecting one of the provided sources.\nReturns:\nfeature_effect_metadata\nReturn type:\nFeatureEffectMetadata\nget_feature_effects_multiclass(source='training', class_=None)\nRetrieve Feature Effects for the multiclass model.\nFeature Effects provide partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how,\nholding all other variables except the feature of interest as they were,\nthe value of this feature affects your prediction.\nRequires that Feature Effects has already been computed with\nrequest_feature_effect.\nSee get_feature_effect_metadata\nfor retrieving information the available sources.\nParameters:\nsource (str) – The source Feature Effects are retrieved for.\nclass (str or None) – The class name Feature Effects are retrieved for.\nReturns:\nThe list of multiclass feature effects.\nReturn type:\nlist\nRaises:\nClientError – If Feature Effects have not been computed or source is not valid value.\nget_feature_impact(with_metadata=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the computed Feature Impact results, a measure of the relevance of each\nfeature in the model.\nFeature Impact is computed for each column by creating new data with that column randomly\npermuted (but the others left unchanged), and seeing how the error metric score for the\npredictions is affected. The ‘impactUnnormalized’ is how much worse the error metric score\nis when making predictions on this modified data. The ‘impactNormalized’ is normalized so\nthat the largest value is 1. In both cases, larger values indicate more important features.\nIf a feature is a redundant feature, i.e. once other features are considered it doesn’t\ncontribute much in addition, the ‘redundantWith’ value is the name of feature that has the\nhighest correlation with this feature. Note that redundancy detection is only available for\njobs run after the addition of this feature. When retrieving data that predates this\nfunctionality, a NoRedundancyImpactAvailable warning will be used.\nElsewhere this technique is sometimes called ‘Permutation Importance’.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nParameters:\nwith_metadata (bool) – The flag indicating if the result should include the metadata as well.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default, this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_feature_impact will raise a ValueError.\nReturns:\nThe feature impact data response depends on the with_metadata parameter. The response is\neither a dict with metadata and a list with actual data or just a list with that data.\nEach List item is a dict with the keys featureName, impactNormalized, and\nimpactUnnormalized, redundantWith and count.\nFor dict response available keys are:\nfeatureImpacts - Feature Impact data as a dictionary. Each item is a dict withkeys: featureName, impactNormalized, and impactUnnormalized, and\nredundantWith.\nshapBased - A boolean that indicates whether Feature Impact was calculated usingShapley values.\nranRedundancyDetection - A boolean that indicates whether redundant featureidentification was run while calculating this Feature Impact.\nrowCount - An integer or None that indicates the number of rows that was used tocalculate Feature Impact. For the Feature Impact calculated with the default\nlogic, without specifying the rowCount, we return None here.\ncount - An integer with the number of features under the featureImpacts.\nReturn type:\nlist or dict\nRaises:\nClientError – If the feature impacts have not been computed.\nValueError – If data_slice_filter passed as None\nget_features_used()\nQuery the server to determine which features were used.\nNote that the data returned by this method is possibly different\nthan the names of the features in the featurelist used by this model.\nThis method will return the raw features that must be supplied in order\nfor predictions to be generated on a new set of data. The featurelist,\nin contrast, would also include the names of derived features.\nReturns:\nfeatures – The names of the features used in the model.\nReturn type:\nList[str]\nget_frozen_child_models()\nRetrieve the IDs for all models that are frozen from this model.\nReturn type:\nA list of Models\nget_labelwise_roc_curves(source, fallback_to_parent_insights=False)\nRetrieve a list of LabelwiseRocCurve instances for a multilabel model for the given source and all labels.\nThis method is valid only for multilabel projects. For binary projects, use Model.get_roc_curve API .\nAdded in version v2.24.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\nReturns:\nLabelwise ROC Curve instances for source and all labels\nReturn type:\nlist of LabelwiseRocCurve\nRaises:\nClientError – If the insight is not available for this model\nget_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\nReturns:\nModel lift chart data\nReturn type:\nLiftChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_missing_report_info()\nRetrieve a report on missing training data that can be used to understand missing\nvalues treatment in the model. The report consists of missing values resolutions for\nfeatures numeric or categorical features that were part of building the model.\nReturns:\nThe queried model missing report, sorted by missing count (DESCENDING order).\nReturn type:\nAn iterable of MissingReportPerFeature\nget_model_blueprint_chart()\nRetrieve a diagram that can be used to understand\ndata flow in the blueprint.\nReturns:\nThe queried model blueprint chart.\nReturn type:\nModelBlueprintChart\nget_model_blueprint_documents()\nGet documentation for tasks used in this model.\nReturns:\nAll documents available for the model.\nReturn type:\nlist of BlueprintTaskDocument\nget_model_blueprint_json()\nGet the blueprint json representation used by this model.\nReturns:\nJson representation of the blueprint stages.\nReturn type:\nBlueprintJson\nget_multiclass_feature_impact()\nFor multiclass it’s possible to calculate feature impact separately for each target class.\nThe method for calculation is exactly the same, calculated in one-vs-all style for each\ntarget class.\nRequires that Feature Impact has already been computed with\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. Each item is a dict with the keys ‘featureImpacts’ (list),\n‘class’ (str). Each item in ‘featureImpacts’ is a dict with the keys ‘featureName’,\n‘impactNormalized’, and ‘impactUnnormalized’, and ‘redundantWith’.\nReturn type:\nlist of dict\nRaises:\nClientError – If the multiclass feature impacts have not been computed.\nget_multiclass_lift_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>, target_class=None)\nRetrieve model Lift chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_lift_chart will raise a ValueError.\ntarget_class (str, optional) – Lift chart target class name.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_multilabel_lift_charts(source, fallback_to_parent_insights=False)\nRetrieve model Lift charts for the specified source.\nAdded in version v2.24.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\nfallback_to_parent_insights (bool) – Optional, if True, this will return lift chart data for this\nmodel’s parent if the lift chart is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return insight data from this model’s parent.\nReturns:\nModel lift chart data for each saved target class\nReturn type:\nlist of LiftChart\nRaises:\nClientError – If the insight is not available for this model\nget_num_iterations_trained()\nRetrieves the number of estimators trained by early-stopping tree-based models.\n– versionadded:: v2.22\nReturns:\nprojectId (str) – id of project containing the model\nmodelId (str) – id of the model\ndata (array) – list of numEstimatorsItem objects, one for each modeling stage.\nnumEstimatorsItem will be of the form\nstage (str) – indicates the modeling stage (for multi-stage models); None of single-stage models\nnumIterations (int) – the number of estimators or iterations trained by the model\nget_or_request_feature_effect(source, max_wait=600, row_count=None, data_slice_id=None)\nRetrieve Feature Effects for the model, requesting a new job if it hasn’t been run previously.\nSee get_feature_effect_metadata\nfor retrieving information of source.\nParameters:\nsource (string) – The source Feature Effects are retrieved for.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effect job to complete before erroring.\nrow_count (Optional[int]) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nfeature_effects – The Feature Effects data.\nReturn type:\nFeatureEffects\nget_or_request_feature_effects_multiclass(source, top_n_features=None, features=None, row_count=None, class_=None, max_wait=600)\nRetrieve Feature Effects for the multiclass model, requesting a job if it hasn’t been run\npreviously.\nParameters:\nsource (string) – The source Feature Effects retrieve for.\nclass (str or None) – The class name Feature Effects retrieve for.\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by Feature Impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nmax_wait (Optional[int]) – The maximum time to wait for a requested Feature Effects job to complete before\nerroring.\nReturns:\nfeature_effects – The list of multiclass feature effects data.\nReturn type:\nlist of FeatureEffectsMulticlass\nget_or_request_feature_impact(max_wait=600, **kwargs)\nRetrieve feature impact for the model, requesting a job if it hasn’t been run previously\nParameters:\nmax_wait (Optional[int]) – The maximum time to wait for a requested feature impact job to complete before erroring\n**kwargs – Arbitrary keyword arguments passed to\nrequest_feature_impact.\nReturns:\nfeature_impacts – The feature impact data. See\nget_feature_impact for the exact\nschema.\nReturn type:\nlist or dict\nget_parameters()\nRetrieve model parameters.\nReturns:\nModel parameters for this model.\nReturn type:\nModelParameters\nget_pareto_front()\nRetrieve the Pareto Front for a Eureqa model.\nThis method is only supported for Eureqa models.\nReturns:\nModel ParetoFront data\nReturn type:\nParetoFront\nget_prime_eligibility()\nCheck if this model can be approximated with DataRobot Prime\nReturns:\nprime_eligibility – a dict indicating whether a model can be approximated with DataRobot Prime\n(key can_make_prime) and why it may be ineligible (key message)\nReturn type:\ndict\nget_residuals_chart(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible\nvalues.\nfallback_to_parent_insights (bool) – Optional, if True, this will return residuals chart data for this model’s parent if\nthe residuals chart is not available for this model and the model has a defined parent\nmodel. If omitted or False, or there is no parent model, will not attempt to return\nresiduals data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_residuals_chart will raise a ValueError.\nReturns:\nModel residuals chart data\nReturn type:\nResidualsChart\nRaises:\nClientError – If the insight is not available for this model\nValueError – If data_slice_filter passed as None\nget_roc_curve(source, fallback_to_parent_insights=False, data_slice_filter=<datarobot.models.model.Sentinel object>)\nRetrieve the ROC curve for a binary model for the specified source.\nThis method is valid only for binary projects. For multilabel projects, use\nModel.get_labelwise_roc_curves.\nParameters:\nsource (str) – ROC curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\n(New in version v2.23) For time series and OTV models, also accepts values backtest_2,\nbacktest_3, …, up to the number of backtests in the model.\nfallback_to_parent_insights (bool) – (New in version v2.14) Optional, if True, this will return ROC curve data for this\nmodel’s parent if the ROC curve is not available for this model and the model has a\ndefined parent model. If omitted or False, or there is no parent model, will not\nattempt to return data from this model’s parent.\ndata_slice_filter (DataSlice, optional) – A dataslice used to filter the return values based on the dataslice.id. By default this function will\nuse data_slice_filter.id == None which returns an unsliced insight. If data_slice_filter is None\nthen get_roc_curve will raise a ValueError.\nReturns:\nModel ROC curve data\nReturn type:\nRocCurve\nRaises:\nClientError – If the insight is not available for this model\n(New in version v3.0) TypeError – If the underlying project type is multilabel\nValueError – If data_slice_filter passed as None\nget_rulesets()\nList the rulesets approximating this model generated by DataRobot Prime\nIf this model hasn’t been approximated yet, will return an empty list.  Note that these\nare rulesets approximating this model, not rulesets used to construct this model.\nReturns:\nrulesets\nReturn type:\nlist of Ruleset\nget_supported_capabilities()\nRetrieves a summary of the capabilities supported by a model.\nAdded in version v2.14.\nReturns:\nsupportsBlending (bool) – whether the model supports blending\nsupportsMonotonicConstraints (bool) – whether the model supports monotonic constraints\nhasWordCloud (bool) – whether the model has word cloud data available\neligibleForPrime (bool) – (Deprecated in version v3.6)\nwhether the model is eligible for Prime\nhasParameters (bool) – whether the model has parameters that can be retrieved\nsupportsCodeGeneration (bool) – (New in version v2.18) whether the model supports code generation\nsupportsShap (bool) –\n(New in version v2.18) True if the model supports Shapley package. i.e. Shapley basedfeature Importance\nsupportsEarlyStopping (bool) – (New in version v2.22) True if this is an early stopping\ntree-based model and number of trained iterations can be retrieved.\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this model at leaderboard.\nReturn type:\nstr\nget_word_cloud(exclude_stop_words=False)\nRetrieve word cloud data for the model.\nParameters:\nexclude_stop_words (Optional[bool]) – Set to True if you want stopwords filtered out of response.\nReturns:\nWord cloud data for the model.\nReturn type:\nWordCloud\nincremental_train(data_stage_id, training_data_name=None)\nSubmit a job to the queue to perform incremental training on an existing model.\nSee train_incremental documentation.\nReturn type:\nModelJob\nclassmethod list(project_id, sort_by_partition='validation', sort_by_metric=None, with_metric=None, search_term=None, featurelists=None, families=None, blueprints=None, labels=None, characteristics=None, training_filters=None, number_of_clusters=None, limit=100, offset=0)\nRetrieve paginated model records, sorted by scores, with optional filtering.\nParameters:\nsort_by_partition (str, one of validation, backtesting, crossValidation or holdout) – Set the partition to use for sorted (by score) list of models. validation is the default.\nsort_by_metric (str) – Set the project metric to use for model sorting. DataRobot-selected project optimization metric\nis the default.\nwith_metric (str) – For a single-metric list of results, specify that project metric.\nsearch_term (str) – If specified, only models containing the term in their name or processes are returned.\nfeaturelists (List[str]) – If specified, only models trained on selected featurelists are returned.\nfamilies (List[str]) – If specified, only models belonging to selected families are returned.\nblueprints (List[str]) – If specified, only models trained on specified blueprint IDs are returned.\nlabels (List[str], starred or prepared for deployment) – If specified, only models tagged with all listed labels are returned.\ncharacteristics (List[str]) – If specified, only models matching all listed characteristics are returned.\ntraining_filters (List[str]) – If specified, only models matching at least one of the listed training conditions are returned.\nThe following formats are supported for autoML and datetime partitioned projects:\n- number of rows in training subset\nFor datetime partitioned projects:\n- <training duration>, example P6Y0M0D\n- <training_duration>-<time_window_sample_percent>-<sampling_method> Example: P6Y0M0D-78-Random,\n(returns models trained on 6 years of data, sampling rate 78%, random sampling).\n- Start/end date\n- Project settings\nnumber_of_clusters (list of int) – Filter models by number of clusters. Applicable only in unsupervised clustering projects.\nlimit (int)\noffset (int)\nReturns:\ngeneric_models\nReturn type:\nlist of GenericModel\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nrequest_approximation()\nRequest an approximation of this model using DataRobot Prime\nThis will create several rulesets that could be used to approximate this model.  After\ncomparing their scores and rule counts, the code used in the approximation can be downloaded\nand run locally.\nReturns:\njob – the job generating the rulesets\nReturn type:\nJob\nrequest_cross_class_accuracy_scores()\nRequest data disparity insights to be computed for the model.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_data_disparity_insights(feature, compared_class_names)\nRequest data disparity insights to be computed for the model.\nParameters:\nfeature (str) – Bias and Fairness protected feature name.\ncompared_class_names (list(str)) – List of two classes to compare\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_external_test(dataset_id, actual_value_column=None)\nRequest external test to compute scores and insights on an external test dataset\nParameters:\ndataset_id (string) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nReturns:\njob – a Job representing external dataset insights computation\nReturn type:\nJob\nrequest_fairness_insights(fairness_metrics_set=None)\nRequest fairness insights to be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – Can be one of <datarobot.enums.FairnessMetricsSet>.\nThe fairness metric used to calculate the fairness scores.\nReturns:\nstatus_id – A statusId of computation request.\nReturn type:\nstr\nrequest_feature_effect(row_count=None, data_slice_id=None)\nSubmit request to compute Feature Effects for the model.\nSee get_feature_effect for more\ninformation on the result of the job.\nParameters:\nrow_count (int) – (New in version v2.21) The sample size to use for Feature Impact computation.\nMinimum is 10 rows. Maximum is 100000 rows or the training sample size of the model,\nwhichever is less.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – A Job representing the feature effect computation. To get the completed feature effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nRaises:\nJobAlreadyRequested – If the feature effect have already been requested.\nrequest_feature_effects_multiclass(row_count=None, top_n_features=None, features=None)\nRequest Feature Effects computation for the multiclass model.\nSee get_feature_effect for\nmore information on the result of the job.\nParameters:\nrow_count (int) – The number of rows from dataset to use for Feature Impact calculation.\ntop_n_features (int or None) – Number of top features (ranked by feature impact) used to calculate Feature Effects.\nfeatures (list or None) – The list of features used to calculate Feature Effects.\nReturns:\njob – A Job representing Feature Effect computation. To get the completed Feature Effect\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob\nrequest_feature_impact(row_count=None, with_metadata=False, data_slice_id=None)\nRequest feature impacts to be computed for the model.\nSee get_feature_impact for more\ninformation on the result of the job.\nParameters:\nrow_count (Optional[int]) – The sample size (specified in rows) to use for Feature Impact computation. This is not\nsupported for unsupervised, multiclass (which has a separate method), and time series\nprojects.\nwith_metadata (Optional[bool]) – Flag indicating whether the result should include the metadata.\nIf true, metadata is included.\ndata_slice_id (Optional[str]) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\njob – Job representing the Feature Impact computation. To retrieve the completed Feature Impact\ndata, use job.get_result or job.get_result_when_complete.\nReturn type:\nJob or status_id\nRaises:\nJobAlreadyRequested – If the feature impacts have already been requested.\nrequest_frozen_datetime_model(training_row_count=None, training_duration=None, training_start_date=None, training_end_date=None, time_window_sample_pct=None, sampling_method=None)\nTrain a new frozen model with parameters from this model.\nRequires that this model belongs to a datetime partitioned project.  If it does not, an\nerror will occur when submitting the job.\nFrozen models use the same tuning parameters as their parent model instead of independently\noptimizing them to allow efficiently retraining models on larger amounts of the training\ndata.\nIn addition of training_row_count and training_duration, frozen datetime models may be\ntrained on an exact date range.  Only one of training_row_count, training_duration, or\ntraining_start_date and training_end_date should be specified.\nModels specified using training_start_date and training_end_date are the only ones that can\nbe trained into the holdout data (once the holdout is unlocked).\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\ntraining_duration may not be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, training_row_count may not be specified.\ntraining_start_date (datetime.datetime, optional) – the start date of the data to train to model on.  Only rows occurring at or after\nthis datetime will be used.  If training_start_date is specified, training_end_date\nmust also be specified.\ntraining_end_date (datetime.datetime, optional) – the end date of the data to train the model on.  Only rows occurring strictly before\nthis datetime will be used.  If training_end_date is specified, training_start_date\nmust also be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates).  An integer between 1 and 99 indicating the percentage to sample by\nwithin the window.  The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nReturns:\nmodel_job – the modeling job training a frozen model\nReturn type:\nModelJob\nrequest_frozen_model(sample_pct=None, training_row_count=None)\nTrain a new frozen model with parameters from this model\nNotes\nThis method only works if project the model belongs to is not datetime\npartitioned.  If it is, use request_frozen_datetime_model instead.\nFrozen models use the same tuning parameters as their parent model instead of independently\noptimizing them to allow efficiently retraining models on larger amounts of the training\ndata.\nParameters:\nsample_pct (float) – optional, the percentage of the dataset to use with the model.  If not provided, will\nuse the value from this model.\ntraining_row_count (int) – (New in version v2.9) optional, the integer number of rows of the dataset to use with\nthe model. Only one of sample_pct and training_row_count should be specified.\nReturns:\nmodel_job – the modeling job training a frozen model\nReturn type:\nModelJob\nrequest_lift_chart(source, data_slice_id=None)\nRequest the model Lift Chart for the specified source.\nParameters:\nsource (str) – Lift chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_per_class_fairness_insights(fairness_metrics_set=None)\nRequest per-class fairness insights be computed for the model.\nParameters:\nfairness_metrics_set (Optional[str]) – The fairness metric used to calculate the fairness scores.\nValue can be any one of <datarobot.enums.FairnessMetricsSet>.\nReturns:\nstatus_check_job – The returned object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_predictions(dataset_id=None, dataset=None, dataframe=None, file_path=None, file=None, include_prediction_intervals=None, prediction_intervals_size=None, forecast_point=None, predictions_start_date=None, predictions_end_date=None, actual_value_column=None, explanation_algorithm=None, max_explanations=None, max_ngram_explanations=None)\nRequests predictions against a previously uploaded dataset.\nParameters:\ndataset_id (string, optional) – The ID of the dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataset (Dataset, optional) – The dataset to make predictions against (as uploaded from Project.upload_dataset)\ndataframe (pd.DataFrame, optional) – (New in v3.0)\nThe dataframe to make predictions against\nfile_path (Optional[str]) – (New in v3.0)\nPath to file to make predictions against\nfile (IOBase, optional) – (New in v3.0)\nFile to make predictions against\ninclude_prediction_intervals (Optional[bool]) – (New in v2.16) For time series projects only.\nSpecifies whether prediction intervals should be calculated for this request. Defaults\nto True if prediction_intervals_size is specified, otherwise defaults to False.\nprediction_intervals_size (Optional[int]) – (New in v2.16) For time series projects only.\nRepresents the percentile to use for the size of the prediction intervals. Defaults to\n80 if include_prediction_intervals is True. Prediction intervals size must be\nbetween 1 and 100 (inclusive).\nforecast_point (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. This is the default point relative\nto which predictions will be generated, based on the forecast window of the project. See\nthe time series prediction documentation for more\ninformation.\npredictions_start_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The end date for bulk\npredictions, exclusive. Note that this parameter is for generating historical\npredictions using the training data. This parameter should be provided in conjunction\nwith predictions_start_date. Can’t be provided with the\nforecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) For time series unsupervised projects only.\nActual value column can be used to calculate the classification metrics and\ninsights on the prediction dataset. Can’t be provided with the forecast_point\nparameter.\nexplanation_algorithm ((New in version v2.21) optional; If set to 'shap', the) – response will include prediction explanations based on the SHAP explainer (SHapley\nAdditive exPlanations). Defaults to null (no prediction explanations).\nmax_explanations ((New in version v2.21) int optional; specifies the maximum number of) – explanation values that should be returned for each row, ordered by absolute value,\ngreatest to least. If null, no limit. In the case of ‘shap’: if the number of features\nis greater than the limit, the sum of remaining values will also be returned as\nshapRemainingTotal. Defaults to null. Cannot be set if explanation_algorithm is\nomitted.\nmax_ngram_explanations (optional;  int or str) – (New in version v2.29) Specifies the maximum number of text explanation values that\nshould be returned. If set to all, text explanations will be computed and all the\nngram explanations will be returned. If set to a non zero positive integer value, text\nexplanations will be computed and this amount of descendingly sorted ngram explanations\nwill be returned. By default text explanation won’t be triggered to be computed.\nReturns:\njob – The job computing the predictions\nReturn type:\nPredictJob\nrequest_residuals_chart(source, data_slice_id=None)\nRequest the model residuals chart for the specified source.\nParameters:\nsource (str) – Residuals chart data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_roc_curve(source, data_slice_id=None)\nRequest the model Roc Curve for the specified source.\nParameters:\nsource (str) – Roc Curve data source. Check datarobot.enums.CHART_DATA_SOURCE for possible values.\ndata_slice_id (string, optional) – ID for the data slice used in the request. If None, request unsliced insight data.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nrequest_training_predictions(data_subset, explanation_algorithm=None, max_explanations=None)\nStart a job to build training predictions\nParameters:\ndata_subset (str) – data set definition to build predictions on.\nChoices are:\ndr.enums.DATA_SUBSET.ALL or string all for all data available. Not valid formodels in datetime partitioned projects\ndr.enums.DATA_SUBSET.VALIDATION_AND_HOLDOUT or string validationAndHoldout forall data except training set. Not valid for models in datetime partitioned\nprojects\ndr.enums.DATA_SUBSET.HOLDOUT or string holdout for holdout data set only\ndr.enums.DATA_SUBSET.ALL_BACKTESTS or string allBacktests for downloadingthe predictions for all backtest validation folds. Requires the model to have\nsuccessfully scored all backtests. Datetime partitioned projects only.\nexplanation_algorithm (dr.enums.EXPLANATIONS_ALGORITHM) – (New in v2.21) Optional. If set to dr.enums.EXPLANATIONS_ALGORITHM.SHAP, the response\nwill include prediction explanations based on the SHAP explainer (SHapley Additive\nexPlanations). Defaults to None (no prediction explanations).\nmax_explanations (int) – (New in v2.21) Optional. Specifies the maximum number of explanation values that should\nbe returned for each row, ordered by absolute value, greatest to least. In the case of\ndr.enums.EXPLANATIONS_ALGORITHM.SHAP:  If not set, explanations are returned for all\nfeatures. If the number of features is greater than the max_explanations, the sum of\nremaining values will also be returned as shap_remaining_total. Max 100. Defaults to\nnull for datasets narrower than 100 columns, defaults to 100 for datasets wider than 100\ncolumns. Is ignored if explanation_algorithm is not set.\nReturns:\nan instance of created async job\nReturn type:\nJob\nretrain(sample_pct=None, featurelist_id=None, training_row_count=None, n_clusters=None)\nSubmit a job to the queue to train a blender model.\nParameters:\nsample_pct (Optional[float]) – The sample size in percents (1 to 100) to use in training. If this parameter is used\nthen training_row_count should not be given.\nfeaturelist_id (Optional[str]) – The featurelist id\ntraining_row_count (Optional[int]) – The number of rows used to train the model. If this parameter is used, then sample_pct\nshould not be given.\nn_clusters (Optional[int]) – (new in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that do not determine\nthe number of clusters automatically.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nset_prediction_threshold(threshold)\nSet a custom prediction threshold for the model.\nMay not be used once prediction_threshold_read_only is True for this model.\nParameters:\nthreshold (float) – only used for binary classification projects. The threshold to when deciding between\nthe positive and negative classes when making predictions.  Should be between 0.0 and\n1.0 (inclusive).\nstar_model()\nMark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nstart_advanced_tuning_session()\nStart an Advanced Tuning session.  Returns an object that helps\nset up arguments for an Advanced Tuning model execution.\nAs of v2.17, all models other than blenders, open source, prime, baseline and\nuser-created support Advanced Tuning.\nReturns:\nSession for setting up and running Advanced Tuning on a model\nReturn type:\nAdvancedTuningSession\nstart_incremental_learning_from_sample(early_stopping_rounds=None, first_iteration_only=False, chunk_definition_id=None)\nSubmit a job to the queue to perform the first incremental learning iteration training on an existing\nsample model. This functionality requires the SAMPLE_DATA_TO_START_PROJECT feature flag to be enabled.\nParameters:\nearly_stopping_rounds (Optional[int]) – The number of chunks in which no improvement is observed that triggers the early stopping mechanism.\nfirst_iteration_only (bool) – Specifies whether incremental learning training should be limited to the first\niteration. If set to True, the training process will be performed only for the first\niteration. If set to False, training will continue until early stopping conditions\nare met or the maximum number of iterations is reached. The default value is False.\nchunk_definition_id (str) – The id of the chunk definition to be use for incremental training.\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\ntrain(sample_pct=None, featurelist_id=None, scoring_type=None, training_row_count=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>)\nTrain the blueprint used in model on a particular featurelist or amount of data.\nThis method creates a new training job for worker and appends it to\nthe end of the queue for this project.\nAfter the job has finished you can get the newly trained model by retrieving\nit from the project leaderboard, or by retrieving the result of the job.\nEither sample_pct or training_row_count can be used to specify the amount of data to\nuse, but not both.  If neither are specified, a default of the maximum amount of data that\ncan safely be used to train any blueprint without going into the validation data will be\nselected.\nIn smart-sampled projects, sample_pct and training_row_count are assumed to be in terms\nof rows of the minority class.\nNotes\nFor datetime partitioned projects, see train_datetime instead.\nParameters:\nsample_pct (Optional[float]) – The amount of data to use for training, as a percentage of the project dataset from\n0 to 100.\nfeaturelist_id (Optional[str]) – The identifier of the featurelist to use. If not defined, the\nfeaturelist of this model is used.\nscoring_type (Optional[str]) – Either validation or crossValidation (also dr.SCORING_TYPE.validation\nor dr.SCORING_TYPE.cross_validation). validation is available for every\npartitioning type, and indicates that the default model validation should be\nused for the project.\nIf the project uses a form of cross-validation partitioning,\ncrossValidation can also be used to indicate\nthat all of the available training/validation combinations\nshould be used to evaluate the model.\ntraining_row_count (Optional[int]) – The number of rows to use to train the requested model.\nmonotonic_increasing_featurelist_id (str) – (new in version 2.11) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (str) – (new in version 2.11) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nReturns:\nmodel_job_id – id of created job, can be used as parameter to ModelJob.get\nmethod or wait_for_async_model_creation function\nReturn type:\nstr\nExamples\nproject = Project.get('project-id')\nmodel = Model.get('project-id', 'model-id')\nmodel_job_id = model.train(training_row_count=project.max_train_rows)\ntrain_datetime(featurelist_id=None, training_row_count=None, training_duration=None, time_window_sample_pct=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>, use_project_settings=False, sampling_method=None, n_clusters=None)\nTrains this model on a different featurelist or sample size.\nRequires that this model is part of a datetime partitioned project; otherwise, an error will\noccur.\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\nfeaturelist_id (Optional[str]) – the featurelist to use to train the model.  If not specified, the featurelist of this\nmodel is used.\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\nneither training_duration nor use_project_settings may be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, neither training_row_count nor use_project_settings may be\nspecified.\nuse_project_settings (Optional[bool]) – (New in version v2.20) defaults to False. If True, indicates that the custom\nbacktest partitioning settings specified by the user will be used to train the model and\nevaluate backtest scores. If specified, neither training_row_count nor\ntraining_duration may be specified.\ntime_window_sample_pct (Optional[int]) – may only be specified when the requested model is a time window (e.g. duration or start\nand end dates). An integer between 1 and 99 indicating the percentage to sample by\nwithin the window. The points kept are determined by a random uniform sample.\nIf specified, training_duration must be specified otherwise, the number of rows used\nto train the model and evaluate backtest scores and an error will occur.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nmonotonic_increasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nn_clusters (Optional[int]) – (New in version 2.27) number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that don’t automatically\ndetermine the number of clusters.\nReturns:\njob – the created job to build the model\nReturn type:\nModelJob\ntrain_incremental(data_stage_id, training_data_name=None, data_stage_encoding=None, data_stage_delimiter=None, data_stage_compression=None)\nSubmit a job to the queue to perform incremental training on an existing model using\nadditional data. The id of the additional data to use for training is specified with the data_stage_id.\nOptionally a name for the iteration can be supplied by the user to help identify the contents of data in\nthe iteration.\nThis functionality requires the INCREMENTAL_LEARNING feature flag to be enabled.\nParameters:\ndata_stage_id (str) – The id of the data stage to use for training.\ntraining_data_name (Optional[str]) – The name of the iteration or data stage to indicate what the incremental learning was performed on.\ndata_stage_encoding (Optional[str]) – The encoding type of the data in the data stage (default: UTF-8).\nSupported formats: UTF-8, ASCII, WINDOWS1252\ndata_stage_encoding – The delimiter used by the data in the data stage (default: ‘,’).\ndata_stage_compression (Optional[str]) – The compression type of the data stage file, e.g. ‘zip’ (default: None).\nSupported formats: zip\nReturns:\njob – The created job that is retraining the model\nReturn type:\nModelJob\nunstar_model()\nUnmark the model as starred.\nModel stars propagate to the web application and the API, and can be used to filter when\nlisting models.\nReturn type:\nNone\nClustering\nclass datarobot.models.ClusteringModel\nClusteringModel extends Model class.\nIt provides provides properties and methods specific to clustering projects.\ncompute_insights(max_wait=600)\nCompute and retrieve cluster insights for model. This method awaits completion of\njob computing cluster insights and returns results after it is finished. If computation\ntakes longer than specified max_wait exception will be raised.\nParameters:\nproject_id (str) – Project to start creation in.\nmodel_id (str) – Project’s model to start creation in.\nmax_wait (int) – Maximum number of seconds to wait before giving up\nReturn type:\nList of ClusterInsight\nRaises:\nClientError – Server rejected creation due to client error.\nMost likely cause is bad project_id or model_id.\nAsyncFailureError – If any of the responses from the server are unexpected\nAsyncProcessUnsuccessfulError – If the cluster insights computation has failed or was cancelled.\nAsyncTimeoutError – If the cluster insights computation did not resolve in time\nproperty insights: List[ClusterInsight]\nReturn actual list of cluster insights if already computed.\nReturn type:\nList of ClusterInsight\nproperty clusters: List[Cluster]\nReturn actual list of Clusters.\nReturn type:\nList of Cluster\nupdate_cluster_names(cluster_name_mappings)\nChange many cluster names at once based on list of name mappings.\nParameters:\ncluster_name_mappings (List of tuples) – Cluster names mapping consisting of current cluster name and old cluster name.\nExample:\ncluster_name_mappings = [\n(\"current cluster name 1\", \"new cluster name 1\"),\n(\"current cluster name 2\", \"new cluster name 2\")]\nReturn type:\nList of Cluster\nRaises:\ndatarobot.errors.ClientError – Server rejected update of cluster names.\nPossible reasons include: incorrect format of mapping, mapping introduces duplicates.\nupdate_cluster_name(current_name, new_name)\nChange cluster name from current_name to new_name.\nParameters:\ncurrent_name (str) – Current cluster name.\nnew_name (str) – New cluster name.\nReturn type:\nList of Cluster\nRaises:\ndatarobot.errors.ClientError – Server rejected update of cluster names.\nclass datarobot.models.cluster.Cluster\nRepresentation of a single cluster.\nVariables:\nname (str) – Current cluster name\npercent (float) – Percent of data contained in the cluster. This value is reported after cluster insights\nare computed for the model.\nclassmethod list(project_id, model_id)\nRetrieve a list of clusters in the model.\nParameters:\nproject_id (str) – ID of the project that the model is part of.\nmodel_id (str) – ID of the model.\nReturn type:\nList of clusters\nclassmethod update_multiple_names(project_id, model_id, cluster_name_mappings)\nUpdate many clusters at once based on list of name mappings.\nParameters:\nproject_id (str) – ID of the project that the model is part of.\nmodel_id (str) – ID of the model.\ncluster_name_mappings (List of tuples) – Cluster name mappings, consisting of current and previous names for each cluster.\nExample:\ncluster_name_mappings = [\n(\"current cluster name 1\", \"new cluster name 1\"),\n(\"current cluster name 2\", \"new cluster name 2\")]\nReturn type:\nList of clusters\nRaises:\ndatarobot.errors.ClientError – Server rejected update of cluster names.\nValueError – Invalid cluster name mapping provided.\nclassmethod update_name(project_id, model_id, current_name, new_name)\nChange cluster name from current_name to new_name\nParameters:\nproject_id (str) – ID of the project that the model is part of.\nmodel_id (str) – ID of the model.\ncurrent_name (str) – Current cluster name\nnew_name (str) – New cluster name\nReturn type:\nList of Cluster\nclass datarobot.models.cluster_insight.ClusterInsight\nHolds data on all insights related to feature as well as breakdown per cluster.\nParameters:\nfeature_name (str) – Name of a feature from the dataset.\nfeature_type (str) – Type of feature.\ninsights (List[ClusterInsight]) – List provides information regarding the importance of a specific feature in relation\nto each cluster. Results help understand how the model is grouping data and what each\ncluster represents.\nfeature_impact (float) – Impact of a feature ranging from 0 to 1.\nclassmethod compute(project_id, model_id, max_wait=600)\nStarts creation of cluster insights for the model and if successful, returns computed\nClusterInsights. This method allows calculation to continue for a specified time and\nif not complete, cancels the request.\nParameters:\nproject_id (str) – ID of the project to begin creation of cluster insights for.\nmodel_id (str) – ID of the project model to begin creation of cluster insights for.\nmax_wait (int) – Maximum number of seconds to wait canceling the request.\nReturn type:\nList[ClusterInsight]\nRaises:\nClientError – Server rejected creation due to client error.\nMost likely cause is bad project_id or model_id.\nAsyncFailureError – Indicates whether any of the responses from the server are unexpected.\nAsyncProcessUnsuccessfulError – Indicates whether the cluster insights computation failed or was cancelled.\nAsyncTimeoutError – Indicates whether the cluster insights computation did not resolve within the specified\ntime limit (max_wait).\nPareto front\nclass datarobot.models.pareto_front.ParetoFront\nPareto front data for a Eureqa model.\nThe pareto front reflects the tradeoffs between error and complexity for particular model. The\nsolutions reflect possible Eureqa models that are different levels of complexity.  By default,\nonly one solution will have a corresponding model, but models can be created for each solution.\nVariables:\nproject_id (str) – the ID of the project the model belongs to\nerror_metric (str) – Eureqa error-metric identifier used to compute error metrics for this search. Note that\nEureqa error metrics do NOT correspond 1:1 with DataRobot error metrics – the available\nmetrics are not the same, and are computed from a subset of the training data rather than\nfrom the validation data.\nhyperparameters (dict) – Hyperparameters used by this run of the Eureqa blueprint\ntarget_type (str) – Indicating what kind of modeling is being done in this project, either ‘Regression’,\n‘Binary’ (Binary classification), or ‘Multiclass’ (Multiclass classification).\nsolutions (list(Solution)) – Solutions that Eureqa has found to model this data.\nSome solutions will have greater accuracy.  Others will have slightly\nless accuracy but will use simpler expressions.\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (list) – List of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nclass datarobot.models.pareto_front.Solution\nEureqa Solution.\nA solution represents a possible Eureqa model; however not all solutions\nhave models associated with them.  It must have a model created before\nit can be used to make predictions, etc.\nVariables:\neureqa_solution_id (str) – ID of this Solution\ncomplexity (int) – Complexity score for this solution. Complexity score is a function\nof the mathematical operators used in the current solution.\nThe Complexity calculation can be tuned via model hyperparameters.\nerror (float or None) – Error for the current solution, as computed by Eureqa using the\n‘error_metric’ error metric. It will be None if model refitted existing solution.\nexpression (str) – Eureqa model equation string.\nexpression_annotated (str) – Eureqa model equation string with variable names tagged for easy identification.\nbest_model (bool) – True, if the model is determined to be the best\ncreate_model()\nAdd this solution to the leaderboard, if it is not already present.\nCombined models\nSee API reference for Combined Model in Segmented Modeling API Reference\nAdvanced tuning\nclass datarobot.models.advanced_tuning.AdvancedTuningSession\nA session enabling users to configure and run advanced tuning for a model.\nEvery model contains a set of one or more tasks.  Every task contains a set of\nzero or more parameters.  This class allows tuning the values of each parameter\non each task of a model, before running that model.\nThis session is client-side only and is not persistent.\nOnly the final model, constructed when run is called, is persisted on the DataRobot server.\nVariables:\ndescription (str) – Description for the new advance-tuned model.\nDefaults to the same description as the base model.\nget_task_names()\nGet the list of task names that are available for this model\nReturns:\nList of task names\nReturn type:\nlist(str)\nget_parameter_names(task_name)\nGet the list of parameter names available for a specific task\nReturns:\nList of parameter names\nReturn type:\nlist(str)\nset_parameter(value, task_name=None, parameter_name=None, parameter_id=None)\nSet the value of a parameter to be used\nThe caller must supply enough of the optional arguments to this function\nto uniquely identify the parameter that is being set.\nFor example, a less-common parameter name such as\n‘building_block__complementary_error_function’ might only be used once (if at all)\nby a single task in a model.  In which case it may be sufficient to simply specify\n‘parameter_name’.  But a more-common name such as ‘random_seed’ might be used by\nseveral of the model’s tasks, and it may be necessary to also specify ‘task_name’\nto clarify which task’s random seed is to be set.\nThis function only affects client-side state. It will not check that the new parameter\nvalue(s) are valid.\nParameters:\ntask_name (str) – Name of the task whose parameter needs to be set\nparameter_name (str) – Name of the parameter to set\nparameter_id (str) – ID of the parameter to set\nvalue (int, float, list, or str) – New value for the parameter, with legal values determined by the parameter being set\nRaises:\nNoParametersFoundException – if no matching parameters are found.\nNonUniqueParametersException – if multiple parameters matched the specified filtering criteria\nReturn type:\nNone\nget_parameters()\nReturns the set of parameters available to this model\nThe returned parameters have one additional key, “value”, reflecting any new values that\nhave been set in this AdvancedTuningSession.  When the session is run, “value” will be used,\nor if it is unset, “current_value”.\nReturn type:\nAdvancedTuningParamsType\nReturns:\nparameters (dict) – “Parameters” dictionary, same as specified on Model.get_advanced_tuning_params.\nAn additional field is added per parameter to the 'tuning_parameters' list in the dictionary\nvalue (int, float, list, or str) – The current value of the parameter.  None if none has been specified.\nrun()\nSubmit this model for Advanced Tuning.\nReturns:\nThe created job to build the model\nReturn type:\ndatarobot.models.modeljob.ModelJob\nRecommended models\nclass datarobot.models.ModelRecommendation\nA collection of information about a recommended model for a project.\nVariables:\nproject_id (str) – the id of the project the model belongs to\nmodel_id (str) – the id of the recommended model\nrecommendation_type (str) – the type of model recommendation\nclassmethod get(project_id, recommendation_type=None)\nRetrieves the default or specified by recommendation_type recommendation.\nParameters:\nproject_id (str) – The project’s id.\nrecommendation_type (enums.RECOMMENDED_MODEL_TYPE) – The type of recommendation to get. If None, returns the default recommendation.\nReturns:\nrecommended_model\nReturn type:\nModelRecommendation\nclassmethod get_all(project_id)\nRetrieves all of the current recommended models for the project.\nParameters:\nproject_id (str) – The project’s id.\nReturns:\nrecommended_models\nReturn type:\nlist of ModelRecommendation\nclassmethod get_recommendation(recommended_models, recommendation_type)\nReturns the model in the given list with the requested type.\nParameters:\nrecommended_models (list of ModelRecommendation)\nrecommendation_type (enums.RECOMMENDED_MODEL_TYPE) – the type of model to extract from the recommended_models list\nReturns:\nrecommended_model\nReturn type:\nModelRecommendation or None if no model with the requested type exists\nget_model()\nReturns the Model associated with this ModelRecommendation.\nReturns:\nrecommended_model\nReturn type:\nModel or DatetimeModel if the project is datetime-partitioned\nClass mapping aggregation settings\nFor multiclass projects with a lot of unique values in target column you can\nspecify the parameters for aggregation of rare values to improve the modeling\nperformance and decrease the runtime and resource usage of resulting models.\nclass datarobot.helpers.ClassMappingAggregationSettings\nClass mapping aggregation settings.\nFor multiclass projects allows fine control over which target values will be\npreserved as classes. Classes which aren’t preserved will be\n- aggregated into a single “catch everything else” class in case of multiclass\n- or will be ignored in case of multilabel.\nAll attributes are optional, if not specified - server side defaults will be used.\nVariables:\nmax_unaggregated_class_values (Optional[int]) – Maximum amount of unique values allowed before aggregation kicks in.\nmin_class_support (Optional[int]) – Minimum number of instances necessary for each target value in the dataset.\nAll values with less instances will be aggregated.\nexcluded_from_aggregation (Optional[List]) – List of target values that should be guaranteed to kept as is,\nregardless of other settings.\naggregation_class_name (Optional[str]) – If some of the values will be aggregated - this is the name of the aggregation class\nthat will replace them.\nModel jobs\ndatarobot.models.modeljob.wait_for_async_model_creation(project_id, model_job_id, max_wait=600)\nGiven a Project id and ModelJob id poll for status of process\nresponsible for model creation until model is created.\nParameters:\nproject_id (str) – The identifier of the project\nmodel_job_id (str) – The identifier of the ModelJob\nmax_wait (Optional[int]) – Time in seconds after which model creation is considered\nunsuccessful\nReturns:\nmodel – Newly created model\nReturn type:\nModel\nRaises:\nAsyncModelCreationError – Raised if status of fetched ModelJob object is error\nAsyncTimeoutError – Model wasn’t created in time, specified by max_wait parameter\nclass datarobot.models.ModelJob\nTracks asynchronous work being done within a project\nVariables:\nid (int) – the id of the job\nproject_id (str) – the id of the project the job belongs to\nstatus (str) – the status of the job - will be one of datarobot.enums.QUEUE_STATUS\njob_type (str) – what kind of work the job is doing - will be ‘model’ for modeling jobs\nis_blocked (bool) – if true, the job is blocked (cannot be executed) until its dependencies are resolved\nsample_pct (float) – the percentage of the project’s dataset used in this modeling job\nmodel_type (str) – the model this job builds (e.g. ‘Nystroem Kernel SVM Regressor’)\nprocesses (List[str]) – the processes used by the model\nfeaturelist_id (str) – the id of the featurelist used in this modeling job\nblueprint (Blueprint) – the blueprint used in this modeling job\nclassmethod from_job(job)\nTransforms a generic Job into a ModelJob\nParameters:\njob (Job) – A generic job representing a ModelJob\nReturns:\nmodel_job – A fully populated ModelJob with all the details of the job\nReturn type:\nModelJob\nRaises:\nValueError: – If the generic Job was not a model job, e.g. job_type != JOB_TYPE.MODEL\nclassmethod get(project_id, model_job_id)\nFetches one ModelJob. If the job finished, raises PendingJobFinished\nexception.\nParameters:\nproject_id (str) – The identifier of the project the model belongs to\nmodel_job_id (str) – The identifier of the model_job\nReturns:\nmodel_job – The pending ModelJob\nReturn type:\nModelJob\nRaises:\nPendingJobFinished – If the job being queried already finished, and the server is\nre-routing to the finished model.\nAsyncFailureError – Querying this resource gave a status code other than 200 or 303\nclassmethod get_model(project_id, model_job_id)\nFetches a finished model from the job used to create it.\nParameters:\nproject_id (str) – The identifier of the project the model belongs to\nmodel_job_id (str) – The identifier of the model_job\nReturns:\nmodel – The finished model\nReturn type:\nModel\nRaises:\nJobNotFinished – If the job has not finished yet\nAsyncFailureError – Querying the model_job in question gave a status code other than 200 or\n303\ncancel()\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nget_result(params=None)\nParameters:\nparams (dict or None) – Query parameters to be added to request to get results.\nNotes\nFor featureEffects, source param is required to define source,\notherwise the default is training.\nReturns:\nresult –\nReturn type depends on the job type\nfor model jobs, a Model is returned\nfor predict jobs, a pandas.DataFrame (with predictions) is returned\nfor featureImpact jobs, a list of dicts by default (see with_metadata\nparameter of the FeatureImpactJob class and its get() method).\nfor primeRulesets jobs, a list of Rulesets\nfor primeModel jobs, a PrimeModel\nfor primeDownloadValidation jobs, a PrimeFile\nfor predictionExplanationInitialization jobs, a PredictionExplanationsInitialization\nfor predictionExplanations jobs, a PredictionExplanations\nfor featureEffects, a FeatureEffects.\nReturn type:\nobject\nRaises:\nJobNotFinished – If the job is not finished, the result is not available.\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nget_result_when_complete(max_wait=600, params=None)\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nparams (dict, optional) – Query parameters to be added to request.\nReturns:\nresult – Return type is the same as would be returned by Job.get_result.\nReturn type:\nobject\nRaises:\nAsyncTimeoutError – If the job does not finish in time\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nrefresh()\nUpdate this object with the latest job data from the server.\nwait_for_completion(max_wait=600)\nWaits for job to complete.\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nReturn type:\nNone\nRegistry jobs\nclass datarobot.models.registry.job.Job\nA DataRobot job.\nAdded in version v3.4.\nVariables:\nid (str) – The ID of the job.\nname (str) – The name of the job.\ncreated_at (str) – ISO-8601 formatted timestamp of when the version was created\nitems (List[JobFileItem]) – A list of file items attached to the job.\ndescription (Optional[str]) – A job description.\nenvironment_id (Optional[str]) – The ID of the environment to use with the job.\nenvironment_version_id (Optional[str]) – The ID of the environment version to use with the job.\nclassmethod create(name, environment_id=None, environment_version_id=None, folder_path=None, files=None, file_data=None, runtime_parameter_values=None)\nCreate a job.\nAdded in version v3.4.\nParameters:\nname (str) – The name of the job.\nenvironment_id (Optional[str]) – The environment ID to use for job runs.\nThe ID must be specified in order to run the job.\nenvironment_version_id (Optional[str]) – The environment version ID to use for job runs.\nIf not specified, the latest version of the execution environment will be used.\nfolder_path (Optional[str]) – The path to a folder containing files to be uploaded.\nEach file in the folder is uploaded under path relative\nto a folder path.\nfiles (Optional[Union[List[Tuple[str, str]], List[str]]]) – The files to be uploaded to the job.\nThe files can be defined in 2 ways:\n1. List of tuples where 1st element is the local path of the file to be uploaded\nand the 2nd element is the file path in the job file system.\n2. List of local paths of the files to be uploaded.\nIn this case files are added to the root of the model file system.\nfile_data (Optional[Dict[str, str]]) – The files content to be uploaded to the job.\nDefined as a dictionary where keys are the file paths in the job file system.\nand values are the files content.\nruntime_parameter_values (Optional[List[RuntimeParameterValue]]) – Additional parameters to be injected into a model at runtime. The fieldName\nmust match a fieldName that is listed in the runtimeParameterDefinitions section\nof the model-metadata.yaml file.\nReturns:\ncreated job\nReturn type:\nJob\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod list()\nList jobs.\nAdded in version v3.4.\nReturns:\na list of jobs\nReturn type:\nList[Job]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(job_id)\nGet job by id.\nAdded in version v3.4.\nParameters:\njob_id (str) – The ID of the job.\nReturns:\nretrieved job\nReturn type:\nJob\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nupdate(name=None, entry_point=None, environment_id=None, environment_version_id=None, description=None, folder_path=None, files=None, file_data=None, runtime_parameter_values=None)\nUpdate job properties.\nAdded in version v3.4.\nParameters:\nname (str) – The job name.\nentry_point (Optional[str]) – The job file item ID to use as an entry point of the job.\nenvironment_id (Optional[str]) – The environment ID to use for job runs.\nMust be specified in order to run the job.\nenvironment_version_id (Optional[str]) – The environment version ID to use for job runs.\nIf not specified, the latest version of the execution environment will be used.\ndescription (str) – The job description.\nfolder_path (Optional[str]) – The path to a folder containing files to be uploaded.\nEach file in the folder is uploaded under path relative\nto a folder path.\nfiles (Optional[Union[List[Tuple[str, str]], List[str]]]) – The files to be uploaded to the job.\nThe files can be defined in 2 ways:\n1. List of tuples where 1st element is the local path of the file to be uploaded\nand the 2nd element is the file path in the job file system.\n2. List of local paths of the files to be uploaded.\nIn this case files are added to the root of the job file system.\nfile_data (Optional[Dict[str, str]]) – The files content to be uploaded to the job.\nDefined as a dictionary where keys are the file paths in the job file system.\nand values are the files content.\nruntime_parameter_values (Optional[List[RuntimeParameterValue]]) – Additional parameters to be injected into a model at runtime. The fieldName\nmust match a fieldName that is listed in the runtimeParameterDefinitions section\nof the model-metadata.yaml file.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nNone\ndelete()\nDelete job.\n:rtype: None\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nrefresh()\nUpdate job with the latest data from server.\n:rtype: None\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod create_from_custom_metric_gallery_template(template_id, name, description=None, sidecar_deployment_id=None)\nCreate a job from a custom metric gallery template.\nParameters:\ntemplate_id (str) – ID of the template.\nname (str) – Name of the job.\ndescription (Optional[str]) – Description of the job.\nsidecar_deployment_id (Optional[str]) – ID of the sidecar deployment. Only relevant for templates that use sidecar deployments.\nReturns:\nretrieved job\nReturn type:\nJob\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nlist_schedules()\nList schedules for the job.\nReturns:\na list of schedules for the job.\nReturn type:\nList[JobSchedule]\nclass datarobot.models.registry.job.JobFileItem\nA file item attached to a DataRobot job.\nAdded in version v3.4.\nVariables:\nid (str) – The ID of the file item.\nfile_name (str) – The name of the file item.\nfile_path (str) – The path of the file item.\nfile_source (str) – The source of the file item.\ncreated_at (str) – ISO-8601 formatted timestamp of when the version was created.\nclass datarobot.models.registry.job_run.JobRun\nA DataRobot job run.\nAdded in version v3.4.\nVariables:\nid (str) – The ID of the job run.\ncustom_job_id (str) – The ID of the parent job.\ndescription (str) – A description of the job run.\ncreated_at (str) – ISO-8601 formatted timestamp of when the version was created\nitems (List[JobFileItem]) – A list of file items attached to the job.\nstatus (JobRunStatus) – The status of the job run.\nduration (float) – The duration of the job run.\nclassmethod create(job_id, max_wait=600, runtime_parameter_values=None)\nCreate a job run.\nAdded in version v3.4.\nParameters:\njob_id (str) – The ID of the job.\nmax_wait (Optional[int]) – max time to wait for a terminal status (“succeeded”, “failed”, “interrupted”, “canceled”).\nIf set to None - method will return without waiting.\nruntime_parameter_values (Optional[List[RuntimeParameterValue]]) – Additional parameters to be injected into a model at runtime. The fieldName\nmust match a fieldName that is listed in the runtimeParameterDefinitions section\nof the model-metadata.yaml file.\nReturns:\ncreated job\nReturn type:\nJob\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nValueError – if execution environment or entry point is not specified for the job\nclassmethod list(job_id)\nList job runs.\nAdded in version v3.4.\nParameters:\njob_id (str) – The ID of the job.\nReturns:\nA list of job runs.\nReturn type:\nList[Job]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(job_id, job_run_id)\nGet job run by id.\nAdded in version v3.4.\nParameters:\njob_id (str) – The ID of the job.\njob_run_id (str) – The ID of the job run.\nReturns:\nThe retrieved job run.\nReturn type:\nJob\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nupdate(description=None)\nUpdate job run properties.\nAdded in version v3.4.\nParameters:\ndescription (str) – new job run description\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nNone\ncancel()\nCancel job run.\n:rtype: None\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nrefresh()\nUpdate job run with the latest data from server.\n:rtype: None\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nget_logs()\nGet log of the job run.\n:rtype: Optional[str]\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndelete_logs()\nGet log of the job run.\n:rtype: None\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclass datarobot.models.registry.job_run.JobRunStatus\nEnum of the job run statuses\nclass datarobot.models.registry.job.JobSchedule\nA job schedule.\nAdded in version v3.5.\nVariables:\nid (str) – The ID of the job schedule.\ncustom_job_id (str) – The ID of the custom job.\nupdated_at (str) – ISO-8601 formatted timestamp of when the schedule was updated.\nupdated_by (Dict[str, Any]) – The user who updated the schedule.\ncreated_at (str) – ISO-8601 formatted timestamp of when the schedule was created.\ncreated_by (Dict[str, Any]) – The user who created the schedule.\nscheduled_job_id (str) – The ID of the scheduled job.\ndeployment (Dict[str, Any]) – The deployment of the scheduled job.\nschedule (Schedule) – The schedule of the job.\nparameter_overrides (List[RuntimeParameterValue]) – The parameter overrides for this schedule.\nupdate(schedule=None, parameter_overrides=None)\nUpdate the job schedule.\nParameters:\nschedule (Optional[Schedule]) – The schedule of the job.\nparameter_overrides (Optional[List[RuntimeParameterValue]]) – The parameter overrides for this schedule.\nReturn type:\nJobSchedule\ndelete()\nDelete the job schedule.\n:rtype: None\nclassmethod create(custom_job_id, schedule, parameter_overrides=None)\nCreate a job schedule.\nParameters:\ncustom_job_id (str) – The ID of the custom job.\nschedule (Schedule) – The schedule of the job.\nparameter_overrides (Optional[List[RuntimeParameterValue]]) – The parameter overrides for this schedule.\nReturn type:\nJobSchedule\nMissing values report\nclass datarobot.models.missing_report.MissingValuesReport\nMissing values report for model, contains list of reports per feature sorted by missing\ncount in descending order.\nNotes\nReport per feature contains:\nfeature : feature name.\ntype : feature type – ‘Numeric’ or ‘Categorical’.\nmissing_count :  missing values count in training data.\nmissing_percentage : missing values percentage in training data.\ntasks : list of information per each task, which was applied to feature.\ntask information contains:\nid : a number of task in the blueprint diagram.\nname : task name.\ndescriptions : human readable aggregated information about how the task handles\nmissing values.  The following descriptions may be present: what value is imputed for\nmissing values, whether the feature being missing is treated as a feature by the task,\nwhether missing values are treated as infrequent values,\nwhether infrequent values are treated as missing values,\nand whether missing values are ignored.\nclassmethod get(project_id, model_id)\nRetrieve a missing report.\nParameters:\nproject_id (str) – The project’s id.\nmodel_id (str) – The model’s id.\nReturns:\nThe queried missing report.\nReturn type:\nMissingValuesReport\nRegistered models\nclass datarobot.models.RegisteredModel\nA registered model is a logical grouping of model packages (versions) that are related to each other.\nVariables:\nid (str) – The ID of the registered model.\nname (str) – The name of the registered model.\ndescription (str) – The description of the registered model.\ncreated_at (str) – The creation time of the registered model.\nmodified_at (str) – The last modification time for the registered model.\nmodified_by (datarobot.models.model_registry.common.UserMetadata) – Information on the user who last modified the registered model.\ntarget (Target) – Information on the target variable.\ncreated_by (datarobot.models.model_registry.common.UserMetadata) – Information on the creator of the registered model.\nlast_version_num (int) – The latest version number associated to this registered model.\nis_archived (bool) – Determines whether the registered model is archived.\nclassmethod get(registered_model_id)\nGet a registered model by ID.\nParameters:\nregistered_model_id (str) – ID of the registered model to retrieve\nReturns:\nregistered_model – Registered Model Object\nReturn type:\nRegisteredModel\nExamples\nfrom datarobot import RegisteredModel\nregistered_model = RegisteredModel.get(registered_model_id='5c939e08962d741e34f609f0')\nregistered_model.id\n>>>'5c939e08962d741e34f609f0'\nregistered_model.name\n>>>'My Registered Model'\nclassmethod list(limit=100, offset=None, sort_key=None, sort_direction=None, search=None, filters=None)\nList all registered models a user can view.\nParameters:\nlimit (Optional[int]) – Maximum number of registered models to return\noffset (Optional[int]) – Number of registered models to skip before returning results\nsort_key (RegisteredModelSortKey, optional) – Key to order result by\nsort_direction (RegisteredModelSortDirection, optional) – Sort direction\nsearch (Optional[str]) – A term to search for in registered model name, description, or target name\nfilters (RegisteredModelListFilters, optional) – An object containing all filters that you’d like to apply to the\nresulting list of registered models.\nReturns:\nregistered_models – A list of registered models user can view.\nReturn type:\nList[RegisteredModel]\nExamples\nfrom datarobot import RegisteredModel\nregistered_models = RegisteredModel.list()\n>>> [RegisteredModel('My Registered Model'), RegisteredModel('My Other Registered Model')]\nfrom datarobot import RegisteredModel\nfrom datarobot.models.model_registry import RegisteredModelListFilters\nfrom datarobot.enums import RegisteredModelSortKey, RegisteredModelSortDirection\nfilters = RegisteredModelListFilters(target_type='Regression')\nregistered_models = RegisteredModel.list(\nfilters=filters,\nsort_key=RegisteredModelSortKey.NAME.value,\nsort_direction=RegisteredModelSortDirection.DESC.value\nsearch='other')\n>>> [RegisteredModel('My Other Registered Model')]\nclassmethod archive(registered_model_id)\nPermanently archive a registered model and all of its versions.\nParameters:\nregistered_model_id (str) – ID of the registered model to be archived\nReturn type:\nNone\nclassmethod update(registered_model_id, name)\nUpdate the name of a registered model.\nParameters:\nregistered_model_id (str) – ID of the registered model to be updated\nname (str) – New name for the registered model\nReturns:\nregistered_model – Updated registered model object\nReturn type:\nRegisteredModel\nget_shared_roles(offset=None, limit=None, id=None)\nRetrieve access control information for this registered model.\nParameters:\noffset (Optional[int]) – The number of records to skip over. Optional. Default is 0.\nlimit (Optional[int]) – The number of records to return. Optional. Default is 100.\nid (Optional[str]) – Return the access control information for a user with this user ID. Optional.\nReturn type:\nList[SharingRole]\nshare(roles)\nShare this registered model or remove access from one or more user(s).\nParameters:\nroles (List[SharingRole]) – A list of SharingRole instances, each of which\nreferences a user and a role to be assigned.\nReturn type:\nNone\nExamples\n>>> from datarobot import RegisteredModel, SharingRole\n>>> from datarobot.enums import SHARING_ROLE, SHARING_RECIPIENT_TYPE\n>>> registered_model = RegisteredModel.get('5c939e08962d741e34f609f0')\n>>> sharing_role = SharingRole(\n...    role=SHARING_ROLE.CONSUMER,\n...    recipient_type=SHARING_RECIPIENT_TYPE.USER,\n...    id='5c939e08962d741e34f609f0',\n...    can_share=True,\n...    )\n>>> registered_model.share(roles=[sharing_role])\nget_version(version_id)\nRetrieve a registered model version.\nParameters:\nversion_id (str) – The ID of the registered model version to retrieve.\nReturns:\nregistered_model_version – A registered model version object.\nReturn type:\nRegisteredModelVersion\nExamples\nfrom datarobot import RegisteredModel\nregistered_model = RegisteredModel.get('5c939e08962d741e34f609f0')\nregistered_model_version = registered_model.get_version('5c939e08962d741e34f609f0')\n>>> RegisteredModelVersion('My Registered Model Version')\nlist_versions(filters=None, search=None, sort_key=None, sort_direction=None, limit=None, offset=None)\nRetrieve a list of registered model versions.\nParameters:\nfilters (Optional[RegisteredModelVersionsListFilters]) – A RegisteredModelVersionsListFilters instance used to filter the list of registered model versions returned.\nsearch (Optional[str]) – A search string used to filter the list of registered model versions returned.\nsort_key (Optional[RegisteredModelVersionSortKey]) – The key to use to sort the list of registered model versions returned.\nsort_direction (Optional[RegisteredModelSortDirection]) – The direction to use to sort the list of registered model versions returned.\nlimit (Optional[int]) – The maximum number of registered model versions to return. Default is 100.\noffset (Optional[int]) – The number of registered model versions to skip over. Default is 0.\nReturns:\nregistered_model_versions – A list of registered model version objects.\nReturn type:\nList[RegisteredModelVersion]\nExamples\nfrom datarobot import RegisteredModel\nfrom datarobot.models.model_registry import RegisteredModelVersionsListFilters\nfrom datarobot.enums import RegisteredModelSortKey, RegisteredModelSortDirection\nregistered_model = RegisteredModel.get('5c939e08962d741e34f609f0')\nfilters = RegisteredModelVersionsListFilters(tags=['tag1', 'tag2'])\nregistered_model_versions = registered_model.list_versions(filters=filters)\n>>> [RegisteredModelVersion('My Registered Model Version')]\nlist_associated_deployments(search=None, sort_key=None, sort_direction=None, limit=None, offset=None)\nRetrieve a list of deployments associated with this registered model.\nParameters:\nsearch (Optional[str])\nsort_key (Optional[RegisteredModelDeploymentSortKey])\nsort_direction (Optional[RegisteredModelSortDirection])\nlimit (Optional[int])\noffset (Optional[int])\nReturns:\ndeployments – A list of deployments associated with this registered model.\nReturn type:\nList[VersionAssociatedDeployment]\nclass datarobot.models.RegisteredModelVersion\nRepresents a version of a registered model.\nParameters:\nid (str) – The ID of the registered model version.\nregistered_model_id (str) – The ID of the parent registered model.\nregistered_model_version (int) – The version of the registered model.\nname (str) – The name of the registered model version.\nmodel_id (str) – The ID of the model.\nmodel_execution_type (str) – Type of model package (version). dedicated (native DataRobot models) and\ncustom_inference_model` (user added inference models) both execute on DataRobot\nprediction servers, external do not\nis_archived (bool) –\nWhether the model package (version) is permanently archived (cannot be used in deployment orreplacement)\nimport_meta (ImportMeta) – Information from when this Model Package (version) was first saved.\nsource_meta (SourceMeta) – Meta information from where this model was generated\nmodel_kind (ModelKind) – Model attribute information.\ntarget (Target) – Target information for the registered model version.\nmodel_description (ModelDescription) – Model description information.\ndatasets (Dataset) – Dataset information for the registered model version.\ntimeseries (Timeseries) – Timeseries information for the registered model version.\nbias_and_fairness (BiasAndFairness) – Bias and fairness information for the registered model version.\nis_deprecated (bool) –\nWhether the model package (version) is deprecated (cannot be used in deployment orreplacement)\npermissions (List[str]) – Permissions for the registered model version.\nactive_deployment_count (int or None) – Number of the active deployments associated with the registered model version.\nbuild_status (str or None) – Model package (version) build status. One of complete, inProgress, failed.\nuser_provided_id (str or None) – User provided ID for the registered model version.\nupdated_at (str or None) – The time the registered model version was last updated.\nupdated_by (UserMetadata or None) – The user who last updated the registered model version.\ntags (List[TagWithId] or None) – The tags associated with the registered model version.\nmlpkg_file_contents (str or None) – The contents of the model package file.\nclassmethod create_for_leaderboard_item(model_id, name=None, prediction_threshold=None, distribution_prediction_model_id=None, description=None, compute_all_ts_intervals=None, registered_model_name=None, registered_model_id=None, tags=None, registered_model_tags=None, registered_model_description=None)\nParameters:\nmodel_id (str) – ID of the DataRobot model.\nname (str or None) – Name of the version (model package).\nprediction_threshold (float or None) – Threshold used for binary classification in predictions.\ndistribution_prediction_model_id (str or None) – ID of the DataRobot distribution prediction model\ntrained on predictions from the DataRobot model.\ndescription (str or None) – Description of the version (model package).\ncompute_all_ts_intervals (bool or None) – Whether to compute all time series prediction intervals (1-100 percentiles).\nregistered_model_name (Optional[str]) – Name of the new registered model that will be created from this model package (version).\nThe model package (version) will be created as version 1 of the created registered model.\nIf neither registeredModelName nor registeredModelId is provided,\nit defaults to the model package (version) name. Mutually exclusive with registeredModelId.\nregistered_model_id (Optional[str]) – Creates a model package (version) as a new version for the provided registered model ID.\nMutually exclusive with registeredModelName.\ntags (Optional[List[Tag]]) – Tags for the registered model version.\nregistered_model_tags (Optional[List[Tag]]) – Tags for the registered model.\nregistered_model_description (Optional[str]) – Description for the registered model.\nReturns:\nregitered_model_version – A new registered model version object.\nReturn type:\nRegisteredModelVersion\nclassmethod create_for_external(name, target, model_id=None, model_description=None, datasets=None, timeseries=None, registered_model_name=None, registered_model_id=None, tags=None, registered_model_tags=None, registered_model_description=None, geospatial_monitoring=None)\nCreate a new registered model version from an external model.\nParameters:\nname (str) – Name of the registered model version.\ntarget (ExternalTarget) – Target information for the registered model version.\nmodel_id (Optional[str]) – Model ID of the registered model version.\nmodel_description (Optional[ModelDescription]) – Information about the model.\ndatasets (Optional[ExternalDatasets]) – Dataset information for the registered model version.\ntimeseries (Optional[Timeseries]) – Timeseries properties for the registered model version.\nregistered_model_name (Optional[str]) – Name of the new registered model that will be created from this model package (version).\nThe model package (version) will be created as version 1 of the created registered model.\nIf neither registeredModelName nor registeredModelId is provided,\nit defaults to the model package (version) name. Mutually exclusive with registeredModelId.\nregistered_model_id (Optional[str]) – Creates a model package (version) as a new version for the provided registered model ID.\nMutually exclusive with registeredModelName.\ntags (Optional[List[Tag]]) – Tags for the registered model version.\nregistered_model_tags (Optional[List[Tag]]) – Tags for the registered model.\nregistered_model_description (Optional[str]) – Description for the registered model.\ngeospatial_monitoring (Optional[ExternalGeospatialMonitoring]) – Geospatial monitoring settings for the registered model version.\nReturns:\nregistered_model_version – A new registered model version object.\nReturn type:\nRegisteredModelVersion\nclassmethod create_for_custom_model_version(custom_model_version_id, name=None, description=None, registered_model_name=None, registered_model_id=None, tags=None, registered_model_tags=None, registered_model_description=None)\nCreate a new registered model version from a custom model version.\nParameters:\ncustom_model_version_id (str) – ID of the custom model version.\nname (Optional[str]) – Name of the registered model version.\ndescription (Optional[str]) – Description of the registered model version.\nregistered_model_name (Optional[str]) – Name of the new registered model that will be created from this model package (version).\nThe model package (version) will be created as version 1 of the created registered model.\nIf neither registeredModelName nor registeredModelId is provided,\nit defaults to the model package (version) name. Mutually exclusive with registeredModelId.\nregistered_model_id (Optional[str]) – Creates a model package (version) as a new version for the provided registered model ID.\nMutually exclusive with registeredModelName.\ntags (Optional[List[Tag]]) – Tags for the registered model version.\nregistered_model_tags (Optional[List[Tag]]) – Tags for the registered model.\nregistered_model_description (Optional[str]) – Description for the registered model.\nReturns:\nregistered_model_version – A new registered model version object.\nReturn type:\nRegisteredModelVersion\nlist_associated_deployments(search=None, sort_key=None, sort_direction=None, limit=None, offset=None)\nRetrieve a list of deployments associated with this registered model version.\nParameters:\nsearch (Optional[str])\nsort_key (Optional[RegisteredModelDeploymentSortKey])\nsort_direction (Optional[RegisteredModelSortDirection])\nlimit (Optional[int])\noffset (Optional[int])\nReturns:\ndeployments – A list of deployments associated with this registered model version.\nReturn type:\nList[VersionAssociatedDeployment]\nclass datarobot.models.model_registry.deployment.VersionAssociatedDeployment\nRepresents a deployment associated with a registered model version.\nParameters:\nid (str) – The ID of the deployment.\ncurrently_deployed (bool) – Whether this version is currently deployed.\nregistered_model_version (int) – The version of the registered model associated with this deployment.\nis_challenger (bool) – Whether the version associated with this deployment is a challenger.\nstatus (str) – The status of the deployment.\nlabel (Optional[str]) – The label of the deployment.\nfirst_deployed_at (datetime.datetime, optional) – The time the version was first deployed.\nfirst_deployed_by (UserMetadata, optional) – The user who first deployed the version.\ncreated_by (UserMetadata, optional) – The user who created the deployment.\nprediction_environment (DeploymentPredictionEnvironment, optional) – The prediction environment of the deployment.\nclass datarobot.models.model_registry.RegisteredModelVersionsListFilters\nFilters for listing of registered model versions.\nParameters:\ntarget_name (str or None) – Name of the target to filter by.\ntarget_type (str or None) – Type of the target to filter by.\ncompatible_with_leaderboard_model_id (str or None.) – If specified, limit results to versions (model packages) of the Leaderboard model with the specified ID.\ncompatible_with_model_package_id (str or None.) – Returns versions compatible with the given model package (version) ID. If used, it will only return versions\nthat match target.name, target.type, target.classNames (for classification models),\nmodelKind.isTimeSeries and modelKind.isMultiseries for the specified model package (version).\nfor_challenger (bool or None) – Can be used with compatibleWithModelPackageId to request similar versions that can be used as challenger\nmodels; for external model packages (versions), instead of returning similar external model packages (versions),\nsimilar DataRobot and Custom model packages (versions) will be retrieved.\nprediction_threshold (float or None) – Return versions with the specified prediction threshold used for binary classification models.\nimported (bool or None) – If specified, return either imported (true) or non-imported (false) versions (model packages).\nprediction_environment_id (str or None) – Can be used to filter versions (model packages) by what is supported by the prediction environment\nmodel_kind (str or None) – Can be used to filter versions (model packages) by model kind.\nbuild_status (str or None) – If specified, filter versions by the build status.\nclass datarobot.models.model_registry.RegisteredModelListFilters\nFilters for listing registered models.\nParameters:\ncreated_at_start (datetime.datetime) – Registered models created on or after this timestamp.\ncreated_at_end (datetime.datetime) – Registered models created before this timestamp. Defaults to the current time.\nmodified_at_start (datetime.datetime) – Registered models modified on or after this timestamp.\nmodified_at_end (datetime.datetime) – Registered models modified before this timestamp. Defaults to the current time.\ntarget_name (str) – Name of the target to filter by.\ntarget_type (str) – Type of the target to filter by.\ncreated_by (str) – Email of the user that created registered model to filter by.\ncompatible_with_leaderboard_model_id (str) – If specified, limit results to registered models containing versions (model packages)\nfor the leaderboard model with the specified ID.\ncompatible_with_model_package_id (str) – Return registered models that have versions (model packages) compatible with given model package (version) ID.\nIf used, will only return registered models which have versions that match target.name, target.type,\ntarget.classNames (for classification models), modelKind.isTimeSeries, and modelKind.isMultiseries\nof the specified model package (version).\nfor_challenger (bool) – Can be used with compatibleWithModelPackageId to request similar registered models that contain\nversions (model packages) that can be used as challenger models; for external model packages (versions),\ninstead of returning similar external model packages (versions), similar DataRobot and Custom model packages\nwill be retrieved.\nprediction_threshold (float) – If specified, return any registered models containing one or more versions matching the prediction\nthreshold used for binary classification models.\nimported (bool) – If specified, return any registered models that contain either imported (true) or non-imported (false)\nversions (model packages).\nprediction_environment_id (str) – Can be used to filter registered models by what is supported by the prediction environment.\nmodel_kind (str) – Return models that contain versions matching a specific format.\nbuild_status (str) – If specified, only return models that have versions with specified build status.\nRulesets\nclass datarobot.models.Ruleset\nRepresents an approximation of a model with DataRobot Prime\nVariables:\nid (str) – the id of the ruleset\nrule_count (int) – the number of rules used to approximate the model\nscore (float) – the validation score of the approximation\nproject_id (str) – the project the approximation belongs to\nparent_model_id (str) – the model being approximated\nmodel_id (str or None) – the model using this ruleset (if it exists).  Will be None if no such model has been\ntrained.\nrequest_model()\nRequest training for a model using this ruleset\nTraining a model using a ruleset is a necessary prerequisite for being able to download\nthe code for a ruleset.\nReturns:\njob – the job fitting the new Prime model\nReturn type:\nJob",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/datarobot-models.html",
      "tags": [
        "api-reference",
        "advanced",
        "beginner",
        "documentation",
        "example"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/datarobot-models.html",
        "content_length": 404562
      },
      "code_examples": [],
      "api_methods": [
        "model.get_forecast_vs_actual_plot_preview",
        "datarobot.models.pareto_front",
        "dr.enums.data_subset",
        "datarobot.helpers.classmappingaggregationsettings",
        "model.get_roc_curve",
        "model.train",
        "datarobot.models.missing_report",
        "datarobot.enums.fairnessmetricsset",
        "model.biasmitigationfeatureinfo",
        "datarobot.models.model",
        "datarobot.enums.series_accuracy_order_by",
        "datarobot.models.ratingtablemodel",
        "datarobot.models.frozenmodel",
        "model.get_version",
        "model.id",
        "model.get_accuracy_over_time_plot_preview",
        "datarobot.models.clusteringmodel",
        "dr.enums.datetime_trend_plots_resolution",
        "datarobot.enums.prime_language",
        "datarobot.models.registry",
        "model.get_all_residuals_charts",
        "datarobot.models.advanced_tuning",
        "model.get_forecast_vs_actual_plot",
        "datarobot.model.get",
        "datarobot.models.datetimemodel",
        "project.get",
        "project.upload_dataset",
        "model.share",
        "model.get_advanced_tuning_params",
        "datarobot.models.model_registry",
        "datarobot.enums.queue_status",
        "datarobot.models.registeredmodelversion",
        "model.list_versions",
        "model.sentinel",
        "model.name",
        "model.get_anomaly_over_time_plot_preview",
        "dr.scoring_type.validation",
        "datarobot.models.cluster",
        "model.get_labelwise_roc_curves",
        "datarobot.errors.servererror",
        "model.advancedtuningparamstype",
        "datarobot.models.cluster_insight",
        "deployment.versionassociateddeployment",
        "datarobot.models.registeredmodel",
        "datarobot.models.blendermodel",
        "model.get_anomaly_over_time_plot",
        "dr.enums.source_type",
        "datarobot.models.genericmodel",
        "datarobot.models.primefile",
        "datarobot.enums.chart_data_source",
        "datarobot.models.ruleset",
        "dr.enums.monotonicity_featurelist_default",
        "model.get_all_feature_impacts",
        "model.get",
        "model.get_all_lift_charts",
        "datarobot.models.primemodel",
        "datarobot.models.modeljob",
        "datarobot.models.modelrecommendation",
        "dr.enums.explanations_algorithm",
        "dr.enums.default_max_wait",
        "project.max_train_rows",
        "model.get_all_roc_curves",
        "dr.scoring_type.cross_validation",
        "model.list",
        "model.get_accuracy_over_time_plot",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_8408766614137855657",
      "title": "Prompting",
      "content": "Prompting\nclass datarobot.models.genai.chat.Chat\nMetadata for a DataRobot GenAI chat.\nVariables:\nid (str) – The chat ID.\nname (str) – The chat name.\nllm_blueprint_id (str) – The ID of the LLM blueprint associated with the chat.\nis_frozen (bool) – Checks whether the chat is frozen. Prompts cannot be submitted to frozen chats.\ncreation_date (str) – The date when the chat was created.\ncreation_user_id (str) – The ID of the creating user.\nwarning (str or None, optional) – The warning about the contents of the chat.\nprompts_count (int) – The number of chat prompts in the chat.\nclassmethod create(name, llm_blueprint)\nCreates a new chat.\nParameters:\nname (str) – The chat name.\nllm_blueprint (LLMBlueprint or str) – The LLM blueprint associated with the created chat, either LLM blueprint or ID.\nReturns:\nchat – The created chat.\nReturn type:\nChat\nclassmethod get(chat)\nRetrieve a single chat.\nParameters:\nchat (Chat or str) – The chat you want to retrieve. Accepts chat or chat ID.\nReturns:\nchat – The requested chat.\nReturn type:\nChat\nclassmethod list(llm_blueprint=None, sort=None)\nList all chats available to the user. If the LLM blueprint is specified,\nresults are restricted to only those chats associated with the LLM blueprint.\nParameters:\nllm_blueprint (Optional[Union[LLMBlueprint, str]], optional) – Returns only those chats associated with a particular LLM blueprint,\nspecified by either the entity or the ID.\nsort (Optional[str]) – The property to sort chats by. Prefix the attribute name with a dash ( - )\nto sort responses in descending order, (for example, ‘-name’).\nSupported options are listed in ListChatsSortQueryParams,\nbut the values can differ depending on platform version.\nThe default sort parameter is None, which results in\nchats returning in order of creation time, descending.\nReturns:\nchats – Returns a list of chats.\nReturn type:\nlist[Chat]\ndelete()\nDelete the single chat.\nReturn type:\nNone\nupdate(name)\nUpdate the chat.\nParameters:\nname (str) – The new name for the chat.\nReturns:\nchat – The updated chat.\nReturn type:\nChat\nclass datarobot.models.genai.chat_prompt.ChatPrompt\nMetadata for a DataRobot GenAI chat prompt.\nVariables:\nid (str) – Chat prompt ID.\ntext (str) – The prompt text.\nllm_blueprint_id (str) – ID of the LLM blueprint associated with the chat prompt.\nllm_id (str) – ID of the LLM type. This must be one of the IDs returned by LLMDefinition.list\nfor this user.\nllm_settings (dict or None) – The LLM settings for the LLM blueprint. The specific keys allowed and the\nconstraints on the values are defined in the response from LLMDefinition.list,\nbut this typically has dict fields. Either:\n- system_prompt - The system prompt that influences the LLM responses.\n- max_completion_length - The maximum number of tokens in the completion.\n- temperature - Controls the variability in the LLM response.\n- top_p - Sets whether the model considers next tokens with top_p probability mass.\nor\n- system_prompt - The system prompt that influences the LLM responses.\n- validation_id - The ID of the external model LLM validation.\n- external_llm_context_size - The external LLM’s context size, in tokens,\nfor external model-based LLM blueprints.\ncreation_date (str) – The date the chat prompt was created.\ncreation_user_id (str) – ID of the creating user.\nvector_database_id (str or None) – ID of the vector database associated with the LLM blueprint, if any.\nvector_database_settings (VectorDatabaseSettings or None) – The settings for the vector database associated with the LLM blueprint, if any.\nresult_metadata (ResultMetadata or None) – Metadata for the result of the chat prompt submission.\nresult_text (str or None) – The result text from the chat prompt submission.\nconfidence_scores (ConfidenceScores or None) – The confidence scores if there is a vector database associated with the chat prompt.\ncitations (list[Citation]) – List of citations from text retrieved from the vector database, if any.\nexecution_status (str) – The execution status of the chat prompt.\nchat_id (Optional[str]) – ID of the chat associated with the chat prompt.\nchat_context_id (Optional[str]) – The ID of the chat context for the chat prompt.\nchat_prompt_ids_included_in_history (Optional[list[str]]) – The IDs of the chat prompts included in the chat history for this chat prompt.\nmetadata_filter (Optional[Dict[str, Any] | None]) – The metadata filter to apply to the vector database.\nSupports:\n- None or empty dict (no filters): Considers all documents\n- Multiple field filters (implicit AND): {“a”: 1, “b”: “b”}\n- Comparison operators: {“field”: {“$gt”: 5}}\n- Logical operators: {“$and”: […], “$or”: […]}\n- Nested combinations of the above\nComparison operators:\n- $eq: equal to (string, int, float, bool)\n- $ne: not equal to (string, int, float, bool)\n- $gt: greater than (int, float)\n- $gte: greater than or equal to (int, float)\n- $lt: less than (int, float)\n- $lte: less than or equal to (int, float)\n- $in: a value is in list (string, int, float, bool)\n- $nin: a value is not in list (string, int, float, bool)\n- $contains: a string contains a value (string)\n- $not_contains: a string does not contain a value (string)\nclassmethod create(text, llm_blueprint=None, chat=None, llm=None, llm_settings=None, vector_database=None, vector_database_settings=None, wait_for_completion=False, metadata_filter=None)\nCreate a new ChatPrompt. This submits the prompt text to the LLM. Either llm_blueprint\nor chat is required.\nParameters:\ntext (str) – The prompt text.\nllm_blueprint (LLMBlueprint or str or None, optional) – The LLM blueprint associated with the created chat prompt, either LLMBlueprint or\nLLM blueprint ID.\nchat (Chat or str or None, optional) – The chat associated with the created chat prompt, either Chat or chat ID.\nllm (LLMDefinition, str, or None, optional) – The LLM to use for the chat prompt, either LLMDefinition or LLM blueprint ID.\nllm_settings (dict or None) – LLM settings to use for the chat prompt. The specific keys allowed and the\nconstraints on the values are defined in the response from LLMDefinition.list\nbut this typically has dict fields:\n- system_prompt - The system prompt that tells the LLM how to behave.\n- max_completion_length - The maximum number of tokens in the completion.\n- temperature - Controls the variability in the LLM response.\n- top_p - Whether the model considers next tokens with top_p probability mass.\nOr\n- system_prompt - The system prompt that tells the LLM how to behave.\n- validation_id - The ID of the custom model LLM validation\nfor custom model LLM blueprints.\nvector_database (VectorDatabase, str, or None, optional) – The vector database to use with this chat prompt submission, either\nVectorDatabase or vector database ID.\nvector_database_settings (VectorDatabaseSettings or None, optional) – Settings for the vector database, if any.\nwait_for_completion (bool) – If set to True, chat prompt result response limit is up to 10 minutes,\nraising a timeout error after that.\nOtherwise, check current status by using ChatPrompt.get with returned ID.\nmetadata_filter (Optional[Dict[str, Any] | None]) – The metadata filter to apply to the vector database.\nSupports:\n- None or empty dict (no filters): Considers all documents\n- Multiple field filters (implicit AND): {“a”: 1, “b”: “b”}\n- Comparison operators: {“field”: {“$gt”: 5}}\n- Logical operators: {“$and”: […], “$or”: […]}\n- Nested combinations of the above\nComparison operators:\n- $eq: equal to (string, int, float, bool)\n- $ne: not equal to (string, int, float, bool)\n- $gt: greater than (int, float)\n- $gte: greater than or equal to (int, float)\n- $lt: less than (int, float)\n- $lte: less than or equal to (int, float)\n- $in: a value is in list (string, int, float, bool)\n- $nin: a value is not in list (string, int, float, bool)\n- $contains: a string contains a value (string)\n- $not_contains: a string does not contain a value (string)\nReturns:\nchat_prompt – The created chat prompt.\nReturn type:\nChatPrompt\nupdate(custom_metrics=None, feedback_metadata=None)\nUpdate the chat prompt.\nParameters:\ncustom_metrics (Optional[list[MetricMetadata]], optional) – The new custom metrics to add to the chat prompt.\nfeedback_metadata (Optional[FeedbackMetadata], optional) – The new feedback to add to the chat prompt.\nReturns:\nchat_prompt – The updated chat prompt.\nReturn type:\nChatPrompt\nclassmethod get(chat_prompt)\nRetrieve a single chat prompt.\nParameters:\nchat_prompt (ChatPrompt or str) – The chat prompt you want to retrieve, either ChatPrompt or chat prompt ID.\nReturns:\nchat_prompt – The requested chat prompt.\nReturn type:\nChatPrompt\nclassmethod list(llm_blueprint=None, playground=None, chat=None)\nList all chat prompts available to the user. If the llm_blueprint, playground, or chat\nis specified then the results are restricted to the chat prompts associated with that\nentity.\nParameters:\nllm_blueprint (Optional[Union[LLMBlueprint, str]], optional) – The returned chat prompts are filtered to those associated with a specific LLM blueprint\nif it is specified. Accepts either LLMBlueprint or LLM blueprint ID.\nplayground (Optional[Union[Playground, str]], optional) – The returned chat prompts are filtered to those associated with a specific playground\nif it is specified. Accepts either Playground or playground ID.\nchat (Optional[Union[Chat, str]], optional) – The returned chat prompts are filtered to those associated with a specific chat\nif it is specified. Accepts either Chat or chat ID.\nReturns:\nchat_prompts – A list of chat prompts available to the user.\nReturn type:\nlist[ChatPrompt]\ndelete()\nDelete the single chat prompt.\nReturn type:\nNone\ncreate_llm_blueprint(name, description='')\nCreate a new LLM blueprint from an existing chat prompt.\nParameters:\nname (str) – LLM blueprint name.\ndescription (Optional[str]) – Description of the LLM blueprint, by default “”.\nReturns:\nllm_blueprint – The created LLM blueprint.\nReturn type:\nLLMBlueprint\nclass datarobot.models.genai.comparison_chat.ComparisonChat\nMetadata for a DataRobot GenAI comparison chat.\nVariables:\nid (str) – The comparison chat ID.\nname (str) – The comparison chat name.\nplayground_id (str) – The ID of the playground associated with the comparison chat.\ncreation_date (str) – The date when the comparison chat was created.\ncreation_user_id (str) – The ID of the creating user.\nclassmethod create(name, playground)\nCreates a new comparison chat.\nParameters:\nname (str) – The comparison chat name.\nplayground (Playground or str) – The playground associated with the created comparison chat, either Playground\nor playground ID.\nReturns:\ncomparison_chat – The created comparison chat.\nReturn type:\nComparisonChat\nclassmethod get(comparison_chat)\nRetrieve a single comparison chat.\nParameters:\ncomparison_chat (ComparisonChat or str) – The comparison chat you want to retrieve. Accepts ComparisonChat or\ncomparison chat ID.\nReturns:\ncomparison_chat – The requested comparison chat.\nReturn type:\nComparisonChat\nclassmethod list(playground=None, sort=None)\nList all comparison chats available to the user. If the playground is specified,\nresults are restricted to only those comparison chats associated with the playground.\nParameters:\nplayground (Optional[Union[Playground, str]], optional) – Returns only those comparison chats associated with a particular playground,\nspecified by either the Playground or the playground ID.\nsort (Optional[str]) – The property to sort comparison chats by. Prefix the attribute name with a dash ( - )\nto sort responses in descending order, (for example, ‘-name’).\nSupported options are listed in ListComparisonChatsSortQueryParams,\nbut the values can differ depending on platform version.\nThe default sort parameter is None, which results in\ncomparison chats returning in order of creation time, descending.\nReturns:\ncomparison_chats – Returns a list of comparison chats.\nReturn type:\nlist[ComparisonChat]\ndelete()\nDelete the single comparison chat.\nReturn type:\nNone\nupdate(name)\nUpdate the comparison chat.\nParameters:\nname (str) – The new name for the comparison chat.\nReturns:\ncomparison_chat – The updated comparison chat.\nReturn type:\nComparisonChat\nclass datarobot.models.genai.comparison_prompt.ComparisonPrompt\nMetadata for a DataRobot GenAI comparison prompt.\nVariables:\nid (str) – Comparison prompt ID.\ntext (str) – The prompt text.\nresults (list[ComparisonPromptResult]) – The list of results for individual LLM blueprints that are part of the comparison prompt.\ncreation_date (str) – The date when the playground was created.\ncreation_user_id (str) – ID of the creating user.\ncomparison_chat_id (str) – The ID of the comparison chat this comparison prompt is associated with.\nmetadata_filter (Optional[Dict[str, Any] | None]) – The metadata filter to apply to the vector database.\nSupports:\n- None or empty dict (no filters): Considers all documents\n- Multiple field filters (implicit AND): {“a”: 1, “b”: “b”}\n- Comparison operators: {“field”: {“$gt”: 5}}\n- Logical operators: {“$and”: […], “$or”: […]}\n- Nested combinations of the above\nComparison operators:\n- $eq: equal to (string, int, float, bool)\n- $ne: not equal to (string, int, float, bool)\n- $gt: greater than (int, float)\n- $gte: greater than or equal to (int, float)\n- $lt: less than (int, float)\n- $lte: less than or equal to (int, float)\n- $in: a value is in list (string, int, float, bool)\n- $nin: a value is not in list (string, int, float, bool)\n- $contains: a string contains a value (string)\n- $not_contains: a string does not contain a value (string)\nupdate(additional_llm_blueprints=None, wait_for_completion=False, feedback_result=None, **kwargs)\nUpdate the comparison prompt.\nParameters:\nadditional_llm_blueprints (list[LLMBlueprint or str]) – The additional LLM blueprints you want to submit the comparison prompt.\nReturns:\ncomparison_prompt – The updated comparison prompt.\nReturn type:\nComparisonPrompt\nclassmethod create(llm_blueprints, text, comparison_chat=None, wait_for_completion=False, metadata_filter=None)\nCreate a new ComparisonPrompt. This submits the prompt text to the LLM blueprints that\nare specified.\nParameters:\nllm_blueprints (list[LLMBlueprint or str]) – The LLM blueprints associated with the created comparison prompt.\nAccepts LLM blueprints or IDs.\ntext (str) – The prompt text.\ncomparison_chat (Optional[ComparisonChat or str], optional) – The comparison chat to add the comparison prompt to. Accepts ComparisonChat or\ncomparison chat ID.\nwait_for_completion (bool) – If set to True code will wait for the chat prompt job to complete before\nreturning the result (up to 10 minutes, raising timeout error after that).\nOtherwise, you can check current status by using ChatPrompt.get with returned ID.\nmetadata_filter (Optional[Dict[str, Any] | None]) – The metadata filter to apply to the vector database.\nSupports:\n- None or empty dict (no filters): Considers all documents\n- Multiple field filters (implicit AND): {“a”: 1, “b”: “b”}\n- Comparison operators: {“field”: {“$gt”: 5}}\n- Logical operators: {“$and”: […], “$or”: […]}\n- Nested combinations of the above\nComparison operators:\n- $eq: equal to (string, int, float, bool)\n- $ne: not equal to (string, int, float, bool)\n- $gt: greater than (int, float)\n- $gte: greater than or equal to (int, float)\n- $lt: less than (int, float)\n- $lte: less than or equal to (int, float)\n- $in: a value is in list (string, int, float, bool)\n- $nin: a value is not in list (string, int, float, bool)\n- $contains: a string contains a value (string)\n- $not_contains: a string does not contain a value (string)\nReturns:\ncomparison_prompt – The created comparison prompt.\nReturn type:\nComparisonPrompt\nclassmethod get(comparison_prompt)\nRetrieve a single comparison prompt.\nParameters:\ncomparison_prompt (str) – The comparison prompt you want to retrieve. Accepts entity or ID.\nReturns:\ncomparison_prompt – The requested comparison prompt.\nReturn type:\nComparisonPrompt\nclassmethod list(llm_blueprints=None, comparison_chat=None)\nList all comparison prompts available to the user that include the specified LLM blueprints\nor from the specified comparison chat.\nParameters:\nllm_blueprints (Optional[List[Union[LLMBlueprint, str]]], optional) – The returned comparison prompts are only those associated with the specified LLM\nblueprints. Accepts either LLMBlueprint or LLM blueprint ID.\ncomparison_chat (Optional[Union[ComparisonChat, str]], optional) – The returned comparison prompts are only those associated with the specified comparison\nchat. Accepts either ComparisonChat or comparison chat ID.\nReturns:\ncomparison_prompts – A list of comparison prompts available to the user that use the specified LLM\nblueprints.\nReturn type:\nlist[ComparisonPrompt]\ndelete()\nDelete the single comparison prompt.\nReturn type:\nNone\nclass datarobot.models.genai.playground.Playground\nMetadata for a DataRobot GenAI playground.\nVariables:\nid (str) – Playground ID.\nname (str) – Playground name.\ndescription (str) – Description of the playground.\nuse_case_id (str) – Linked use case ID.\ncreation_date (str) – The date when the playground was created.\ncreation_user_id (str) – ID of the creating user.\nlast_update_date (str) – Date when the playground was most recently updated.\nlast_update_user_id (str) – ID of the user who most recently updated the playground.\nsaved_llm_blueprints_count (int) – Number of saved LLM blueprints in the playground.\nllm_blueprints_count (int) – Number of LLM blueprints in the playground.\nuser_name (str) – The name of the user who created the playground.\nplayground_type (Optional[PlaygroundType])\nclassmethod create(name, description='', use_case=None, copy_insights=None, playground_type=PlaygroundType.RAG)\nCreate a new playground.\nParameters:\nname (str) – Playground name.\ndescription (Optional[str]) – Description of the playground, by default “”.\nuse_case (Optional[Union[UseCase, str]], optional) – Use case to link to the created playground.\ncopy_insights (CopyInsightsRequest, optional) – If present, copies insights from the source playground to the created playground.\nReturns:\nplayground – The created playground.\nReturn type:\nPlayground\nclassmethod get(playground_id)\nRetrieve a single playground.\nParameters:\nplayground_id (str) – The ID of the playground you want to retrieve.\nReturns:\nplayground – The requested playground.\nReturn type:\nPlayground\nclassmethod list(use_case=None, search=None, sort=None)\nList all playgrounds available to the user. If the use_case is specified or can be\ninferred from the Context then the results are restricted to the playgrounds\nassociated with the UseCase.\nParameters:\nuse_case (Optional[UseCaseLike], optional) – The returned playgrounds are filtered to those associated with a specific Use Case\nor Cases if specified or can be inferred from the Context.\nAccepts either the entity or the ID.\nsearch (Optional[str]) – String for filtering playgrounds.\nPlaygrounds that contain the string in name will be returned.\nIf not specified, all playgrounds will be returned.\nsort (Optional[str]) – Property to sort playgrounds by.\nPrefix the attribute name with a dash to sort in descending order,\ne.g. sort=’-creationDate’.\nCurrently supported options are listed in ListPlaygroundsSortQueryParams\nbut the values can differ with different platform versions.\nBy default, the sort parameter is None which will result in\nplaygrounds being returned in order of creation time descending.\nReturns:\nplaygrounds – A list of playgrounds available to the user.\nReturn type:\nlist[Playground]\nupdate(name=None, description=None)\nUpdate the playground.\nParameters:\nname (str) – The new name for the playground.\ndescription (str) – The new description for the playground.\nReturns:\nplayground – The updated playground.\nReturn type:\nPlayground\ndelete()\nDelete the playground.\nReturn type:\nNone\nclass datarobot.enums.PromptType\nSupported LLM blueprint prompting types.\nclass datarobot.models.genai.user_limits.UserLimits\nCounts for user limits for LLM APIs and vector databases.\nclassmethod get_vector_database_count()\nGet the count of vector databases for the user.\nReturn type:\nAPIObject\nclassmethod get_llm_requests_count()\nGet the count of LLMs requests made by the user.\nReturn type:\nAPIObject\nclass datarobot.models.genai.chat_prompt.ResultMetadata\nMetadata for the result of a chat prompt submission.\nVariables:\noutput_token_count (int) – The number of tokens in the output.\ninput_token_count (int) – The number of tokens in the input. This includes the chat history and documents\nretrieved from a vector database, if any.\ntotal_token_count (int) – The total number of tokens processed.\nestimated_docs_token_count (int) – The estimated number of tokens from the documents retrieved from a vector database, if any.\nlatency_milliseconds (int) – The latency of the chat prompt submission in milliseconds.\nfeedback_result (FeedbackResult) – The lists of user_ids providing positive and negative feedback.\nmetrics (MetricMetadata) – The evaluation metrics for this prompt.\nfinal_prompt (Optional[Union[str, dict]], optional) – Representation of the final prompt sent to the LLM.\nerror_message (str or None, optional) – The error message from the LLM response.\ncost (float or None, optional) – The cost of the chat prompt submission.\nclass datarobot.models.genai.prompt_trace.PromptTrace\nPrompt trace contains aggregated information about a prompt execution.\nVariables:\ntimestamp (str) – The timestamp of the trace (ISO 8601 formatted).\nuser (dict) – The user who submitted the prompt.\nchat_prompt_id (str) – The ID of the chat prompt associated with the trace.\nuse_case_id (str) – The ID of the Use Case the playground is in.\ncomparison_prompt_id (str) – The ID of the comparison prompts associated with the trace.\nllm_blueprint_id (str) – The ID of the LLM blueprint that the prompt was submitted to.\nllm_blueprint_name (str) – The name of the LLM blueprint.\nllm_name (str) – The name of the LLM in the LLM blueprint.\nllm_vendor (str) – The vendor name of the LLM.\nllm_license (str) – What type of license the LLM has.\nllm_settings (dict or None) – The LLM settings for the LLM blueprint. The specific keys allowed and the\nconstraints on the values are defined in the response from LLMDefinition.list,\nbut this typically has dict fields. Either:\n- system_prompt - The system prompt that influences the LLM responses.\n- max_completion_length - The maximum number of tokens in the completion.\n- temperature - Controls the variability in the LLM response.\n- top_p - Sets whether the model considers next tokens with top_p probability mass.\nor\n- system_prompt - The system prompt that influences the LLM responses.\n- validation_id - The ID of the external model LLM validation.\n- external_llm_context_size - The external LLM’s context size, in tokens,\nfor external model-based LLM blueprints.\nchat_name (str or None) – The name of the chat associated with the Trace.\nchat_id (str or None) – The ID of the chat associated with the Trace.\nvector_database_id (str or None) – ID of the vector database associated with the LLM blueprint, if any.\nvector_database_settings (VectorDatabaseSettings or None) – The settings for the vector database associated with the LLM blueprint, if any.\nresult_metadata (ResultMetadata or None) – Metadata for the result of the prompt submission.\nresult_text (str or None) – The result text from the prompt submission.\nconfidence_scores (ConfidenceScores or None) – The confidence scores if there is a vector database associated with the prompt.\ntext (str) – The prompt text submitted to the LLM.\nexecution_status (str) – The execution status of the chat prompt.\nprompt_type (str or None) – The type of prompting strategy, for example history aware.\nevaluation_dataset_configuration_id (str or None) – The ID of the evaluation dataset configuration associated with the trace.\nwarning (str or None) – Any warnings associated with the trace.\nclassmethod list(playground)\nList all prompt traces for a playground.\nParameters:\nplayground (str) – The ID of the playground to list prompt traces for.\nReturns:\nprompt_traces – List of prompt traces for the playground.\nReturn type:\nlist[PromptTrace]\nclassmethod export_to_ai_catalog(playground)\nExport prompt traces to AI Catalog as a CSV.\nParameters:\nplayground (str) – The ID of the playground to export prompt traces for.\nReturns:\nstatus_url – The URL where the status of the job can be monitored\nReturn type:\nstr\nclass datarobot.models.genai.prompt_trace.TraceMetadata\nTrace metadata contains information about all the users and chats that are relevant to\nthis playground.\nVariables:\nusers (list[dict]) – The users who submitted the prompt.\nclassmethod get(playground)\nGet trace metadata for a playground.\nParameters:\nplayground (str) – The ID of the playground to get trace metadata for.\nReturns:\ntrace_metadata – The trace metadata for the playground.\nReturn type:\nTraceMetadata",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-prompting.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-prompting.html",
        "content_length": 24839
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.genai",
        "datarobot.enums.prompttype"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2795828310055220296",
      "title": "AI Robustness Tests",
      "content": "AI Robustness Tests\nclass datarobot.models.genai.insights_configuration.InsightsConfiguration\nConfiguration information for a specific insight.\nVariables:\ninsight_name (str) – The name of the insight.\ninsight_type (InsightTypes, optional) – The type of the insight.\ndeployment_id (Optional[str]) – The deployment ID the insight is applied to.\nmodel_id (Optional[str]) – The model ID for the insight.\nsidecar_model_metric_validation_id (Optional[str]) – Validation ID for the sidecar model metric.\ncustom_metric_id (Optional[str]) – The ID for a custom model metric.\nevaluation_dataset_configuration_id (Optional[str]) – The ID for the evaluation dataset configuration.\ncost_configuration_id (Optional[str]) – The ID for the cost configuration information.\nresult_unit (Optional[str]) – The unit of the result, for example “USD”.\nootb_metric_id (Optional[str]) – The ID of the Datarobot-provided metric that does not require additional configuration.\nootb_metric_name (Optional[str]) – The name of the Datarobot-provided metric that does not require additional configuration.\nguard_conditions (list[dict], optional) – The guard conditions to be used with the insight.\nmoderation_configuration (dict, optional) – The moderation configuration for the insight.\nexecution_status (Optional[str]) – The execution status of the insight.\nerror_message (Optional[str]) – The error message for the insight, for example if it is missing specific configuration\nfor deployed models.\nerror_resolution (Optional[str]) – An indicator of which field must be edited to resolve an error state.\nnemo_metric_id (Optional[str]) – The ID for the NEMO metric.\nllm_id (Optional[str]) – The LLM ID for OOTB metrics that use LLMs.\ncustom_model_llm_validation_id (Optional[str]) – The ID for the custom model LLM validation if using a custom model LLM for OOTB metrics.\naggregation_types (list[str], optional) – The aggregation types to be used for the insight.\nstage (Optional[str]) – The stage (prompt or response) when the metric is calculated.\nsidecar_model_metric_metadata (dict, optional) – Metadata specific to sidecar model metrics.\nguard_template_id (Optional[str]) – The ID for the guard template that applies to the insight.\nguard_configuration_id (Optional[str]) – The ID for the guard configuration that applies to the insight.\nmodel_package_registered_model_id (Optional[str]) – The ID of the registered model package associated with deploymentId.\ncustom_model_guard (Optional[CustomModelGuard]) – The custom model guard configuration, if applicable.\nextra_metric_settings (Optional[ExtraMetricSettings]) – Additional settings for the insight.\nclassmethod from_data(data)\nProperly convert composition classes.\nReturn type:\nInsightsConfiguration\nclass datarobot.models.genai.cost_metric_configurations.LLMCostConfiguration\nCost configuration for a specific LLM model; used for cost metric calculation.\nPrice-per-token is price/reference token count.\nVariables:\n(float) (output_token_price)\n(int) (reference_output_token_count)\n(float)\n(int)\n(str) (llm_id)\n(str)\n(Optional[str]) (custom_model_llm_validation_id)\nclass datarobot.models.genai.cost_metric_configurations.CostMetricConfiguration\nCost metric configuration for a use case.\nVariables:\n(str) (use_case_id)\n(str)\n(List[LLMCostConfiguration]) (cost_metric_configurations)\nclassmethod get(cost_metric_configuration_id)\nGet cost metric configuration by ID.\nReturn type:\nCostMetricConfiguration\nupdate(cost_metric_configurations, name=None)\nUpdate the cost configurations.\nReturn type:\nCostMetricConfiguration\nclassmethod create(use_case_id, playground_id, name, cost_metric_configurations)\nCreate a new cost metric configuration.\nReturn type:\nCostMetricConfiguration\ndelete()\nDelete the cost metric configuration.\nReturn type:\nNone\nclass datarobot.models.genai.evaluation_dataset_configuration.EvaluationDatasetConfiguration\nAn evaluation dataset configuration used to evaluate the performance of LLMs.\nVariables:\nid (str) – The evaluation dataset configuration ID.\nname (str) – The name of the evaluation dataset configuration.\nsize (int) – The size of the evaluation dataset (in bytes).\nrows_count (int) – The row count of the evaluation dataset.\nuse_case_id (str) – The ID of the Use Case associated with the evaluation dataset configuration.\nplayground_id (Optional[str]) – The ID of the playground associated with the evaluation dataset configuration.\ndataset_id (str) – The ID of the evaluation dataset.\ndataset_name (str) – The name of the evaluation dataset.\nprompt_column_name (str) – The name of the dataset column containing the prompt text.\nresponse_column_name (Optional[str]) – The name of the dataset column containing the response text.\ntool_calls_column_name (Optional[str]) – The name of the dataset column containing the expected tool calls.  It is required to evaluate\nthe tool call accuracy metric for agentic workflows.\nagent_goals_column_name (Optional[str]) – The name of the dataset column containing the expected agent goals. It is required to\nevaluate the agent goal accuracy with reference metrics for agentic workflows.\ncorrectness_enabled (Optional[bool]) – Whether correctness is enabled for the evaluation dataset configuration.\ncreation_user_id (str) – The ID of the user who created the evaluation dataset configuration.\ncreation_date (str) – The creation date of the evaluation dataset configuration (ISO-8601 formatted).\ntenant_id (str) – The ID of the DataRobot tenant this evaluation dataset configuration belongs to.\nexecution_status (str) – The execution status of the evaluation dataset configuration.\nerror_message (Optional[str]) – The error message associated with the evaluation dataset configuration.\nclassmethod get(id)\nGet an evaluation dataset configuration by ID.\nParameters:\nid (str) – The evaluation dataset configuration ID to fetch.\nReturns:\nevaluation_dataset_configuration – The evaluation dataset configuration.\nReturn type:\nEvaluationDatasetConfiguration\nclassmethod list(use_case_id, playground_id, evaluation_dataset_configuration_id=None, offset=0, limit=100, sort=None, search=None, correctness_only=False, completed_only=False)\nList all evaluation dataset configurations for a Use Case.\nParameters:\nuse_case_id (str) – The ID of the Use Case that evaluation datasets are returned for.\nplayground_id (str) – The ID of the playground that evaluation datasets are returned for. Default is None.\nevaluation_dataset_configuration_id (Optional[str]) – The ID of the evaluation dataset configuration to fetch. Default is None.\noffset (Optional[int]) – The offset to start fetching evaluation datasets from. Default is 0.\nlimit (Optional[int]) – The maximum number of evaluation datasets to return. Default is 100.\nsort (Optional[str]) – The order of return for evaluation datasets. Default is None, which returns sorting\nby creation time.\nsearch (Optional[str]) – A search term that filters results so that only evaluation datasets with names\nmatching the string are returned. Default is None.\ncorrectness_only (Optional[bool]) – Whether to return only completed datasets (particularly applicable to completion of generated\nsynthetic datasets). Default is False.\ncompleted_only (Optional[bool]) – Whether to return only completed datasets. Default is False.\nReturns:\nevaluation_dataset_configurations – A list of evaluation dataset configurations.\nReturn type:\nList[EvaluationDatasetConfiguration]\nclassmethod create(name, use_case_id, dataset_id, prompt_column_name, playground_id, is_synthetic_dataset=False, response_column_name=None, tool_calls_column_name=None, agent_goals_column_name=None)\nCreate an evaluation dataset configuration for an existing dataset.\nParameters:\nname (str) – The name of the evaluation dataset configuration.\nuse_case_id (str) – The Use Case ID that the evaluation dataset configuration will be added to.\ndataset_id (str) – An ID, to add to the configuration, that identifies the evaluation dataset.\nplayground_id (str) – The ID of the playground that the evaluation dataset configuration will be added to.\nDefault is None.\nprompt_column_name (str) – The name of the prompt column in the dataset.\nresponse_column_name (str) – The name of the response column in the dataset.\ntool_calls_column_name (Optional[str]) – The name of the dataset column containing the expected tool calls.  It is required to evaluate\nthe tool call accuracy metric for agentic workflows.\nagent_goals_column_name (Optional[str]) – The name of the dataset column containing the expected agent goals. It is required to\nevaluate the agent goal accuracy with reference metrics for agentic workflows.\nis_synthetic_dataset (bool) – Whether the evaluation dataset is synthetic.\nReturns:\nevaluation_dataset_configuration – The created evaluation dataset configuration.\nReturn type:\nEvaluationDatasetConfiguration\nupdate(name=None, dataset_id=None, prompt_column_name=None, response_column_name=None, tool_calls_column_name=None, agent_goals_column_name=None)\nUpdate the evaluation dataset configuration.\nParameters:\nname (Optional[str]) – The name of the evaluation dataset configuration.\ndataset_id (Optional[str]) – The ID of the dataset used in this configuration.\nprompt_column_name (Optional[str]) – The name of the prompt column in the dataset.\nresponse_column_name (Optional[str]) – The name of the response column in the dataset.\ntool_calls_column_name (Optional[str]) – The name of the dataset column containing the expected tool calls.  It is required to evaluate\nthe tool call accuracy metric for agentic workflows.\nagent_goals_column_name (Optional[str]) – The name of the dataset column containing the expected agent goals. It is required to\nevaluate the agent goal accuracy with reference metrics for agentic workflows.\nReturns:\nevaluation_dataset_configuration – The updated evaluation dataset configuration.\nReturn type:\nEvaluationDatasetConfiguration\ndelete()\nDelete the evaluation dataset configuration.\nReturn type:\nNone\nclass datarobot.models.genai.evaluation_dataset_metric_aggregation.EvaluationDatasetMetricAggregation\nInformation about an evaluation dataset metric aggregation job.This job runs a metric against LLMs using an evaluation dataset and aggregates the results.\nVariables:\nllm_blueprint_id (str) – The LLM blueprint ID.\nevaluation_dataset_configuration_id (str) – The evaluation dataset configuration ID.\nootb_dataset_name (str | None) – The name of the Datarobot-provided dataset that does not require additional configuration.\nmetric_name (str) – The name of the metric.\ndeployment_id (str | None) – A deployment ID if the evaluation was run against a deployment.\ndataset_id (str | None) – The ID of the dataset used in the evaluation.\ndataset_name (str | None) – The name of the dataset used in the evaluation.\nchat_id (str) – The ID of the chat created to run the evaluation.\nchat_name (str) – The name of the chat that was created to run the evaluation.\naggregation_value (float | List[Dict[str, float]]) – The aggregated metric result.\naggregation_type (AggregationType) – The type of aggregation used for the metric results.\ncreation_date (str) – The date the evaluation job was created.\ncreation_user_id (str) – The ID of the user who created the evaluation job.\ntenant_id (str) – The ID of the tenant that owns the evaluation job.\ncustom_model_guard_id (str | None) – The ID of the custom model’s guard associated with the metric aggregation, if applicable.\nclassmethod create(chat_name, llm_blueprint_ids, evaluation_dataset_configuration_id, insights_configuration)\nCreate a new evaluation dataset metric aggregation job.  The job will run the\nspecified metric for the specified LLM blueprint IDs using the prompt-response pairs in\nthe evaluation dataset.\nParameters:\nchat_name (str) – The name of the chat that will be created to run the evaluation in.\nllm_blueprint_ids (List[str]) – The LLM blueprint IDs to evaluate.\nevaluation_dataset_configuration_id (str) – The ID evaluation dataset configuration to use during the evaluation.\ninsights_configuration (List[InsightsConfiguration]) – The insights configurations to use during the evaluation.\nReturns:\nThe ID of the evaluation dataset metric aggregation job.\nReturn type:\nstr\nclassmethod list(llm_blueprint_ids=None, chat_ids=None, evaluation_dataset_configuration_ids=None, metric_names=None, aggregation_types=None, current_configuration_only=False, sort=None, offset=0, limit=100, non_errored_only=True)\nList evaluation dataset metric aggregations.  The results will be filtered by the provided\nLLM blueprint IDs and chat IDs.\nParameters:\nllm_blueprint_ids (List[str]) – The LLM blueprint IDs to filter on.\nchat_ids (List[str]) – The chat IDs to filter on.\nevaluation_dataset_configuration_ids (List[str]) – The evaluation dataset configuration IDs to filter on.\nmetric_names (List[str]) – The metric names to filter on.\naggregation_types (List[str]) – The aggregation types to filter on.\ncurrent_configuration_only (Optional[bool]) – If True, only results that are associated with the current configuration of the LLM blueprint\nwill be returned.  Defaults to False.\nsort (Optional[str]) – The field to sort on.  Defaults to None.\noffset (Optional[int]) – The offset to start at.  Defaults to 0.\nlimit (Optional[int]) – The maximum number of results to return.  Defaults to 100.\nnon_errored_only (Optional[bool]) – If True, only results that did not encounter an error will be returned.  Defaults to True.\nReturns:\nA list of evaluation dataset metric aggregations.\nReturn type:\nList[EvaluationDatasetMetricAggregation]\nclassmethod delete(llm_blueprint_ids, chat_ids)\nDelete the associated evaluation dataset metric aggregations.  Either llm_blueprint_ids\nor chat_ids must be provided.  If both are provided, only results matching both will be removed.\nParameters:\nllm_blueprint_ids (List[str]) – The LLM blueprint IDs to filter on.\nchat_ids (List[str]) – The chat IDs to filter on.\nReturn type:\nNone\nclass datarobot.models.genai.synthetic_evaluation_dataset_generation.SyntheticEvaluationDataset\nA synthetically generated evaluation dataset for LLMs.\nVariables:\n(str) (response_column_name)\n(str)\n(str)\nclassmethod create(llm_id, vector_database_id, llm_settings=None, dataset_name=None, language=None)\nCreate a synthetic evaluation dataset generation job.  This will\ncreate a synthetic dataset to be used for evaluation of a language model.\nParameters:\n(str) (language)\n(Dict[Optional[str][Union[bool (llm_settings) – model used for dataset generation.\nint (The settings to use for the language) – model used for dataset generation.\nfloat (The settings to use for the language) – model used for dataset generation.\nstr]]]) (The settings to use for the language) – model used for dataset generation.\n(str)\n(str)\n(str)\nReturns:\nSyntheticEvaluationDataset\nReturn type:\nReference to the synthetic evaluation dataset that was created.\nclass datarobot.models.genai.sidecar_model_metric.SidecarModelMetricValidation\nA sidecar model metric validation for LLMs.\nVariables:\nid (str) – The ID of the sidecar model metric validation.\nprompt_column_name (str) – The name of the prompt column for the sidecar model.\ndeployment_id (str) – The ID of the deployment associated with the sidecar model.\nmodel_id (str) – The ID of the sidecar model.\nvalidation_status (str) – The status of the validation job.\ndeployment_access_data (dict) – The data that will be used for accessing the deployment prediction server.\nThis field is only available for deployments that pass validation.\nDict fields are as follows:\n- prediction_api_url - The URL for the deployment prediction server.\n- datarobot_key - The first of two auth headers for the prediction server.\n- authorization_header - The second of two auth headers for the prediction server.\n- input_type - The input type the model expects, either JSON or CSV.\n- model_type - The target type of the deployed custom model.\ntenant_id (str) – The ID of the tenant that created the sidecar model metric validation.\nname (str) – The name of the sidecar model metric.\ncreation_date (str) – The date the sidecar model metric validation was created.\nuser_id (str) – The ID of the user that created the sidecar model metric validation.\ndeployment_name (str) – The name of the deployment associated with the sidecar model.\nuser_name (str) – The name of the user that created the sidecar model metric validation.\nuse_case_id (str) – The ID of the Use Case associated with the sidecar model metric validation.\nprediction_timeout (int) – The timeout in seconds for the prediction API used in this sidecar model metric validation.\nerror_message (str) – Additional information for the errored validation.\ncitations_prefix_column_name (str) – The name of the prefix in the citations column for the sidecar model.\nresponse_column_name (str) – The name of the response column for the sidecar model.\nexpected_response_column_name (str) – The name of the expected response column for the sidecar model.\ntarget_column_name (str) – The name of the target column for the sidecar model.\nclassmethod create(deployment_id, name, prediction_timeout, model_id=None, use_case_id=None, playground_id=None, prompt_column_name=None, target_column_name=None, response_column_name=None, citation_prefix_column_name=None, expected_response_column_name=None)\nCreate a sidecar model metric validation.\nParameters:\ndeployment_id (str) – The ID of the deployment to validate.\nname (str) – The name of the validation.\nprediction_timeout (int) – The timeout in seconds for the prediction API used in this validation.\nmodel_id (Optional[str]) – The ID of the model to validate.\nuse_case_id (Optional[str]) – The ID of the Use Case associated with the validation.\nplayground_id (Optional[str]) – The ID of the playground associated with the validation.\nprompt_column_name (Optional[str]) – The name of the prompt column for the sidecar model.\ntarget_column_name (Optional[str]) – The name of the target column for the sidecar model.\nresponse_column_name (Optional[str]) – The name of the response column for the sidecar model.\ncitation_prefix_column_name (Optional[str]) – The name of the prefix for citations column for the sidecar model.\nexpected_response_column_name (Optional[str]) – The name of the expected response column for the sidecar model.\nReturns:\nThe created sidecar model metric validation.\nReturn type:\nSidecarModelMetricValidation\nclassmethod list(use_case_ids=None, offset=None, limit=None, search=None, sort=None, completed_only=True, deployment_id=None, model_id=None, prompt_column_name=None, target_column_name=None, citation_prefix_column_name=None)\nList sidecar model metric validations.\nParameters:\nuse_case_ids (List[str], optional) – The IDs of the use cases to filter by.\noffset (Optional[int]) – The number of records to skip.\nlimit (Optional[int]) – The maximum number of records to return.\nsearch (Optional[str]) – The search string.\nsort (Optional[str]) – The sort order.\ncompleted_only (Optional[bool]) – Whether to return only completed validations.\ndeployment_id (Optional[str]) – The ID of the deployment to filter by.\nmodel_id (Optional[str]) – The ID of the model to filter by.\nprompt_column_name (Optional[str]) – The name of the prompt column to filter by.\ntarget_column_name (Optional[str]) – The name of the target column to filter by.\ncitation_prefix_column_name (Optional[str]) – The name of the prefix for citations column to filter by.\nReturns:\nThe list of sidecar model metric validations.\nReturn type:\nList[SidecarModelMetricValidation]\nclassmethod get(validation_id)\nGet a sidecar model metric validation by ID.\nParameters:\nvalidation_id (str) – The ID of the validation to get.\nReturns:\nThe sidecar model metric validation.\nReturn type:\nSidecarModelMetricValidation\nrevalidate()\nRevalidate the sidecar model metric validation.\nReturns:\nThe sidecar model metric validation.\nReturn type:\nSidecarModelMetricValidation\nupdate(name=None, prompt_column_name=None, target_column_name=None, response_column_name=None, expected_response_column_name=None, citation_prefix_column_name=None, deployment_id=None, model_id=None, prediction_timeout=None)\nUpdate the sidecar model metric validation.\nParameters:\nname (Optional[str]) – The name of the validation.\nprompt_column_name (Optional[str]) – The name of the prompt column for the sidecar model.\ntarget_column_name (Optional[str]) – The name of the target column for the sidecar model.\nresponse_column_name (Optional[str]) – The name of the response column for the sidecar model.\nexpected_response_column_name (Optional[str]) – The name of the expected response column for the sidecar model.\ncitation_prefix_column_name (Optional[str]) – The name of the prefix for citations column for the sidecar model.\ndeployment_id (Optional[str]) – The ID of the deployment to validate.\nmodel_id (Optional[str]) – The ID of the model to validate.\nprediction_timeout (Optional[int]) – The timeout in seconds for the prediction API used in this validation.\nReturns:\nThe updated sidecar model metric validation.\nReturn type:\nSidecarModelMetricValidation\ndelete()\nDelete the sidecar model metric validation.\nReturn type:\nNone\nclass datarobot.models.genai.llm_test_configuration.LLMTestConfiguration\nMetadata for a DataRobot GenAI LLM test configuration.\nVariables:\nid (str) – The LLM test configuration ID.\nname (str) – The LLM test configuration name.\ndescription (str) – The LLM test configuration description.\ndataset_evaluations (list[DatasetEvaluation]) – The dataset/insight combinations that make up the LLM test configuration.\nllm_test_grading_criteria (LLMTestGradingCriteria) – The criteria used to grade the result of the LLM test configuration.\nis_out_of_the_box_test_configuration (bool) – Whether this is an out-of-the-box configuration.\nuse_case_id (Optional[str]) – The ID of the linked Use Case, if any.\ncreation_date (Optional[str]) – The date the LLM test configuration was created, if any.\ncreation_user_id (Optional[str]) – The ID of the creating user, if any.\nwarnings (Optional[list[Dict[str, str]]]) – The warnings for the LLM test configuration, if any.\nclassmethod create(name, dataset_evaluations, llm_test_grading_criteria, use_case=None, description=None)\nCreates a new LLM test configuration.\nParameters:\nname (str) – The LLM test configuration name.\ndataset_evaluations (list[DatasetEvaluationRequestDict]) – The LLM test dataset evaluation requests.\nllm_test_grading_criteria (LLMTestGradingCriteria) – The LLM test grading criteria.\nuse_case (Optional[Union[UseCase, str]], optional) – Use case to link to the created llm test configuration.\ndescription (Optional[str]) – The LLM test configuration description. If None, the default,\ndescription returns an empty string.\nReturns:\nllm_test_configuration – The created LLM test configuration.\nReturn type:\nLLMTestConfiguration\nclassmethod get(llm_test_configuration)\nRetrieve a single LLM Test configuration.\nParameters:\nllm_test_configuration (LLMTestConfiguration or str) – The LLM test configuration to retrieve, either LLMTestConfiguration or LLMTestConfiguration ID.\nReturns:\nllm_test_configuration – The requested LLM Test configuration.\nReturn type:\nLLMTestConfiguration\nclassmethod list(use_case=None, test_config_type=None)\nList all LLM test configurations available to the user. If a Use Case is specified,\nresults are restricted to only those configurations associated with that Use Case.\nParameters:\nuse_case (Optional[UseCaseLike], optional) – Returns only those configurations associated with a particular Use Case,\nspecified by either the Use Case name or ID.\ntest_config_type (Optional[LLMTestConfigurationType], optional) – Returns only configurations of the specified type. If not specified,\nthe custom test configurations are returned.\nReturns:\nllm_test_configurations – Returns a list of LLM test configurations.\nReturn type:\nlist[LLMTestConfiguration]\nupdate(name=None, description=None, dataset_evaluations=None, llm_test_grading_criteria=None)\nUpdate the LLM test configuration.\nParameters:\nname (Optional[str]) – The new LLM test configuration name.\ndescription (Optional[str]) – The new LLM test configuration description.\ndataset_evaluations (list[DatasetEvaluationRequestDict], optional) – The new dataset evaluation requests.\nllm_test_grading_criteria (LLMTestGradingCriteria, optional) – The new grading criteria.\nReturns:\nllm_test_configuration – The updated LLM test configuration.\nReturn type:\nLLMTestConfiguration\ndelete()\nDelete a single LLM test configuration.\nReturn type:\nNone\nclass datarobot.models.genai.llm_test_configuration.LLMTestConfigurationSupportedInsights\nMetadata for a DataRobot GenAI LLM test configuration supported insights.\nVariables:\nsupported_insight_configurations (list[InsightsConfiguration]) – The supported insights for LLM test configurations.\nclassmethod list(use_case=None, playground=None)\nList all supported insights for a LLM test configuration.\nParameters:\nuse_case (Optional[Union[UseCase, str]], optional) – Returns only those supported insight configurations\nassociated with a particular Use Case, specified by\neither the Use Case name or ID.\nplayground (Optional[Union[Playground, str]], optional) – Returns only those supported insight configurations\nassociated with a particular playground, specified by\neither the Playground or ID.\nReturns:\nllm_test_configuration_supported_insights – Returns the supported insight configurations for the\nLLM test configuration.\nReturn type:\nLLMTestConfigurationSupportedInsights\nclass datarobot.models.genai.llm_test_result.LLMTestResult\nMetadata for a DataRobot GenAI LLM test result.\nVariables:\nid (str) – The LLM test result ID.\nllm_test_configuration_id (str) – The LLM test configuration ID associated with this LLM test result.\nllm_test_configuration_name (str) – The LLM test configuration name associated with this LLM test result.\nuse_case_id (str) – The ID of the Use Case associated with this LLM test result.\nllm_blueprint_id (str) – The ID of the LLM blueprint for this LLM test result.\nllm_test_grading_criteria (LLMTestGradingCriteria) – The criteria used to grade the result of the LLM test configuration.\ngrading_result (GradingResult) – The overall grading result for the LLM test.\npass_percentage (float) – The percentage of insight evaluation results that passed the grading criteria.\nexecution_status (str) – The execution status of the job that evaluated the LLM test result.\ninsight_evaluation_result (list[InsightEvaluationResult]) – The results for the individual insights that make up the LLM test result.\ncreation_date (str) – The date of the LLM test result.\ncreation_user_id (str) – The ID of the user who executed the LLM test.\ncreation_user_name (str) – The name of the user who executed the LLM test.\nclassmethod create(llm_test_configuration, llm_blueprint)\nCreate a new LLMTestResult. This executes the LLM test configuration using the\nspecified LLM blueprint. To check the status of the LLM test, use the\nLLMTestResult.get method with the returned ID.\nParameters:\nllm_test_configuration (LLMTestConfiguration or str) – The LLM test configuration to execute, either LLMTestConfiguration or\nthe LLM test configuration ID.\nllm_blueprint (LLMBlueprint or str) – The LLM blueprint to test, either LLMBlueprint or\nthe LLM blueprint ID.\nReturns:\nllm_test_result – The created LLM test result.\nReturn type:\nLLMTestResult\nclassmethod get(llm_test_result)\nRetrieve a single LLM test result.\nParameters:\nllm_test_result (LLMTestResult or str) – The LLM test result to retrieve, specified by either LLM test result or test ID.\nReturns:\nllm_test_result – The requested LLM test result.\nReturn type:\nLLMTestResult\nclassmethod list(llm_test_configuration=None, llm_blueprint=None)\nList all LLM test results available to the user. If the LLM test configuration or LLM\nblueprint is specified, results are restricted to only those LLM test results associated\nwith the LLM test configuration or LLM blueprint.\nParameters:\nllm_test_configuration (Optional[Union[LLMTestConfiguration, str]]) – The returned LLM test results are filtered to those associated with a specific\nLLM test configuration, if specified.\nllm_blueprint (Optional[Union[LLMBlueprint, str]]) – The returned LLM test results, filtered by those associated with a specific\nLLM blueprint, if specified.\nReturns:\nllm_test_results – Returns a list of LLM test results.\nReturn type:\nList[LLMTestResult]\ndelete()\nDelete a single LLM test result.\nReturn type:\nNone\nclass datarobot.models.genai.llm_test_configuration.DatasetEvaluation\nMetadata for a DataRobot GenAI dataset evaluation.\nVariables:\nevaluation_name (str) – The name of the evaluation.\nevaluation_dataset_configuration_id (str or None, optional) – The ID of the evaluation dataset configuration for custom datasets.\nevaluation_dataset_name (str) – The name of the evaluation dataset.\nootb_dataset (OOTBDataset or None, optional) – Out-of-the-box dataset.\ninsight_configuration (InsightsConfiguration) – The insight to calculate for this dataset.\ninsight_grading_criteria (InsightGradingCriteria) – The criteria to use for grading the results.\nmax_num_prompts (int) – The maximum number of prompts to use for the evaluation.\nprompt_sampling_strategy (PromptSamplingStrategy) – The prompt sampling strategy for the dataset evaluation.\nclass datarobot.models.genai.llm_test_result.InsightEvaluationResult\nMetadata for a DataRobot GenAI insight evaluation result.\nVariables:\nid (str) – The ID of the insight evaluation result.\nllm_test_result_id (str) – The ID of the LLM test result associated with this insight evaluation result.\nevaluation_dataset_configuration_id (str) – The ID of the evaluation dataset configuration.\nevaluation_dataset_name (str) – The name of the evaluation dataset.\nmetric_name (str) – The name of the metric.\nchat_id (str) – The ID of the chat containing the prompts and responses.\nchat_name (str) – The name of the chat containing the prompts and responses.\naggregation_type (AggregationType) – The type of aggregation used for the metric results.\ngrading_result (GradingResult) – The overall grade for the LLM test.\nexecution_status (str) – The execution status of the LLM test.\nevaluation_name (str) – The name of the evaluation.\ninsight_grading_criteria (InsightGradingCriteria) – The criteria to grade the results.\nlast_update_date (str) – The date the result was most recently updated.\naggregation_value (float | List[Dict[str, float]] | None) – The aggregated metric result.\nclass datarobot.models.genai.llm_test_configuration.OOTBDatasetDict\nclass datarobot.models.genai.llm_test_configuration.DatasetEvaluationRequestDict\nclass datarobot.models.genai.llm_test_configuration.DatasetEvaluationDict\nclass datarobot.models.genai.nemo_configuration.NemoConfiguration\nConfiguration for the Nemo Pipeline.\nVariables:\nprompt_pipeline_metric_name (Optional[str]) – The name of the metric for the prompt pipeline.\nprompt_pipeline_files (NemoFileContentsResponse, optional) – The files used in the prompt pipeline.\nprompt_llm_configuration (NemoLLMConfiguration, optional) – The LLM configuration for the prompt pipeline.\nprompt_moderation_configuration (ModerationConfigurationWithoutID, optional) – The moderation configuration for the prompt pipeline.\nprompt_pipeline_template_id (Optional[str]) – The ID of the prompt pipeline template. This parameter defines the actions.py file.\nresponse_pipeline_metric_name (Optional[str]) – The name of the metric for the response pipeline.\nresponse_pipeline_files (NemoFileContentsResponse, optional) – The files used in the response pipeline.\nresponse_llm_configuration (NemoLLMConfiguration, optional) – The LLM configuration for the response pipeline.\nresponse_moderation_configuration (ModerationConfigurationWithoutID, optional) – The moderation configuration for the response pipeline.\nresponse_pipeline_template_id (Optional[str]) – The ID of the response pipeline template. This parameter defines the actions.py file.\nblocked_terms_file_contents (str) – The contents of the blocked terms file.  This is shared between the prompt and response pipelines.\nclassmethod get(playground)\nGet the Nemo configuration for a playground.\nParameters:\nplayground (str or Playground) – The playground to get the configuration for\nReturns:\nThe Nemo configuration for the playground.\nReturn type:\nNemoConfiguration\nclassmethod upsert(playground, blocked_terms_file_contents, prompt_pipeline_metric_name=None, prompt_pipeline_files=None, prompt_llm_configuration=None, prompt_moderation_configuration=None, prompt_pipeline_template_id=None, response_pipeline_metric_name=None, response_pipeline_files=None, response_llm_configuration=None, response_moderation_configuration=None, response_pipeline_template_id=None)\nCreate or update the nemo configuration for a playground.\nParameters:\nplayground (str or Playground) – The playground for the configuration\nblocked_terms_file_contents (str) – The contents of the blocked terms file.\nprompt_pipeline_metric_name (Optional[str]) – The name of the metric for the prompt pipeline.\nprompt_pipeline_files (NemoFileContents, optional) – The files used in the prompt pipeline.\nprompt_llm_configuration (NemoLLMConfiguration, optional) – The LLM configuration for the prompt pipeline.\nprompt_moderation_configuration (ModerationConfigurationWithoutID, optional) – The moderation configuration for the prompt pipeline.\nprompt_pipeline_template_id (Optional[str]) – The ID of the prompt pipeline template, this will define the action.py file.\nresponse_pipeline_metric_name (Optional[str]) – The name of the metric for the response pipeline.\nresponse_pipeline_files (NemoFileContents, optional) – The files used in the response pipeline.\nresponse_llm_configuration (NemoLLMConfiguration, optional) – The LLM configuration for the response pipeline.\nresponse_moderation_configuration (ModerationConfigurationWithoutID, optional) – The moderation configuration for the response pipeline.\nresponse_pipeline_template_id (Optional[str]) – The ID of the response pipeline template, this will define the action.py file.\nReturns:\nThe Nemo configuration for the playground.\nReturn type:\nNemoConfiguration\nclass datarobot.models.genai.llm_test_configuration.OOTBDataset\nMetadata for a DataRobot GenAI out-of-the-box LLM compliance test dataset.\nVariables:\ndataset_name (str) – The name of the dataset.\nprompt_column_name (str) – The name of the prompt column.\nresponse_column_name (str or None, optional) – The name of the response column, if any.\ndataset_url (str or None, optional) – The URL of the dataset.\nrows_count (int) – The number of rows in the dataset.\nwarning (str or None, optional) – A warning message regarding the contents of the dataset, if any.\nclassmethod list()\nList all out-of-the-box datasets available to the user.\nReturns:\nootb_datasets – Returns a list of out-of-the-box datasets.\nReturn type:\nlist[OOTBDataset]\nclass datarobot.models.genai.llm_test_configuration.NonOOTBDataset\nMetadata for a DataRobot GenAI non out-of-the-box (OOTB) LLM compliance test dataset.\nclassmethod list(use_case=None)\nList all non out-of-the-box datasets available to the user.\nReturns:\nnon_ootb_datasets – Returns a list of non out-of-the-box datasets.\nReturn type:\nlist[NonOOTBDataset]\nclass datarobot.models.genai.metric_insights.MetricInsights\nMetric insights for playground.\nclassmethod list(playground, llm_blueprint_ids=None)\nGet metric insights for playground.\nParameters:\nplayground (str or Playground) – Playground to get the supported metrics from.\nllm_blueprint_ids (Optional[Sequence[str]]) – LLM Blueprint IDs to check for additional metrics support for.\nReturns:\ninsights – Metric insights for playground.\nReturn type:\nlist[InsightsConfiguration]\nclassmethod copy_to_playground(source_playground, target_playground, add_to_existing=True, with_evaluation_datasets=False)\nCopy metric insights from one playground to another.\nParameters:\nsource_playground (str or Playground) – Playground to copy metric insights from.\ntarget_playground (str or Playground) – Playground to copy metric insights to.\nadd_to_existing (Optional[bool]) – Add metric insights to existing ones in the target playground, by default True.\nwith_evaluation_datasets (Optional[bool]) – Copy evaluation datasets from the source playground.\nReturn type:\nNone\nclass datarobot.models.genai.ootb_metric_configuration.PlaygroundOOTBMetricConfiguration\nOOTB metric configurations for a playground.\nVariables:\nootb_metric_configurations ((List[OOTBMetricConfigurationResponse]): The list of the OOTB metric configurations.)\nclassmethod get(playground_id)\nGet OOTB metric configurations for the playground.\nReturn type:\nPlaygroundOOTBMetricConfiguration\nclassmethod create(playground_id, ootb_metric_configurations)\nCreate a new OOTB metric configurations.\nReturn type:\nPlaygroundOOTBMetricConfiguration\nclass datarobot.models.genai.evaluation_dataset_utils.ReferenceToolCall\nReference tool call for an evaluation dataset.  This is a convenience stand in\nfor the Ragas ToolCall class.\njson()\nConvert the tool call to a JSON string.\nReturn type:\nstr\nclassmethod from_json(json_str)\nCreate a ReferenceToolCall object from a JSON string.\nReturn type:\nReferenceToolCall\nclass datarobot.models.genai.evaluation_dataset_utils.ReferenceToolCalls\nUtility for creating a list of reference tool calls for an evaluation dataset. This\nclass represents a list of tool calls for a single row in the evaluation dataset.\nExample usage:\n>>> df = pandas.DataFrame()\n>>> tool_calls_1 = ReferenceToolCalls([\n>>>     ReferenceToolCall(name=”get_weather”, args={“location”: “New York”}),\n>>>     ReferenceToolCall(name=”get_news”, args={“topic”: “technology”})\n>>> ])\n>>> tool_calls_2 = ReferenceToolCalls([\n>>>     ReferenceToolCall(name=”get_weather”, args={“location”: “Los Angeles”}),\n>>>     ReferenceToolCall(name=”get_news”, args={“topic”: “sports”})\n>>> ])\n>>> df[‘prompts’] = [‘what is the weather for the tech conference in NYC?’,\n>>> ‘what is the weather in LA?, and will it affect the game?’]\n>>> df[‘reference_tool_calls’] = [tool_calls_1.json(), tool_calls_2.json()]\nclassmethod from_json(json_str)\nCreate a ReferenceToolCalls object from a JSON string.\nReturn type:\nReferenceToolCalls",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-testing.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-testing.html",
        "content_length": 38183
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.genai"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_1354903506282083515",
      "title": "Vector Databases",
      "content": "Vector Databases\nclass datarobot.models.genai.vector_database.CustomModelVectorDatabaseValidation\nValidation record checking the ability of the deployment to serve as a vector database.\nVariables:\nid (str) – The ID of the validation.\nprompt_column_name (str) – The column name the deployed model expect as the input.\ntarget_column_name (str) – The target name deployed model will output.\ndeployment_id (str) – ID of the deployment.\nmodel_id (str) – ID of the underlying deployment model.\nCan be found from the API as Deployment.model[“id”].\nvalidation_status (str) – Can be TESTING, FAILED and PASSED. Only PASSED allowed for use.\ndeployment_access_data (dict, optional) – The data that will be used for accessing the deployment prediction server.\nThis field is only available for deployments that pass validation.\nDict fields are as follows:\n- prediction_api_url - The URL for the deployment prediction server.\n- datarobot_key - The first of two auth headers for the prediction server.\n- authorization_header - The second of two auth headers for the prediction server.\n- input_type - The input type the model expects, either JSON or CSV.\n- model_type - The target type of the deployed custom model.\ntenant_id (str) – The creating user’s tenant ID.\nname (str) – The display name of the validated custom model.\ncreation_date (str) – The creation date of the validation (ISO 8601 formatted).\nuser_id (str) – The ID of the creating user.\nerror_message (Optional[str]) – Additional information for the errored validation.\ndeployment_name (Optional[str]) – The name of the validated deployment.\nuser_name (Optional[str]) – The name of the creating user.\nuse_case_id (Optional[str]) – The ID of the Use Case associated with the validation.\nclass datarobot.models.genai.vector_database.SupportedEmbeddings\nAll supported embedding models including the recommended default model.\nVariables:\nembedding_models (list[EmbeddingModel]) – All supported embedding models.\ndefault_embedding_model (str) – Name of the default recommended text embedding model.\nCurrently supported options are listed in VectorDatabaseEmbeddingModel\nbut the values can differ with different platform versions.\ncustom_model_embedding_validations (List[str]) – External embedding models that have been validated\nclass datarobot.models.genai.vector_database.SupportedTextChunkings\nSupported text chunking configurations which includes a set of\nrecommended chunking parameters for each supported embedding model.\nVariables:\ntext_chunking_configs – All supported text chunking configurations.\nclass datarobot.models.genai.vector_database.VectorDatabase\nMetadata for a DataRobot vector database accessible to the user.\nVariables:\nid (str) – Vector database ID.\nname (str) – Vector database name.\nsize (int) – Size of the vector database assets in bytes.\nuse_case_id (str) – Linked use case ID.\ndataset_id (str) – ID of the dataset used for creation.\nembedding_model (str) – Name of the text embedding model.\nCurrently supported options are listed in VectorDatabaseEmbeddingModel\nbut the values can differ with different platform versions.\nchunking_method (str or None) – Name of the method to split dataset documents.\nCurrently supported options are listed in VectorDatabaseChunkingMethod\nbut the values can differ with different platform versions.\nchunk_size (int or None) – Size of each text chunk in number of tokens.\nchunk_overlap_percentage (int or None) – Overlap percentage between chunks.\nchunks_count (int) – Total number of text chunks.\ncustom_chunking (bool) – Determines if the chunking is custom. With custom chunking,\ndataset rows are not split into chunks automatically;\ninstead, the user provides the chunks.\nseparators (list[string] or None) – Separators for document splitting.\ncreation_date (str) – Date when the database was created.\ncreation_user_id (str) – The ID of the creating user.\norganization_id (str) – The creating user’s organization ID.\ntenant_id (str) – The creating user’s tenant ID.\nlast_update_date (str) – Last update date for the database.\nexecution_status (str) – Database execution status.\nCurrently supported options are listed in VectorDatabaseExecutionStatus\nbut the values can differ with different platform versions.\nplaygrounds_count (int) – Number of using playgrounds.\ndataset_name (str) – Name of the used dataset.\nuser_name (str) – Name of the creating user.\nsource (str) – Source of the vector database.\nCurrently supported options are listed in VectorDatabaseSource\nbut the values can differ with different platform versions.\nvalidation_id (Optional[str]) – ID of custom model vector database validation.\nOnly filled for external vector databases.\nerror_message (Optional[str]) – Additional information for errored vector database.\nembedding_validation_id (Optional[str]) – ID of the custom embedding validation, if any.\nis_separator_regex (bool) – Whether the separators should be treated as regular expressions.\nexternal_vector_database_connection (Optional[dict]) – Parameters defining the external vector database connection to use.\nmetadata_dataset_id (Optional[str]) – The ID of the dataset used to add additional metadata to the vector database.\nmetadata_dataset_name (Optional[str]) – The name of the dataset used to add additional metadata to the vector database.\nmetadata_combination_strategy (Optional[VectorDatabaseMetadataCombinationStrategy]) – The strategy used to combine metadata when there is duplication between the dataset and\nthe metadata dataset.\nadded_metadata_dataset_pairs (Optional[List[Dict[str, str]]) – Pairs of dataset_id and metadata_dataset_id that have been added to the vector database.\nclassmethod get_supported_embeddings(dataset_id=None, use_case=None)\nGet all supported and the recommended embedding models.\nParameters:\ndataset_id (Optional[str]) – ID of a dataset for which the recommended model is returned\nbased on the detected language of that dataset.\nuse_case (Optional[UseCase, str]) – May be Use Case ID or the Use Case entity.\nReturns:\nsupported_embeddings – The supported embedding models.\nReturn type:\nSupportedEmbeddings\nsubmit_export_dataset_job()\nSubmit the vector database dataset export job.\nReturns:\nresult – The result of the vector database dataset export job containing the exported dataset id.\nReturn type:\nVectorDatabaseDatasetExportJob\nclassmethod get_supported_retrieval_settings()\nGet supported retrieval settings.\nReturns:\nsupported_retrieval_settings – The supported retriever settings.\nReturn type:\nSupportedRetrievalSettings\nclassmethod create(dataset_id, chunking_parameters=None, use_case=None, name=None, parent_vector_database_id=None, update_llm_blueprints=None, update_deployments=None, external_vector_database_connection=None, metadata_dataset_id=None, metadata_combination_strategy=None)\nCreate a new vector database.\nParameters:\ndataset_id (str) – ID of the dataset used for creation.\nchunking_parameters (ChunkingParameters) – Parameters defining how documents are split and embedded.\nuse_case (Optional[Union[UseCase, str]], optional) – Use case to link to the created vector database.\nname (Optional[str]) – Vector database name, by default None\nwhich leads to the default name ‘Vector Database for <dataset name>’.\nparent_vector_database_id (Optional[str]) – ID of the parent vector database to base the update on.\nupdate_llm_blueprints (Optional[bool]) – Whether to update LLM blueprints related to the parent vector database.\nupdate_deployments (Optional[bool]) – Whether to update deployments related to the parent vector database.\nexternal_vector_database_connection (Optional[dict]) – Parameters defining the external vector database connection to use.\nmetadata_dataset_id (Optional[str]) – The ID of the dataset used to provide additional metadata.\nmetadata_combination_strategy (Optional[VectorDatabaseMetadataCombinationStrategy]) – Strategy used to combine the metadata columns if there are duplicates between the\ndataset and the metadata dataset.\nReturns:\nvector database – The created vector database with execution status ‘new’.\nReturn type:\nVectorDatabase\nclassmethod create_from_custom_model(name, use_case=None, validation_id=None, prompt_column_name=None, target_column_name=None, deployment_id=None, model_id=None)\nCreate a new vector database from validated custom model deployment.\nParameters:\nname (str) – Vector database name.\nuse_case (Optional[Union[UseCase, str]], optional) – Use case to link to the created vector database.\nvalidation_id (Optional[str]) – ID of CustomModelVectorDatabaseValidation for the deployment.\nAlternatively, you can specify ALL the following fields.\nprompt_column_name (Optional[str]) – The column name the deployed model expect as the input.\ntarget_column_name (Optional[str]) – The target name deployed model will output.\ndeployment_id (Optional[str]) – ID of the deployment.\nmodel_id (Optional[str]) – ID of the underlying deployment model.\nCan be found from the API as Deployment.model[“id”].\nReturns:\nvector database – The created vector database.\nReturn type:\nVectorDatabase\nclassmethod get(vector_database_id)\nRetrieve a single vector database.\nParameters:\nvector_database_id (str) – The ID of the vector database you want to retrieve.\nReturns:\nvector database – The requested vector database.\nReturn type:\nVectorDatabase\nclassmethod list(use_case=None, playground=None, search=None, sort=None, completed_only=None)\nList all vector databases associated with a specific use case available to the user.\nParameters:\nuse_case (Optional[UseCaseLike], optional) – The returned vector databases are filtered to those associated with a specific Use Case\nor Cases if specified or can be inferred from the Context.\nAccepts either the entity or the ID.\nplayground (Optional[Union[Playground, str]], optional) – The returned vector databases are filtered to those associated with a specific playground\nif it is specified. Accepts either the entity or the ID.\nsearch (Optional[str]) – String for filtering vector databases.\nVector databases that contain the string in name will be returned.\nIf not specified, all vector databases will be returned.\nsort (Optional[str]) – Property to sort vector databases by.\nPrefix the attribute name with a dash to sort in descending order,\ne.g. sort=’-creationDate’.\nCurrently supported options are listed in ListVectorDatabasesSortQueryParams\nbut the values can differ with different platform versions.\nBy default, the sort parameter is None which will result in\nvector databases being returned in order of creation time descending.\ncompleted_only (Optional[bool]) – A filter to retrieve only vector databases that have been successfully created.\nBy default, all vector databases regardless of execution status are retrieved.\nReturns:\nvectorbases – A list of vector databases available to the user.\nReturn type:\nlist[VectorDatabase]\nupdate(name=None, credential_id=None)\nUpdate the vector database.\nParameters:\nname (Optional[str]) – The new name for the vector database.\ncredential_id (Optional[str]) – The new credential id to access the connected vector database.\nReturns:\nvector database – The updated vector database.\nReturn type:\nVectorDatabase\nupdate_connected(dataset_id, metadata_dataset_id=None, metadata_combination_strategy=None)\nUpdate a connected vector database.\nParameters:\ndataset_id (str) – The ID of the dataset to add to the vector database.\nmetadata_dataset_id (Optional[str]) – The ID of the dataset used to provide additional metadata.\nmetadata_combination_strategy (Optional[VectorDatabaseMetadataCombinationStrategy]) – The strategy used to combine the metadata columns if there are duplicates between the\ndataset and the metadata dataset.\nReturns:\nvector database – The updated vector database.\nReturn type:\nVectorDatabase\ndelete()\nDelete the vector database.\nReturn type:\nNone\nclassmethod get_supported_text_chunkings()\nGet all supported text chunking configurations which includes\na set of recommended chunking parameters for each supported embedding model.\nReturns:\nsupported_text_chunkings – The supported text chunking configurations.\nReturn type:\nSupportedTextChunkings\ndownload_text_and_embeddings_asset(file_path=None, part=None)\nDownload a parquet file with text chunks and corresponding embeddings created\nby a vector database.\nParameters:\nfile_path (Optional[str]) – File path to save the asset. By default, it saves in the current directory\nautogenerated by server name.\npart (Optional[int]) – Part of the text chunks to download. Connected vector databases have a\npart for each dataset that is added.\nReturn type:\nNone\nsend_to_custom_model_workshop(maximum_memory=None, resource_bundle_id=None, replicas=None, network_egress_policy=None)\nCreate a new CustomModelVersion for this vector database.\nParameters:\nmaximum_memory (Optional[int]) – The maximum memory that will be allocated to this custom model.\nresource_bundle_id (Optional[str]) – The ID of a datarobot.models.resource_bundle.ResourceBundle that will be used by this\ncustom model.\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed for this custom model.\nnetwork_egress_policy (Optional[str]) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE,\ndatarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nReturn type:\nCustomModelVersion\ndeploy(default_prediction_server_id=None, prediction_environment_id=None, credential_id=None, maximum_memory=None, resource_bundle_id=None, replicas=None, network_egress_policy=None)\nCreate a new Custom Model for this vector database and deploy it on a new Deployment.\nParameters:\ndefault_prediction_server_id (Optional[str]) – An identifier of a prediction server to be used as the default prediction server.\nWhen working with prediction environments, the default prediction server ID should not\nbe provided.\nprediction_environment_id (Optional[str]) – An identifier of a prediction environment to be used for model deployment.\ncredential_id (Optional[str]:) – The ID of credentials to access a connected vector database. This is only needed for vector\ndatabases with an external source when the user does not have access to the credentials\nassociated with this vector database.\nmaximum_memory (Optional[int]) – The maximum memory that will be allocated to the new custom model.\nresource_bundle_id (Optional[str]) – The ID of a datarobot.models.resource_bundle.ResourceBundle that will be used by the new\ncustom model.\nreplicas (Optional[int]) – A fixed number of replicas that will be deployed for this custom model.\nnetwork_egress_policy (Optional[str]) – Determines whether the given custom model is isolated, or can access the public network.\nValues: [datarobot.NETWORK_EGRESS_POLICY.NONE,\ndatarobot.NETWORK_EGRESS_POLICY.PUBLIC].\nReturn type:\nDeployment\nclass datarobot.models.genai.vector_database.SupportedRetrievalSetting\nA single supported retrieval setting.\nVariables:\nname (str) – The name of the setting.\ntype (str or list[str]) – The type of the setting.\ndescription (str) – The description of the setting.\ntitle (str) – The title of the setting.\ndefault (str, int, bool, or None) – The default value of the setting.\nminimum (int or None) – The minimum value of the setting.\nmaximum (int or None) – The maximum value of the setting.\nenum (list[str] or None) – The enum values of the setting.\nsettings (list[SupportedRetrievalSetting] or None) – The supported retriever settings.\ngroup_id (str or None) – The group ID of the setting.\nclass datarobot.models.genai.vector_database.VectorDatabaseDatasetExportJob\nResponse for the vector database dataset export job.\nVariables:\njob_id (str) – ID of the export job.\nvector_database_id (str) – ID of the vector database.\nexport_dataset_id (str) – ID of the exported dataset.\nclass datarobot.models.genai.chat_prompt.Citation\nCitation for documents retrieved from a vector database.\nVariables:\ntext (str) – The text retrieved from a vector database.\nsource (str or None, optional) – The source of the retrieved text.\nsimilarity_score – The similarity score between the citation and the user prompt.\nmetadata (dict or None, optional) – Additional metadata for the citation.\nclass datarobot.models.genai.llm_blueprint.VectorDatabaseSettings\nSettings for a DataRobot GenAI vector database associated with an LLM blueprint.\nVariables:\nmax_documents_retrieved_per_prompt (int or None, optional) – The maximum number of documents to retrieve for each prompt.\nmax_tokens (int or None, optional) – The maximum number of tokens to retrieve for each document.\nretriever (VectorDatabaseRetrievers) – The vector database retriever name.\nadd_neighbor_chunks – Whether to add neighboring documents to the retrieved documents.\nclass datarobot.models.genai.vector_database.ChunkingParameters\nParameters defining how documents are split and embedded.\nVariables:\nembedding_model (Optional[str]) – Name of the text embedding model.\nCurrently supported options are listed in VectorDatabaseEmbeddingModel\nbut the values can differ with different platform versions.\nchunking_method (str) – Name of the method to split dataset documents.\nCurrently supported options are listed in VectorDatabaseChunkingMethod\nbut the values can differ with different platform versions.\nchunk_size (int) – Size of each text chunk in number of tokens.\nchunk_overlap_percentage (int) – Overlap percentage between chunks.\nseparators (list[str]) – Strings used to split documents into text chunks.\nembedding_validation (Optional[CustomModelEmbeddingValidation, SupportedCustomModelEmbedding, str]) – ID or object for custom embedding validation.\ncustom_chunking (bool) – Determines if the chunking is custom. With custom chunking,\ndataset rows are not split into chunks automatically;\ninstead, the user provides the chunks.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-vector-databases.html",
      "tags": [
        "advanced",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/gen-vector-databases.html",
        "content_length": 17759
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.resource_bundle",
        "deployment.model",
        "datarobot.network_egress_policy.none",
        "datarobot.models.genai",
        "datarobot.network_egress_policy.public"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_7907002315364816359",
      "title": "Key-Values",
      "content": "Key-Values\nclass datarobot.models.key_values.KeyValue\nA DataRobot Key-Value.\nAdded in version v3.4.\nVariables:\nid (str) – ID of the Key-Value\ncreated_at (str) – creation time of the Key-Value\nentity_id (str) – ID of the related Entity\nentity_type (KeyValueEntityType) – type of the related Entity\nname (str) – Key-Value name\nvalue (str) – Key-Value value\nnumeric_value (float) – Key-Value numeric value\nboolean_value (bool) – Key-Value boolean value\nvalue_type (KeyValueType) – Key-Value type\ndescription (str) – Key-Value description\ncreator_id (str) – ID of the user who created the Key-Value\ncreator_name (str) – ID of the user who created the Key-Value\ncategory (KeyValueCategory) – Key-Value category\nartifact_size (int) – size in bytes of associated image, if applicable\noriginal_file_name (str) – name of uploaded original image or dataset file\nis_editable (bool) – true if a user with permissions can edit or delete\nis_dataset_missing (bool) – true if the key-value type is “dataset” and its dataset is not visible to the user\nerror_message (str) – additional information if “isDataSetMissing” is true. Blank if there are no errors\nclassmethod get(key_value_id)\nGet Key-Value by id.\nAdded in version v3.4.\nParameters:\nkey_value_id (str) – ID of the Key-Value\nReturns:\nretrieved Key-Value\nReturn type:\nKeyValue\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nclassmethod list(entity_id, entity_type)\nList Key-Values.\nAdded in version v3.4.\nParameters:\nentity_id (str) – ID of the related Entity\nentity_type (KeyValueEntityType) – type of the related Entity\nReturns:\na list of Key-Values\nReturn type:\nList[KeyValue]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod find(entity_id, entity_type, name)\nFind Key-Value by name.\nAdded in version v3.4.\nParameters:\nentity_id (str) – ID of the related Entity\nentity_type (KeyValueEntityType) – type of the related Entity\nname (str) – name of the Key-Value\nReturns:\na list of Key-Values\nReturn type:\nList[KeyValue]\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod create(entity_id, entity_type, name, category, value_type, value=None, description=None)\nCreate a Key-Value.\nAdded in version v3.4.\nParameters:\nentity_id (str) – ID of the associated resource\nentity_type (KeyValueEntityType) – type of the associated resource\nname (str) – name of the Key-Value. Cannot contain: { } ; |\ncategory (KeyValueCategory) – category of the Key-Value\nvalue_type (KeyValueType) – type of the Key-Value value\nvalue (Optional[Union[str, float, bool]]) – value of Key-Value\ndescription (Optional[str]) – description of the Key-Value\nReturns:\ncreated Key-Value\nReturn type:\nKeyValue\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nupdate(entity_id=None, entity_type=None, name=None, category=None, value_type=None, value=None, description=None, comment=None)\nUpdate Key-Value.\nAdded in version v3.4.\nParameters:\nentity_id (Optional[str]) – ID of the associated resource\nentity_type (Optional[KeyValueEntityType]) – type of the associated resource\nname (Optional[str]) – name of the Key-Value. Cannot contain: { } ; |\ncategory (Optional[KeyValueCategory]) – category of the Key-Value\nvalue_type (Optional[KeyValueType]) – type of the Key-Value value\nvalue (Optional[[Union[str, float, bool]]) – value of Key-Value\ndescription (Optional[str]) – description of the Key-Value\ncomment (Optional[str]) – user comment explaining the change\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nReturn type:\nNone\nrefresh()\nUpdate Key-Value with the latest data from server.\n:rtype: None\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\ndelete()\nDelete Key-Value.\n:rtype: None\nAdded in version v3.4.\nRaises:\ndatarobot.errors.ClientError – If the server responded with 4xx status.\ndatarobot.errors.ServerError – If the server responded with 5xx status.\nget_value()\nGet a value of Key-Value.\nAdded in version v3.4.\nReturns:\nvalue depending on the value type\nReturn type:\nUnion[str, float, boolean]\nclass datarobot.enums.KeyValueCategory\nKey-Value category\nclass datarobot.enums.KeyValueEntityType\nKey-Value entity type\nclass datarobot.enums.KeyValueType\nKey-Value type",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/key_values.html",
      "tags": [
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/key_values.html",
        "content_length": 4757
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.enums.keyvalueentitytype",
        "datarobot.enums.keyvaluetype",
        "datarobot.models.key_values",
        "datarobot.errors.servererror",
        "datarobot.enums.keyvaluecategory",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 0.6000000000000001,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_8307093062739848616",
      "title": "Job",
      "content": "Job\nclass datarobot.models.Job\nTracks asynchronous work being done within a project\nVariables:\nid (int) – the id of the job\nproject_id (str) – the id of the project the job belongs to\nstatus (str) – the status of the job - will be one of datarobot.enums.QUEUE_STATUS\njob_type (str) – what kind of work the job is doing - will be one of datarobot.enums.JOB_TYPE\nis_blocked (bool) – if true, the job is blocked (cannot be executed) until its dependencies are resolved\nclassmethod get(project_id, job_id)\nFetches one job.\nParameters:\nproject_id (str) – The identifier of the project in which the job resides\njob_id (str) – The job id\nReturns:\njob – The job\nReturn type:\nJob\nRaises:\nAsyncFailureError – Querying this resource gave a status code other than 200 or 303\ncancel()\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nget_result(params=None)\nParameters:\nparams (dict or None) – Query parameters to be added to request to get results.\nNotes\nFor featureEffects, source param is required to define source,\notherwise the default is training.\nReturns:\nresult –\nReturn type depends on the job type\nfor model jobs, a Model is returned\nfor predict jobs, a pandas.DataFrame (with predictions) is returned\nfor featureImpact jobs, a list of dicts by default (see with_metadata\nparameter of the FeatureImpactJob class and its get() method).\nfor primeRulesets jobs, a list of Rulesets\nfor primeModel jobs, a PrimeModel\nfor primeDownloadValidation jobs, a PrimeFile\nfor predictionExplanationInitialization jobs, a PredictionExplanationsInitialization\nfor predictionExplanations jobs, a PredictionExplanations\nfor featureEffects, a FeatureEffects.\nReturn type:\nobject\nRaises:\nJobNotFinished – If the job is not finished, the result is not available.\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nget_result_when_complete(max_wait=600, params=None)\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nparams (dict, optional) – Query parameters to be added to request.\nReturns:\nresult – Return type is the same as would be returned by Job.get_result.\nReturn type:\nobject\nRaises:\nAsyncTimeoutError – If the job does not finish in time\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nrefresh()\nUpdate this object with the latest job data from the server.\nwait_for_completion(max_wait=600)\nWaits for job to complete.\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nReturn type:\nNone\nclass datarobot.models.TrainingPredictionsJob\nclassmethod get(project_id, job_id, model_id=None, data_subset=None)\nFetches one training predictions job.\nThe resulting\nTrainingPredictions\nobject will be annotated with model_id and data_subset.\nParameters:\nproject_id (str) – The identifier of the project in which the job resides\njob_id (str) – The job id\nmodel_id (str) – The identifier of the model used for computing training predictions\ndata_subset (dr.enums.DATA_SUBSET, optional) – Data subset used for computing training predictions\nReturns:\njob – The job\nReturn type:\nTrainingPredictionsJob\nrefresh()\nUpdate this object with the latest job data from the server.\ncancel()\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nget_result(params=None)\nParameters:\nparams (dict or None) – Query parameters to be added to request to get results.\nNotes\nFor featureEffects, source param is required to define source,\notherwise the default is training.\nReturns:\nresult –\nReturn type depends on the job type\nfor model jobs, a Model is returned\nfor predict jobs, a pandas.DataFrame (with predictions) is returned\nfor featureImpact jobs, a list of dicts by default (see with_metadata\nparameter of the FeatureImpactJob class and its get() method).\nfor primeRulesets jobs, a list of Rulesets\nfor primeModel jobs, a PrimeModel\nfor primeDownloadValidation jobs, a PrimeFile\nfor predictionExplanationInitialization jobs, a PredictionExplanationsInitialization\nfor predictionExplanations jobs, a PredictionExplanations\nfor featureEffects, a FeatureEffects.\nReturn type:\nobject\nRaises:\nJobNotFinished – If the job is not finished, the result is not available.\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nget_result_when_complete(max_wait=600, params=None)\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nparams (dict, optional) – Query parameters to be added to request.\nReturns:\nresult – Return type is the same as would be returned by Job.get_result.\nReturn type:\nobject\nRaises:\nAsyncTimeoutError – If the job does not finish in time\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nwait_for_completion(max_wait=600)\nWaits for job to complete.\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nReturn type:\nNone\nclass datarobot.models.ShapMatrixJob\nclassmethod get(project_id, job_id, model_id=None, dataset_id=None)\nFetches one SHAP matrix job.\nParameters:\nproject_id (str) – The identifier of the project in which the job resides\njob_id (str) – The job identifier\nmodel_id (str) – The identifier of the model used for computing prediction explanations\ndataset_id (str) – The identifier of the dataset against which prediction explanations should be computed\nReturns:\njob – The job\nReturn type:\nShapMatrixJob\nRaises:\nAsyncFailureError – Querying this resource gave a status code other than 200 or 303\nrefresh()\nUpdate this object with the latest job data from the server.\nReturn type:\nNone\ncancel()\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nget_result(params=None)\nParameters:\nparams (dict or None) – Query parameters to be added to request to get results.\nNotes\nFor featureEffects, source param is required to define source,\notherwise the default is training.\nReturns:\nresult –\nReturn type depends on the job type\nfor model jobs, a Model is returned\nfor predict jobs, a pandas.DataFrame (with predictions) is returned\nfor featureImpact jobs, a list of dicts by default (see with_metadata\nparameter of the FeatureImpactJob class and its get() method).\nfor primeRulesets jobs, a list of Rulesets\nfor primeModel jobs, a PrimeModel\nfor primeDownloadValidation jobs, a PrimeFile\nfor predictionExplanationInitialization jobs, a PredictionExplanationsInitialization\nfor predictionExplanations jobs, a PredictionExplanations\nfor featureEffects, a FeatureEffects.\nReturn type:\nobject\nRaises:\nJobNotFinished – If the job is not finished, the result is not available.\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nget_result_when_complete(max_wait=600, params=None)\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nparams (dict, optional) – Query parameters to be added to request.\nReturns:\nresult – Return type is the same as would be returned by Job.get_result.\nReturn type:\nobject\nRaises:\nAsyncTimeoutError – If the job does not finish in time\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nwait_for_completion(max_wait=600)\nWaits for job to complete.\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nReturn type:\nNone\nclass datarobot.models.FeatureImpactJob\nCustom Feature Impact job to handle different return value structures.\nThe original implementation had just the the data and the new one also includes some metadata.\nIn general, we aim to keep the number of Job classes low by just utilizing the job_type\nattribute to control any specific formatting; however in this case when we needed to support\na new representation with the _same_ job_type, customizing the behavior of\n_make_result_from_location allowed us to achieve our ends without complicating the\n_make_result_from_json method.\nclassmethod get(project_id, job_id, with_metadata=False)\nFetches one job.\nParameters:\nproject_id (str) – The identifier of the project in which the job resides\njob_id (str) – The job id\nwith_metadata (bool) – To make this job return the metadata (i.e. the full object of the completed resource)\nset the with_metadata flag to True.\nReturns:\njob – The job\nReturn type:\nJob\nRaises:\nAsyncFailureError – Querying this resource gave a status code other than 200 or 303\ncancel()\nCancel this job. If this job has not finished running, it will be\nremoved and canceled.\nget_result(params=None)\nParameters:\nparams (dict or None) – Query parameters to be added to request to get results.\nNotes\nFor featureEffects, source param is required to define source,\notherwise the default is training.\nReturns:\nresult –\nReturn type depends on the job type\nfor model jobs, a Model is returned\nfor predict jobs, a pandas.DataFrame (with predictions) is returned\nfor featureImpact jobs, a list of dicts by default (see with_metadata\nparameter of the FeatureImpactJob class and its get() method).\nfor primeRulesets jobs, a list of Rulesets\nfor primeModel jobs, a PrimeModel\nfor primeDownloadValidation jobs, a PrimeFile\nfor predictionExplanationInitialization jobs, a PredictionExplanationsInitialization\nfor predictionExplanations jobs, a PredictionExplanations\nfor featureEffects, a FeatureEffects.\nReturn type:\nobject\nRaises:\nJobNotFinished – If the job is not finished, the result is not available.\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nget_result_when_complete(max_wait=600, params=None)\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nparams (dict, optional) – Query parameters to be added to request.\nReturns:\nresult – Return type is the same as would be returned by Job.get_result.\nReturn type:\nobject\nRaises:\nAsyncTimeoutError – If the job does not finish in time\nAsyncProcessUnsuccessfulError – If the job errored or was aborted\nrefresh()\nUpdate this object with the latest job data from the server.\nwait_for_completion(max_wait=600)\nWaits for job to complete.\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish.\nReturn type:\nNone",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/jobs.html",
      "tags": [
        "advanced",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/jobs.html",
        "content_length": 9988
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.featureimpactjob",
        "datarobot.enums.job_type",
        "dr.enums.data_subset",
        "datarobot.models.trainingpredictionsjob",
        "datarobot.models.shapmatrixjob",
        "datarobot.enums.queue_status",
        "datarobot.models.job"
      ],
      "complexity_score": 0.95,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-7018210490175246532",
      "title": "MLOps event",
      "content": "MLOps event\nclass datarobot.mlops.events.MLOpsEvent\nAn MLOps Event Object: An object representing an important MLOps activity that\nhappened.  For example, health, service issues with the DataRobot deployment\nor a prediction environment or a particular phase in a long operation (like\ncreation of deployment or processing training data) is completed or errored.\nThis class allows the client to report such event to the DataRobot service.\nNotes\nDataRobot backend support lots of events and all these events are categorized\ninto different categories.  This class does not yet support ALL events, but\nwe will gradually add support for them\nSupported Event Categories:\nmoderation\nclassmethod report_moderation_event(event_type, timestamp=None, title=None, message=None, deployment_id=None, org_id=None, guard_name=None, metric_name=None)\nReports a moderation event\nParameters:\nevent_type (str) – The type of the moderation event.\ntimestamp (Optional[str]) – The timestamp of the event, datetime, or string in RFC3339 format. If the datetime provided\ndoes not have a timezone, DataRobot assumes it is UTC.\ntitle (Optional[str]) – The title of the moderation event.\nmessage (Optional[str]) – A description of the moderation event.\ndeployment_id (Optional[str]) – The ID of the deployment associated with the event.\norg_id (Optional[str]) – The ID of the organization associated with the event.\nguard_name (Optional[str]) – The name or label of the guard.\nmetric_name (Optional[str]) – The name or label of the metric.\nReturn type:\nNone\nRaises:\nValueError – If event_type is not one of the moderation event types.\nIf fails to create the event.\nExamples\n>>> from datarobot.mlops.events import MLOpsEvent\n>>> MLOpsEvent.report_moderation_event(\n...     event_type=\"moderationMetricCreationError\",\n...     title=\"Failed to create moderation metric\",\n...     message=\"Maximum number of custom metrics reached\",\n...     deployment_id=\"5c939e08962d741e34f609f0\",\n...     metric_name=\"Blocked Prompts\",\n... )",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/mlops_event.html",
      "tags": [
        "advanced",
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/mlops_event.html",
        "content_length": 1996
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.mlops.events"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2498062704879742640",
      "title": "Insights",
      "content": "Insights\nclass datarobot.insights.ShapMatrix\nClass for SHAP Matrix calculations. Use the standard methods of BaseInsight to compute\nand retrieve: compute, create, list, get.\nproperty matrix: Any\nSHAP matrix values.\nproperty base_value: float\nSHAP base value for the matrix values\nproperty columns: List[str]\nList of columns associated with the SHAP matrix\nproperty link_function: str\nLink function used to generate the SHAP matrix\nclassmethod compute(entity_id, source=INSIGHTS_SOURCES.VALIDATION, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, **kwargs)\nSubmit an insight compute request. You can use create if you want to\nwait synchronously for the completion of the job. May be overridden by insight subclasses to\naccept additional parameters.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nReturns:\nStatus check job entity for the asynchronous insight calculation.\nReturn type:\nStatusCheckJob\nclassmethod create(entity_id, source=INSIGHTS_SOURCES.VALIDATION, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, max_wait=600, **kwargs)\nCreate an insight and wait for completion. May be overridden by insight subclasses to\naccept additional parameters.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nmax_wait (int) – The number of seconds to wait for the result.\nReturns:\nEntity of the newly or already computed insights.\nReturn type:\nSelf\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nOverride from_server_data to handle paginated responses\nReturn type:\nSelf\nclassmethod get(entity_id, source=INSIGHTS_SOURCES.VALIDATION, quick_compute=None, **kwargs)\nReturn the first matching insight based on the entity id and kwargs.\nParameters:\nentity_id (str) – The ID of the entity to retrieve generated insights.\nsource (str) – The source type to use when retrieving the insight.\nquick_compute (Optional[bool]) – Sets whether to retrieve the insight that was computed using quick-compute. If not\nspecified, quick_compute is not used for matching.\nReturns:\nPreviously computed insight.\nReturn type:\nSelf\nclassmethod get_as_csv(entity_id, **kwargs)\nRetrieve a specific insight represented in CSV format.\nParameters:\nentity_id (str) – ID of the entity to retrieve the insight.\n**kwargs (Any) – Additional keyword arguments to pass to the retrieve function.\nReturns:\nThe retrieved insight.\nReturn type:\nstr\nclassmethod get_as_dataframe(entity_id, **kwargs)\nRetrieve a specific insight represented as a pandas DataFrame.\nParameters:\nentity_id (str) – ID of the entity to retrieve the insight.\n**kwargs (Any) – Additional keyword arguments to pass to the retrieve function.\nReturns:\nThe retrieved insight.\nReturn type:\nDataFrame\nget_uri()\nThis should define the URI to their browser based interactions\nReturn type:\nstr\nclassmethod list(entity_id)\nList all generated insights.\nParameters:\nentity_id (str) – The ID of the entity queried for listing all generated insights.\nReturns:\nList of newly or previously computed insights.\nReturn type:\nList[Self]\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nsort(key_name)\nSorts insights data\nReturn type:\nNone\nclass datarobot.insights.ShapPreview\nClass for SHAP Preview calculations. Use the standard methods of BaseInsight to compute\nand retrieve: compute, create, list, get.\nproperty previews: List[Dict[str, Any]]\nSHAP preview values.\nReturns:\npreview – A list of the ShapPreview values for each row.\nReturn type:\nList[Dict[str, Any]]\nproperty previews_count: int\nThe number of shap preview rows.\nReturn type:\nint\nclassmethod get(entity_id, source=INSIGHTS_SOURCES.VALIDATION, quick_compute=None, prediction_filter_row_count=None, prediction_filter_percentiles=None, prediction_filter_operand_first=None, prediction_filter_operand_second=None, prediction_filter_operator=None, feature_filter_count=None, feature_filter_name=None, **kwargs)\nReturn the first matching ShapPreview insight based on the entity id and kwargs.\nParameters:\nentity_id (str) – The ID of the entity to retrieve generated insights.\nsource (str) – The source type to use when retrieving the insight.\nquick_compute (Optional[bool]) – Sets whether to retrieve the insight that was computed using quick-compute. If not\nspecified, quick_compute is not used for matching.\nprediction_filter_row_count (Optional[int]) – The maximum number of preview rows to return.\nprediction_filter_percentiles (Optional[int]) – The number of percentile intervals to select from the total number of rows.\nThis field will supersede predictionFilterRowCount if both are present.\nprediction_filter_operand_first (Optional[float]) – The first operand to apply to filtered predictions.\nprediction_filter_operand_second (Optional[float]) – The second operand to apply to filtered predictions.\nprediction_filter_operator (Optional[str]) – The operator to apply to filtered predictions.\nfeature_filter_count (Optional[int]) – The maximum number of features to return for each preview.\nfeature_filter_name (Optional[str]) – The names of specific features to return for each preview.\nReturns:\nList of newly or already computed insights.\nReturn type:\nList[Any]\nclassmethod compute(entity_id, source=INSIGHTS_SOURCES.VALIDATION, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, **kwargs)\nSubmit an insight compute request. You can use create if you want to\nwait synchronously for the completion of the job. May be overridden by insight subclasses to\naccept additional parameters.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nReturns:\nStatus check job entity for the asynchronous insight calculation.\nReturn type:\nStatusCheckJob\nclassmethod create(entity_id, source=INSIGHTS_SOURCES.VALIDATION, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, max_wait=600, **kwargs)\nCreate an insight and wait for completion. May be overridden by insight subclasses to\naccept additional parameters.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nmax_wait (int) – The number of seconds to wait for the result.\nReturns:\nEntity of the newly or already computed insights.\nReturn type:\nSelf\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nOverride from_server_data to handle paginated responses\nReturn type:\nSelf\nget_uri()\nThis should define the URI to their browser based interactions\nReturn type:\nstr\nclassmethod list(entity_id)\nList all generated insights.\nParameters:\nentity_id (str) – The ID of the entity queried for listing all generated insights.\nReturns:\nList of newly or previously computed insights.\nReturn type:\nList[Self]\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nsort(key_name)\nSorts insights data\nReturn type:\nNone\nclass datarobot.insights.ShapImpact\nClass for SHAP Impact calculations. Use the standard methods of BaseInsight to compute\nand retrieve: compute, create, list, get.\nclassmethod compute(entity_id, source=INSIGHTS_SOURCES.TRAINING, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, **kwargs)\nSubmit an insight compute request. You can use create if you want to\nwait synchronously for the completion of the job.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nReturns:\nStatus check job entity for the asynchronous insight calculation.\nReturn type:\nStatusCheckJob\nclassmethod create(entity_id, source=INSIGHTS_SOURCES.TRAINING, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, max_wait=600, **kwargs)\nCreate an insight and wait for completion.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nmax_wait (int) – The number of seconds to wait for the result.\nReturns:\nEntity of the newly or already computed insights.\nReturn type:\nSelf\nsort(key_name='-impact_normalized')\nSorts insights data by key name.\nParameters:\nkey_name (str) – item key name to sort data.\nOne of ‘feature_name’, ‘impact_normalized’ or ‘impact_unnormalized’.\nStarting with ‘-’ reverses sort order. Default ‘-impact_normalized’\nReturn type:\nNone\nproperty shap_impacts: List[List[Any]]\nSHAP impact values\nReturns:\nA list of the SHAP impact values\nReturn type:\nshap impacts\nproperty base_value: List[float]\nA list of base prediction values\nproperty capping: Dict[str, Any] | None\nCapping for the models in the blender\nproperty link: str | None\nShared link function of the models in the blender\nproperty row_count: int | None\nNumber of SHAP impact rows. This is deprecated.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nOverride from_server_data to handle paginated responses\nReturn type:\nSelf\nclassmethod get(entity_id, source=INSIGHTS_SOURCES.VALIDATION, quick_compute=None, **kwargs)\nReturn the first matching insight based on the entity id and kwargs.\nParameters:\nentity_id (str) – The ID of the entity to retrieve generated insights.\nsource (str) – The source type to use when retrieving the insight.\nquick_compute (Optional[bool]) – Sets whether to retrieve the insight that was computed using quick-compute. If not\nspecified, quick_compute is not used for matching.\nReturns:\nPreviously computed insight.\nReturn type:\nSelf\nget_uri()\nThis should define the URI to their browser based interactions\nReturn type:\nstr\nclassmethod list(entity_id)\nList all generated insights.\nParameters:\nentity_id (str) – The ID of the entity queried for listing all generated insights.\nReturns:\nList of newly or previously computed insights.\nReturn type:\nList[Self]\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nclass datarobot.insights.ShapDistributions\nClass for SHAP Distributions calculations. Use the standard methods of BaseInsight to compute\nand retrieve: compute, create, list, get.\nproperty features: List[Dict[str, Any]]\nSHAP feature values\nReturns:\nfeatures – A list of the ShapDistributions values for each row\nReturn type:\nList[Dict[str, Any]]\nproperty total_features_count: int\nNumber of shap distributions features\nReturn type:\nint\nclassmethod compute(entity_id, source=INSIGHTS_SOURCES.VALIDATION, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, **kwargs)\nSubmit an insight compute request. You can use create if you want to\nwait synchronously for the completion of the job. May be overridden by insight subclasses to\naccept additional parameters.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nReturns:\nStatus check job entity for the asynchronous insight calculation.\nReturn type:\nStatusCheckJob\nclassmethod create(entity_id, source=INSIGHTS_SOURCES.VALIDATION, data_slice_id=None, external_dataset_id=None, entity_type=ENTITY_TYPES.DATAROBOT_MODEL, quick_compute=None, max_wait=600, **kwargs)\nCreate an insight and wait for completion. May be overridden by insight subclasses to\naccept additional parameters.\nParameters:\nentity_id (str) – The ID of the entity to compute the insight.\nsource (str) – The source type to use when computing the insight.\ndata_slice_id (Optional[str]) – Data slice ID to use when computing the insight.\nexternal_dataset_id (Optional[str]) – External dataset ID to use when computing the insight.\nentity_type (Optional[ENTITY_TYPES]) – The type of the entity associated with the insight. Select one of the ENTITY_TYPE enum\nvalues, or accept the default, “datarobotModel”.\nquick_compute (Optional[bool]) – Sets whether to use quick-compute for the insight. If True or unspecified, the insight\nis computed using a 2500-row data sample. If False, the insight is computed using all\nrows in the chosen source.\nmax_wait (int) – The number of seconds to wait for the result.\nReturns:\nEntity of the newly or already computed insights.\nReturn type:\nSelf\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nOverride from_server_data to handle paginated responses\nReturn type:\nSelf\nclassmethod get(entity_id, source=INSIGHTS_SOURCES.VALIDATION, quick_compute=None, **kwargs)\nReturn the first matching insight based on the entity id and kwargs.\nParameters:\nentity_id (str) – The ID of the entity to retrieve generated insights.\nsource (str) – The source type to use when retrieving the insight.\nquick_compute (Optional[bool]) – Sets whether to retrieve the insight that was computed using quick-compute. If not\nspecified, quick_compute is not used for matching.\nReturns:\nPreviously computed insight.\nReturn type:\nSelf\nget_uri()\nThis should define the URI to their browser based interactions\nReturn type:\nstr\nclassmethod list(entity_id)\nList all generated insights.\nParameters:\nentity_id (str) – The ID of the entity queried for listing all generated insights.\nReturns:\nList of newly or previously computed insights.\nReturn type:\nList[Self]\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nsort(key_name)\nSorts insights data\nReturn type:\nNone\nTypes\nclass datarobot.models.RocCurveEstimatedMetric\nTyped dict for estimated metric\nclass datarobot.models.AnomalyAssessmentRecordMetadata\nTyped dict for record metadata\nclass datarobot.models.AnomalyAssessmentPreviewBin\nTyped dict for preview bin\nclass datarobot.models.ShapleyFeatureContribution\nTyped dict for shapley feature contribution\nclass datarobot.models.AnomalyAssessmentDataPoint\nTyped dict for data points\nclass datarobot.models.RegionExplanationsData\nTyped dict for region explanations\nAnomaly assessment\nclass datarobot.models.anomaly_assessment.AnomalyAssessmentRecord\nObject which keeps metadata about anomaly assessment insight for the particular\nsubset, backtest and series and the links to proceed to get the anomaly assessment data.\nAdded in version v2.25.\nVariables:\nrecord_id (str) – The ID of the record.\nproject_id (str) – The ID of the project record belongs to.\nmodel_id (str) – The ID of the model record belongs to.\nbacktest (int or \"holdout\") – The backtest of the record.\nsource (\"training\" or \"validation\") – The source of the record\nseries_id (str or None) – The series id of the record for the multiseries projects. Defined only for the multiseries\nprojects.\nstatus (str) – The status of the insight. One of datarobot.enums.AnomalyAssessmentStatus\nstatus_details (str) – The explanation of the status.\nstart_date (str or None) – The ISO-formatted timestamp of the first prediction in the subset. Will be None if status is\nnot AnomalyAssessmentStatus.COMPLETED.\nend_date (str or None) – The ISO-formatted timestamp of the last prediction in the subset. Will be None if status is\nnot AnomalyAssessmentStatus.COMPLETED.\nprediction_threshold (float or None) – The threshold, all rows with anomaly scores greater or equal to it have shap explanations computed.\npreview_location (str or None) – The URL to retrieve predictions preview for the subset. Will be None if status is\nnot AnomalyAssessmentStatus.COMPLETED.\nlatest_explanations_location (str or None) – The URL to retrieve the latest predictions with the shap explanations. Will be None if status is\nnot AnomalyAssessmentStatus.COMPLETED.\ndelete_location (str) – The URL to delete anomaly assessment record and relevant insight data.\nclassmethod list(project_id, model_id, backtest=None, source=None, series_id=None, limit=100, offset=0, with_data_only=False)\nRetrieve the list of the anomaly assessment records for the project and model.\nOutput can be filtered and limited.\nParameters:\nproject_id (str) – The ID of the project record belongs to.\nmodel_id (str) – The ID of the model record belongs to.\nbacktest (int or \"holdout\") – The backtest to filter records by.\nsource (\"training\" or \"validation\") – The source to filter records by.\nseries_id (Optional[str]) – The series id to filter records by. Can be specified for multiseries projects.\nlimit (Optional[int]) – 100 by default. At most this many results are returned.\noffset (Optional[int]) – This many results will be skipped.\nwith_data_only (bool, False by default) – Filter by status == AnomalyAssessmentStatus.COMPLETED. If True, records with\nno data or not supported will be omitted.\nReturns:\nThe anomaly assessment record.\nReturn type:\nAnomalyAssessmentRecord\nclassmethod compute(project_id, model_id, backtest, source, series_id=None)\nRequest anomaly assessment insight computation on the specified subset.\nParameters:\nproject_id (str) – The ID of the project to compute insight for.\nmodel_id (str) – The ID of the model to compute insight for.\nbacktest (int or \"holdout\") – The backtest to compute insight for.\nsource (\"training\" or \"validation\") – The source  to compute insight for.\nseries_id (Optional[str]) – The series id to compute insight for. Required for multiseries projects.\nReturns:\nThe anomaly assessment record.\nReturn type:\nAnomalyAssessmentRecord\ndelete()\nDelete anomaly assessment record with preview and explanations.\nReturn type:\nNone\nget_predictions_preview()\nRetrieve aggregated predictions statistics for the anomaly assessment record.\nReturn type:\nAnomalyAssessmentPredictionsPreview\nget_latest_explanations()\nRetrieve latest predictions along with shap explanations for the most anomalous records.\nReturn type:\nAnomalyAssessmentExplanations\nget_explanations(start_date=None, end_date=None, points_count=None)\nRetrieve predictions along with shap explanations for the most anomalous records\nin the specified date range/for defined number of points.\nTwo out of three parameters: start_date, end_date or points_count must be specified.\nParameters:\nstart_date (Optional[str]) – The start of the date range to get explanations in.\nExample: 2020-01-01T00:00:00.000000Z\nend_date (Optional[str]) – The end of the date range to get explanations in.\nExample: 2020-10-01T00:00:00.000000Z\npoints_count (Optional[int]) – The number of the rows to return.\nReturn type:\nAnomalyAssessmentExplanations\nget_explanations_data_in_regions(regions, prediction_threshold=0.0)\nGet predictions along with explanations for the specified regions, sorted by\npredictions in descending order.\nParameters:\nregions (list of AnomalyAssessmentPreviewBin) – For each region explanations will be retrieved and merged.\nprediction_threshold (Optional[float]) – If specified, only points with score greater or equal to the threshold will be returned.\nReturns:\ndict in a form of {‘explanations’: explanations, ‘shap_base_value’: shap_base_value}\nReturn type:\nRegionExplanationsData\nclass datarobot.models.anomaly_assessment.AnomalyAssessmentExplanations\nObject which keeps predictions along with shap explanations for the most anomalous records\nin the specified date range/for defined number of points.\nAdded in version v2.25.\nVariables:\nrecord_id (str) – The ID of the record.\nproject_id (str) – The ID of the project record belongs to.\nmodel_id (str) – The ID of the model record belongs to.\nbacktest (int or \"holdout\") – The backtest of the record.\nsource (\"training\" or \"validation\") – The source of the record.\nseries_id (str or None) – The series id of the record for the multiseries projects. Defined only for the multiseries\nprojects.\nstart_date (str or None) – The ISO-formatted datetime of the first row in the data. Will be None of there is no data\nin the specified range.\nend_date (str or None) – The ISO-formatted datetime of the last row in the data. Will be None of there is no data\nin the specified range.\nshap_base_value (float) – Shap base value.\ncount (int) – The number of points in data.\ndata (array of DataPoint objects or None) – The list of DataPoint objects in the specified date range.\nNotes\nDataPoint contains:\nshap_explanation : None or an array of up to 10 ShapleyFeatureContribution objects.\nOnly rows with the highest anomaly scores have Shapley explanations calculated.\nValue is None if prediction is lower than prediction_threshold.\ntimestamp (str) : ISO-formatted timestamp for the row.\nprediction (float) : The output of the model for this row.\nShapleyFeatureContribution contains:\nfeature_value (str) : the feature value for this row. First 50 characters are returned.\nstrength (float) : the shap value for this feature and row.\nfeature (str) : the feature name.\nclassmethod get(project_id, record_id, start_date=None, end_date=None, points_count=None)\nRetrieve predictions along with shap explanations for the most anomalous records\nin the specified date range/for defined number of points.\nTwo out of three parameters: start_date, end_date or points_count must be specified.\nParameters:\nproject_id (str) – The ID of the project.\nrecord_id (str) – The ID of the anomaly assessment record.\nstart_date (Optional[str]) – The start of the date range to get explanations in.\nExample: 2020-01-01T00:00:00.000000Z\nend_date (Optional[str]) – The end of the date range to get explanations in.\nExample: 2020-10-01T00:00:00.000000Z\npoints_count (Optional[int]) – The number of the rows to return.\nReturn type:\nAnomalyAssessmentExplanations\nclass datarobot.models.anomaly_assessment.AnomalyAssessmentPredictionsPreview\nAggregated predictions over time for the corresponding anomaly assessment record.\nIntended to find the bins with highest anomaly scores.\nAdded in version v2.25.\nVariables:\nrecord_id (str) – The ID of the record.\nproject_id (str) – The ID of the project record belongs to.\nmodel_id (str) – The ID of the model record belongs to.\nbacktest (int or \"holdout\") – The backtest of the record.\nsource (\"training\" or \"validation\") – The source of the record\nseries_id (str or None) – The series id of the record for the multiseries projects. Defined only for the multiseries\nprojects.\nstart_date (str) – the ISO-formatted timestamp of the first prediction in the subset.\nend_date (str) – the ISO-formatted timestamp of the last prediction in the subset.\npreview_bins (list of preview_bin objects.) – The aggregated predictions for the subset.  Bins boundaries may differ from actual start/end\ndates because this is an aggregation.\nNotes\nPreviewBin contains:\nstart_date (str) : the ISO-formatted datetime of the start of the bin.\nend_date (str) : the ISO-formatted datetime of the end of the bin.\navg_predicted (float or None) : the average prediction of the model in the bin. None if\nthere are no entries in the bin.\nmax_predicted (float or None) : the maximum prediction of the model in the bin. None if\nthere are no entries in the bin.\nfrequency (int) : the number of the rows in the bin.\nclassmethod get(project_id, record_id)\nRetrieve aggregated predictions over time.\nParameters:\nproject_id (str) – The ID of the project.\nrecord_id (str) – The ID of the anomaly assessment record.\nReturn type:\nAnomalyAssessmentPredictionsPreview\nfind_anomalous_regions(max_prediction_threshold=0.0)\nSort preview bins by max_predicted value and select those with max predicted valuegreater or equal to max prediction threshold.\nSort the result by max predicted value in descending order.\nParameters:\nmax_prediction_threshold (Optional[float]) – Return bins with maximum anomaly score greater or equal to max_prediction_threshold.\nReturns:\npreview_bins – Filtered and sorted preview bins\nReturn type:\nlist of preview_bin\nConfusion chart\nclass datarobot.models.confusion_chart.ConfusionChart\nConfusion Chart data for model.\nNotes\nClassMetrics is a dict containing the following:\nclass_name (string) name of the class\nactual_count (int) number of times this class is seen in the validation data\npredicted_count (int) number of times this class has been predicted for the           validation data\nf1 (float) F1 score\nrecall (float) recall score\nprecision (float) precision score\nwas_actual_percentages (list of dict) one vs all actual percentages in format           specified below.\nother_class_name (string) the name of the other class\npercentage (float) the percentage of the times this class was predicted when is               was actually class (from 0 to 1)\nwas_predicted_percentages (list of dict) one vs all predicted percentages in format           specified below.\nother_class_name (string) the name of the other class\npercentage (float) the percentage of the times this class was actual predicted               (from 0 to 1)\nconfusion_matrix_one_vs_all (list of list) 2d list representing 2x2 one vs all matrix.\nThis represents the True/False Negative/Positive rates as integer for each class.               The data structure looks like:\n[ [ True Negative, False Positive ], [ False Negative, True Positive ] ]\nVariables:\nsource (str) – Confusion Chart data source. Can be ‘validation’, ‘crossValidation’ or ‘holdout’.\nraw_data (dict) – All of the raw data for the Confusion Chart\nconfusion_matrix (list of list) – The N x N confusion matrix\nclasses (list) – The names of each of the classes\nclass_metrics (list of dicts) – List of dicts with schema described as ClassMetrics above.\nsource_model_id (str) – ID of the model this Confusion chart represents; in some cases,\ninsights from the parent of a frozen model may be used\nLift chart\nclass datarobot.models.lift_chart.LiftChart\nLift chart data for model.\nNotes\nLiftChartBin is a dict containing the following:\nactual (float) Sum of actual target values in bin\npredicted (float) Sum of predicted target values in bin\nbin_weight (float) The weight of the bin. For weighted projects, it is the sum of           the weights of the rows in the bin. For unweighted projects, it is the number of rows in           the bin.\nVariables:\nsource (str) – Lift chart data source. Can be ‘validation’, ‘crossValidation’ or ‘holdout’.\nbins (list of dict) – List of dicts with schema described as LiftChartBin above.\nsource_model_id (str) – ID of the model this lift chart represents; in some cases,\ninsights from the parent of a frozen model may be used\ntarget_class (Optional[str]) – For multiclass lift - target class for this lift chart data.\ndata_slice_id (string or None) – The slice to retrieve Lift Chart for; if None, retrieve unsliced data.\nclassmethod from_server_data(data, keep_attrs=None, use_insights_format=False, **kwargs)\nOverwrite APIObject.from_server_data to handle lift chart data retrieved\nfrom either legacy URL or /insights/ new URL.\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nuse_insights_format (Optional[bool]) – Whether to repack the data from the format used in the GET /insights/liftChart/ URL\nto the format used in the legacy URL.\nData slices\nclass datarobot.models.data_slice.DataSlice\nDefinition of a data slice\nVariables:\nid (str) – ID of the data slice.\nname (str) – Name of the data slice definition.\nfilters (list[DataSliceFiltersType]) –\nList of DataSliceFiltersType with params\noperand (str)\nName of the feature to use in the filter.\noperator (str)\nOperator to use in the filter - eq, in, <, or >.\nvalues (Union[str, int, float])\nValues to use from the feature.\nproject_id (str) – ID of the project that the model is part of.\nclassmethod list(project, offset=0, limit=100)\nList the data slices in the same project\nParameters:\nproject (Union[str, Project]) – ID of the project or Project object from which to list data slices.\noffset (Optional[int]) – Number of items to skip.\nlimit (Optional[int]) – Number of items to return.\nReturns:\ndata_slices\nReturn type:\nlist[DataSlice]\nExamples\n>>> import datarobot as dr\n>>> ...  # set up your Client\n>>> data_slices = dr.DataSlice.list(\"646d0ea0cd8eb2355a68b0e5\")\n>>> data_slices\n[DataSlice(...), DataSlice(...), ...]\nclassmethod create(name, filters, project)\nCreates a data slice in the project with the given name and filters\nParameters:\nname (str) – Name of the data slice definition.\nfilters (list[DataSliceFiltersType]) –\nList of filters (dict) with params:\noperand (str)Name of the feature to use in filter.\noperator (str)Operator to use: ‘eq’, ‘in’, ‘<’, or ‘>’.\nvalues (Union[str, int, float])Values to use from the feature.\nproject (Union[str, Project]) – Project ID or Project object from which to list data slices.\nReturns:\ndata_slice – The data slice object created\nReturn type:\nDataSlice\nExamples\n>>> import datarobot as dr\n>>> ...  # set up your Client and retrieve a project\n>>> data_slice = dr.DataSlice.create(\n>>> ...    name='yes',\n>>> ...    filters=[{'operand': 'binary_target', 'operator': 'eq', 'values': ['Yes']}],\n>>> ...    project=project,\n>>> ...  )\n>>> data_slice\nDataSlice(\nfilters=[{'operand': 'binary_target', 'operator': 'eq', 'values': ['Yes']}],\nid=646d1296bd0c543d88923c9d,\nname=yes,\nproject_id=646d0ea0cd8eb2355a68b0e5\n)\ndelete()\nDeletes the data slice from storage\n:rtype: None\nExamples\n>>> import datarobot as dr\n>>> data_slice = dr.DataSlice.get('5a8ac9ab07a57a0001be501f')\n>>> data_slice.delete()\n>>> import datarobot as dr\n>>> ... # get project or project_id\n>>> data_slices = dr.DataSlice.list(project)  # project object or project_id\n>>> data_slice = data_slices[0]  # choose a data slice from the list\n>>> data_slice.delete()\nrequest_size(source, model=None)\nSubmits a request to validate the data slice’s filters and\ncalculate the data slice’s number of rows on a given source\nParameters:\nsource (INSIGHTS_SOURCES) – Subset of data (partition or “source”) on which to apply the data slice\nfor estimating available rows.\nmodel (Optional[Union[str, Model]]) – Model object or ID of the model. It is only required when source is “training”.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nExamples\n>>> import datarobot as dr\n>>> ... # get project or project_id\n>>> data_slices = dr.DataSlice.list(project)  # project object or project_id\n>>> data_slice = data_slices[0]  # choose a data slice from the list\n>>> status_check_job = data_slice.request_size(\"validation\")\nModel is required when source is ‘training’\n>>> import datarobot as dr\n>>> ... # get project or project_id\n>>> data_slices = dr.DataSlice.list(project)  # project object or project_id\n>>> data_slice = data_slices[0]  # choose a data slice from the list\n>>> status_check_job = data_slice.request_size(\"training\", model)\nget_size_info(source, model=None)\nGet information about the data slice applied to a source\nParameters:\nsource (INSIGHTS_SOURCES) – Source (partition or subset) to which the data slice was applied\nmodel (Optional[Union[str, Model]]) – ID for the model whose training data was sliced with this data slice.\nRequired when the source is “training”, and not used for other sources.\nReturns:\nslice_size_info – Information of the data slice applied to a source\nReturn type:\nDataSliceSizeInfo\nExamples\n>>> import datarobot as dr\n>>> ...  # set up your Client\n>>> data_slices = dr.DataSlice.list(\"646d0ea0cd8eb2355a68b0e5\")\n>>> data_slice = slices[0]  # can be any slice in the list\n>>> data_slice_size_info = data_slice.get_size_info(\"validation\")\n>>> data_slice_size_info\nDataSliceSizeInfo(\ndata_slice_id=6493a1776ea78e6644382535,\nmessages=[\n{\n'level': 'WARNING',\n'description': 'Low Observation Count',\n'additional_info': 'Insufficient number of observations to compute some insights.'\n}\n],\nmodel_id=None,\nproject_id=646d0ea0cd8eb2355a68b0e5,\nslice_size=1,\nsource=validation,\n)\n>>> data_slice_size_info.to_dict()\n{\n'data_slice_id': '6493a1776ea78e6644382535',\n'messages': [\n{\n'level': 'WARNING',\n'description': 'Low Observation Count',\n'additional_info': 'Insufficient number of observations to compute some insights.'\n}\n],\n'model_id': None,\n'project_id': '646d0ea0cd8eb2355a68b0e5',\n'slice_size': 1,\n'source': 'validation',\n}\n>>> import datarobot as dr\n>>> ...  # set up your Client\n>>> data_slice = dr.DataSlice.get(\"6493a1776ea78e6644382535\")\n>>> data_slice_size_info = data_slice.get_size_info(\"validation\")\nWhen using source=’training’, the model param is required.\n>>> import datarobot as dr\n>>> ...  # set up your Client\n>>> model = dr.Model.get(project_id, model_id)\n>>> data_slice = dr.DataSlice.get(\"6493a1776ea78e6644382535\")\n>>> data_slice_size_info = data_slice.get_size_info(\"training\", model)\n>>> import datarobot as dr\n>>> ...  # set up your Client\n>>> data_slice = dr.DataSlice.get(\"6493a1776ea78e6644382535\")\n>>> data_slice_size_info = data_slice.get_size_info(\"training\", model_id)\nclassmethod get(data_slice_id)\nRetrieve a specific data slice.\nParameters:\ndata_slice_id (str) – The identifier of the data slice to retrieve.\nReturns:\ndata_slice – The required data slice.\nReturn type:\nDataSlice\nExamples\n>>> import datarobot as dr\n>>> dr.DataSlice.get('648b232b9da812a6aaa0b7a9')\nDataSlice(filters=[{'operand': 'binary_target', 'operator': 'eq', 'values': ['Yes']}],\nid=648b232b9da812a6aaa0b7a9,\nname=test,\nproject_id=644bc575572480b565ca42cd\n)\nclass datarobot.models.data_slice.DataSliceSizeInfo\nDefinition of a data slice applied to a source\nVariables:\ndata_slice_id (str) – ID of the data slice\nproject_id (str) – ID of the project\nsource (str) – Data source used to calculate the number of rows (slice size) after applying the data slice’s filters\nmodel_id (Optional[str]) – ID of the model, required when source (subset) is ‘training’\nslice_size (int) – Number of rows in the data slice for a given source\nmessages (list[DataSliceSizeMessageType]) – List of user-relevant messages related to a data slice\nDatetime trend plots\nclass datarobot.models.datetime_trend_plots.AccuracyOverTimePlotsMetadata\nAccuracy over Time metadata for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nforecast_distance (int or None) – The forecast distance for which the metadata was retrieved. None for OTV projects.\nresolutions (list of string) – A list of datarobot.enums.DATETIME_TREND_PLOTS_RESOLUTION, which represents\navailable time resolutions for which plots can be retrieved.\nbacktest_metadata (list of dict) – List of backtest metadata dicts.\nThe list index of metadata dict is the backtest index.\nSee backtest/holdout metadata info in Notes for more details.\nholdout_metadata (dict) – Holdout metadata dict. See backtest/holdout metadata info in Notes for more details.\nbacktest_statuses (list of dict) – List of backtest statuses dict. The list index of status dict is the backtest index.\nSee backtest/holdout status info in Notes for more details.\nholdout_statuses (dict) – Holdout status dict. See backtest/holdout status info in Notes for more details.\nNotes\nBacktest/holdout status is a dict containing the following:\ntraining: stringStatus backtest/holdout training. One of datarobot.enums.DATETIME_TREND_PLOTS_STATUS\nvalidation: stringStatus backtest/holdout validation. One of datarobot.enums.DATETIME_TREND_PLOTS_STATUS\nBacktest/holdout metadata is a dict containing the following:\ntraining: dictStart and end dates for the backtest/holdout training.\nvalidation: dictStart and end dates for the backtest/holdout validation.\nEach dict in the training and validation in backtest/holdout metadata is structured like:\nstart_date: datetime.datetime or NoneThe datetime of the start of the chart data (inclusive). None if chart data is not computed.\nend_date: datetime.datetime or NoneThe datetime of the end of the chart data (exclusive). None if chart data is not computed.\nclass datarobot.models.datetime_trend_plots.AccuracyOverTimePlot\nAccuracy over Time plot for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nresolution (string) – The resolution that is used for binning.\nOne of datarobot.enums.DATETIME_TREND_PLOTS_RESOLUTION\nstart_date (datetime.datetime) – The datetime of the start of the chart data (inclusive).\nend_date (datetime.datetime) – The datetime of the end of the chart data (exclusive).\nbins (list of dict) – List of plot bins. See bin info in Notes for more details.\nstatistics (dict) – Statistics for plot. See statistics info in Notes for more details.\ncalendar_events (list of dict) – List of calendar events for the plot. See calendar events info in Notes for more details.\nNotes\nBin is a dict containing the following:\nstart_date: datetime.datetimeThe datetime of the start of the bin (inclusive).\nend_date: datetime.datetimeThe datetime of the end of the bin (exclusive).\nactual: float or NoneAverage actual value of the target in the bin. None if there are no entries in the bin.\npredicted: float or NoneAverage prediction of the model in the bin. None if there are no entries in the bin.\nfrequency: int or NoneIndicates number of values averaged in bin.\nStatistics is a dict containing the following:\ndurbin_watson: float or NoneThe Durbin-Watson statistic for the chart data.\nValue is between 0 and 4. Durbin-Watson statistic\nis a test statistic used to detect the presence of\nautocorrelation at lag 1 in the residuals (prediction errors)\nfrom a regression analysis. More info\nhttps://wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic\nCalendar event is a dict containing the following:\nname: stringName of the calendar event.\ndate: datetimeDate of the calendar event.\nseries_id: string or NoneThe series ID for the event. If this event does not specify a series ID,\nthen this will be None, indicating that the event applies to all series.\nclass datarobot.models.datetime_trend_plots.AccuracyOverTimePlotPreview\nAccuracy over Time plot preview for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nstart_date (datetime.datetime) – The datetime of the start of the chart data (inclusive).\nend_date (datetime.datetime) – The datetime of the end of the chart data (exclusive).\nbins (list of dict) – List of plot bins. See bin info in Notes for more details.\nNotes\nBin is a dict containing the following:\nstart_date: datetime.datetimeThe datetime of the start of the bin (inclusive).\nend_date: datetime.datetimeThe datetime of the end of the bin (exclusive).\nactual: float or NoneAverage actual value of the target in the bin. None if there are no entries in the bin.\npredicted: float or NoneAverage prediction of the model in the bin. None if there are no entries in the bin.\nclass datarobot.models.datetime_trend_plots.ForecastVsActualPlotsMetadata\nForecast vs Actual plots metadata for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nresolutions (list of string) – A list of datarobot.enums.DATETIME_TREND_PLOTS_RESOLUTION, which represents\navailable time resolutions for which plots can be retrieved.\nbacktest_metadata (list of dict) – List of backtest metadata dicts.\nThe list index of metadata dict is the backtest index.\nSee backtest/holdout metadata info in Notes for more details.\nholdout_metadata (dict) – Holdout metadata dict. See backtest/holdout metadata info in Notes for more details.\nbacktest_statuses (list of dict) – List of backtest statuses dict. The list index of status dict is the backtest index.\nSee backtest/holdout status info in Notes for more details.\nholdout_statuses (dict) – Holdout status dict. See backtest/holdout status info in Notes for more details.\nNotes\nBacktest/holdout status is a dict containing the following:\ntraining: dictDict containing each of datarobot.enums.DATETIME_TREND_PLOTS_STATUS as dict key,\nand list of forecast distances for particular status as dict value.\nvalidation: dictDict containing each of datarobot.enums.DATETIME_TREND_PLOTS_STATUS as dict key,\nand list of forecast distances for particular status as dict value.\nBacktest/holdout metadata is a dict containing the following:\ntraining: dictStart and end dates for the backtest/holdout training.\nvalidation: dictStart and end dates for the backtest/holdout validation.\nEach dict in the training and validation in backtest/holdout metadata is structured like:\nstart_date: datetime.datetime or NoneThe datetime of the start of the chart data (inclusive). None if chart data is not computed.\nend_date: datetime.datetime or NoneThe datetime of the end of the chart data (exclusive). None if chart data is not computed.\nclass datarobot.models.datetime_trend_plots.ForecastVsActualPlot\nForecast vs Actual plot for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nforecast_distances (list of int) – A list of forecast distances that were retrieved.\nresolution (string) – The resolution that is used for binning.\nOne of datarobot.enums.DATETIME_TREND_PLOTS_RESOLUTION\nstart_date (datetime.datetime) – The datetime of the start of the chart data (inclusive).\nend_date (datetime.datetime) – The datetime of the end of the chart data (exclusive).\nbins (list of dict) – List of plot bins. See bin info in Notes for more details.\ncalendar_events (list of dict) – List of calendar events for the plot. See calendar events info in Notes for more details.\nNotes\nBin is a dict containing the following:\nstart_date: datetime.datetimeThe datetime of the start of the bin (inclusive).\nend_date: datetime.datetimeThe datetime of the end of the bin (exclusive).\nactual: float or NoneAverage actual value of the target in the bin. None if there are no entries in the bin.\nforecasts: list of floatA list of average forecasts for the model for each forecast distance.\nEmpty if there are no forecasts in the bin.\nEach index in the forecasts list maps to forecastDistances list index.\nerror: float or NoneAverage absolute residual value of the bin.\nNone if there are no entries in the bin.\nnormalized_error: float or NoneNormalized average absolute residual value of the bin.\nNone if there are no entries in the bin.\nfrequency: int or NoneIndicates number of values averaged in bin.\nCalendar event is a dict containing the following:\nname: stringName of the calendar event.\ndate: datetimeDate of the calendar event.\nseries_id: string or NoneThe series ID for the event. If this event does not specify a series ID,\nthen this will be None, indicating that the event applies to all series.\nclass datarobot.models.datetime_trend_plots.ForecastVsActualPlotPreview\nForecast vs Actual plot preview for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nstart_date (datetime.datetime) – The datetime of the start of the chart data (inclusive).\nend_date (datetime.datetime) – The datetime of the end of the chart data (exclusive).\nbins (list of dict) – List of plot bins. See bin info in Notes for more details.\nNotes\nBin is a dict containing the following:\nstart_date: datetime.datetimeThe datetime of the start of the bin (inclusive).\nend_date: datetime.datetimeThe datetime of the end of the bin (exclusive).\nactual: float or NoneAverage actual value of the target in the bin. None if there are no entries in the bin.\npredicted: float or NoneAverage prediction of the model in the bin. None if there are no entries in the bin.\nclass datarobot.models.datetime_trend_plots.AnomalyOverTimePlotsMetadata\nAnomaly over Time metadata for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nresolutions (list of string) – A list of datarobot.enums.DATETIME_TREND_PLOTS_RESOLUTION, which represents\navailable time resolutions for which plots can be retrieved.\nbacktest_metadata (list of dict) – List of backtest metadata dicts.\nThe list index of metadata dict is the backtest index.\nSee backtest/holdout metadata info in Notes for more details.\nholdout_metadata (dict) – Holdout metadata dict. See backtest/holdout metadata info in Notes for more details.\nbacktest_statuses (list of dict) – List of backtest statuses dict. The list index of status dict is the backtest index.\nSee backtest/holdout status info in Notes for more details.\nholdout_statuses (dict) – Holdout status dict. See backtest/holdout status info in Notes for more details.\nNotes\nBacktest/holdout status is a dict containing the following:\ntraining: stringStatus backtest/holdout training. One of datarobot.enums.DATETIME_TREND_PLOTS_STATUS\nvalidation: stringStatus backtest/holdout validation. One of datarobot.enums.DATETIME_TREND_PLOTS_STATUS\nBacktest/holdout metadata is a dict containing the following:\ntraining: dictStart and end dates for the backtest/holdout training.\nvalidation: dictStart and end dates for the backtest/holdout validation.\nEach dict in the training and validation in backtest/holdout metadata is structured like:\nstart_date: datetime.datetime or NoneThe datetime of the start of the chart data (inclusive). None if chart data is not computed.\nend_date: datetime.datetime or NoneThe datetime of the end of the chart data (exclusive). None if chart data is not computed.\nclass datarobot.models.datetime_trend_plots.AnomalyOverTimePlot\nAnomaly over Time plot for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nresolution (string) – The resolution that is used for binning.\nOne of datarobot.enums.DATETIME_TREND_PLOTS_RESOLUTION\nstart_date (datetime.datetime) – The datetime of the start of the chart data (inclusive).\nend_date (datetime.datetime) – The datetime of the end of the chart data (exclusive).\nbins (list of dict) – List of plot bins. See bin info in Notes for more details.\ncalendar_events (list of dict) – List of calendar events for the plot. See calendar events info in Notes for more details.\nNotes\nBin is a dict containing the following:\nstart_date: datetime.datetimeThe datetime of the start of the bin (inclusive).\nend_date: datetime.datetimeThe datetime of the end of the bin (exclusive).\npredicted: float or NoneAverage prediction of the model in the bin. None if there are no entries in the bin.\nfrequency: int or NoneIndicates number of values averaged in bin.\nCalendar event is a dict containing the following:\nname: stringName of the calendar event.\ndate: datetimeDate of the calendar event.\nseries_id: string or NoneThe series ID for the event. If this event does not specify a series ID,\nthen this will be None, indicating that the event applies to all series.\nclass datarobot.models.datetime_trend_plots.AnomalyOverTimePlotPreview\nAnomaly over Time plot preview for datetime model.\nAdded in version v2.25.\nVariables:\nproject_id (string) – The project ID.\nmodel_id (string) – The model ID.\nprediction_threshold (float) – Only bins with predictions exceeding\nthis threshold are returned in the response.\nstart_date (datetime.datetime) – The datetime of the start of the chart data (inclusive).\nend_date (datetime.datetime) – The datetime of the end of the chart data (exclusive).\nbins (list of dict) – List of plot bins. See bin info in Notes for more details.\nNotes\nBin is a dict containing the following:\nstart_date: datetime.datetimeThe datetime of the start of the bin (inclusive).\nend_date: datetime.datetimeThe datetime of the end of the bin (exclusive).\nExternal scores and insights\nclass datarobot.ExternalScores\nMetric scores on prediction dataset with target or actual value column in unsupervised\ncase. Contains project metrics for supervised and special classification metrics set for\nunsupervised projects.\nAdded in version v2.21.\nVariables:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model\ndataset_id (str) – id of the prediction dataset with target or actual value column for unsupervised case\nactual_value_column (Optional[str]) – For unsupervised projects only.\nActual value column which was used to calculate the classification metrics and\ninsights on the prediction dataset.\nscores (list of dicts in a form of {'label': metric_name, 'value': score}) – Scores on the dataset.\nExamples\nList all scores for a dataset\nfrom datarobot.models.external_dataset_scores_insights.external_scores import ExternalScores\nscores = ExternalScores.list(project_id, dataset_id=dataset_id)\nclassmethod create(project_id, model_id, dataset_id, actual_value_column=None)\nCompute an external dataset insights for the specified model.\nParameters:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model for which insights is requested\ndataset_id (str) – id of the dataset for which insights is requested\nactual_value_column (Optional[str]) – actual values column label, for unsupervised projects only\nReturns:\njob – an instance of created async job\nReturn type:\nJob\nclassmethod list(project_id, model_id=None, dataset_id=None, offset=0, limit=100)\nFetch external scores list for the project and optionally for model and dataset.\nParameters:\nproject_id (str) – id of the project\nmodel_id (Optional[str]) – if specified, only scores for this model will be retrieved\ndataset_id (Optional[str]) – if specified, only scores for this dataset will be retrieved\noffset (Optional[int]) – this many results will be skipped, default: 0\nlimit (Optional[int]) – at most this many results are returned, default: 100, max 1000.\nTo return all results, specify 0\nReturn type:\nList[ExternalScores]\nReturns:\nA list of External Scores objects\nclassmethod get(project_id, model_id, dataset_id)\nRetrieve external scores for the project, model and dataset.\nParameters:\nproject_id (str) – id of the project\nmodel_id (str) – if specified, only scores for this model will be retrieved\ndataset_id (str) – if specified, only scores for this dataset will be retrieved\nReturn type:\nExternalScores\nReturns:\nExternal Scores object\nclass datarobot.ExternalLiftChart\nLift chart for the model and prediction dataset with target or actual value column in\nunsupervised case.\nAdded in version v2.21.\nLiftChartBin is a dict containing the following:\nactual (float) Sum of actual target values in bin\npredicted (float) Sum of predicted target values in bin\nbin_weight (float) The weight of the bin. For weighted projects, it is the sum of           the weights of the rows in the bin. For unweighted projects, it is the number of rows in           the bin.\nVariables:\ndataset_id (str) – id of the prediction dataset with target or actual value column for unsupervised case\nbins (list of dict) – List of dicts with schema described as LiftChartBin above.\nclassmethod list(project_id, model_id, dataset_id=None, offset=0, limit=100)\nRetrieve list of the lift charts for the model.\nParameters:\nproject_id (str) – id of the project\nmodel_id (str) – if specified, only lift chart for this model will be retrieved\ndataset_id (Optional[str]) – if specified, only lift chart for this dataset will be retrieved\noffset (Optional[int]) – this many results will be skipped, default: 0\nlimit (Optional[int]) – at most this many results are returned, default: 100, max 1000.\nTo return all results, specify 0\nReturn type:\nList[ExternalLiftChart]\nReturns:\nA list of ExternalLiftChart objects\nclassmethod get(project_id, model_id, dataset_id)\nRetrieve lift chart for the model and prediction dataset.\nParameters:\nproject_id (str) – project id\nmodel_id (str) – model id\ndataset_id (str) – prediction dataset id with target or actual value column for unsupervised case\nReturn type:\nExternalLiftChart\nReturns:\nExternalLiftChart object\nclass datarobot.ExternalRocCurve\nROC curve data for the model and prediction dataset with target or actual value column in\nunsupervised case.\nAdded in version v2.21.\nVariables:\ndataset_id (str) – id of the prediction dataset with target or actual value column for unsupervised case\nroc_points (list of dict) – List of precalculated metrics associated with thresholds for ROC curve.\nnegative_class_predictions (list of float) – List of predictions from example for negative class\npositive_class_predictions (list of float) – List of predictions from example for positive class\nclassmethod list(project_id, model_id, dataset_id=None, offset=0, limit=100)\nRetrieve list of the roc curves for the model.\nParameters:\nproject_id (str) – id of the project\nmodel_id (str) – if specified, only lift chart for this model will be retrieved\ndataset_id (Optional[str]) – if specified, only lift chart for this dataset will be retrieved\noffset (Optional[int]) – this many results will be skipped, default: 0\nlimit (Optional[int]) – at most this many results are returned, default: 100, max 1000.\nTo return all results, specify 0\nReturn type:\nList[ExternalRocCurve]\nReturns:\nA list of ExternalRocCurve objects\nclassmethod get(project_id, model_id, dataset_id)\nRetrieve ROC curve chart for the model and prediction dataset.\nParameters:\nproject_id (str) – project id\nmodel_id (str) – model id\ndataset_id (str) – prediction dataset id with target or actual value column for unsupervised case\nReturn type:\nExternalRocCurve\nReturns:\nExternalRocCurve object\nFeature association\nclass datarobot.models.FeatureAssociationMatrix\nFeature association statistics for a project.\nNotes\nProjects created prior to v2.17 are not supported by this feature.\nVariables:\nproject_id (str) – Id of the associated project.\nstrengths (list of dict) – Pairwise statistics for the available features as structured below.\nfeatures (list of dict) – Metadata for each feature and where it goes in the matrix.\nExamples\nimport datarobot as dr\n# retrieve feature association matrix\nfeature_association_matrix = dr.FeatureAssociationMatrix.get(project_id)\nfeature_association_matrix.strengths\nfeature_association_matrix.features\n# retrieve feature association matrix for a metric, association type or a feature list\nfeature_association_matrix = dr.FeatureAssociationMatrix.get(\nproject_id,\nmetric=enums.FEATURE_ASSOCIATION_METRIC.SPEARMAN,\nassociation_type=enums.FEATURE_ASSOCIATION_TYPE.CORRELATION,\nfeaturelist_id=featurelist_id,\n)\nclassmethod get(project_id, metric=None, association_type=None, featurelist_id=None)\nGet feature association statistics.\nParameters:\nproject_id (str) – Id of the project that contains the requested associations.\nmetric (enums.FEATURE_ASSOCIATION_METRIC) – The name of a metric to get pairwise data for. Since ‘v2.19’ this is optional and\ndefaults to enums.FEATURE_ASSOCIATION_METRIC.MUTUAL_INFO.\nassociation_type (enums.FEATURE_ASSOCIATION_TYPE) – The type of dependence for the data. Since ‘v2.19’ this is optional and defaults to\nenums.FEATURE_ASSOCIATION_TYPE.ASSOCIATION.\nfeaturelist_id (str or None) – Optional, the feature list to lookup FAM data for. By default, depending on the type of\nthe project “Informative Features” or “Timeseries Informative Features” list will be\nused.\n(New in version v2.19)\nReturns:\nFeature association pairwise metric strength data, feature clustering data, and\nordering data for Feature Association Matrix visualization.\nReturn type:\nFeatureAssociationMatrix\nclassmethod create(project_id, featurelist_id)\nCompute the Feature Association Matrix for a Feature List\nParameters:\nproject_id (str) – The ID of the project that the feature list belongs to.\nfeaturelist_id (str) – The ID of the feature list for which insights are requested.\nReturns:\nstatus_check_job – Object contains all needed logic for a periodical status check of an async job.\nReturn type:\nStatusCheckJob\nFeature association matrix details\nclass datarobot.models.FeatureAssociationMatrixDetails\nPlotting details for a pair of passed features present in the feature association matrix.\nNotes\nProjects created prior to v2.17 are not supported by this feature.\nVariables:\nproject_id (str) – Id of the project that contains the requested associations.\nchart_type (str) – Which type of plotting the pair of features gets in the UI.\ne.g. ‘HORIZONTAL_BOX’, ‘VERTICAL_BOX’, ‘SCATTER’ or ‘CONTINGENCY’\nvalues (list) – The data triplets for pairwise plotting e.g.\n{“values”: [[460.0, 428.5, 0.001], [1679.3, 259.0, 0.001], …]\nThe first entry of each list is a value of feature1, the second entry of each list is a\nvalue of feature2, and the third is the relative frequency of the pair of datapoints in the\nsample.\nfeatures (list) – A list of the requested features, [feature1, feature2]\ntypes (list) – The type of feature1 and feature2. Possible values: “CATEGORICAL”, “NUMERIC”\nfeaturelist_id (str) – Id of the feature list to lookup FAM details for.\nclassmethod get(project_id, feature1, feature2, featurelist_id=None)\nGet a sample of the actual values used to measure the association between a pair of features\nAdded in version v2.17.\nParameters:\nproject_id (str) – Id of the project of interest.\nfeature1 (str) – Feature name for the first feature of interest.\nfeature2 (str) – Feature name for the second feature of interest.\nfeaturelist_id (str) – Optional, the feature list to lookup FAM data for. By default, depending on the type of\nthe project “Informative Features” or “Timeseries Informative Features” list will be\nused.\nReturns:\nThe feature association plotting for provided pair of features.\nReturn type:\nFeatureAssociationMatrixDetails\nFeature association featurelists\nclass datarobot.models.FeatureAssociationFeaturelists\nFeaturelists with feature association matrix availability flags for a project.\nVariables:\nproject_id (str) – Id of the project that contains the requested associations.\nfeaturelists (list fo dict) – The featurelists with the featurelist_id, title and the has_fam flag.\nclassmethod get(project_id)\nGet featurelists with feature association status for each.\nParameters:\nproject_id (str) – Id of the project of interest.\nReturns:\nFeaturelist with feature association status for each.\nReturn type:\nFeatureAssociationFeaturelists\nFeature effects\nclass datarobot.models.FeatureEffects\nFeature Effects provides partial dependence and predicted vs actual values for top-500\nfeatures ordered by feature impact score.\nThe partial dependence shows marginal effect of a feature on the target variable after\naccounting for the average effects of all other predictive features. It indicates how, holding\nall other variables except the feature of interest as they were, the value of this feature\naffects your prediction.\nVariables:\nproject_id (string) – The project that contains requested model\nmodel_id (string) – The model to retrieve Feature Effects for\nsource (string) – The source to retrieve Feature Effects for\ndata_slice_id (string or None) – The slice to retrieve Feature Effects for; if None, retrieve unsliced data\nfeature_effects (list) – Feature Effects for every feature\nbacktest_index (string, required only for DatetimeModels,) – The backtest index to retrieve Feature Effects for.\nNotes\nfeatureEffects is a dict containing the following:\nfeature_name (string) Name of the feature\nfeature_type (string) dr.enums.FEATURE_TYPE,           Feature type either numeric, categorical or datetime\nfeature_impact_score (float) Feature impact score\nweight_label (string) optional, Weight label if configured for the project else null\npartial_dependence (List) Partial dependence results\npredicted_vs_actual (List) optional, Predicted versus actual results,           may be omitted if there are insufficient qualified samples\npartial_dependence is a dict containing the following:\nis_capped (bool) Indicates whether the data for computation is capped\ndata (List) partial dependence results in the following format\ndata is a list of dict containing the following:\nlabel (string) Contains label for categorical and numeric features as string\ndependence (float) Value of partial dependence\npredicted_vs_actual is a dict containing the following:\nis_capped (bool) Indicates whether the data for computation is capped\ndata (List) pred vs actual results in the following format\ndata is a list of dict containing the following:\nlabel (string) Contains label for categorical features           for numeric features contains range or numeric value.\nbin (List) optional, For numeric features contains           labels for left and right bin limits\npredicted (float) Predicted value\nactual (float) Actual value. Actual value is null           for unsupervised timeseries models\nrow_count (int or float) Number of rows for the label and bin.           Type is float if weight or exposure is set for the project.\nclassmethod from_server_data(data, *args, use_insights_format=False, **kwargs)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing.\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nuse_insights_format (Optional[bool]) – Whether to repack the data from the format used in the GET /insights/featureEffects/ URL\nto the format used in the legacy URL.\nclass datarobot.models.FeatureEffectMetadata\nFeature Effect Metadata for model, contains status and available model sources.\nNotes\nsource is expected parameter to retrieve Feature Effect. One of provided sources\nshall be used.\nclass datarobot.models.FeatureEffectMetadataDatetime\nFeature Effect Metadata for datetime model, contains list of\nfeature effect metadata per backtest.\nNotes\nfeature effect metadata per backtest contains:\nstatus : str.\nbacktest_index : str.\nsources : List[str].\nsource is expected parameter to retrieve Feature Effect. One of provided sources\nshall be used.\nbacktest_index is expected parameter to submit compute request and retrieve Feature Effect.\nOne of provided backtest indexes shall be used.\nVariables:\ndata (list[FeatureEffectMetadataDatetimePerBacktest]) – List feature effect metadata per backtest\nclass datarobot.models.FeatureEffectMetadataDatetimePerBacktest\nConvert dictionary into feature effect metadata per backtest which contains backtest_index,\nstatus and sources.\nPayoff matrix\nclass datarobot.models.PayoffMatrix\nRepresents a Payoff Matrix, a costs/benefit scenario used for creating a profit curve.\nVariables:\nproject_id (str) – id of the project with which the payoff matrix is associated.\nid (str) – id of the payoff matrix.\nname (str) – User-supplied label for the payoff matrix.\ntrue_positive_value (float) – Cost or benefit of a true positive classification\ntrue_negative_value (float) – Cost or benefit of a true negative classification\nfalse_positive_value (float) – Cost or benefit of a false positive classification\nfalse_negative_value (float) – Cost or benefit of a false negative classification\nExamples\nimport datarobot as dr\n# create a payoff matrix\npayoff_matrix = dr.PayoffMatrix.create(\nproject_id,\nname,\ntrue_positive_value=100,\ntrue_negative_value=10,\nfalse_positive_value=0,\nfalse_negative_value=-10,\n)\n# list available payoff matrices\npayoff_matrices = dr.PayoffMatrix.list(project_id)\npayoff_matrix = payoff_matrices[0]\nclassmethod create(project_id, name, true_positive_value=1, true_negative_value=1, false_positive_value=-1, false_negative_value=-1)\nCreate a payoff matrix associated with a specific project.\nParameters:\nproject_id (str) – id of the project with which the payoff matrix will be associated\nReturns:\npayoff_matrix – The newly created payoff matrix\nReturn type:\nPayoffMatrix\nclassmethod list(project_id)\nFetch all the payoff matrices for a project.\nParameters:\nproject_id (str) – id of the project\nReturns:\nA list of PayoffMatrix objects\nReturn type:\nList of PayoffMatrix\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(project_id, id)\nRetrieve a specified payoff matrix.\nParameters:\nproject_id (str) – id of the project the model belongs to\nid (str) – id of the payoff matrix\nReturn type:\nPayoffMatrix\nReturns:\nPayoffMatrix object representing specified\npayoff matrix\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod update(project_id, id, name, true_positive_value, true_negative_value, false_positive_value, false_negative_value)\nUpdate (replace) a payoff matrix. Note that all data fields are required.\nParameters:\nproject_id (str) – id of the project to which the payoff matrix belongs\nid (str) – id of the payoff matrix\nname (str) – User-supplied label for the payoff matrix\ntrue_positive_value (float) – True positive payoff value to use for the profit curve\ntrue_negative_value (float) – True negative payoff value to use for the profit curve\nfalse_positive_value (float) – False positive payoff value to use for the profit curve\nfalse_negative_value (float) – False negative payoff value to use for the profit curve\nReturns:\nPayoffMatrix with updated values\nReturn type:\npayoff_matrix\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod delete(project_id, id)\nDelete a specified payoff matrix.\nParameters:\nproject_id (str) – id of the project the model belongs to\nid (str) – id of the payoff matrix\nReturns:\nresponse – Empty response (204)\nReturn type:\nrequests.Response\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nPrediction explanations\nclass datarobot.PredictionExplanationsInitialization\nRepresents a prediction explanations initialization of a model.\nVariables:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model the prediction explanations initialization is for\nprediction_explanations_sample (list of dict) – a small sample of prediction explanations that could be generated for the model\nclassmethod get(project_id, model_id)\nRetrieve the prediction explanations initialization for a model.\nPrediction explanations initializations are a prerequisite for computing prediction\nexplanations, and include a sample what the computed prediction explanations for a\nprediction dataset would look like.\nParameters:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model the prediction explanations initialization is for\nReturns:\nprediction_explanations_initialization – The queried instance.\nReturn type:\nPredictionExplanationsInitialization\nRaises:\nClientError – If the project or model does not exist or the initialization has not been computed.\nclassmethod create(project_id, model_id)\nCreate a prediction explanations initialization for the specified model.\nParameters:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model for which initialization is requested\nReturns:\njob – an instance of created async job\nReturn type:\nJob\ndelete()\nDelete this prediction explanations initialization.\nclass datarobot.PredictionExplanations\nRepresents prediction explanations metadata and provides access to computation results.\nExamples\nprediction_explanations = dr.PredictionExplanations.get(project_id, explanations_id)\nfor row in prediction_explanations.get_rows():\nprint(row)  # row is an instance of PredictionExplanationsRow\nVariables:\nid (str) – id of the record and prediction explanations computation result\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model the prediction explanations are for\ndataset_id (str) – id of the prediction dataset prediction explanations were computed for\nmax_explanations (int) – maximum number of prediction explanations to supply per row of the dataset\nthreshold_low (float) – the lower threshold, below which a prediction must score in order for prediction\nexplanations to be computed for a row in the dataset\nthreshold_high (float) – the high threshold, above which a prediction must score in order for prediction\nexplanations to be computed for a row in the dataset\nnum_columns (int) – the number of columns prediction explanations were computed for\nfinish_time (float) – timestamp referencing when computation for these prediction explanations finished\nprediction_explanations_location (str) – where to retrieve the prediction explanations\nsource (str) – For OTV/TS in-training predictions. Holds the portion of the training dataset used to generate\npredictions.\nclassmethod get(project_id, prediction_explanations_id)\nRetrieve a specific prediction explanations metadata.\nParameters:\nproject_id (str) – id of the project the explanations belong to\nprediction_explanations_id (str) – id of the prediction explanations\nReturns:\nprediction_explanations – The queried instance.\nReturn type:\nPredictionExplanations\nclassmethod create(project_id, model_id, dataset_id, max_explanations=None, threshold_low=None, threshold_high=None, mode=None)\nCreate prediction explanations for the specified dataset.\nIn order to create PredictionExplanations for a particular model and dataset, you must\nfirst:\nCompute feature impact for the model via datarobot.Model.get_feature_impact()\nCompute a PredictionExplanationsInitialization for the model via\ndatarobot.PredictionExplanationsInitialization.create(project_id, model_id)\nCompute predictions for the model and dataset via\ndatarobot.Model.request_predictions(dataset_id)\nthreshold_high and threshold_low are optional filters applied to speed up\ncomputation.  When at least one is specified, only the selected outlier rows will have\nprediction explanations computed. Rows are considered to be outliers if their predicted\nvalue (in case of regression projects) or probability of being the positive\nclass (in case of classification projects) is less than threshold_low or greater than\nthresholdHigh.  If neither is specified, prediction explanations will be computed for\nall rows.\nParameters:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model for which prediction explanations are requested\ndataset_id (str) – id of the prediction dataset for which prediction explanations are requested\nthreshold_low (Optional[float]) – the lower threshold, below which a prediction must score in order for prediction\nexplanations to be computed for a row in the dataset. If neither threshold_high nor\nthreshold_low is specified, prediction explanations will be computed for all rows.\nthreshold_high (Optional[float]) – the high threshold, above which a prediction must score in order for prediction\nexplanations to be computed. If neither threshold_high nor threshold_low is\nspecified, prediction explanations will be computed for all rows.\nmax_explanations (Optional[int]) – the maximum number of prediction explanations to supply per row of the dataset,\ndefault: 3.\nmode (PredictionExplanationsMode, optional) – mode of calculation for multiclass models, if not specified - server default is\nto explain only the predicted class, identical to passing TopPredictionsMode(1).\nReturns:\njob – an instance of created async job\nReturn type:\nJob\nclassmethod create_on_training_data(project_id, model_id, dataset_id, max_explanations=None, threshold_low=None, threshold_high=None, mode=None, datetime_prediction_partition=None)\nCreate prediction explanations for the the dataset used to train the model.\nThis can be retrieved by calling dr.Model.get().featurelist_id.\nFor OTV and timeseries projects, datetime_prediction_partition is required and limited to the\nfirst backtest (‘0’) or holdout (‘holdout’).\nIn order to create PredictionExplanations for a particular model and dataset, you must\nfirst:\nCompute Feature Impact for the model via datarobot.Model.get_feature_impact()/\nCompute a PredictionExplanationsInitialization for the model via\ndatarobot.PredictionExplanationsInitialization.create(project_id, model_id).\nCompute predictions for the model and dataset via\ndatarobot.Model.request_predictions(dataset_id).\nthreshold_high and threshold_low are optional filters applied to speed up\ncomputation.  When at least one is specified, only the selected outlier rows will have\nprediction explanations computed. Rows are considered to be outliers if their predicted\nvalue (in case of regression projects) or probability of being the positive\nclass (in case of classification projects) is less than threshold_low or greater than\nthresholdHigh.  If neither is specified, prediction explanations will be computed for\nall rows.\nParameters:\nproject_id (str) – The ID of the project the model belongs to.\nmodel_id (str) – The ID of the model for which prediction explanations are requested.\ndataset_id (str) – The ID of the prediction dataset for which prediction explanations are requested.\nthreshold_low (Optional[float]) – The lower threshold, below which a prediction must score in order for prediction\nexplanations to be computed for a row in the dataset. If neither threshold_high nor\nthreshold_low is specified, prediction explanations will be computed for all rows.\nthreshold_high (Optional[float]) – The high threshold, above which a prediction must score in order for prediction\nexplanations to be computed. If neither threshold_high nor threshold_low is\nspecified, prediction explanations will be computed for all rows.\nmax_explanations (Optional[int]) – The maximum number of prediction explanations to supply per row of the dataset\n(default: 3).\nmode (PredictionExplanationsMode, optional) – The mode of calculation for multiclass models. If not specified, the server default is\nto explain only the predicted class, identical to passing TopPredictionsMode(1).\ndatetime_prediction_partition (str) – Options: ‘0’, ‘holdout’ or None.\nUsed only by time series and OTV projects to indicate what part of the dataset\nwill be used to generate predictions for computing prediction explanation. Current\noptions are ‘0’ (first backtest) and ‘holdout’.\nNote that only the validation partition of the first backtest will be used to\ngeneration predictions.\nReturns:\njob – An instance of created async job.\nReturn type:\nJob\nclassmethod list(project_id, model_id=None, limit=None, offset=None)\nList of prediction explanations metadata for a specified project.\nParameters:\nproject_id (str) – id of the project to list prediction explanations for\nmodel_id (Optional[str]) – if specified, only prediction explanations computed for this model will be returned\nlimit (int or None) – at most this many results are returned, default: no limit\noffset (int or None) – this many results will be skipped, default: 0\nReturns:\nprediction_explanations\nReturn type:\nlist[PredictionExplanations]\nget_rows(batch_size=None, exclude_adjusted_predictions=True)\nRetrieve prediction explanations rows.\nParameters:\nbatch_size (int or None, optional) – maximum number of prediction explanations rows to retrieve per request\nexclude_adjusted_predictions (bool) – Optional, defaults to True. Set to False to include adjusted predictions, which will\ndiffer from the predictions on some projects, e.g. those with an exposure column\nspecified.\nYields:\nprediction_explanations_row (PredictionExplanationsRow) – Represents prediction explanations computed for a prediction row.\nis_multiclass()\nWhether these explanations are for a multiclass project or a non-multiclass project\nis_unsupervised_clustering_or_multiclass()\nClustering and multiclass XEMP always has either one of num_top_classes or class_names\nparameters set\nget_number_of_explained_classes()\nHow many classes we attempt to explain for each row\nget_all_as_dataframe(exclude_adjusted_predictions=True)\nRetrieve all prediction explanations rows and return them as a pandas.DataFrame.\nReturned dataframe has the following structure:\nrow_id : row id from prediction dataset\nprediction : the output of the model for this row\nadjusted_prediction : adjusted prediction values (only appears for projects that\nutilize prediction adjustments, e.g. projects with an exposure column)\nclass_0_label : a class level from the target (only appears for classification\nprojects)\nclass_0_probability : the probability that the target is this class (only appears for\nclassification projects)\nclass_1_label : a class level from the target (only appears for classification\nprojects)\nclass_1_probability : the probability that the target is this class (only appears for\nclassification projects)\nexplanation_0_feature : the name of the feature contributing to the prediction for\nthis explanation\nexplanation_0_feature_value : the value the feature took on\nexplanation_0_label : the output being driven by this explanation.  For regression\nprojects, this is the name of the target feature.  For classification projects, this\nis the class label whose probability increasing would correspond to a positive\nstrength.\nexplanation_0_qualitative_strength : a human-readable description of how strongly the\nfeature affected the prediction (e.g. ‘+++’, ‘–’, ‘+’) for this explanation\nexplanation_0_per_ngram_text_explanations : Text prediction explanations data in json\nformatted string.\nexplanation_0_strength : the amount this feature’s value affected the prediction\n…\nexplanation_N_feature : the name of the feature contributing to the prediction for\nthis explanation\nexplanation_N_feature_value : the value the feature took on\nexplanation_N_label : the output being driven by this explanation.  For regression\nprojects, this is the name of the target feature.  For classification projects, this\nis the class label whose probability increasing would correspond to a positive\nstrength.\nexplanation_N_qualitative_strength : a human-readable description of how strongly the\nfeature affected the prediction (e.g. ‘+++’, ‘–’, ‘+’) for this explanation\nexplanation_N_per_ngram_text_explanations : Text prediction explanations data in json\nformatted string.\nexplanation_N_strength : the amount this feature’s value affected the prediction\nFor classification projects, the server does not guarantee any ordering on the prediction\nvalues, however within this function we sort the values so that class_X corresponds to\nthe same class from row to row.\nParameters:\nexclude_adjusted_predictions (bool) – Optional, defaults to True. Set this to False to include adjusted prediction values in\nthe returned dataframe.\nReturns:\ndataframe\nReturn type:\npandas.DataFrame\ndownload_to_csv(filename, encoding='utf-8', exclude_adjusted_predictions=True)\nSave prediction explanations rows into CSV file.\nParameters:\nfilename (str or file object) – path or file object to save prediction explanations rows\nencoding (string, optional) – A string representing the encoding to use in the output file, defaults to ‘utf-8’\nexclude_adjusted_predictions (bool) – Optional, defaults to True. Set to False to include adjusted predictions, which will\ndiffer from the predictions on some projects, e.g. those with an exposure column\nspecified.\nget_prediction_explanations_page(limit=None, offset=None, exclude_adjusted_predictions=True)\nGet prediction explanations.\nIf you don’t want use a generator interface, you can access paginated prediction\nexplanations directly.\nParameters:\nlimit (int or None) – the number of records to return, the server will use a (possibly finite) default if not\nspecified\noffset (int or None) – the number of records to skip, default 0\nexclude_adjusted_predictions (bool) – Optional, defaults to True. Set to False to include adjusted predictions, which will\ndiffer from the predictions on some projects, e.g. those with an exposure column\nspecified.\nReturns:\nprediction_explanations\nReturn type:\nPredictionExplanationsPage\ndelete()\nDelete these prediction explanations.\nclass datarobot.models.prediction_explanations.PredictionExplanationsRow\nRepresents prediction explanations computed for a prediction row.\nNotes\nPredictionValue contains:\nlabel : describes what this model output corresponds to.  For regression projects,\nit is the name of the target feature.  For classification projects, it is a level from\nthe target feature.\nvalue : the output of the prediction.  For regression projects, it is the predicted\nvalue of the target.  For classification projects, it is the predicted probability the\nrow belongs to the class identified by the label.\nPredictionExplanation contains:\nlabel : described what output was driven by this explanation.  For regression\nprojects, it is the name of the target feature.  For classification projects, it is the\nclass whose probability increasing would correspond to a positive strength of this\nprediction explanation.\nfeature : the name of the feature contributing to the prediction\nfeature_value : the value the feature took on for this row\nstrength : the amount this feature’s value affected the prediction\nqualitative_strength : a human-readable description of how strongly the feature\naffected the prediction. A large positive effect is denoted ‘+++’, medium ‘++’, small ‘+’,\nvery small ‘<+’. A large negative effect is denoted ‘—’, medium ‘–’, small ‘-’, very\nsmall ‘<-‘.\nVariables:\nrow_id (int) – which row this PredictionExplanationsRow describes\nprediction (float) – the output of the model for this row\nadjusted_prediction (float or None) – adjusted prediction value for projects that provide this information, None otherwise\nprediction_values (list) – an array of dictionaries with a schema described as PredictionValue\nadjusted_prediction_values (list) – same as prediction_values but for adjusted predictions\nprediction_explanations (list) – an array of dictionaries with a schema described as PredictionExplanation\nclass datarobot.models.prediction_explanations.PredictionExplanationsPage\nRepresents a batch of prediction explanations received by one request.\nVariables:\nid (str) – id of the prediction explanations computation result\ndata (list[dict]) – list of raw prediction explanations; each row corresponds to a row of the prediction dataset\ncount (int) – total number of rows computed\nprevious_page (str) – where to retrieve previous page of prediction explanations, None if current page is the\nfirst\nnext_page (str) – where to retrieve next page of prediction explanations, None if current page is the last\nprediction_explanations_record_location (str) – where to retrieve the prediction explanations metadata\nadjustment_method (str) – Adjustment method that was applied to predictions, or ‘N/A’ if no adjustments were done.\nclassmethod get(project_id, prediction_explanations_id, limit=None, offset=0, exclude_adjusted_predictions=True)\nRetrieve prediction explanations.\nParameters:\nproject_id (str) – id of the project the model belongs to\nprediction_explanations_id (str) – id of the prediction explanations\nlimit (int or None) – the number of records to return; the server will use a (possibly finite) default if not\nspecified\noffset (int or None) – the number of records to skip, default 0\nexclude_adjusted_predictions (bool) – Optional, defaults to True. Set to False to include adjusted predictions, which will\ndiffer from the predictions on some projects, e.g. those with an exposure column\nspecified.\nReturns:\nprediction_explanations – The queried instance.\nReturn type:\nPredictionExplanationsPage\nclass datarobot.models.ShapMatrix\nRepresents SHAP based prediction explanations and provides access to score values.\nVariables:\nproject_id (str) – id of the project the model belongs to\nshap_matrix_id (str) – id of the generated SHAP matrix\nmodel_id (str) – id of the model used to\ndataset_id (str) – id of the prediction dataset SHAP values were computed for\nExamples\nimport datarobot as dr\n# request SHAP matrix calculation\nshap_matrix_job = dr.ShapMatrix.create(project_id, model_id, dataset_id)\nshap_matrix = shap_matrix_job.get_result_when_complete()\n# list available SHAP matrices\nshap_matrices = dr.ShapMatrix.list(project_id)\nshap_matrix = shap_matrices[0]\n# get SHAP matrix as dataframe\nshap_matrix_values = shap_matrix.get_as_dataframe()\nclassmethod create(cls, project_id, model_id, dataset_id)\nCalculate SHAP based prediction explanations against previously uploaded dataset.\nParameters:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model for which prediction explanations are requested\ndataset_id (str) – id of the prediction dataset for which prediction explanations are requested (as\nuploaded from Project.upload_dataset)\nReturns:\njob – The job computing the SHAP based prediction explanations\nReturn type:\nShapMatrixJob\nRaises:\nClientError – If the server responded with 4xx status. Possible reasons are project, model or dataset\ndon’t exist, user is not allowed or model doesn’t support SHAP based prediction\nexplanations\nServerError – If the server responded with 5xx status\nclassmethod list(cls, project_id)\nFetch all the computed SHAP prediction explanations for a project.\nParameters:\nproject_id (str) – id of the project\nReturns:\nA list of ShapMatrix objects\nReturn type:\nList of ShapMatrix\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status\ndatarobot.errors.ServerError – if the server responded with 5xx status\nclassmethod get(cls, project_id, id)\nRetrieve the specific SHAP matrix.\nParameters:\nproject_id (str) – id of the project the model belongs to\nid (str) – id of the SHAP matrix\nReturn type:\nShapMatrix object representing specified record\nget_as_dataframe(read_timeout=60)\nRetrieve SHAP matrix values as dataframe.\nReturn type:\nDataFrame\nReturns:\ndataframe (pandas.DataFrame) – A dataframe with SHAP scores\nread_timeout (int (optional, default 60)) – .. versionadded:: 2.29\nWait this many seconds for the server to respond.\nRaises:\ndatarobot.errors.ClientError – if the server responded with 4xx status.\ndatarobot.errors.ServerError – if the server responded with 5xx status.\nclass datarobot.models.ClassListMode\nCalculate prediction explanations for the specified classes in each row.\nVariables:\nclass_names (list) – List of class names that will be explained for each dataset row.\nget_api_parameters(batch_route=False)\nGet parameters passed in corresponding API call\nParameters:\nbatch_route (bool) – Batch routes describe prediction calls with all possible parameters, so to\ndistinguish explanation parameters from others they have prefix in parameters.\nReturn type:\ndict\nclass datarobot.models.TopPredictionsMode\nCalculate prediction explanations for the number of top predicted classes in each row.\nVariables:\nnum_top_classes (int) – Number of top predicted classes [1..10] that will be explained for each dataset row.\nget_api_parameters(batch_route=False)\nGet parameters passed in corresponding API call\nParameters:\nbatch_route (bool) – Batch routes describe prediction calls with all possible parameters, so to\ndistinguish explanation parameters from others they have prefix in parameters.\nReturn type:\ndict\nRating table\nclass datarobot.models.RatingTable\nInterface to modify and download rating tables.\nVariables:\nid (str) – The id of the rating table.\nproject_id (str) – The id of the project this rating table belongs to.\nrating_table_name (str) – The name of the rating table.\noriginal_filename (str) – The name of the file used to create the rating table.\nparent_model_id (str) – The model id of the model the rating table was validated against.\nmodel_id (str) – The model id of the model that was created from the rating table.\nCan be None if a model has not been created from the rating table.\nmodel_job_id (str) – The id of the job to create a model from this rating table.\nCan be None if a model has not been created from the rating table.\nvalidation_job_id (str) – The id of the created job to validate the rating table.\nCan be None if the rating table has not been validated.\nvalidation_error (str) – Contains a description of any errors caused during validation.\nclassmethod from_server_data(data, should_warn=True, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nshould_warn (bool) – Whether or not to issue a warning if an invalid rating table is being retrieved.\nReturn type:\nRatingTable\nclassmethod get(project_id, rating_table_id)\nRetrieve a single rating table\nParameters:\nproject_id (str) – The ID of the project the rating table is associated with.\nrating_table_id (str) – The ID of the rating table\nReturns:\nrating_table – The queried instance\nReturn type:\nRatingTable\nclassmethod create(project_id, parent_model_id, filename, rating_table_name='Uploaded Rating Table')\nUploads and validates a new rating table CSV\nParameters:\nproject_id (str) – id of the project the rating table belongs to\nparent_model_id (str) – id of the model for which this rating table should be validated against\nfilename (str) – The path of the CSV file containing the modified rating table.\nrating_table_name (Optional[str]) – A human friendly name for the new rating table. The string may be\ntruncated and a suffix may be added to maintain unique names of all\nrating tables.\nReturns:\njob – an instance of created async job\nReturn type:\nJob\nRaises:\nInputNotUnderstoodError – Raised if filename isn’t one of supported types.\nClientError – Raised if parent_model_id is invalid.\ndownload(filepath)\nDownload a csv file containing the contents of this rating table\nParameters:\nfilepath (str) – The path at which to save the rating table file.\nReturn type:\nNone\nrename(rating_table_name)\nRenames a rating table to a different name.\nParameters:\nrating_table_name (str) – The new name to rename the rating table to.\nReturn type:\nNone\ncreate_model()\nCreates a new model from this rating table record. This rating table\nmust not already be associated with a model and must be valid.\nReturns:\njob – an instance of created async job\nReturn type:\nJob\nRaises:\nClientError – Raised if creating model from a RatingTable that failed validation\nJobAlreadyRequested – Raised if creating model from a RatingTable that is already\nassociated with a RatingTableModel\nROC curve\nclass datarobot.models.roc_curve.RocCurve\nROC curve data for model.\nVariables:\nsource (str) – ROC curve data source. Can be ‘validation’, ‘crossValidation’ or ‘holdout’.\nroc_points (list of dict) – List of precalculated metrics associated with thresholds for ROC curve.\nnegative_class_predictions (list of float) – List of predictions from example for negative class\npositive_class_predictions (list of float) – List of predictions from example for positive class\nsource_model_id (str) – ID of the model this ROC curve represents; in some cases,\ninsights from the parent of a frozen model may be used\ndata_slice_id (str) – ID of the data slice this ROC curve represents.\nclassmethod from_server_data(data, keep_attrs=None, use_insights_format=False, **kwargs)\nOverwrite APIObject.from_server_data to handle roc curve data retrieved\nfrom either legacy URL or /insights/ new URL.\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place.\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nuse_insights_format (Optional[bool]) – Whether to repack the data from the format used in the GET /insights/RocCur/ URL\nto the format used in the legacy URL.\nReturn type:\nRocCurve\nclass datarobot.models.roc_curve.LabelwiseRocCurve\nLabelwise ROC curve data for one label and one source.\nVariables:\nsource (str) – ROC curve data source. Can be ‘validation’, ‘crossValidation’ or ‘holdout’.\nroc_points (list of dict) – List of precalculated metrics associated with thresholds for ROC curve.\nnegative_class_predictions (list of float) – List of predictions from example for negative class\npositive_class_predictions (list of float) – List of predictions from example for positive class\nsource_model_id (str) – ID of the model this ROC curve represents; in some cases,\ninsights from the parent of a frozen model may be used\nlabel (str) – Label name for\nkolmogorov_smirnov_metric (float) – Kolmogorov-Smirnov metric value for label\nauc (float) – AUC metric value for label\nWord Cloud\nclass datarobot.models.word_cloud.WordCloud\nWord cloud data for the model.\nNotes\nWordCloudNgram is a dict containing the following:\nngram (str) Word or ngram value.\ncoefficient (float) Value from [-1.0, 1.0] range, describes effect of this ngram on           the target. Large negative value means strong effect toward negative class in           classification and smaller target value in regression models. Large positive - toward           positive class and bigger value respectively.\ncount (int) Number of rows in the training sample where this ngram appears.\nfrequency (float) Value from (0.0, 1.0] range, relative frequency of given ngram to           most frequent ngram.\nis_stopword (bool) True for ngrams that DataRobot evaluates as stopwords.\nclass (str or None) For classification - values of the target class for\ncorresponding word or ngram. For regression - None.\nVariables:\nngrams (list of dict) – List of dicts with schema described as WordCloudNgram above.\nmost_frequent(top_n=5)\nReturn most frequent ngrams in the word cloud.\nParameters:\ntop_n (int) – Number of ngrams to return\nReturns:\nUp to top_n top most frequent ngrams in the word cloud.\nIf top_n bigger then total number of ngrams in word cloud - return all sorted by\nfrequency in descending order.\nReturn type:\nlist of dict\nmost_important(top_n=5)\nReturn most important ngrams in the word cloud.\nParameters:\ntop_n (int) – Number of ngrams to return\nReturns:\nUp to top_n top most important ngrams in the word cloud.\nIf top_n bigger then total number of ngrams in word cloud - return all sorted by\nabsolute coefficient value in descending order.\nReturn type:\nlist of dict\nngrams_per_class()\nSplit ngrams per target class values. Useful for multiclass models.\nReturns:\nDictionary in the format of (class label) -> (list of ngrams for that class)\nReturn type:\ndict\nclass datarobot.models.word_cloud.WordCloudNgram",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/insights.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/insights.html",
        "content_length": 99939
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.model.get_feature_impact",
        "datarobot.models.shapmatrix",
        "datarobot.insights.shapdistributions",
        "datarobot.models.featureeffectmetadata",
        "datarobot.models.anomalyassessmentrecordmetadata",
        "datarobot.enums.datetime_trend_plots_resolution",
        "model.request_predictions",
        "datarobot.models.featureeffects",
        "datarobot.models.lift_chart",
        "datarobot.enums.datetime_trend_plots_status",
        "datarobot.models.prediction_explanations",
        "datarobot.models.confusion_chart",
        "datarobot.models.regionexplanationsdata",
        "dr.predictionexplanations.get",
        "datarobot.model.request_predictions",
        "datarobot.models.featureassociationmatrix",
        "datarobot.models.featureassociationfeaturelists",
        "project.upload_dataset",
        "datarobot.models.featureeffectmetadatadatetimeperbacktest",
        "dr.enums.feature_type",
        "datarobot.models.roc_curve",
        "datarobot.insights.shappreview",
        "dr.payoffmatrix.create",
        "dr.shapmatrix.create",
        "dr.featureassociationmatrix.get",
        "datarobot.models.word_cloud",
        "datarobot.insights.shapimpact",
        "datarobot.errors.servererror",
        "datarobot.models.roccurveestimatedmetric",
        "datarobot.models.shapleyfeaturecontribution",
        "datarobot.models.datetime_trend_plots",
        "datarobot.models.classlistmode",
        "dr.shapmatrix.list",
        "datarobot.models.anomaly_assessment",
        "dr.dataslice.list",
        "datarobot.insights.shapmatrix",
        "datarobot.models.data_slice",
        "model.get",
        "datarobot.predictionexplanationsinitialization.create",
        "datarobot.models.anomalyassessmentdatapoint",
        "datarobot.enums.anomalyassessmentstatus",
        "dr.dataslice.create",
        "model.get_feature_impact",
        "datarobot.models.anomalyassessmentpreviewbin",
        "datarobot.models.payoffmatrix",
        "datarobot.models.featureeffectmetadatadatetime",
        "datarobot.models.external_dataset_scores_insights",
        "dr.dataslice.get",
        "dr.model.get",
        "dr.payoffmatrix.list",
        "datarobot.models.ratingtable",
        "datarobot.models.featureassociationmatrixdetails",
        "datarobot.models.toppredictionsmode",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 0.95,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_3344011203136962100",
      "title": "Notebooks",
      "content": "Notebooks\nclass datarobot.models.notebooks.enums.NotebookType\nTypes of notebooks.\nclass datarobot.models.notebooks.enums.RunType\nTypes of notebook job runs.\nclass datarobot.models.notebooks.enums.ManualRunType\nA subset of RunType\nTo be used in API schemas.\nclass datarobot.models.notebooks.enums.SessionType\nTypes of notebook sessions. Triggered sessions include notebook job runs whether manually triggered or scheduled.\nclass datarobot.models.notebooks.enums.ScheduleStatus\nPossible statuses for notebook schedules.\nclass datarobot.models.notebooks.enums.ScheduledRunStatus\nPossible statuses for scheduled notebook runs.\nclass datarobot.models.notebooks.enums.NotebookPermissions\nPermissions for notebooks.\nclass datarobot.models.notebooks.enums.NotebookStatus\nPossible statuses for notebook sessions.\nclass datarobot.models.notebooks.enums.KernelExecutionStatus\nPossible statuses for kernel execution.\nclass datarobot.models.notebooks.enums.CellType\nTypes of cells in a notebook.\nclass datarobot.models.notebooks.enums.RuntimeLanguage\nLanguages as used in notebook jupyter kernels.\nclass datarobot.models.notebooks.enums.ImageLanguage\nLanguages as used and supported in notebook images.\nclass datarobot.models.notebooks.enums.KernelSpec\nKernel specifications for Jupyter notebook kernels.\nclass datarobot.models.notebooks.enums.KernelState\nPossible states for notebook kernels.\nexception datarobot.models.notebooks.exceptions.KernelNotAssignedError\nRaised when a Codespace notebook does not have a kernel assigned.\nclass datarobot.models.notebooks.notebook.ManualRunPayload\nclass datarobot.models.notebooks.notebook.Notebook\nMetadata for a DataRobot Notebook accessible to the user.\nVariables:\nid (str) – The ID of the Notebook.\nname (str) – The name of the Notebook.\ntype (NotebookType) – The type of the Notebook. Can be “plain” or “codespace”.\npermissions (List[NotebookPermissions]) – The permissions the user has for the Notebook.\ntags (List[str]) – Any tags that have been added to the Notebook. Default is an empty list.\ncreated (NotebookActivity) – Information on when the Notebook was created and who created it.\nupdated (NotebookActivity) – Information on when the Notebook was updated and who updated it.\nlast_viewed (NotebookActivity) – Information on when the Notebook was last viewed and who viewed it.\nsettings (NotebookSettings) – Information on global settings applied to the Notebook.\norg_id (Optional[str]) – The organization ID associated with the Notebook.\ntenant_id (Optional[str]) – The tenant ID associated with the Notebook.\ndescription (Optional[str]) – The description of the Notebook. Optional.\nsession (Optional[NotebookSession]) – Metadata on the current status of the Notebook and its kernel. Optional.\nuse_case_id (Optional[str]) – The ID of the Use Case the Notebook is associated with. Optional.\nuse_case_name (Optional[str]) – The name of the Use Case the Notebook is associated with. Optional.\nhas_schedule (bool) – Whether or not the notebook has a schedule.\nhas_enabled_schedule (bool) – Whether or not the notebook has a currently enabled schedule.\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this Notebook in its Use Case or standalone.\nReturn type:\nstr\nclassmethod get(notebook_id)\nRetrieve a single notebook.\nParameters:\nnotebook_id (str) – The ID of the notebook you want to retrieve.\nReturns:\nnotebook – The requested notebook.\nReturn type:\nNotebook\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\ncreate_revision(name=None, notebook_path=None, is_auto=False)\nCreate a new revision for the notebook.\nParameters:\nname (Optional[str]) – The name of the revision. Optional.\nnotebook_path (Optional[str]) – The path of the notebook to execute within the codespace. Required if notebook is in a codespace.\nis_auto (bool) – Indicates whether the revision was auto-saved versus a user interaction. Default is False.\nReturns:\nnotebook_revision – Information about the created notebook revision.\nReturn type:\nNotebookRevision\ndownload_revision(revision_id, file_path=None, filelike=None)\nDownloads the notebook as a JSON (.ipynb) file for the specified revision.\nParameters:\nfile_path (string, optional) – The destination to write the file to.\nfilelike (file, optional) – A file-like object to write to.  The object must be able to write bytes. The user is\nresponsible for closing the object.\nReturn type:\nNone\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\nmanual_run = notebook.run_as_job()\nrevision_id = manual_run.wait_for_completion()\nnotebook.download_revision(revision_id=revision_id, file_path=\"./results.ipynb\")\ndelete()\nDelete a single notebook\n:rtype: None\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\nnotebook.delete()\nclassmethod list(created_before=None, created_after=None, order_by=None, tags=None, owners=None, query=None, use_cases=None)\nList all Notebooks available to the user.\nParameters:\ncreated_before (Optional[str]) – List Notebooks created before a certain date. Optional.\ncreated_after (Optional[str]) – List Notebooks created after a certain date. Optional.\norder_by (Optional[str]) – Property to sort returned Notebooks. Optional.\nSupported properties are “name”, “created”, “updated”, “tags”, and “lastViewed”.\nPrefix the attribute name with a dash to sort in descending order,\ne.g. order_by=’-created’.\nBy default, the order_by parameter is None.\ntags (Optional[List[str]]) – A list of tags that returned Notebooks should be associated with. Optional.\nowners (Optional[List[str]]) – A list of user IDs used to filter returned Notebooks.\nThe respective users share ownership of the Notebooks. Optional.\nquery (Optional[str]) – A specific regex query to use when filtering Notebooks. Optional.\nuse_cases (Optional[UseCase or List[UseCase] or str or List[str]]) – Filters returned Notebooks by a specific Use Case or Cases. Accepts either the entity or the ID. Optional.\nIf set to [None], the method filters the notebook’s datasets by those not linked to a UseCase.\nReturns:\nnotebooks – A list of Notebooks available to the user.\nReturn type:\nList[Notebook]\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebooks = Notebook.list()\nis_running()\nCheck if the notebook session is currently running.\nReturn type:\nbool\nget_session_status()\nGet the status of the notebook session.\nReturn type:\nNotebookStatus\nstart_session(is_triggered_run=False, parameters=None, open_file_paths=None, clone_repository=None)\nStart a new session for the notebook.\nParameters:\nis_triggered_run (bool) – Whether the session being started is considered an “interactive” or “triggered” session. Default is False.\nparameters (Optional[List[StartSessionParameters]]) – A list of dictionaries in the format {“name”: “FOO”, “value”: “my_value”}  representing environment\nvariables propagated to the notebook session.\nopen_file_paths (Optional[List[str]]) – A list of file paths to open upon instantiation of the notebook session.\nclone_repository (Optional[CloneRepositorySchema]) – Information used to clone a remote repository as part of the environment setup flow.\nReturns:\nnotebook_session – The created notebook session.\nReturn type:\nNotebookSession\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\nsession = notebook.start_session()\nstop_session()\nStop the current session for the notebook.\nReturns:\nnotebook_session – The stopped notebook session.\nReturn type:\nNotebookSession\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\nsession = notebook.stop_session()\nexecute(notebook_path=None, cell_ids=None)\nExecute the notebook. Assumes session is already started.\nParameters:\nnotebook_path (Optional[str]) – The path of the notebook to execute within the Codespace. Required if the notebook is in a Codespace.\ncell_ids (Optional[List[str]]) – The list of cell IDs to execute for a notebook. Not supported if the notebook is in a Codespace. Optional.\nIf not provided, the whole notebook will be executed.\nReturn type:\nNone\nget_execution_status()\nGet the execution status information of the notebook.\nReturns:\nexecution_status – The notebook execution status information.\nReturn type:\nNotebookExecutionStatus\nis_finished_executing(notebook_path=None)\nCheck if the notebook is finished executing.\nParameters:\nnotebook_path (Optional[str]) – The path of the notebook the Codespace. Required only if the notebook is in a Codespace.\nWill raise an error if working with a standalone notebook.\nReturns:\nis_finished_executing – Whether or not the notebook has finished executing.\nReturn type:\nbool\nRaises:\nInvalidUsageError – If attempting to check if a standalone notebook has finished executing and incorrectly\npassing a notebook path.\nIf attempting to check if a codespace notebook has finished executing without passing a\nnotebook path.\nKernelNotAssignedError – If attempting to check if a codespace notebook has finished executing but the notebook\ndoes not have a kernel assigned.\nrun_as_job(title=None, notebook_path=None, parameters=None, manual_run_type=ManualRunType.MANUAL)\nCreate a manual scheduled job that runs the notebook.\nNotes\nThe notebook must be part of a Use Case.\nIf the notebook is in a Codespace then notebook_path is required.\nParameters:\ntitle (Optional[str]) – The title of the background job. Optional.\nnotebook_path (Optional[str]) – The path of the notebook to execute within the Codespace. Required if notebook is in a Codespace.\nparameters (Optional[List[StartSessionParameters]]) – A list of dictionaries in the format {“name”: “FOO”, “value”: “my_value”}  representing environment\nvariables predefined in the notebook session. Optional.\nmanual_run_type (Optional[ManualRunType]) – The type of manual run being triggered. Defaults to “manual” as opposed to “pipeline”.\nReturns:\nnotebook_scheduled_job – The created notebook schedule job.\nReturn type:\nNotebookScheduledJob\nRaises:\nInvalidUsageError – If attempting to create a manual scheduled run for a Codespace without a notebook path.\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\nmanual_run = notebook.run_as_job()\n# Alternatively, with title and parameters:\n# manual_run = notebook.run_as_job(title=\"My Run\", parameters=[{\"name\": \"FOO\", \"value\": \"bar\"}])\nrevision_id = manual_run.wait_for_completion()\nlist_schedules(enabled_only=False)\nList all NotebookScheduledJobs associated with the notebook.\nParameters:\nenabled_only (bool) – Whether or not to return only enabled schedules.\nReturns:\nnotebook_schedules – A list of schedules for the notebook.\nReturn type:\nList[NotebookScheduledJob]\nRaises:\nInvalidUsageError – If attempting to list schedules for a notebook not associated with a Use Case.\nExamples\nfrom datarobot.models.notebooks import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\nenabled_schedules = notebook.list_schedules(enabled_only=True)\nclass datarobot.models.notebooks.execution_environment.ExecutionEnvironmentAssignPayload\nPayload for assigning an execution environment to a notebook.\nclass datarobot.models.notebooks.execution_environment.Image\nExecution environment image information.\nVariables:\nid (str) – The ID of the image.\nname (str) – The name of the image.\ndefault (bool) – Whether the image is the default image.\ndescription (str) – The description of the image.\nenvironment_id (str) – The ID of the environment.\ngpu_optimized (bool) – Whether the image is GPU optimized.\nlanguage (ImageLanguage) – The runtime language of the image. For example “Python” or “R”\nlanguage_version (str) – The version of the language. For example “3.11” or “4.3”\nlibraries (list[str]) – A list of pre-installed libraries on the image.\nclass datarobot.models.notebooks.execution_environment.Machine\nExecution environment machine information.\nVariables:\nid (str) – The ID of the machine.\nname (str) – The name of the machine. Values include “XS”, “S”, “M”, “L” etc.\ndefault (bool) – Whether the machine is the default machine.\ncpu (str) – The CPU of the machine. For example a value like “2000m”.\ncpu_cores (int) – The number of CPU cores.\nephemeral_storage (str) – The ephemeral storage of the machine. For example a value like “15Gi”.\nhas_gpu (bool) – Whether the machine has a GPU.\nmemory (str) – The memory of the machine. For example a value like “8Gi”.\nram_gb (int) – The amount of RAM of the machine.\nclass datarobot.models.notebooks.execution_environment.ExecutionEnvironment\nAn execution environment associated with a notebook.\nVariables:\nimage (Image) – The image associated with the execution environment.\nmachine (Machine) – The machine associated with the execution environment.\ntime_to_live (int) – The inactivity timeout for notebook session.\nclassmethod get(notebook_id)\nGet a notebook execution environment by its notebook ID.\nParameters:\nnotebook_id (str) – The ID of the notebook.\nReturns:\nThe notebook execution environment.\nReturn type:\nExecutionEnvironment\nclassmethod assign_environment(notebook_id, payload)\nAssign execution environment values to a notebook.\nParameters:\nnotebook_id (str) – The ID of the notebook.\npayload (ExecutionEnvironmentAssignPayload) – The payload for the assignment/update.\nReturns:\nThe assigned execution environment.\nReturn type:\nExecutionEnvironment\nExamples\nfrom datarobot.models.notebooks import ExecutionEnvironment, ExecutionEnvironmentAssignPayload\npayload = ExecutionEnvironmentAssignPayload(machine_slug='medium', time_to_live=10)\nexec_env = ExecutionEnvironment.assign_environment('67914bfab0279fd832dc3fd1', payload)\nclass datarobot.models.notebooks.kernel.NotebookKernel\nA kernel associated with a codespace notebook.\nVariables:\nid (str) – The kernel ID.\nname (str) – The kernel name.\nlanguage (RuntimeLanguage) – The kernel language. Supports Python and R.\nrunning (bool) – Whether the kernel is running.\nexecution_state (KernelState) – The kernel execution state.\nclass datarobot.models.notebooks.revision.CreateRevisionPayload\nPayload for creating a notebook revision.\nclass datarobot.models.notebooks.revision.NotebookRevision\nRepresents a notebook revision.\nVariables:\nrevision_id (str) – The ID of the notebook revision.\nnotebook_id (str) – The ID of the notebook.\nis_auto (bool) – Whether the revision was auto-saved.\nclassmethod create(notebook_id, payload=None)\nCreate a new notebook revision.\nParameters:\nnotebook_id (str) – The ID of the notebook.\npayload (CreateRevisionPayload) – The payload to create the revision.\nReturns:\nInformation about the created notebook revision.\nReturn type:\nNotebookRevision\nclass datarobot.models.notebooks.scheduled_job.NotebookScheduledJob\nDataRobot Notebook Schedule. A scheduled job that runs a notebook.\nVariables:\nid (str) – The ID of the scheduled notebook job.\nenabled (bool) – Whether job is enabled or not.\nrun_type (RunType) – The type of the run - either manual (triggered via UI or API) or scheduled.\nnotebook_type (NotebookType) – The type of the notebook - either plain or codespace.\njob_payload (ScheduledJobPayload) – The payload used for the background job.\nnext_run_time (Optional[str]) – The next time the job is scheduled to run (assuming it is enabled).\ntitle (Optional[str]) – The title of the job. Optional.\nschedule (Optional[str]) – Cron-like string to define how frequently job should be run. Optional.\nschedule_localized (Optional[str]) – A human-readable localized version of the schedule. Example in English is ‘At 42 minutes past the hour’.\nOptional.\nlast_successful_run (Optional[str]) – The last time the job was run successfully. Optional.\nlast_failed_run (Optional[str]) – The last time the job failed. Optional.\nlast_run_time (Optional[str]) – The last time the job was run (failed or successful). Optional.\nclassmethod get(use_case_id, scheduled_job_id)\nRetrieve a single notebook schedule.\nParameters:\nscheduled_job_id (str) – The ID of the notebook schedule you want to retrieve.\nReturns:\nnotebook_schedule – The requested notebook schedule.\nReturn type:\nNotebookScheduledJob\nExamples\nfrom datarobot.models.notebooks import NotebookScheduledJob\nnotebook_schedule = NotebookScheduledJob.get(\nuse_case_id=\"654ad653c6c1e889e8eab12e\",\nscheduled_job_id=\"65734fe637157200e28bf688\",\n)\nclassmethod list(notebook_ids=None, statuses=None)\nList all NotebookScheduledJobs available to the user.\nParameters:\nnotebook_ids (Optional[List[str]]) – Notebook IDs to filter listed schedules by. Optional.\nstatuses (Optional[List[ScheduleStatus]]) – Statuses to filter listed schedules by. Includes values “disabled” and “enabled”. Optional.\nReturns:\nnotebook_schedules – A list of NotebookScheduledJobs available to the user.\nReturn type:\nList[NotebookScheduledJob]\ncancel()\nCancel a running notebook schedule.\nReturn type:\nNone\nget_most_recent_run()\nRetrieve the most recent run for the notebook schedule.\nReturns:\nnotebook_scheduled_run – The most recent run for the notebook schedule, or None if no runs have been made.\nReturn type:\nOptional[NotebookScheduledRun]\nExamples\nfrom datarobot.models.notebooks import NotebookScheduledJob\nnotebook_schedule = NotebookScheduledJob.get(\nuse_case_id=\"654ad653c6c1e889e8eab12e\",\nscheduled_job_id=\"65734fe637157200e28bf688\",\n)\nmost_recent_run = notebook_schedule.get_most_recent_run()\nget_job_history()\nRetrieve list of historical runs for the notebook schedule. Gets the most recent runs first.\nReturns:\nnotebook_scheduled_runs – The list of historical runs for the notebook schedule.\nReturn type:\nList[NotebookScheduledRun]\nExamples\nfrom datarobot.models.notebooks import NotebookScheduledJob\nnotebook_schedule = NotebookScheduledJob.get(\nuse_case_id=\"654ad653c6c1e889e8eab12e\",\nscheduled_job_id=\"65734fe637157200e28bf688\",\n)\nnotebook_scheduled_runs = notebook_schedule.get_job_history()\nwait_for_completion(max_wait=600)\nWait for the completion of a scheduled notebook and return the revision ID corresponding to the run’s output.\nParameters:\nmax_wait (int) – The number of seconds to wait before giving up.\nReturns:\nrevision_id – Returns either revision ID or message describing current state.\nReturn type:\nstr\nExamples\nfrom datarobot.models.notebooks.notebook import Notebook\nnotebook = Notebook.get(notebook_id='6556b00dcc4ea0bb7ea48121')\nmanual_run = notebook.run_as_job()\nrevision_id = manual_run.wait_for_completion()\nclass datarobot.models.notebooks.scheduled_run.ScheduledJobParam\nDataRobot Schedule Job Parameter.\nVariables:\nname (str) – The name of the parameter.\nvalue (str) – The value of the parameter.\nclass datarobot.models.notebooks.scheduled_run.ScheduledJobPayload\nDataRobot Schedule Job Payload.\nVariables:\nuid (str) – The ID of the user who created the notebook schedule.\norg_id (str) – The ID of the user’s organization who created the notebook schedule.\nuse_case_id (str) – The ID of the Use Case that the notebook belongs to.\nnotebook_id (str) – The ID of the notebook being run on a schedule.\nnotebook_name (str) – The name of the notebook being run on a schedule.\nrun_type (RunType) – The type of the run - either manual (triggered via UI or API) or scheduled.\nnotebook_type (NotebookType) – The type of the notebook - either plain or codespace.\nparameters (List[ScheduledJobParam]) – The parameters being used in the notebook schedule. Can be an empty list.\nnotebook_path (Optional[str]) – The path of the notebook to execute within the codespace. Optional. Required if notebook is in a codespace.\nuse_case_name (Optional[str]) – The name of the Use Case that the notebook belongs to.\nclass datarobot.models.notebooks.scheduled_run.ScheduledRunRevisionMetadata\nDataRobot Notebook Revision Metadata specifically for a scheduled run.\nBoth id and name can be null if for example the job is still running or has failed.\nVariables:\nid (Optional[str]) – The ID of the Notebook Revision. Optional.\nname (Optional[str]) – The name of the Notebook Revision. Optional.\nclass datarobot.models.notebooks.scheduled_run.NotebookScheduledRun\nDataRobot Notebook Scheduled Run. A historical run of a notebook schedule.\nVariables:\nid (str) – The ID of the scheduled notebook job.\nuse_case_id (str) – The Use Case ID of the scheduled notebook job.\nstatus (str) – The status of the run.\npayload (ScheduledJobPayload) – The payload used for the background job.\ntitle (Optional[str]) – The title of the job. Optional.\nstart_time (Optional[str]) – The start time of the job. Optional.\nend_time (Optional[str]) – The end time of the job. Optional.\nrevision (ScheduledRunRevisionMetadata) – Notebook revision data - ID and name.\nduration (Optional[int]) – The job duration in seconds. May be None for example while the job is running. Optional.\nrun_type (Optional[RunType]) – The type of the run - either manual (triggered via UI or API) or scheduled. Optional.\nnotebook_type (Optional[NotebookType]) – The type of the notebook - either plain or codespace. Optional.\nclass datarobot.models.notebooks.session.CloneRepositorySchema\nSchema for cloning a repository when starting a notebook session.\nclass datarobot.models.notebooks.session.StartSessionParameters\nParameters used as environment variables in a notebook session.\nclass datarobot.models.notebooks.session.StartSessionPayload\nPayload for starting a notebook session.\nclass datarobot.models.notebooks.session.NotebookExecutionStatus\nNotebook execution status information.\nVariables:\nstatus (str) – The status of the notebook execution.\ncell_id (Optional[bson.ObjectId]) – The ID of the cell being executed. Optional.\nqueued_cell_ids (Optional[List[bson.ObjectId]]) – The list of cell IDs that are queued for execution. Optional.\nclass datarobot.models.notebooks.session.CodespaceNotebookCell\nRepresents a cell in a codespace notebook.\nclass datarobot.models.notebooks.session.CodespaceNotebookState\nNotebook state information for a codespace notebook.\nVariables:\nname (str) – The name of the notebook.\npath (str) – The path of the notebook.\ngeneration (int) – The generation of the notebook.\nnbformat (int) – The notebook format version.\nnbformat_minor (int) – The notebook format minor version.\nmetadata (dict) – The metadata of the notebook.\ncells (List[CodespaceNotebookCell]) – The list of cells in the notebook.\nkernel_id (Optional[str]) – The ID of the kernel. Optional.\nclass datarobot.models.notebooks.session.NotebookSession\nNotebook session information.\nVariables:\nstatus (NotebookStatus) – The current status of the notebook kernel.\nnotebook_id (str) – The ID of the notebook.\nsession_id (str) – The ID of the session. Incorporates the notebook_id as part of this ID.\nstarted_at (Optional[str]) – The date and time when the notebook was started. Optional.\nsession_type (Optional[SessionType]) – The type of the run - either manual (triggered via UI or API) or scheduled. Optional.\nephemeral_session_key (Optional[str]) – The ID specific to ephemeral session if being used. Optional.\nclassmethod get(notebook_id)\nGet a notebook session by its notebook ID.\nParameters:\nnotebook_id (str) – The ID of the notebook.\nReturns:\nThe notebook session information.\nReturn type:\nNotebookSession\nclassmethod start(notebook_id, payload)\nStart a notebook session.\nParameters:\nnotebook_id (str) – The ID of the notebook.\npayload (StartSessionPayload) – The payload to start the session.\nReturns:\nThe notebook session information.\nReturn type:\nNotebookSession\nclassmethod stop(notebook_id)\nStop a notebook session.\nParameters:\nnotebook_id (str) – The ID of the notebook.\nReturns:\nThe notebook session information.\nReturn type:\nNotebookSession\nclassmethod execute_notebook(notebook_id, cell_ids=None)\nExecute a notebook.\nParameters:\nnotebook_id (str) – The ID of the notebook.\ncell_ids (Optional[List[bson.ObjectId]]) – The list of cell IDs to execute. Optional. If not provided, the whole notebook will be executed.\nReturn type:\nNone\nclassmethod execute_codespace_notebook(notebook_id, notebook_path, generation, cells)\nExecute a notebook.\nParameters:\nnotebook_id (str) – The ID of the notebook.\nnotebook_path (str) – The path of the notebook.\ngeneration (int) – The generation of the notebook.\ncells (List[CodespaceNotebookCell]) – The list of cells to execute.\nReturn type:\nNone\nclassmethod get_execution_status(notebook_id)\nGet the execution status information of a notebook.\nParameters:\nnotebook_id (str) – The ID of the notebook.\nReturns:\nThe execution status information of the notebook.\nReturn type:\nNotebookExecutionStatus\nclass datarobot.models.notebooks.settings.NotebookSettings\nSettings for a DataRobot Notebook.\nVariables:\nshow_line_numbers (bool) – Whether line numbers in cells should be displayed.\nhide_cell_titles (bool) – Whether cell titles should be displayed.\nhide_cell_outputs (bool) – Whether the cell outputs should be displayed.\nshow_scrollers (bool) – Whether scroll bars should be shown on cells.\nhide_cell_footers (bool) – Whether footers should be shown on cells.\nhighlight_whitespace (bool) – Whether whitespace should be highlighted or not.\nclass datarobot.models.notebooks.user.NotebookUser\nA user associated with a Notebook.\nVariables:\nid (str) – The ID of the user.\nactivated (bool) – Whether or not the user is enabled.\nusername (str) – The username of the user, usually their email address.\nfirst_name (str) – The first name of the user.\nlast_name (str) – The last name of the user.\ngravatar_hash (Optional[str]) – The gravatar hash of the user. Optional.\ntenant_phase (Optional[str]) – The phase that the user’s tenant is in. Optional.\nclass datarobot.models.notebooks.user.NotebookActivity\nA record of activity (i.e. last run, updated, etc.) in a Notebook.\nVariables:\nat (str) – The time of the activity in the notebook.\nby (NotebookUser) – The user who performed the activity.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/notebooks.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/notebooks.html",
        "content_length": 25926
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.notebooks"
      ],
      "complexity_score": 0.4,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_4825973087977578137",
      "title": "Credentials",
      "content": "Credentials\nYou can store credentials for use with databases and data connections.\nTo interact with credentials API, use the Credential class.\nList credentials\nTo retrieve the list of all credentials accessible to you, use\nCredential.list.\nimport datarobot as dr\ncredentials = dr.Credential.list()\nEach credential object contains the credential_id string field which\ncan be used in, for example, Batch predictions.\nBasic credentials\nUse the code below to store generic username and password credentials:\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_basic(\n...     name='my_db_cred',\n...     user='<user>',\n...     password='<password>',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e0f', 'my_db_cred', 'basic'),\n# Store cred.credential_id\n>>> cred = dr.Credential.get(credential_id)\n>>> cred.credential_id\n'5e429d6ecf8a5f36c5693e0f'\nStored credentials can be used in Batch predictions for JDBC intake or output.\nS3 credentials\nYou can store AWS credentials either using the following three parameters:\naws_access_key_id\naws_secret_access_key\naws_session_token\nor by using the ID of the saved shared secure configuration:\nconfig_id\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_s3(\n...     name='my_s3_cred',\n...     aws_access_key_id='<aws access key id>',\n...     aws_secret_access_key='<aws secret access key>',\n...     aws_session_token='<aws session token>',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e03', 'my_s3_cred', 's3'),\n# Using config_id\n>>> cred = dr.Credential.dr.Credential.create_s3(\n...     name='my_s3_cred_with_config_id',\n...     config_id='<id_of_shared_secure_configuration>',\n... )\n>>> cred\nCredential('65ef55ef4cec97f0f733835c', 'my_s3_cred_with_config_id', 's3')\n# Store cred.credential_id\n>>> cred = dr.Credential.get(credential_id)\n>>> cred.credential_id\n'5e429d6ecf8a5f36c5693e03'\nStored credential can be used, for example, in Batch predictions for S3 intake or output.\nOAuth credentials\nYou can store OAuth credentials in the data store.\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_oauth(\n...     name='my_oauth_cred',\n...     token='<token>',\n...     refresh_token='<refresh_token>',\n... )\n>>> cred\nCredential('5e429d6ecf8a5f36c5693e0f', 'my_oauth_cred', 'oauth'),\n# Store cred.credential_id\n>>> cred = dr.Credential.get(credential_id)\n>>> cred.credential_id\n'5e429d6ecf8a5f36c5693e0f'\nSnowflake key pair credentials\nYou can store Snowflake key pair credentials in the store. It accepts either of the following parameters:\nprivate_key\npassphrase\nOr you can use the ID of the saved shared secure configuration.\nconfig_id\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_snowflake_key_pair(\n...     name='my_snowflake_key_pair_cred',\n...     user='<user>',\n...     private_key=\"\"\"<private_key>\"\"\",\n...     passphrase='<passphrase>',\n... )\n>>> cred\nCredential('65e9b55e4b0d925c678bb847', 'my_snowflake_key_pair_cred', 'snowflake_key_pair_user_account')\n>>> cred = dr.Credential.create_snowflake_key_pair(\n...     name='my_snowflake_key_pair_cred_with_config_id',\n...     config_id='<id_of_shared_secure_configuration>',\n... )\n>>> cred\nCredential('65e9b9494b0d925c678bb84d', 'my_snowflake_key_pair_cred_with_config_id', 'snowflake_key_pair_user_account')\nDatabricks access token credentials\nYou can store Databricks access token credentials in the data store.\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_databricks_access_token(\n...     name='my_databricks_access_token_cred',\n...     databricks_access_token='<databricks_access_token>',\n... )\n>>> cred\nCredential('65e9bace4b0d925c678bb850', 'my_databricks_access_token_cred', 'databricks_access_token_account')\nDatabricks service principal credentials\nYou can store Databricks service principal credentials in the store. It accepts either of the following parameters:\nclient_id\nclient_secret\nYou can also use the ID of the saved shared secure configuration.\nconfig_id\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_databricks_service_principal(\n...     name='my_databricks_service_principal_cred',\n...     client_id='<client_id>',\n...     client_secret='<client_secret>',\n... )\n>>> cred\nCredential('65e9bb864b0d925c678bb853', 'my_databricks_service_principal_cred', 'databricks_service_principal_account')\n>>> cred = dr.Credential.create_databricks_service_principal(\n...     name='my_databricks_service_principal_cred_with_config_id',\n...     config_id='<id_of_shared_secure_configuration>',\n... )\n>>> cred\nCredential('65e9bcc14b0d925c678bb85e', 'my_databricks_service_principal_cred_with_config_id', 'databricks_service_principal_account')\nAzure Service Principal credentials\nYou can store Azure Service Principal credentials using any of the three following parameters:\nclient_id\nclient_secret\nazure_tenant_id\nYou can also use the ID of the saved shared secure configuration.\nconfig_id\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_azure_service_principal(\n...     name='my_azure_service_principal_cred',\n...     client_id='<client id>',\n...     client_secret='<client secret>',\n...     azure_tenant_id='<azure tenant id>',\n... )\n>>> cred\nCredential('66c920fc4ef80072a8225e56', 'my_azure_service_principal_cred2', 'azure_service_principal')\n# Using config_id\n>>> cred = dr.Credential.dr.Credential.create_azure_service_principal(\n...     name='my_azure_service_principal_cred_with_config_id',\n...     config_id='<id_of_shared_secure_configuration>',\n... )\n>>> cred\nCredential('66c921aa0ff7aea1ce225e2d', 'my_azure_service_principal_cred_with_config_id', 'azure_service_principal')\n# Store cred.credential_id\n>>> cred = dr.Credential.get(credential_id)\n>>> cred.credential_id\n'66c921aa0ff7aea1ce225e2d'\nADLS OAuth credentials\nYou can store ADLS OAuth credentials using any of the three following parameters:\nclient_id\nclient_secret\noauth_scopes\nYou can also use the ID of the saved shared secure configuration.\nconfig_id\n>>> import datarobot as dr\n>>> cred = dr.Credential.create_adls_oauth(\n...     name='my_adls_oauth_cred',\n...     client_id='<client id>',\n...     client_secret='<client secret>',\n...     oauth_scopes=['<oauth scope>'],\n... )\n>>> cred\nCredential('66c9227e3b268d3278225e41', 'my_adls_oauth_cred', 'adls_gen2_oauth')\n# Using config_id\n>>> cred = dr.Credential.dr.Credential.create_adls_oauth(\n...     name='my_adls_oauth_cred_with_config_id',\n...     config_id='<id_of_shared_secure_configuration>',\n... )\n>>> cred\nCredential('66c922b3ae75806f1d126f06', 'my_adls_oauth_cred_with_config_id', 'adls_gen2_oauth')\n# Store cred.credential_id\n>>> cred = dr.Credential.get(credential_id)\n>>> cred.credential_id\n'66c922b3ae75806f1d126f06'\nCredential data\nFor methods that accept credential data instead of a username/password or credential ID:\n{\n\"credentialType\": \"basic\",\n\"user\": \"user123\",\n\"password\": \"pass123\",\n}\n{\n\"credentialType\": \"s3\",\n\"awsAccessKeyId\": \"key123\",\n\"awsSecretAccessKey\": \"secret123\",\n}\n{\n\"credentialType\": \"s3\",\n\"configId\": \"id123\",\n}\n{\n\"credentialType\": \"oauth\",\n\"oauthRefreshToken\": \"token123\",\n\"oauthClientId\": \"client123\",\n\"oauthClientSecret\": \"secret123\",\n}\n{\n\"credentialType\": \"snowflake_key_pair_user_account\",\n\"user\": \"user123\",\n\"privateKey\": \"privatekey123\",\n\"passphrase\": \"passphrase123\",\n}\n{\n\"credentialType\": \"snowflake_key_pair_user_account\",\n\"configId\": \"id123\",\n}\n{\n\"credentialType\": \"databricks_access_token_account\",\n\"databricksAccessToken\": \"token123\",\n}\n{\n\"credentialType\": \"databricks_service_principal_account\",\n\"clientId\": \"client123\",\n\"clientSecret\": \"secret123\",\n}\n{\n\"credentialType\": \"databricks_service_principal_account\",\n\"configId\": \"id123\",\n}\n{\n\"credentialType\": \"azure_service_principal\",\n\"clientId\": \"client123\",\n\"clientSecret\": \"secret123\",\n\"azureTenantId\": \"tenant123\"\n}\n{\n\"credentialType\": \"azure_service_principal\",\n\"configId\": \"id123\",\n}\n{\n\"credentialType\": \"adls_gen2_oauth\",\n\"clientId\": \"client123\",\n\"clientSecret\": \"secret123\",\n\"oauthScopes\": [\"scope123\"]\n}\n{\n\"credentialType\": \"adls_gen2_oauth\",\n\"configId\": \"id123\",\n}",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/credentials.html",
      "tags": [
        "beginner",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/credentials.html",
        "content_length": 7983
      },
      "code_examples": [],
      "api_methods": [
        "dr.credential.list",
        "dr.credential.dr",
        "dr.credential.get",
        "dr.credential.create_azure_service_principal",
        "dr.credential.create_databricks_access_token",
        "dr.credential.create_databricks_service_principal",
        "dr.credential.create_s3",
        "dr.credential.create_basic",
        "dr.credential.create_oauth",
        "dr.credential.create_adls_oauth",
        "dr.credential.create_snowflake_key_pair"
      ],
      "complexity_score": 0.8,
      "use_case_category": "deployment"
    },
    {
      "id": "readthedocs_6385994318683533508",
      "title": "Administration",
      "content": "Administration\nThe administration section provides details for users and administrators about managing credentials and sharing permissions.",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/index.html",
        "content_length": 140
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "general"
    },
    {
      "id": "readthedocs_4399047544326378654",
      "title": "Sharing",
      "content": "Sharing\nOnce you have created entities in DataRobot, you may want to share them with collaborators.\nDataRobot provides an API for sharing the following entities:\nData sources and data stores ( see Database connectivity for more info on connecting to JDBC databases)\nDatasets\nProjects\nCalendar files\nModel deployments (see Deployment sharing for more information on sharing deployments)\nUse Cases (Sharing for Use Cases is slightly different than what’s documented on this page. See Use Case sharing for more information and examples.)\nAccess levels\nEntities can be shared at varying access levels. For example, you can allow someone to\ncreate projects from a data source you have built without allowing them to delete it.\nEach entity type uses slightly different permission names intended to specifically convey what\nkind of actions are available. These roles fall into three categories. These generic role names\ncan be used in the sharing API for any entity.\nFor the complete set of actions granted by each role on a given entity, see the UI documentation for roles and permissions.\nOWNER\nUsed for all entities.\nAllows any action including deletion.\nREAD_WRITE\nKnown as as EDITOR on data sources and data stores.\nAllows modifications to the state, such as renaming and creating data sources from a data store, but not deleting the entity.\nREAD_ONLY\nKnown as CONSUMER on data sources and data stores.\nFor data sources, enables creating projects and predictions; for data stores, only allows you to view them.\nWhen a user’s new role is specified as None, their access will be revoked.\nIn addition to the role, some entities (data sources and data stores) allow\nseparate control over whether a new user should be able to share that entity further. When granting access to a user,\nthe can_share parameter determines whether that user can, in turn, share this entity with another user.\nWhen this parameter is set as false, the user in question has all the access to the entity granted by their\nrole and can remove themselves if desired, but are unable to change the role of any other user.\nExamples\nTransfer access to the data source from mailto:old_user@datarobot.com to mailto:new_user@datarobot.com.\nimport datarobot as dr\nnew_access = dr.SharingAccess(\n\"[email protected]\",\ndr.enums.SHARING_ROLE.OWNER,\ncan_share=True,\n)\naccess_list = [dr.SharingAccess(\"[email protected]\", None), new_access]\ndr.DataSource.get('my-data-source-id').share(access_list)\nTo check access to a project:\nimport datarobot as dr\nproject = dr.Project.create('mydata.csv', project_name='My Data')\naccess_list = project.get_access_list()\naccess_list[0].username\nTo transfer ownership of all projects owned by your account to mailto:new_user@datarobot.com without sending notifications:\nimport datarobot as dr\n# Put path to YAML credentials below\ndr.Client(config_path= '.yaml')\n# Get all projects for your account and store the ids in a list\nprojects = dr.Project.list()\nproject_ids = [project.id for project in projects]\n# List of emails to share with\nshare_targets = ['[email protected]']\n# Target role\ntarget_role = dr.enums.SHARING_ROLE.OWNER\nfor pid in project_ids:\nproject = dr.Project.get(project_id=pid)\nshares = []\nfor user in share_targets:\nshares.append(dr.SharingAccess(username=user, role=target_role))\nproject.share(shares, send_notification=False)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/sharing.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/sharing.html",
        "content_length": 3337
      },
      "code_examples": [],
      "api_methods": [
        "project.get_access_list",
        "project.create",
        "dr.project.list",
        "project.share",
        "project.list",
        "project.id",
        "dr.project.create",
        "dr.enums.sharing_role",
        "dr.project.get",
        "project.get",
        "dr.datasource.get"
      ],
      "complexity_score": 0.8499999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-8865399365475895851",
      "title": "Database connectivity",
      "content": "Database connectivity\nDatabases are a widely used tool for carrying valuable business data. To enable integration\nwith a variety of enterprise databases, DataRobot provides a “self-service” JDBC product\nfor database connectivity setup. Once configured, you can read data from production databases\nfor model building and predictions. This allows you to quickly train and retrain models\non that data, and avoids the unnecessary step of exporting data from your enterprise database\nto a CSV for ingest to DataRobot. With access to more diverse data, you can build more accurate models.\nDatabase connection configuration uses the following terminology:\nData store: A configured connection to a database. It has a name, a specified driver,\nand a JDBC URL. You can register data stores with DataRobot for ease of re-use.\nA data store has one connector but can have many data sources.\nData source: A configured connection to the backing data store (the location of data\nwithin a given endpoint). A data source specifies, via a SQL query or a selected table\nand schema data, which data to extract from the data store to use for modeling or predictions.\nA data source has one data store and one connector but can have many datasets.\nData driver: The software that allows the application to interact with a database;\neach data store is associated with either a driver or a connector (created by the administrator). The\ndriver configuration saves the storage location in the application of the JAR file and any additional\ndependency files associated with the driver.\nConnector: Similarly to data drivers, a connector allows the application to interact\nwith a database; each data store is associated with either a driver or a connector (created by the\nadministrator). The connector configuration saves the storage location in the application of the JAR file and\nany additional dependency files associated with the connector.\nDataset: Data, a file or the content of a data source, at a particular point in time.\nA data source can produce multiple datasets; a dataset has exactly one data source.\nReview the workflow to set up projects or prediction datasets below.\nThe administrator sets up a datarobot.DataDriver for accessing a particular database.\nFor any particular driver, this setup is performed once for the entire system and\nthe resulting driver is used by all users.\nUsers create a datarobot.DataStore which represents an interface\nto a particular database, using that driver.\nUsers create a datarobot.DataSource representing a particular set of data\nto be extracted from the data store.\nUsers create projects and prediction datasets from a data source.\nUsers can also manage their data stores and data sources and administrators can manage drivers by listing, retrieving, updating, and deleting existing instances.\nCreate drivers\nAdministrators should specify the class_name, the name of the Java class in the Java archive\nwhich implements the java.sql.Driver interface; canonical_name, a user-friendly name\nfor resulting driver to display in the API and the GUI; and files, a list of local files which\ncontain the driver.\n>>> import datarobot as dr\n>>> driver = dr.DataDriver.create(\n...     class_name='org.postgresql.Driver',\n...     canonical_name='PostgreSQL',\n...     files=['/tmp/postgresql-42.2.2.jar']\n... )\n>>> driver\nDataDriver('PostgreSQL')\nTo retrieve information about existing drivers, such as the driver ID for data store creation,\nyou can use dr.DataDriver.list().\nCreate data stores\nAfter an administrator has created drivers, any user can use them for DataStore creation.\nA data store represents a JDBC database. When creating them, you should specify the type,\nwhich must be jdbc; canonical_name, a user-friendly name to display\nin the API and GUI for the data store; driver_id, the ID of the driver to use to connect\nto the database; and jdbc_url, the full URL specifying the database connection settings\nsuch as the database type, server address, port, and database name.\nNote that you can only create data stores with drivers when using the Python client.\nDrivers and connectors are not interchangeable for this method. To create a data store with\na connector, instead use the REST API.\n>>> import datarobot as dr\n>>> data_store = dr.DataStore.create(\n...     data_store_type='jdbc',\n...     canonical_name='Demo DB',\n...     driver_id='5a6af02eb15372000117c040',\n...     jdbc_url='jdbc:postgresql://my.db.address.org:5432/perftest'\n... )\n>>> data_store\nDataStore('Demo DB')\n>>> data_store.test(username='username', password='password')\n{'message': 'Connection successful'}\nCreate data sources\nOnce you have a data store, you can can query datasets via the data source.\nWhen creating a data source, first create a\ndatarobot.DataSourceParameters object from a data store’s ID and a query,\nand then create the data source with a type, currently always jdbc; a canonical_name,\nthe user-friendly name to display in the API and GUI, and params, the DataSourceParameters\nobject.\n>>> import datarobot as dr\n>>> params = dr.DataSourceParameters(\n...     data_store_id='5a8ac90b07a57a0001be501e',\n...     query='SELECT * FROM airlines10mb WHERE \"Year\" >= 1995;'\n... )\n>>> data_source = dr.DataSource.create(\n...     data_source_type='jdbc',\n...     canonical_name='airlines stats after 1995',\n...     params=params\n... )\n>>> data_source\nDataSource('airlines stats after 1995')\nCreate projects\nYou can create new projects from a data source, demonstrated below.\n>>> import datarobot as dr\n>>> project = dr.Project.create_from_data_source(\n...     data_source_id='5ae6eee9962d740dd7b86886',\n...     username='username',\n...     password='password'\n... )\nAs of v3.0 of the Python API client, you can alternatively pass in the credential_id of an existing\nDataset.Credential object.\n>>> import datarobot as dr\n>>> project = dr.Project.create_from_data_source(\n...     data_source_id='5ae6eee9962d740dd7b86886',\n...     credential_id='9963d544d5ce3se783r12190'\n... )\nOr, pass in credential_data which conforms to CredentialDataSchema.\n>>> import datarobot as dr\n>>> s3_credential_data = {\"credentialType\": \"s3\", \"awsAccessKeyId\": \"key123\", \"awsSecretAccessKey\": \"secret123\"}\n>>> project = dr.Project.create_from_data_source(\n...     data_source_id='5ae6eee9962d740dd7b86886',\n...     credential_data=s3_credential_data\n... )\nCreate prediction datasets\nGiven a data source, new prediction datasets can be created for any project.\n>>> import datarobot as dr\n>>> project = dr.Project.get('5ae6f296962d740dd7b86887')\n>>> prediction_dataset = project.upload_dataset_from_data_source(\n...     data_source_id='5ae6eee9962d740dd7b86886',\n...     username='username',\n...     password='password'\n... )",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/database_connectivity.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/database_connectivity.html",
        "content_length": 6704
      },
      "code_examples": [],
      "api_methods": [
        "project.upload_dataset_from_data_source",
        "project.create_from_data_source",
        "dr.datasource.create",
        "dr.datastore.create",
        "dr.datadriver.create",
        "dr.project.get",
        "dr.project.create_from_data_source",
        "project.get",
        "dr.datadriver.list"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_5588913512627629802",
      "title": "Datasets",
      "content": "Datasets\nTo create a project and begin modeling, you first need to upload your data and prepare a dataset.\nCreate a dataset\nThere are several ways to create a dataset.\nDataset.upload takes either a path to a\nlocal file, a streamable file object via external URL, or a pandas DataFrame.\n>>> import datarobot as dr\n>>> # Upload a local file\n>>> dataset_one = dr.Dataset.upload(\"./data/examples.csv\")\n>>> # Create a dataset with a URL\n>>> dataset_two = dr.Dataset.upload(\"https://raw.githubusercontent.com/curran/data/gh-pages/dbpedia/cities/data.csv\")\n>>> # Create a dataset using a pandas DataFrame\n>>> dataset_three = dr.Dataset.upload(my_df)\n>>> # Create a dataset using a local file\n>>> with open(\"./data/examples.csv\", \"rb\") as file_pointer:\n...     dataset_four = dr.Dataset.create_from_file(filelike=file_pointer)\nDataset.create_from_file can take either a path to a\nlocal file or any streamable file object.\n>>> import datarobot as dr\n>>> dataset = dr.Dataset.create_from_file(file_path='data_dir/my_data.csv')\n>>> with open('data_dir/my_data.csv', 'rb') as f:\n...     other_dataset = dr.Dataset.create_from_file(filelike=f)\nDataset.create_from_in_memory_data takes\neither a pandas.Dataframe or a list of dictionaries representing rows of data.  Note that the\ndictionaries representing the rows of data must contain the same keys.\n>>> import pandas as pd\n>>> data_frame = pd.read_csv('data_dir/my_data.csv')\n>>> pandas_dataset = dr.Dataset.create_from_in_memory_data(data_frame=data_frame)\n>>> in_memory_data = [{'key1': 'value', 'key2': 'other_value', ...},\n...                   {'key1': 'new_value', 'key2': 'other_new_value', ...}, ...]\n>>> in_memory_dataset = dr.Dataset.create_from_in_memory_data(records=other_data)\nDataset.create_from_url takes CSV data from a URL. If you\nhave set DISABLE_CREATE_SNAPSHOT_DATASOURCE, you must set do_snapshot=False.\n>>> url_dataset = dr.Dataset.create_from_url('https://s3.amazonaws.com/my_data/my_dataset.csv',\n...                                          do_snapshot=False)\nDataset.create_from_data_source takes data\nfrom a data source.\nIf you have set DISABLE_CREATE_SNAPSHOT_DATASOURCE, you must set do_snapshot=False.\n>>> data_source_dataset = dr.Dataset.create_from_data_source(data_source.id, do_snapshot=False)\nor\n>>> data_source_dataset = data_source.create_dataset(do_snapshot=False)\nUse datasets\nAfter creating a dataset, you can create Projects from it and begin training models. You can also combine project creation and a dataset upload in one method using Project.create.\nHowever, using this method means the data is only accessible to the project which created it.\n>>> project = dataset.create_project(project_name='New Project')\n>>> project.analyze_and_model('some target')\nProject(New Project)\nGet information from a dataset\nThe dataset object contains some basic information that you can query, as shown in the snippet below.\n>>> dataset.id\nu'5e31cdac39782d0f65842518'\n>>> dataset.name\nu'my_data.csv'\n>>> dataset.categories\n[\"TRAINING\", \"PREDICTION\"]\n>>> dataset.created_at\ndatetime.datetime(2020, 2, 7, 16, 51, 10, 311000, tzinfo=tzutc())\nThe snippet below outlines several methods available to retrieve details from a dataset.\n# Details\n>>> details = dataset.get_details()\n>>> details.last_modification_date\ndatetime.datetime(2020, 2, 7, 16, 51, 10, 311000, tzinfo=tzutc())\n>>> details.feature_count_by_type\n[FeatureTypeCount(count=1, feature_type=u'Text'),\nFeatureTypeCount(count=1, feature_type=u'Boolean'),\nFeatureTypeCount(count=16, feature_type=u'Numeric'),\nFeatureTypeCount(count=3, feature_type=u'Categorical')]\n>>> details.to_dataset().id == details.dataset_id\nTrue\n# Projects\n>>> dr.Project.create_from_dataset(dataset.id, project_name='Project One')\nProject(Project One)\n>>> dr.Project.create_from_dataset(dataset.id, project_name='Project Two')\nProject(Project Two)\n>>> dataset.get_projects()\n[ProjectLocation(url=u'https://app.datarobot.com/api/v2/projects/5e3c94aff86f2d10692497b5/', id=u'5e3c94aff86f2d10692497b5'),\nProjectLocation(url=u'https://app.datarobot.com/api/v2/projects/5e3c94eb9525d010a9918ec1/', id=u'5e3c94eb9525d010a9918ec1')]\n>>> first_id = dataset.get_projects()[0].id\n>>> dr.Project.get(first_id).project_name\n'Project One'\n# Features\n>>> all_features = dataset.get_all_features()\n>>> feature = next(dataset.iterate_all_features(offset=2, limit=1))\n>>> feature.name == all_features[2].name\nTrue\n>>> print(feature.name, feature.feature_type, feature.dataset_id)\n(u'Partition', u'Numeric', u'5e31cdac39782d0f65842518')\n>>> feature.get_histogram().plot\n[{'count': 3522, 'target': None, 'label': u'0.0'},\n{'count': 3521, 'target': None, 'label': u'1.0'}, ... ]\n# The raw data\n>>> with open('myfile.csv', 'wb') as f:\n...     dataset.get_file(filelike=f)\nRetrieve datasets\nYou can retrieve specific datasets, a list of all datasets, or an iterator that retrieves\nall or some datasets.\n>>> dataset_id = '5e387c501a438646ed7bf0f2'\n>>> dataset = dr.Dataset.get(dataset_id)\n>>> dataset.id == dataset_id\nTrue\n# A blocking call that returns all datasets\n>>> dr.Dataset.list()\n[Dataset(name=u'Untitled Dataset', id=u'5e3c51e0f86f2d1087249728'),\nDataset(name=u'my_data.csv', id=u'5e3c2028162e6a5fe9a0d678'), ...]\n# Avoid listing datasets that fail to properly upload\n>>> dr.Dataset.list(filter_failed=True)\n[Dataset(name=u'my_data.csv', id=u'5e3c2028162e6a5fe9a0d678'),\nDataset(name=u'my_other_data.csv', id=u'3efc2428g62eaa5f39a6dg7a'), ...]\n# An iterator that lazily retrieves from the server page-by-page\n>>> from itertools import islice\n>>> iterator = dr.Dataset.iterate(offset=2)\n>>> for element in islice(iterator, 3):\n...    print(element)\nDataset(name='some_data.csv', id='5e8df2f21a438656e7a23d12')\nDataset(name='other_data.csv', id='5e8df2e31a438656e7a23d0b')\nDataset(name='Untitled Dataset', id='5e6127681a438666cc73c2b0')\nManage datasets\nYou can modify, delete, and restore datasets.  Note that you need the dataset’s ID in order to restore\nit from deletion. If you do not keep track of the ID, you will be unable to restore a dataset. If your deleted dataset was used to create a project, that project can still access it, but you will not be able to create new projects using that dataset.\n>>> dataset.modify(name='A Better Name')\n>>> dataset.name\n'A Better Name'\n>>> new_project = dr.Project.create_from_dataset(dataset.id)\n>>> stored_id = dataset.id\n>>> dr.Dataset.delete(dataset.id)\n# new_project is still ok\n>>> dr.Project.create_from_dataset(stored_id)\nTraceback (most recent call last):\n...\ndatarobot.errors.ClientError: 410 client error: {u'message': u'Requested Dataset 5e31cdac39782d0f65842518 was previously deleted.'}\n>>> dr.Dataset.un_delete(stored_id)\n>>> dr.Project.create_from_dataset(stored_id, project_name='Successful')\nProject(Successful)\nYou can share a dataset as demonstrated in the following code snippet.\n>>> from datarobot.enums import SHARING_ROLE\n>>> from datarobot.models.dataset import Dataset\n>>> from datarobot.models.sharing import SharingAccess\n>>>\n>>> new_access = SharingAccess(\n>>>     \"[email protected]\",\n>>>     SHARING_ROLE.OWNER,\n>>>     can_share=True,\n>>> )\n>>> access_list = [\n>>>     SharingAccess(\"[email protected]\", SHARING_ROLE.OWNER, can_share=True),\n>>>     new_access,\n>>> ]\n>>>\n>>> Dataset.get('my-dataset-id').share(access_list)\nManage dataset feature lists\nYou can create, modify, and delete custom feature lists on a given dataset. Some feature lists are\nautomatically created by DataRobot and cannot be modified or deleted. Note that you cannot\nrestore a deleted feature list.\n>>> dataset.get_featurelists()\n[DatasetFeaturelist(Raw Features),\nDatasetFeaturelist(universe),\nDatasetFeaturelist(Informative Features)]\n>>> dataset_features = [feature.name for feature in dataset.get_all_features()]\n>>> custom_featurelist = dataset.create_featurelist('Custom Features', dataset_features[:5])\n>>> custom_featurelist\nDatasetFeaturelist(Custom Features)\n>>> dataset.get_featurelists()\n[DatasetFeaturelist(Raw Features),\nDatasetFeaturelist(universe),\nDatasetFeaturelist(Informative Features),\nDatasetFeaturelist(Custom Features)]\n>>> custom_featurelist.update('New Name')\n>>> custom_featurelist.name\n'New Name'\n>>> custom_featurelist.delete()\n>>> dataset.get_featurelists()\n[DatasetFeaturelist(Raw Features),\nDatasetFeaturelist(universe),\nDatasetFeaturelist(Informative Features)]\nUse credential data\nFor methods that accept credential data instead of username and password or a credential ID, see the  Credential data section.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/dataset.html",
      "tags": [
        "advanced",
        "api_reference",
        "beginner",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/dataset.html",
        "content_length": 8484
      },
      "code_examples": [],
      "api_methods": [
        "dr.dataset.upload",
        "datarobot.models.sharing",
        "dr.dataset.create_from_file",
        "dr.dataset.un_delete",
        "project.get",
        "dr.dataset.create_from_data_source",
        "project.create",
        "datarobot.models.dataset",
        "dr.dataset.create_from_url",
        "dr.dataset.delete",
        "dr.dataset.create_from_in_memory_data",
        "dr.dataset.get",
        "datarobot.errors.clienterror",
        "dr.dataset.iterate",
        "dr.dataset.list",
        "dr.project.get",
        "project.analyze_and_model",
        "project.create_from_dataset",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-5854469769145174148",
      "title": "Data",
      "content": "Data\nData integrity and quality are cornerstones for creating highly accurate predictive models. These sections describe the tools and visualizations provided to ensure that your project doesn’t suffer the “garbage in, garbage out” outcome.",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/index.html",
        "content_length": 241
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.15,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-5951938635478590258",
      "title": "Feature Discovery",
      "content": "Feature Discovery\nFeature Discovery allows you to generate features automatically\nfrom secondary datasets connected to a primary dataset (training data).\nYou can create this type of connection using relationship configuration.\nRegister a primary dataset to create a project\nTo create a Feature Discovery project, upload the primary (training) dataset:\nProjects\nimport datarobot as dr\nprimary_dataset = dr.Dataset.create_from_file(file_path='your-training_file.csv')\nproject = dr.Project.create_from_dataset(primary_dataset.id, project_name='Lending Club')\nNext, register all the secondary datasets which you want to connect with the primary dataset.\nRegister secondary datasets\nYou can register the dataset using\nDataset.create_from_file, which can take either\na path to a local file or any streamable file object.\nprofile_dataset = dr.Dataset.create_from_file(file_path='your_profile_file.csv')\ntransaction_dataset = dr.Dataset.create_from_file(file_path='your_transaction_file.csv')\nCreate dataset definitions and relationships\nCreate the DatasetDefinition and Relationship for the profile and transaction datasets created above using helper functions.\nprofile_catalog_id = profile_dataset.id\nprofile_catalog_version_id = profile_dataset.version_id\ntransac_catalog_id = transaction_dataset.id\ntransac_catalog_version_id = transaction_dataset.version_id\nprofile_dataset_definition = dr.DatasetDefinition(\nidentifier='profile',\ncatalog_id=profile_catalog_id,\ncatalog_version_id=profile_catalog_version_id\n)\ntransaction_dataset_definition = dr.DatasetDefinition(\nidentifier='transaction',\ncatalog_id=transac_catalog_id,\ncatalog_version_id=transac_catalog_version_id,\nprimary_temporal_key='Date'\n)\nprofile_transaction_relationship = dr.Relationship(\ndataset1_identifier='profile',\ndataset2_identifier='transaction',\ndataset1_keys=['CustomerID'],\ndataset2_keys=['CustomerID']\n)\nprimary_profile_relationship = dr.Relationship(\ndataset2_identifier='profile',\ndataset1_keys=['CustomerID'],\ndataset2_keys=['CustomerID'],\nfeature_derivation_window_start=-14,\nfeature_derivation_window_end=-1,\nfeature_derivation_window_time_unit='DAY',\nprediction_point_rounding=1,\nprediction_point_rounding_time_unit='DAY'\n)\ndataset_definitions = [profile_dataset_definition, transaction_dataset_definition]\nrelationships = [primary_profile_relationship, profile_transaction_relationship]\nCreate a relationship configuration\nCreate a relationship configuration using the dataset definitions and relationships created above.\n# Create the relationships configuration to define connection between the datasets\nrelationship_config = dr.RelationshipsConfiguration.create(dataset_definitions=dataset_definitions, relationships=relationships)\nCreate a Feature Discovery project\nOnce you have configured relationships for your datasets, you can create a Feature Discovery project.\n# Set the datetime partitionining column (`date` in this example)\npartitioning_spec = dr.DatetimePartitioningSpecification('date')\n# As of v3.0, use ``Project.set_datetime_partitioning`` instead of passing the spec to ``Project.analyze_and_model`` via ``partitioning_method``.\nproject.set_datetime_partitioning(datetime_partition_spec=partitioning_spec)\n# Set the target for the project and start Feature discovery (if ``Project.set_datetime_partitioning`` was used there is no need to pass ``partitioning_method``)\nproject.analyze_and_model(target='BadLoan', relationships_configuration_id=relationship_config.id, mode='manual', partitioning_method=partitioning_spec)\nProject(train.csv)\nTo start training a model, reference the ref:modeling <model> documentation.\nCreate secondary dataset configuration for predictions\nCreate configurations for your secondary datasets with Secondary Dataset:\nnew_secondary_dataset_config = dr.SecondaryDatasetConfigurations.create(\nproject_id=project.id,\nname='My config',\nsecondary_datasets=secondary_datasets\n)\nFor more details, reference the Secondary Dataset configuration documentation.\nMake predictions with a trained model\nTo make predictions with a trained model, reference the Predictions documentation.\ndataset_from_path = project.upload_dataset(\n'./data_to_predict.csv',\nsecondary_datasets_config_id=new_secondary_dataset_config.id\n)\npredict_job_1 = model.request_predictions(dataset_from_path.id)\nCommon errors\nDataset registration failed\ndatasetdr.Dataset.create_from_file(file_path='file.csv')\ndatarobot.errors.AsyncProcessUnsuccessfulError: The job did not complete successfully.\nSolutions:\nCheck the internet connectivity. Sometimes network flakiness can cause upload errors.\nCheck the dataset file size. If a file is too large, you should consider uploading the dataset via a URL rather than uploading the file directly.\nRelationship configuration errors\ndatarobot.errors.ClientError: 422 client error: {u'message': u'Invalid field data',\nu'errors': {u'datasetDefinitions': {u'1': {u'identifier': u'value cannot contain characters: $ - \" . { } / \\\\'},\nu'0': {u'identifier': u'value cannot contain characters: $ - \" . { } / \\\\'}}}}\nSolution:\nCheck the identifier name passed in datasets_definitions and relationships.\nTip: Do not use the name of the dataset if you did not specify it when registering the dataset to the AI Catalog.\ndatarobot.errors.ClientError: 422 client error: {u'message': u'Invalid field data',\nu'errors': {u'datasetDefinitions': {u'1': {u'primaryTemporalKey': u'date column doesnt exist'},\n}}}\nSolution:\nCheck if the name of the column passed as primaryTemporalKey is correct, as it is case-sensitive.\nConfigure relationships\nA relationship’s configuration specifies additional datasets to be included to a project, how these datasets are related to each other, and the primary dataset.\nWhen a relationships configuration is specified for a project,\nFeature Discovery will create features automatically from these datasets.\nYou can create a relationship configuration from uploaded AI Catalog items.\nAfter uploading all the secondary datasets in the AI Catalog:\nCreate the dataset’s definition to specify which datasets to be used as secondary datasets along with its details\nConfigure relationships among the above datasets.\nrelationship_config = dr.RelationshipsConfiguration.create(dataset_definitions=dataset_definitions, relationships=relationships)\n>>> relationship_config.id\nu'5506fcd38bd88f5953219da0'\nDataset definitions and relationships using helper functions\nCreate the DatasetDefinition and Relationship for the profile and transaction dataset using helper functions.\nprofile_catalog_id = '5ec4aec1f072bc028e3471ae'\nprofile_catalog_version_id = '5ec4aec2f072bc028e3471b1'\ntransac_catalog_id = '5ec4aec268f0f30289a03901'\ntransac_catalog_version_id = '5ec4aec268f0f30289a03900'\nprofile_dataset_definition = dr.DatasetDefinition(\nidentifier='profile',\ncatalog_id=profile_catalog_id,\ncatalog_version_id=profile_catalog_version_id\n)\ntransaction_dataset_definition = dr.DatasetDefinition(\nidentifier='transaction',\ncatalog_id=transac_catalog_id,\ncatalog_version_id=transac_catalog_version_id,\nprimary_temporal_key='Date'\n)\nprofile_transaction_relationship = dr.Relationship(\ndataset1_identifier='profile',\ndataset2_identifier='transaction',\ndataset1_keys=['CustomerID'],\ndataset2_keys=['CustomerID']\n)\nprimary_profile_relationship = dr.Relationship(\ndataset2_identifier='profile',\ndataset1_keys=['CustomerID'],\ndataset2_keys=['CustomerID'],\nfeature_derivation_window_start=-14,\nfeature_derivation_window_end=-1,\nfeature_derivation_window_time_unit='DAY',\nprediction_point_rounding=1,\nprediction_point_rounding_time_unit='DAY'\n)\ndataset_definitions = [profile_dataset_definition, transaction_dataset_definition]\nrelationships = [primary_profile_relationship, profile_transaction_relationship]\nDataset definition and relationship using a dictionary\nCreate the dataset definitions and relationships for the profile and transaction dataset using dict directly.\nprofile_catalog_id = profile_dataset.id\nprofile_catalog_version_id = profile_dataset.version_id\ntransac_catalog_id = transaction_dataset.id\ntransac_catalog_version_id = transaction_dataset.version_id\ndataset_definitions = [\n{\n'identifier': 'transaction',\n'catalogVersionId': transac_catalog_version_id,\n'catalogId': transac_catalog_id,\n'primaryTemporalKey': 'Date',\n'snapshotPolicy': 'latest',\n},\n{\n'identifier': 'profile',\n'catalogId': profile_catalog_id,\n'catalogVersionId': profile_catalog_version_id,\n'snapshotPolicy': 'latest',\n},\n]\nrelationships = [\n{\n'dataset2Identifier': 'profile',\n'dataset1Keys': ['CustomerID'],\n'dataset2Keys': ['CustomerID'],\n'featureDerivationWindowStart': -14,\n'featureDerivationWindowEnd': -1,\n'featureDerivationWindowTimeUnit': 'DAY',\n'predictionPointRounding': 1,\n'predictionPointRoundingTimeUnit': 'DAY',\n},\n{\n'dataset1Identifier': 'profile',\n'dataset2Identifier': 'transaction',\n'dataset1Keys': ['CustomerID'],\n'dataset2Keys': ['CustomerID'],\n},\n]\nRetrieving relationship configuration\nYou can retrieve a specific relationship’s configuration using the ID of the relationship configuration.\nrelationship_config_id = '5506fcd38bd88f5953219da0'\nrelationship_config = dr.RelationshipsConfiguration(id=relationship_config_id).get()\n>>> relationship_config.id == relationship_config_id\nTrue\n# Get all the datasets used in this relationship's configuration\n>> len(relationship_config.dataset_definitions) == 2\nTrue\n>> relationship_config.dataset_definitions[0]\n{\n'feature_list_id': '5ec4af93603f596525d382d3',\n'snapshot_policy': 'latest',\n'catalog_id': '5ec4aec268f0f30289a03900',\n'catalog_version_id': '5ec4aec268f0f30289a03901',\n'primary_temporal_key': 'Date',\n'is_deleted': False,\n'identifier': 'transaction',\n'feature_lists':\n[\n{\n'name': 'Raw Features',\n'description': 'System created featurelist',\n'created_by': 'User1',\n'creation_date': datetime.datetime(2020, 5, 20, 4, 18, 27, 150000, tzinfo=tzutc()),\n'user_created': False,\n'dataset_id': '5ec4aec268f0f30289a03900',\n'id': '5ec4af93603f596525d382d1',\n'features': [u'CustomerID', u'AccountID', u'Date', u'Amount', u'Description']\n},\n{\n'name': 'universe',\n'description': 'System created featurelist',\n'created_by': 'User1',\n'creation_date': datetime.datetime(2020, 5, 20, 4, 18, 27, 172000, tzinfo=tzutc()),\n'user_created': False,\n'dataset_id': '5ec4aec268f0f30289a03900',\n'id': '5ec4af93603f596525d382d2',\n'features': [u'CustomerID', u'AccountID', u'Date', u'Amount', u'Description']\n},\n{\n'features': [u'CustomerID', u'AccountID', u'Date', u'Amount', u'Description'],\n'description': 'System created featurelist',\n'created_by': u'Garvit Bansal',\n'creation_date': datetime.datetime(2020, 5, 20, 4, 18, 27, 179000, tzinfo=tzutc()),\n'dataset_version_id': '5ec4aec268f0f30289a03901',\n'user_created': False,\n'dataset_id': '5ec4aec268f0f30289a03900',\n'id': u'5ec4af93603f596525d382d3',\n'name': 'Informative Features'\n}\n]\n}\n# Get information regarding how the datasets are connected among themselves as well as  theprimary dataset\n>> relationship_config.relationships\n[\n{\n'dataset2Identifier': 'profile',\n'dataset1Keys': ['CustomerID'],\n'dataset2Keys': ['CustomerID'],\n'featureDerivationWindowStart': -14,\n'featureDerivationWindowEnd': -1,\n'featureDerivationWindowTimeUnit': 'DAY',\n'predictionPointRounding': 1,\n'predictionPointRoundingTimeUnit': 'DAY',\n},\n{\n'dataset1Identifier': 'profile',\n'dataset2Identifier': 'transaction',\n'dataset1Keys': ['CustomerID'],\n'dataset2Keys': ['CustomerID'],\n},\n]\nUpdate details of a relationship configuration\nUse the snippet below as an example of how to update the details of the existing relationship configuration.\nrelationship_config_id = '5506fcd38bd88f5953219da0'\nrelationship_config = dr.RelationshipsConfiguration(id=relationship_config_id)\n# Remove obsolete dataset definitions and its relationships\nnew_datasets_definiton =\n[\n{\n'identifier': 'user',\n'catalogVersionId': '5c88a37770fc42a2fcc62759',\n'catalogId': '5c88a37770fc42a2fcc62759',\n'snapshotPolicy': 'latest',\n},\n]\n# Get information regarding how the datasets are connected among themselves as well as the primary dataset\nnew_relationships =\n[\n{\n'dataset2Identifier': 'user',\n'dataset1Keys': ['user_id', 'dept_id'],\n'dataset2Keys': ['user_id', 'dept_id'],\n},\n]\nnew_config = relationship_config.replace(new_datasets_definiton, new_relationships)\n>>> new_config.id == relationship_config_id\nTrue\n>>> new_config.datasets_definition\n[\n{\n'identifier': 'user',\n'catalogVersionId': '5c88a37770fc42a2fcc62759',\n'catalogId': '5c88a37770fc42a2fcc62759',\n'snapshotPolicy': 'latest',\n},\n]\n>>> new_config.relationships\n[\n{\n'dataset2Identifier': 'user',\n'dataset1Keys': ['user_id', 'dept_id'],\n'dataset2Keys': ['user_id', 'dept_id'],\n},\n]\nDelete relationships configuration\nYou can delete a relationship configuration that is not used by any project.\nrelationship_config_id = '5506fcd38bd88f5953219da0'\nrelationship_config = dr.RelationshipsConfiguration(id=relationship_config_id)\nresult = relationship_config.get()\n>>> result.id == relationship_config_id\nTrue\n# Delete the relationships configuration\n>>> relationship_config.delete()\n>>> relationship_config.get()\nClientError: Relationships Configuration 5506fcd38bd88f5953219da0 not found\n(secondary-dataset-configuration)=\nSecondary dataset configuration\nSecondary dataset configuration allows you to use the different secondary datasets\nfor a Feature Discovery project when making predictions.\nSecondary datasets using helper functions\nCreate the Secondary Dataset using helper functions.\n>>> profile_catalog_id = '5ec4aec1f072bc028e3471ae'\n>>> profile_catalog_version_id = '5ec4aec2f072bc028e3471b1'\n>>> transac_catalog_id = '5ec4aec268f0f30289a03901'\n>>> transac_catalog_version_id = '5ec4aec268f0f30289a03900'\nprofile_secondary_dataset = dr.SecondaryDataset(\nidentifier='profile',\ncatalog_id=profile_catalog_id,\ncatalog_version_id=profile_catalog_version_id,\nsnapshot_policy='latest'\n)\ntransaction_secondary_dataset = dr.SecondaryDataset(\nidentifier='transaction',\ncatalog_id=transac_catalog_id,\ncatalog_version_id=transac_catalog_version_id,\nsnapshot_policy='latest'\n)\nsecondary_datasets = [profile_secondary_dataset, transaction_secondary_dataset]\nCreate secondary datasets with dict\nYou can create secondary datasets using raw dict structure.\nsecondary_datasets = [\n{\n'snapshot_policy': u'latest',\n'identifier': u'profile',\n'catalog_version_id': u'5fd06b4af24c641b68e4d88f',\n'catalog_id': u'5fd06b4af24c641b68e4d88e'\n},\n{\n'snapshot_policy': u'dynamic',\n'identifier': u'transaction',\n'catalog_version_id': u'5fd1e86c589238a4e635e98e',\n'catalog_id': u'5fd1e86c589238a4e635e98d'\n}\n]\nCreate a secondary dataset configuration\nCreate a secondary dataset configuration for a Feature Discovery Project which uses\ntwo secondary datasets: profile and transaction.\nimport datarobot as dr\nproject = dr.Project.get(project_id='54e639a18bd88f08078ca831')\nnew_secondary_dataset_config = dr.SecondaryDatasetConfigurations.create(\nproject_id=project.id,\nname='My config',\nsecondary_datasets=secondary_datasets\n)\n>>> new_secondary_dataset_config.id\n'5fd1e86c589238a4e635e93d'\nRetrieve a secondary dataset configuration\nYou can retrieve specific secondary dataset configurations using the configuration ID.\n>>> config_id = '5fd1e86c589238a4e635e93d'\nsecondary_dataset_config = dr.SecondaryDatasetConfigurations(id=config_id).get()\n>>> secondary_dataset_config.id == config_id\nTrue\n>>> secondary_dataset_config\n{\n'created': datetime.datetime(2020, 12, 9, 6, 16, 22, tzinfo=tzutc()),\n'creator_full_name': u'[email protected]',\n'creator_user_id': u'asdf4af1gf4bdsd2fba1de0a',\n'credential_ids': None,\n'featurelist_id': None,\n'id': u'5fd1e86c589238a4e635e93d',\n'is_default': True,\n'name': u'My config',\n'project_id': u'5fd06afce2456ec1e9d20457',\n'project_version': None,\n'secondary_datasets': [\n{\n'snapshot_policy': u'latest',\n'identifier': u'profile',\n'catalog_version_id': u'5fd06b4af24c641b68e4d88f',\n'catalog_id': u'5fd06b4af24c641b68e4d88e'\n},\n{\n'snapshot_policy': u'dynamic',\n'identifier': u'transaction',\n'catalog_version_id': u'5fd1e86c589238a4e635e98e',\n'catalog_id': u'5fd1e86c589238a4e635e98d'\n}\n]\n}\nList all secondary dataset configurations\nYou can list all secondary dataset configurations created in the project.\n>>> secondary_dataset_configs = dr.SecondaryDatasetConfigurations.list(project.id)\n>>> secondary_dataset_configs[0]\n{\n'created': datetime.datetime(2020, 12, 9, 6, 16, 22, tzinfo=tzutc()),\n'creator_full_name': u'[email protected]',\n'creator_user_id': u'asdf4af1gf4bdsd2fba1de0a',\n'credential_ids': None,\n'featurelist_id': None,\n'id': u'5fd1e86c589238a4e635e93d',\n'is_default': True,\n'name': u'My config',\n'project_id': u'5fd06afce2456ec1e9d20457',\n'project_version': None,\n'secondary_datasets': [\n{\n'snapshot_policy': u'latest',\n'identifier': u'profile',\n'catalog_version_id': u'5fd06b4af24c641b68e4d88f',\n'catalog_id': u'5fd06b4af24c641b68e4d88e'\n},\n{\n'snapshot_policy': u'dynamic',\n'identifier': u'transaction',\n'catalog_version_id': u'5fd1e86c589238a4e635e98e',\n'catalog_id': u'5fd1e86c589238a4e635e98d'\n}\n]\n}",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/feature_discovery.html",
      "tags": [
        "advanced",
        "api_reference",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/feature_discovery.html",
        "content_length": 16952
      },
      "code_examples": [],
      "api_methods": [
        "project.id",
        "datarobot.errors.asyncprocessunsuccessfulerror",
        "project.upload_dataset",
        "dr.project.get",
        "project.get",
        "model.request_predictions",
        "dr.relationshipsconfiguration.create",
        "dr.secondarydatasetconfigurations.create",
        "project.set_datetime_partitioning",
        "project.create_from_dataset",
        "datarobot.errors.clienterror",
        "project.analyze_and_model",
        "dr.secondarydatasetconfigurations.list",
        "dr.project.create_from_dataset",
        "dr.dataset.create_from_file"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-7790153666340864334",
      "title": "Projects",
      "content": "Projects\nProject\nclass datarobot.models.Project\nA project built from a particular training dataset\nVariables:\nid (str) – the id of the project\nproject_name (str) – the name of the project\nproject_description (str) – an optional description for the project\nmode (int) – The current autopilot mode. 0: Full Autopilot. 2: Manual Mode.\n4: Comprehensive Autopilot. null: Mode not set.\ntarget (str) – the name of the selected target features\ntarget_type (str) – Indicating what kind of modeling is being done in this project Options are: ‘Regression’,\n‘Binary’ (Binary classification), ‘Multiclass’ (Multiclass classification),\n‘Multilabel’ (Multilabel classification)\nholdout_unlocked (bool) – whether the holdout has been unlocked\nmetric (str) – the selected project metric (e.g. LogLoss)\nstage (str) – the stage the project has reached - one of datarobot.enums.PROJECT_STAGE\npartition (dict) – information about the selected partitioning options\npositive_class (str) – for binary classification projects, the selected positive class; otherwise, None\ncreated (datetime.datetime) – the time the project was created\nadvanced_options (AdvancedOptions) – information on the advanced options that were selected for the project settings,\ne.g. a weights column or a cap of the runtime of models that can advance autopilot stages\nmax_train_pct (float) – The maximum percentage of the project dataset that can be used without going into the\nvalidation data or being too large to submit any blueprint for training\nmax_train_rows (int) – the maximum number of rows that can be trained on without going into the validation data\nor being too large to submit any blueprint for training\nfile_name (str) – The name of the file uploaded for the project dataset\ncredentials (Optional[List]) – A list of credentials for the datasets used in relationship configuration\n(previously graphs). For Feature Discovery projects, the list must be formatted\nin dictionary record format. Provide the catalogVersionId and credentialId\nfor each dataset that is to be used in the project that requires authentication.\nfeature_engineering_prediction_point (Optional[str]) – For time-aware Feature Engineering, this parameter specifies the column from the\nprimary dataset to use as the prediction point.\nunsupervised_mode (Optional[bool]) – (New in version v2.20) defaults to False, indicates whether this is an unsupervised project.\nrelationships_configuration_id (Optional[str]) – (New in version v2.21) id of the relationships configuration to use\nquery_generator_id (Optional[str]) – (New in version v2.27) id of the query generator applied for time series data prep\nsegmentation (dict, optional) – information on the segmentation options for segmented project\npartitioning_method (PartitioningMethod, optional) – (New in version v3.0) The partitioning class for this project. This attribute should only be used\nwith newly-created projects and before calling Project.analyze_and_model(). After the project has been\naimed, see Project.partition for actual partitioning options.\ncatalog_id (str) – (New in version v3.0) ID of the dataset used during creation of the project.\ncatalog_version_id (str) – (New in version v3.0) The object ID of the catalog_version which the project’s dataset belongs to.\nuse_gpu (bool) – (New in version v3.2) Whether project allows usage of GPUs\nuse_case_id (Optional[str]) – (New in version v3.8) The object ID of the use case which the project belongs to.\nset_options(options=None, **kwargs)\nUpdate the advanced options of this project.\nEither accepts an AdvancedOptions object or individual keyword arguments.\nThis is an inplace update.\nRaises:\nValueError – Raised if an object passed to the options parameter is not an AdvancedOptions instance,\na valid keyword argument from the AdvancedOptions class, or a combination of an AdvancedOptions\ninstance AND keyword arguments.\nReturn type:\nNone\nget_options()\nReturn the stored advanced options for this project.\nReturn type:\nAdvancedOptions\nclassmethod get(project_id)\nGets information about a project.\nParameters:\nproject_id (str) – The identifier of the project you want to load.\nReturns:\nproject – The queried project\nReturn type:\nProject\nExamples\nimport datarobot as dr\np = dr.Project.get(project_id='54e639a18bd88f08078ca831')\np.id\n>>>'54e639a18bd88f08078ca831'\np.project_name\n>>>'Some project name'\nclassmethod create(cls, sourcedata, project_name='Untitled Project', max_wait=600, read_timeout=600, dataset_filename=None, *, use_case=None)\nCreates a project with provided data.\nProject creation is asynchronous process, which means that after\ninitial request we will keep polling status of async process\nthat is responsible for project creation until it’s finished.\nFor SDK users this only means that this method might raise\nexceptions related to it’s async nature.\nParameters:\nsourcedata (basestring, file, pathlib.Path or pandas.DataFrame) – Dataset to use for the project.\nIf string can be either a path to a local file, url to publicly\navailable file or raw file content. If using a file, the filename\nmust consist of ASCII characters only.\nproject_name (str, unicode, optional) – The name to assign to the empty project.\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered\nunsuccessful\nread_timeout (int) – The maximum number of seconds to wait for the server to respond indicating that the\ninitial upload is complete\ndataset_filename (string or None, optional) – (New in version v2.14) File name to use for dataset.\nIgnored for url and file path sources.\nuse_case (UseCase | string, optional) – A single UseCase object or ID to add this new Project to. Must be a kwarg.\nReturns:\nproject – Instance with initialized data.\nReturn type:\nProject\nRaises:\nInputNotUnderstoodError – Raised if sourcedata isn’t one of supported types.\nAsyncFailureError – Polling for status of async process resulted in response\nwith unsupported status code. Beginning in version 2.1, this\nwill be ProjectAsyncFailureError, a subclass of AsyncFailureError\nAsyncProcessUnsuccessfulError – Raised if project creation was unsuccessful\nAsyncTimeoutError – Raised if project creation took more time, than specified\nby max_wait parameter\nExamples\np = Project.create('/home/datasets/somedataset.csv',\nproject_name=\"New API project\")\np.id\n>>> '5921731dkqshda8yd28h'\np.project_name\n>>> 'New API project'\nclassmethod encrypted_string(plaintext)\nSends a string to DataRobot to be encrypted\nThis is used for passwords that DataRobot uses to access external data sources\nParameters:\nplaintext (str) – The string to encrypt\nReturns:\nciphertext – The encrypted string\nReturn type:\nstr\nclassmethod create_from_hdfs(cls, url, port=None, project_name=None, max_wait=600)\nCreate a project from a datasource on a WebHDFS server.\nParameters:\nurl (str) – The location of the WebHDFS file, both server and full path. Per the DataRobot\nspecification, must begin with hdfs://, e.g. hdfs:///tmp/10kDiabetes.csv\nport (Optional[int]) – The port to use. If not specified, will default to the server default (50070)\nproject_name (Optional[str]) – A name to give to the project\nmax_wait (int) – The maximum number of seconds to wait before giving up.\nReturn type:\nProject\nExamples\np = Project.create_from_hdfs('hdfs:///tmp/somedataset.csv',\nproject_name=\"New API project\")\np.id\n>>> '5921731dkqshda8yd28h'\np.project_name\n>>> 'New API project'\nclassmethod create_from_data_source(cls, data_source_id, username=None, password=None, credential_id=None, use_kerberos=None, credential_data=None, project_name=None, max_wait=600, *, use_case=None)\nCreate a project from a data source. Either data_source or data_source_id\nshould be specified.\nParameters:\ndata_source_id (str) – the identifier of the data source.\nusername (Optional[str]) – The username for database authentication. If supplied password must also be supplied.\npassword (Optional[str]) – The password for database authentication. The password is encrypted\nat server side and never saved / stored. If supplied username must also be supplied.\ncredential_id (Optional[str]) – The ID of the set of credentials to\nuse instead of user and password. Note that with this change, username and password\nwill become optional.\nuse_kerberos (Optional[bool]) – Server default is False.\nIf true, use kerberos authentication for database authentication.\ncredential_data (dict, optional) – The credentials to authenticate with the database, to use instead of user/password or\ncredential ID.\nproject_name (Optional[str]) – optional, a name to give to the project.\nmax_wait (int) – optional, the maximum number of seconds to wait before giving up.\nuse_case (UseCase | string, optional) – A single UseCase object or ID to add this new Project to. Must be a kwarg.\nRaises:\nInvalidUsageError – Raised if either username or password is passed without the other.\nReturn type:\nProject\nclassmethod create_from_dataset(cls, dataset_id, dataset_version_id=None, project_name=None, user=None, password=None, credential_id=None, use_kerberos=None, use_sample_from_dataset=None, credential_data=None, max_wait=600, *, use_case=None)\nCreate a Project from a datarobot.models.Dataset\nParameters:\ndataset_id (string) – The ID of the dataset entry to user for the project’s Dataset\ndataset_version_id (string, optional) – The ID of the dataset version to use for the project dataset. If not specified - uses\nlatest version associated with dataset_id\nproject_name (string, optional) – The name of the project to be created.\nIf not specified, will be “Untitled Project” for database connections, otherwise\nthe project name will be based on the file used.\nuser (string, optional) – The username for database authentication.\npassword (string, optional) – The password (in cleartext) for database authentication. The password\nwill be encrypted on the server side in scope of HTTP request and never saved or stored\ncredential_id (string, optional) – The ID of the set of credentials to use instead of user and password.\nuse_kerberos (Optional[bool]) – Server default is False.\nIf true, use kerberos authentication for database authentication.\nuse_sample_from_dataset (Optional[bool]) – Server default is False\nIf true, use the EDA sample for the project instead of the full data.\nIt is optional for datasets between 500 MB and 10 GB.\nFor datasets over 10 GB, this is always set to True on the server side.\ncredential_data (dict, optional) – The credentials to authenticate with the database, to use instead of user/password or\ncredential ID.\nmax_wait (int) – optional, the maximum number of seconds to wait before giving up.\nuse_case (UseCase | string, optional) – A single UseCase object or ID to add this new Project to. Must be a kwarg.\nReturn type:\nProject\nclassmethod create_from_recipe(cls, recipe_id, *, use_case=None)\nCreate a project from a recipe\nParameters:\nrecipe_id (string) – The ID of the recipe entry to use to create the project’s dataset.\nReturn type:\nProject\nclassmethod create_segmented_project_from_clustering_model(cls, clustering_project_id, clustering_model_id, target, max_wait=600, *, use_case=None)\nCreate a new segmented project from a clustering model\nParameters:\nclustering_project_id (str) – The identifier of the clustering project you want to use as\nthe base.\nclustering_model_id (str) – The identifier of the clustering model you want to use as the\nsegmentation method.\ntarget (str) – The name of the target column that will be used from the\nclustering project.\nmax_wait (int) – optional, the maximum number of seconds to wait before giving up.\nuse_case (UseCase | string, optional) – A single UseCase object or ID to add this new Project to. Must be a kwarg.\nReturns:\nproject – The created project\nReturn type:\nProject\nclassmethod from_async(async_location, max_wait=600)\nGiven a temporary async status location poll for no more than max_wait seconds\nuntil the async process (project creation or setting the target, for example)\nfinishes successfully, then return the ready project\nParameters:\nasync_location (str) – The URL for the temporary async status resource. This is returned\nas a header in the response to a request that initiates an\nasync process\nmax_wait (int) – The maximum number of seconds to wait before giving up.\nReturns:\nproject – The project, now ready\nReturn type:\nProject\nRaises:\nProjectAsyncFailureError – If the server returned an unexpected response while polling for the\nasynchronous operation to resolve\nAsyncProcessUnsuccessfulError – If the final result of the asynchronous operation was a failure\nAsyncTimeoutError – If the asynchronous operation did not resolve within the time\nspecified\nclassmethod start(cls, sourcedata, target=None, project_name='Untitled Project', worker_count=None, metric=None, autopilot_on=True, blueprint_threshold=None, response_cap=None, partitioning_method=None, positive_class=None, target_type=None, unsupervised_mode=False, blend_best_models=None, prepare_model_for_deployment=None, consider_blenders_in_recommendation=None, scoring_code_only=None, min_secondary_validation_model_count=None, shap_only_mode=None, relationships_configuration_id=None, autopilot_with_feature_discovery=None, feature_discovery_supervised_feature_reduction=None, unsupervised_type=None, autopilot_cluster_list=None, bias_mitigation_feature_name=None, bias_mitigation_technique=None, include_bias_mitigation_feature_as_predictor_variable=None, incremental_learning_only_mode=None, incremental_learning_on_best_model=None, number_of_incremental_learning_iterations_before_best_model_selection=None, *, use_case=None)\nChain together project creation, file upload, and target selection.\nNotes\nWhile this function provides a simple means to get started, it does not expose\nall possible parameters. For advanced usage, using create, set_advanced_options\nand analyze_and_model directly is recommended.\nParameters:\nsourcedata (str or pandas.DataFrame) – The path to the file to upload. Can be either a path to a\nlocal file or a publicly accessible URL (starting with http://, https://,\nfile://, or s3://). If the source is a DataFrame, it will be serialized to a\ntemporary buffer.\nIf using a file, the filename must consist of ASCII\ncharacters only.\ntarget (Optional[str]) – The name of the target column in the uploaded file. Should not be provided if\nunsupervised_mode is True.\nproject_name (str) – The project name.\nworker_count (Optional[int]) – The number of workers that you want to allocate to this project.\nmetric (Optional[str]) – The name of metric to use.\nautopilot_on (boolean, default True) – Whether or not to begin modeling automatically.\nblueprint_threshold (Optional[int]) – Number of hours the model is permitted to run.\nMinimum 1\nresponse_cap (Optional[float]) – Quantile of the response distribution to use for response capping\nMust be in range 0.5 .. 1.0\npartitioning_method (PartitioningMethod object, optional) – Instance of one of the Partition Classes defined in\ndatarobot.helpers.partitioning_methods.  As an alternative, use\nProject.set_partitioning_method\nor Project.set_datetime_partitioning\nto set the partitioning for the project.\npositive_class (str, float, or int; optional) – Specifies a level of the target column that should be treated as the\npositive class for binary classification.  May only be specified\nfor binary classification targets.\ntarget_type (Optional[str]) – Override the automatically selected target_type. An example usage would be setting the\ntarget_type=’Multiclass’ when you want to preform a multiclass classification task on a\nnumeric column that has a low cardinality.\nYou can use TARGET_TYPE enum.\nunsupervised_mode (boolean, default False) – Specifies whether to create an unsupervised project.\nblend_best_models (Optional[bool]) – blend best models during Autopilot run\nscoring_code_only (Optional[bool]) – Keep only models that can be converted to scorable java code during Autopilot run.\nshap_only_mode (Optional[bool]) – Keep only models that support SHAP values during Autopilot run. Use SHAP-based insights\nwherever possible. Defaults to False.\nprepare_model_for_deployment (Optional[bool]) – Prepare model for deployment during Autopilot run.\nThe preparation includes creating reduced feature list models, retraining best model on\nhigher sample size, computing insights and assigning “RECOMMENDED FOR DEPLOYMENT” label.\nconsider_blenders_in_recommendation (Optional[bool]) – Include blenders when selecting a model to prepare for deployment in an Autopilot Run.\nDefaults to False.\nmin_secondary_validation_model_count (Optional[int]) – Compute “All backtest” scores (datetime models) or cross validation scores\nfor the specified number of highest ranking models on the Leaderboard,\nif over the Autopilot default.\nrelationships_configuration_id (Optional[str]) – (New in version v2.23) id of the relationships configuration to use\nautopilot_with_feature_discovery (Optional[bool].) – (New in version v2.23) If true, autopilot will run on a feature list that includes\nfeatures found via search for interactions.\nfeature_discovery_supervised_feature_reduction (Optional[bool]) – (New in version v2.23) Run supervised feature reduction for feature discovery projects.\nunsupervised_type (UnsupervisedTypeEnum, optional) – (New in version v2.27) Specifies whether an unsupervised project is anomaly detection\nor clustering.\nautopilot_cluster_list (list(int), optional) – (New in version v2.27) Specifies the list of clusters to build for each model during\nAutopilot. Specifying multiple values in a list will build models with each number\nof clusters for the Leaderboard.\nbias_mitigation_feature_name (Optional[str]) – The feature from protected features that will be used in a bias mitigation task to\nmitigate bias\nbias_mitigation_technique (Optional[str]) – One of datarobot.enums.BiasMitigationTechnique\nOptions:\n- ‘preprocessingReweighing’\n- ‘postProcessingRejectionOptionBasedClassification’\nThe technique by which we’ll mitigate bias, which will inform which bias mitigation task\nwe insert into blueprints\ninclude_bias_mitigation_feature_as_predictor_variable (Optional[bool]) – Whether we should also use the mitigation feature as in input to the modeler just like\nany other categorical used for training, i.e. do we want the model to “train on” this\nfeature in addition to using it for bias mitigation\nuse_case (UseCase | string, optional) – A single UseCase object or ID to add this new Project to. Must be a kwarg.\nReturns:\nproject – The newly created and initialized project.\nReturn type:\nProject\nRaises:\nAsyncFailureError – Polling for status of async process resulted in response\nwith unsupported status code\nAsyncProcessUnsuccessfulError – Raised if project creation or target setting was unsuccessful\nAsyncTimeoutError – Raised if project creation or target setting timed out\nExamples\nProject.start(\"./tests/fixtures/file.csv\",\n\"a_target\",\nproject_name=\"test_name\",\nworker_count=4,\nmetric=\"a_metric\")\nThis is an example of using a URL to specify the datasource:\nProject.start(\"https://example.com/data/file.csv\",\n\"a_target\",\nproject_name=\"test_name\",\nworker_count=4,\nmetric=\"a_metric\")\nclassmethod list(search_params=None, use_cases=None, offset=None, limit=None)\nReturns the projects associated with this account.\nParameters:\nsearch_params (dict, optional.) – If not None, the returned projects are filtered by lookup.\nCurrently you can query projects by:\nproject_name\nuse_cases (Union[UseCase, List[UseCase], str, List[str]], optional.) – If not None, the returned projects are filtered to those associated\nwith a specific Use Case or Use Cases. Accepts either the entity or the ID.\noffset (Optional[int]) – If provided, specifies the number of results to skip.\nlimit (Optional[int]) – If provided, specifies the maximum number of results to return. If not provided,\nreturns a maximum of 1000 results.\nReturns:\nprojects – Contains a list of projects associated with this user\naccount.\nReturn type:\nlist of Project instances\nRaises:\nTypeError – Raised if search_params parameter is provided,\nbut is not of supported type.\nExamples\nList all projects\np_list = Project.list()\np_list\n>>> [Project('Project One'), Project('Two')]\nSearch for projects by name\nProject.list(search_params={'project_name': 'red'})\n>>> [Project('Prediction Time'), Project('Fred Project')]\nList 2nd and 3rd projects\nProject.list(offset=1, limit=2)\n>>> [Project('Project 2'), Project('Project 3')]\nrefresh()\nFetches the latest state of the project, and updates this object\nwith that information. This is an in place update, not a new object.\nReturn type:\nNone\ndelete()\nRemoves this project from your account.\nReturn type:\nNone\nanalyze_and_model(target=None, mode='quick', metric=None, worker_count=None, positive_class=None, partitioning_method=None, featurelist_id=None, advanced_options=None, max_wait=600, target_type=None, credentials=None, feature_engineering_prediction_point=None, unsupervised_mode=False, relationships_configuration_id=None, class_mapping_aggregation_settings=None, segmentation_task_id=None, unsupervised_type=None, autopilot_cluster_list=None, use_gpu=None)\nSet target variable of an existing project and begin the autopilot process or send data to DataRobot\nfor feature analysis only if manual mode is specified.\nAny options saved using set_options will be used if nothing is passed to advanced_options.\nHowever, saved options will be ignored if advanced_options are passed.\nTarget setting is an asynchronous process, which means that after\ninitial request we will keep polling status of async process\nthat is responsible for target setting until it’s finished.\nFor SDK users this only means that this method might raise\nexceptions related to it’s async nature.\nWhen execution returns to the caller, the autopilot process will already have commenced\n(again, unless manual mode is specified).\nParameters:\ntarget (Optional[str]) – The name of the target column in the uploaded file. Should not be provided if\nunsupervised_mode is True.\nmode (Optional[str]) – You can use AUTOPILOT_MODE enum to choose between AUTOPILOT_MODE.FULL_AUTO,\nAUTOPILOT_MODE.MANUAL, AUTOPILOT_MODE.QUICK, and AUTOPILOT_MODE.COMPREHENSIVE.\nCOMPREHENSIVE` runs all blueprints in the repository (this may be extremely slow).\nIf unspecified, ``QUICK is used. If the MANUAL value is used, the model\ncreation process will need to be started by executing the start_autopilot\nfunction with the desired featurelist. It will start immediately otherwise.\nmetric (Optional[str]) – Name of the metric to use for evaluating models. You can query\nthe metrics available for the target by way of\nProject.get_metrics. If none is specified, then the default\nrecommended by DataRobot is used.\nworker_count (Optional[int]) – The number of concurrent workers to request for this project. If\nNone, then the default is used.\n(New in version v2.14) Setting this to -1 will request the maximum number\navailable to your account.\npartitioning_method (PartitioningMethod object, optional) – Instance of one of the Partition Classes defined in\ndatarobot.helpers.partitioning_methods.  As an alternative, use\nProject.set_partitioning_method\nor Project.set_datetime_partitioning\nto set the partitioning for the project.\npositive_class (str, float, or int; optional) – Specifies a level of the target column that should be treated as the\npositive class for binary classification.  May only be specified\nfor binary classification targets.\nfeaturelist_id (Optional[str]) – Specifies which feature list to use.\nadvanced_options (AdvancedOptions, optional) – Used to set advanced options of project creation. Will override any options saved using set_options.\nmax_wait (Optional[int]) – Time in seconds after which target setting is considered\nunsuccessful.\ntarget_type (Optional[str]) – Override the automatically selected target_type. An example usage would be setting the\ntarget_type=’Multiclass’ when you want to preform a multiclass classification task on a\nnumeric column that has a low cardinality. You can use TARGET_TYPE enum.\ncredentials (Optional[List]) – a list of credentials for the datasets used in relationship configuration\n(previously graphs).\nfeature_engineering_prediction_point (Optional[str]) – additional aim parameter.\nunsupervised_mode (boolean, default False) – (New in version v2.20) Specifies whether to create an unsupervised project. If True,\ntarget may not be provided.\nrelationships_configuration_id (Optional[str]) – (New in version v2.21) ID of the relationships configuration to use.\nsegmentation_task_id (str or SegmentationTask, optional) – (New in version v2.28) The segmentation task that should be used to split the project\nfor segmented modeling.\nunsupervised_type (UnsupervisedTypeEnum, optional) – (New in version v2.27) Specifies whether an unsupervised project is anomaly detection\nor clustering.\nautopilot_cluster_list (list(int), optional) – (New in version v2.27) Specifies the list of clusters to build for each model during\nAutopilot. Specifying multiple values in a list will build models with each number\nof clusters for the Leaderboard.\nuse_gpu (Optional[bool]) – (New in version v3.2) Specifies whether project should use GPUs\nReturns:\nproject – The instance with updated attributes.\nReturn type:\nProject\nRaises:\nAsyncFailureError – Polling for status of async process resulted in response\nwith unsupported status code\nAsyncProcessUnsuccessfulError – Raised if target setting was unsuccessful\nAsyncTimeoutError – Raised if target setting took more time, than specified\nby max_wait parameter\nTypeError – Raised if advanced_options, partitioning_method or target_type is\nprovided, but is not of supported type\nSee also\ndatarobot.models.Project.startcombines project creation, file upload, and target selection. Provides fewer options, but is useful for getting started quickly.\nset_target(target=None, mode='quick', metric=None, worker_count=None, positive_class=None, partitioning_method=None, featurelist_id=None, advanced_options=None, max_wait=600, target_type=None, credentials=None, feature_engineering_prediction_point=None, unsupervised_mode=False, relationships_configuration_id=None, class_mapping_aggregation_settings=None, segmentation_task_id=None, unsupervised_type=None, autopilot_cluster_list=None)\nSet target variable of an existing project and begin the Autopilot process (unless manual\nmode is specified).\nTarget setting is an asynchronous process, which means that after\ninitial request DataRobot keeps polling status of an async process\nthat is responsible for target setting until it’s finished.\nFor SDK users, this method might raise\nexceptions related to its async nature.\nWhen execution returns to the caller, the Autopilot process will already have commenced\n(again, unless manual mode is specified).\nParameters:\ntarget (Optional[str]) – The name of the target column in the uploaded file. Should not be provided if\nunsupervised_mode is True.\nmode (Optional[str]) – You can use AUTOPILOT_MODE enum to choose between\nAUTOPILOT_MODE.FULL_AUTO\nAUTOPILOT_MODE.MANUAL\nAUTOPILOT_MODE.QUICK\nAUTOPILOT_MODE.COMPREHENSIVE: Runs all blueprints in the repository (warning:\nthis may be extremely slow).\nIf unspecified, QUICK mode is used. If the MANUAL value is used, the model\ncreation process needs to be started by executing the start_autopilot\nfunction with the desired feature list. It will start immediately otherwise.\nmetric (Optional[str]) – Name of the metric to use for evaluating models. You can query\nthe metrics available for the target by way of\nProject.get_metrics. If none is specified, then the default\nrecommended by DataRobot is used.\nworker_count (Optional[int]) – The number of concurrent workers to request for this project. If\nNone, then the default is used.\n(New in version v2.14) Setting this to -1 will request the maximum number\navailable to your account.\npositive_class (str, float, or int; optional) – Specifies a level of the target column that should be treated as the\npositive class for binary classification.  May only be specified\nfor binary classification targets.\npartitioning_method (PartitioningMethod object, optional) – Instance of one of the Partition Classes defined in\ndatarobot.helpers.partitioning_methods.  As an alternative, use\nProject.set_partitioning_method\nor Project.set_datetime_partitioning\nto set the partitioning for the project.\nfeaturelist_id (Optional[str]) – Specifies which feature list to use.\nadvanced_options (AdvancedOptions, optional) – Used to set advanced options of project creation.\nmax_wait (Optional[int]) – Time in seconds after which target setting is considered\nunsuccessful.\ntarget_type (Optional[str]) – Override the automatically selected target_type. An example usage would be setting the\ntarget_type=Multiclass’ when you want to preform a multiclass classification task on a\nnumeric column that has a low cardinality. You can use ``TARGET_TYPE` enum.\ncredentials (Optional[List]) – A list of credentials for the datasets used in relationship configuration\n(previously graphs).\nfeature_engineering_prediction_point (Optional[str]) – For time-aware Feature Engineering, this parameter specifies the column from the\nprimary dataset to use as the prediction point.\nunsupervised_mode (boolean, default False) – (New in version v2.20) Specifies whether to create an unsupervised project. If True,\ntarget may not be provided.\nrelationships_configuration_id (Optional[str]) – (New in version v2.21) ID of the relationships configuration to use.\nclass_mapping_aggregation_settings (ClassMappingAggregationSettings, optional) – Instance of datarobot.helpers.ClassMappingAggregationSettings\nsegmentation_task_id (str or SegmentationTask, optional) – (New in version v2.28) The segmentation task that should be used to split the project\nfor segmented modeling.\nunsupervised_type (Optional[UnsupervisedTypeEnum]) – (New in version v2.27) Specifies whether an unsupervised project is anomaly detection\nor clustering.\nautopilot_cluster_list (Optional[List[int]]) – (New in version v2.27) Specifies the list of clusters to build for each model during\nAutopilot. Specifying multiple values in a list will build models with each number\nof clusters for the Leaderboard.\nReturns:\nproject – The instance with updated attributes.\nReturn type:\nProject\nRaises:\nAsyncFailureError – Polling for status of async process resulted in response\nwith unsupported status code.\nAsyncProcessUnsuccessfulError – Raised if target setting was unsuccessful.\nAsyncTimeoutError – Raised if target setting took more time, than specified\nby max_wait parameter.\nTypeError – Raised if advanced_options, partitioning_method or target_type is\nprovided, but is not of supported type.\nSee also\ndatarobot.models.Project.startCombines project creation, file upload, and target selection. Provides fewer options, but is useful for getting started quickly.\ndatarobot.models.Project.analyze_and_modelthe method replacing set_target after it is removed.\nget_model_records(sort_by_partition='validation', sort_by_metric=None, with_metric=None, search_term=None, featurelists=None, families=None, blueprints=None, labels=None, characteristics=None, training_filters=None, number_of_clusters=None, limit=100, offset=0)\nRetrieve paginated model records, sorted by scores, with optional filtering.\nParameters:\nsort_by_partition (str, one of validation, backtesting, crossValidation or holdout) – Set the partition to use for sorted (by score) list of models. validation is the default.\nsort_by_metric (str) –\nSet the project metric to use for model sorting. DataRobot-selected project optimization metricis the default.\nwith_metric (str) – For a single-metric list of results, specify that project metric.\nsearch_term (str) – If specified, only models containing the term in their name or processes are returned.\nfeaturelists (List[str]) – If specified, only models trained on selected featurelists are returned.\nfamilies (List[str]) – If specified, only models belonging to selected families are returned.\nblueprints (List[str]) – If specified, only models trained on specified blueprint IDs are returned.\nlabels (List[str], starred or prepared for deployment) – If specified, only models tagged with all listed labels are returned.\ncharacteristics (List[str]) – If specified, only models matching all listed characteristics are returned. Possible values\n“frozen”,”trained on gpu”,”with exportable coefficients”,”with mono constraints”,”with rating table”,\n“with scoring code”,”new series optimized”\ntraining_filters (List[str]) – If specified, only models matching at least one of the listed training conditions are returned.\nThe following formats are supported for autoML and datetime partitioned projects:\n- number of rows in training subset\nFor datetime partitioned projects:\n- <training duration>, example P6Y0M0D\n- <training_duration>-<time_window_sample_percent>-<sampling_method> Example: P6Y0M0D-78-Random,\n(returns models trained on 6 years of data, sampling rate 78%, random sampling).\n- Start/end date\n- Project settings\nnumber_of_clusters (list of int) – Filter models by number of clusters. Applicable only in unsupervised clustering projects.\nlimit (int)\noffset (int)\nReturns:\ngeneric_models\nReturn type:\nlist of GenericModel\nget_models(order_by=None, search_params=None, with_metric=None, use_new_models_retrieval=False)\nList all completed, successful models in the leaderboard for the given project.\nParameters:\norder_by (str or list of strings, optional) – If not None, the returned models are ordered by this\nattribute. If None, the default return is the order of\ndefault project metric.\nAllowed attributes to sort by are:\nmetric\nsample_pct\nIf the sort attribute is preceded by a hyphen, models will be sorted in descending\norder, otherwise in ascending order.\nMultiple sort attributes can be included as a comma-delimited string or in a list\ne.g. order_by=`sample_pct,-metric` or order_by=[sample_pct, -metric]\nUsing metric to sort by will result in models being sorted according to their\nvalidation score by how well they did according to the project metric.\nsearch_params (dict, optional.) – If not None, the returned models are filtered by lookup.\nCurrently you can query models by:\nname\nsample_pct\nis_starred\nwith_metric (Optional[str].) – If not None, the returned models will only have scores for this\nmetric. Otherwise all the metrics are returned.\nuse_new_models_retrieval (bool, False by default) – If true, new retrieval route is used, which supports filtering and returns fewer attributes per\nindividual model. Following attributes are absent and could be retrieved from the blueprint level:\nmonotonic_increasing_featurelist_id, monotonic_decreasing_featurelist_id, supports_composable_ml\nand supports_monotonic_constraints. Following attributes are absent and could be retrieved from\nthe individual model level: has_empty_clusters, is_n_clusters_dynamically_determined,\nprediction_threshold and prediction_threshold_read_only. Attribute n_clusters in Model is\nrenamed to number_of_clusters in GenericModel and is returned for unsupervised clustering models.\nReturns:\nmodels – All models trained in the project.\nReturn type:\na list of Model or a list of GenericModel if `use_new_models_retrieval is True.`\nRaises:\nTypeError – Raised if order_by or search_params parameter is provided,\nbut is not of supported type.\nExamples\nProject.get('pid').get_models(order_by=['-sample_pct',\n'metric'])\n# Getting models that contain \"Ridge\" in name\nProject.get('pid').get_models(\nsearch_params={\n'name': \"Ridge\"\n})\n# Filtering models based on 'starred' flag:\nProject.get('pid').get_models(search_params={'is_starred': True})\n# retrieve additional attributes for the model\nmodel_records = project.get_models(use_new_models_retrieval=True)\nmodel_record = model_records[0]\nblueprint_id = model_record.blueprint_id\nblueprint = dr.Blueprint.get(project.id, blueprint_id)\nmodel_record.number_of_clusters\nblueprint.supports_composable_ml\nblueprint.supports_monotonic_constraints\nblueprint.monotonic_decreasing_featurelist_id\nblueprint.monotonic_increasing_featurelist_id\nmodel = dr.Model.get(project.id, model_record.id)\nmodel.prediction_threshold\nmodel.prediction_threshold_read_only\nmodel.has_empty_clusters\nmodel.is_n_clusters_dynamically_determined\nrecommended_model()\nReturns the default recommended model, or None if there is no default recommended model.\nReturns:\nrecommended_model – The default recommended model.\nReturn type:\nModel or None\nget_top_model(metric=None)\nObtain the top ranked model for a given metric/\nIf no metric is passed in, it uses the project’s default metric.\nModels that display score of N/A in the UI are not included in the ranking (see\nhttps://docs.datarobot.com/en/docs/modeling/reference/model-detail/leaderboard-ref.html#na-scores).\nParameters:\nmetric (Optional[str]) – Metric to sort models\nReturns:\nmodel – The top model\nReturn type:\nModel\nRaises:\nValueError – Raised if the project is unsupervised.\nRaised if the project has no target set.\nRaised if no metric was passed or the project has no metric.\nRaised if the metric passed is not used by the models on the leaderboard.\nExamples\nfrom datarobot.models.project import Project\nproject = Project.get(\"<MY_PROJECT_ID>\")\ntop_model = project.get_top_model()\nget_datetime_models()\nList all models in the project as DatetimeModels\nRequires the project to be datetime partitioned.  If it is not, a ClientError will occur.\nReturns:\nmodels – the datetime models\nReturn type:\nlist of DatetimeModel\nget_prime_models()\nList all DataRobot Prime models for the project\nPrime models were created to approximate a parent model, and have downloadable code.\nReturns:\nmodels\nReturn type:\nlist of PrimeModel\nget_prime_files(parent_model_id=None, model_id=None)\nList all downloadable code files from DataRobot Prime for the project\nParameters:\nparent_model_id (Optional[str]) – Filter for only those prime files approximating this parent model\nmodel_id (Optional[str]) – Filter for only those prime files with code for this prime model\nReturns:\nfiles\nReturn type:\nlist of PrimeFile\nget_dataset()\nRetrieve the dataset used to create a project.\nReturns:\nDataset used for creation of project or None if no catalog_id present.\nReturn type:\nDataset\nExamples\nfrom datarobot.models.project import Project\nproject = Project.get(\"<MY_PROJECT_ID>\")\ndataset = project.get_dataset()\nget_datasets()\nList all the datasets that have been uploaded for predictions\nReturns:\ndatasets\nReturn type:\nlist of PredictionDataset instances\nupload_dataset(sourcedata, max_wait=600, read_timeout=600, forecast_point=None, predictions_start_date=None, predictions_end_date=None, dataset_filename=None, relax_known_in_advance_features_check=None, credentials=None, actual_value_column=None, secondary_datasets_config_id=None)\nUpload a new dataset to make predictions against\nParameters:\nsourcedata (str, file or pandas.DataFrame) – Data to be used for predictions. If string, can be either a path to a local file,\na publicly accessible URL (starting with http://, https://, file://), or\nraw file content. If using a file on disk, the filename must consist of ASCII\ncharacters only.\nmax_wait (Optional[int]) – The maximum number of seconds to wait for the uploaded dataset to be processed before\nraising an error.\nread_timeout (Optional[int]) – The maximum number of seconds to wait for the server to respond indicating that the\ninitial upload is complete\nforecast_point (datetime.datetime or None, optional) – (New in version v2.8) May only be specified for time series projects, otherwise the\nupload will be rejected. The time in the dataset relative to which predictions should be\ngenerated in a time series project.  See the Time Series documentation for more information. If not provided, will default to using the\nlatest forecast point in the dataset.\npredictions_start_date (datetime.datetime or None, optional) – (New in version v2.11) May only be specified for time series projects. The start date\nfor bulk predictions. Note that this parameter is for generating historical predictions\nusing the training data. This parameter should be provided in conjunction with\npredictions_end_date. Cannot be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in version v2.11) May only be specified for time series projects. The end date\nfor bulk predictions, exclusive. Note that this parameter is for generating\nhistorical predictions using the training data. This parameter should be provided in\nconjunction with predictions_start_date.\nCannot be provided with the forecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) Actual value column name, valid for the prediction\nfiles if the project is unsupervised and the dataset is considered as bulk predictions\ndataset. Cannot be provided with the forecast_point parameter.\ndataset_filename (string or None, optional) – (New in version v2.14) File name to use for the dataset.\nIgnored for url and file path sources.\nrelax_known_in_advance_features_check (Optional[bool]) – (New in version v2.15) For time series projects only. If True, missing values in the\nknown in advance features are allowed in the forecast window at the prediction time.\nIf omitted or False, missing values are not allowed.\ncredentials (Optional[List] a list of credentials for the datasets used) – in Feature discovery project\nsecondary_datasets_config_id (string or None, optional) – (New in version v2.23) The Id of the alternative secondary dataset config\nto use during prediction for Feature discovery project.\nReturns:\ndataset – The newly uploaded dataset.\nReturn type:\nPredictionDataset\nRaises:\nInputNotUnderstoodError – Raised if sourcedata isn’t one of supported types.\nAsyncFailureError – Raised if polling for the status of an async process resulted in a response with an\nunsupported status code.\nAsyncProcessUnsuccessfulError – Raised if project creation was unsuccessful (i.e. the server reported an error in\nuploading the dataset).\nAsyncTimeoutError – Raised if processing the uploaded dataset took more time than specified\nby the max_wait parameter.\nValueError – Raised if forecast_point or predictions_start_date and predictions_end_date\nare provided, but are not of the supported type.\nupload_dataset_from_data_source(data_source_id, username, password, max_wait=600, forecast_point=None, relax_known_in_advance_features_check=None, credentials=None, predictions_start_date=None, predictions_end_date=None, actual_value_column=None, secondary_datasets_config_id=None)\nUpload a new dataset from a data source to make predictions against\nParameters:\ndata_source_id (str) – The identifier of the data source.\nusername (str) – The username for database authentication.\npassword (str) – The password for database authentication. The password is encrypted\nat server side and never saved / stored.\nmax_wait (Optional[int]) – Optional, the maximum number of seconds to wait before giving up.\nforecast_point (datetime.datetime or None, optional) – (New in version v2.8) For time series projects only. This is the default point relative\nto which predictions will be generated, based on the forecast window of the project. See\nthe time series prediction documentation for more\ninformation.\nrelax_known_in_advance_features_check (Optional[bool]) – (New in version v2.15) For time series projects only. If True, missing values in the\nknown in advance features are allowed in the forecast window at the prediction time.\nIf omitted or False, missing values are not allowed.\ncredentials (Optional[List] a list of credentials for the datasets used) – in Feature discovery project\npredictions_start_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – (New in version v2.20) For time series projects only. The end date for bulk predictions,\nexclusive. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_start_date. Can’t be provided with the forecast_point parameter.\nactual_value_column (string, optional) – (New in version v2.21) Actual value column name, valid for the prediction\nfiles if the project is unsupervised and the dataset is considered as bulk predictions\ndataset. Cannot be provided with the forecast_point parameter.\nsecondary_datasets_config_id (string or None, optional) – (New in version v2.23) The Id of the alternative secondary dataset config\nto use during prediction for Feature discovery project.\nReturns:\ndataset – the newly uploaded dataset\nReturn type:\nPredictionDataset\nupload_dataset_from_catalog(dataset_id, credential_id=None, credential_data=None, dataset_version_id=None, max_wait=600, forecast_point=None, relax_known_in_advance_features_check=None, credentials=None, predictions_start_date=None, predictions_end_date=None, actual_value_column=None, secondary_datasets_config_id=None)\nUpload a new dataset from a catalog dataset to make predictions against\nParameters:\ndataset_id (str) – The identifier of the dataset.\ncredential_id (Optional[str]) – The credential ID of the AI Catalog dataset to upload.\ncredential_data (BasicCredentialsDataDict | S3CredentialsDataDict | OAuthCredentialsDataDict, optional) – Credential data of the catalog dataset to upload. credential_data can be in\none of the following forms:\nBasic Credentials:\ncredentialType (str)The credential type. For basic credentials, this value must be CredentialTypes.BASIC.\nuser (str)The username for database authentication.\npassword (str)The password for database authentication.\nThe password is encrypted at rest and never saved or stored.\nS3 Credentials\ncredentialType (str)The credential type. For S3 credentials, this value must be CredentialTypes.S3.\nawsAccessKeyId (Optional[str])The S3 AWS access key ID.\nawsSecretAccessKey (Optional[str])The S3 AWS secret access key.\nawsSessionToken (Optional[str])The S3 AWS session token.\nconfig_id (Optional[str])The ID of the saved shared secure configuration. If specified, cannot include awsAccessKeyId,\nawsSecretAccessKey or awsSessionToken.\nOAuth Credentials\ncredentialType (str)The credential type. For OAuth credentials, this value must be CredentialTypes.OAUTH.\noauthRefreshToken (str)The oauth refresh token.\noauthClientId (str)The oauth client ID.\noauthClientSecret (str)The oauth client secret.\noauthAccessToken (str)The oauth access token.\nSnowflake Key Pair Credentials\ncredentialType (str)The credential type. For Snowflake Key Pair, this value must be\nCredentialTypes.SNOWFLAKE_KEY_PAIR_AUTH.\nuser (Optional[str])The Snowflake login name.\nprivateKeyStr (Optional[str])The private key copied exactly from user private key file. Since it contains\nmultiple lines, when assign to a variable,\nput the key string inside triple-quotes\npassphrase (Optional[str])The string used to encrypt the private key.\nconfigId (Optional[str])The ID of the saved shared secure configuration. If specified, cannot include user,\nprivateKeyStr or passphrase.\nDatabricks Access Token Credentials\ncredentialType (str)The credential type. For a Databricks access token, this value must be\nCredentialTypes.DATABRICKS_ACCESS_TOKEN.\ndatabricksAccessToken (str)The Databricks personal access token.\nDatabricks Service Principal Credentials\ncredentialType (str)The credential type. For Databricks service principal, this value must be\nCredentialTypes.DATABRICKS_SERVICE_PRINCIPAL.\nclientId (Optional[str])The client ID for Databricks service principal.\nclientSecret (Optional[str])The client secret for Databricks service principal.\nconfigId (Optional[str])The ID of the saved shared secure configuration. If specified, cannot include clientId\nand clientSecret.\nAzure Service Principal Credentials\ncredentialType (str)The credential type. For Azure service principal, this value must be\nCredentialTypes.AZURE_SERVICE_PRINCIPAL.\nclientId (Optional[str])The client ID for Azure service principal.\nclientSecret (Optional[str])The client secret for Azure service principal.\nazureTenantId (Optional[str])The azure tenant ID for Azure service principal.\nconfigId (Optional[str])The ID of the saved shared secure configuration. If specified, cannot include clientId\nand clientSecret.\ndataset_version_id (Optional[str]) – The version id of the dataset to use.\nmax_wait (Optional[int]) – Optional, the maximum number of seconds to wait before giving up.\nforecast_point (datetime.datetime or None, optional) – For time series projects only. This is the default point relative\nto which predictions will be generated, based on the forecast window of the project. See\nthe time series prediction documentation for more\ninformation.\nrelax_known_in_advance_features_check (Optional[bool]) – For time series projects only. If True, missing values in the\nknown in advance features are allowed in the forecast window at the prediction time.\nIf omitted or False, missing values are not allowed.\ncredentials (list[BasicCredentialsDict | CredentialIdCredentialsDict], optional) – A list of credentials for the datasets used in Feature discovery project.\nItems in credentials can have the following forms:\nBasic Credentials\nuser (str)The username for database authentication.\npassword (str)The password (in cleartext) for database authentication. The password\nwill be encrypted on the server side in scope of HTTP request\nand never saved or stored.\nCredential ID\ncredentialId (str)The ID of the set of credentials to use instead of user and password.\nNote that with this change, username and password will become optional.\npredictions_start_date (datetime.datetime or None, optional) – For time series projects only. The start date for bulk\npredictions. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_end_date. Can’t be provided with the forecast_point parameter.\npredictions_end_date (datetime.datetime or None, optional) – For time series projects only. The end date for bulk predictions,\nexclusive. Note that this parameter is for generating historical predictions using the\ntraining data. This parameter should be provided in conjunction with\npredictions_start_date. Can’t be provided with the forecast_point parameter.\nactual_value_column (string, optional) – Actual value column name, valid for the prediction\nfiles if the project is unsupervised and the dataset is considered as bulk predictions\ndataset. Cannot be provided with the forecast_point parameter.\nsecondary_datasets_config_id (string or None, optional) – The Id of the alternative secondary dataset config\nto use during prediction for Feature discovery project.\nReturns:\ndataset – the newly uploaded dataset\nReturn type:\nPredictionDataset\nget_blueprints()\nList all blueprints recommended for a project.\nReturns:\nmenu – All blueprints in a project’s repository.\nReturn type:\nlist of Blueprint instances\nget_features()\nList all features for this project\nReturns:\nall features for this project\nReturn type:\nlist of Feature\nget_modeling_features(batch_size=None)\nList all modeling features for this project\nOnly available once the target and partitioning settings have been set.  For more\ninformation on the distinction between input and modeling features, see the\ntime series documentation.\nParameters:\nbatch_size (Optional[int]) – The number of features to retrieve in a single API call.  If specified, the client may\nmake multiple calls to retrieve the full list of features.  If not specified, an\nappropriate default will be chosen by the server.\nReturns:\nAll modeling features in this project\nReturn type:\nlist of ModelingFeature\nget_featurelists()\nList all featurelists created for this project\nReturns:\nAll featurelists created for this project\nReturn type:\nlist of Featurelist\nget_associations(assoc_type, metric, featurelist_id=None)\nGet the association statistics and metadata for a project’s\ninformative features\nAdded in version v2.17.\nParameters:\nassoc_type (string or None) – The type of association, must be either ‘association’ or ‘correlation’\nmetric (string or None) – The specified association metric, belongs under either association\nor correlation umbrella\nfeaturelist_id (string or None) – The desired featurelist for which to get association statistics\n(New in version v2.19)\nReturns:\nassociation_data – Pairwise metric strength data, feature clustering data,\nand ordering data for Feature Association Matrix visualization\nReturn type:\ndict\nget_association_featurelists()\nList featurelists and get feature association status for each\nAdded in version v2.19.\nReturns:\nfeature_lists – Dict with ‘featurelists’ as key, with list of featurelists as values\nReturn type:\ndict\nget_association_matrix_details(feature1, feature2)\nGet a sample of the actual values used to measure the association\nbetween a pair of features\nAdded in version v2.17.\nParameters:\nfeature1 (str) – Feature name for the first feature of interest\nfeature2 (str) – Feature name for the second feature of interest\nReturns:\ndict – This data has 3 keys: chart_type, features, values, and types\nchart_type (str) – Type of plotting the pair of features gets in the UI.\ne.g. ‘HORIZONTAL_BOX’, ‘VERTICAL_BOX’, ‘SCATTER’ or ‘CONTINGENCY’\nvalues (list) – A list of triplet lists e.g.\n{“values”: [[460.0, 428.5, 0.001], [1679.3, 259.0, 0.001], …]\nThe first entry of each list is a value of feature1, the second entry of\neach list is a value of feature2, and the third is the relative frequency of\nthe pair of datapoints in the sample.\nfeatures (List[str]) – A list of the passed features, [feature1, feature2]\ntypes (List[str]) – A list of the passed features’ types inferred by DataRobot.\ne.g. [‘NUMERIC’, ‘CATEGORICAL’]\nget_modeling_featurelists(batch_size=None)\nList all modeling featurelists created for this project\nModeling featurelists can only be created after the target and partitioning options have\nbeen set for a project.  In time series projects, these are the featurelists that can be\nused for modeling; in other projects, they behave the same as regular featurelists.\nSee the time series documentation for more information.\nParameters:\nbatch_size (Optional[int]) – The number of featurelists to retrieve in a single API call.  If specified, the client\nmay make multiple calls to retrieve the full list of features.  If not specified, an\nappropriate default will be chosen by the server.\nReturns:\nall modeling featurelists in this project\nReturn type:\nlist of ModelingFeaturelist\nget_discarded_features()\nRetrieve discarded during feature generation features. Applicable for time\nseries projects. Can be called at the modeling stage.\nReturns:\ndiscarded_features_info\nReturn type:\nDiscardedFeaturesInfo\nrestore_discarded_features(features, max_wait=600)\nRestore discarded during feature generation features. Applicable for time\nseries projects. Can be called at the modeling stage.\nReturns:\nstatus – information about features requested to be restored.\nReturn type:\nFeatureRestorationStatus\ncreate_type_transform_feature(name, parent_name, variable_type, replacement=None, date_extraction=None, max_wait=600)\nCreate a new feature by transforming the type of an existing feature in the project\nNote that only the following transformations are supported:\nText to categorical or numeric\nCategorical to text or numeric\nNumeric to categorical\nDate to categorical or numeric\nNotes\nSpecial considerations when casting numeric to categorical\nThere are two parameters which can be used for variableType to convert numeric\ndata to categorical levels. These differ in the assumptions they make about the input\ndata, and are very important when considering the data that will be used to make\npredictions. The assumptions that each makes are:\ncategorical : The data in the column is all integral, and there are no missing\nvalues. If either of these conditions do not hold in the training set, the\ntransformation will be rejected. During predictions, if any of the values in the\nparent column are missing, the predictions will error.\ncategoricalInt : New in v2.6\nAll of the data in the column should be considered categorical in its string form when\ncast to an int by truncation. For example the value 3 will be cast as the string\n3 and the value 3.14 will also be cast as the string 3. Further, the\nvalue -3.6 will become the string -3.\nMissing values will still be recognized as missing.\nFor convenience these are represented in the enum VARIABLE_TYPE_TRANSFORM with the\nnames CATEGORICAL and CATEGORICAL_INT.\nParameters:\nname (str) – The name to give to the new feature\nparent_name (str) – The name of the feature to transform\nvariable_type (str) – The type the new column should have. See the values within\ndatarobot.enums.VARIABLE_TYPE_TRANSFORM.\nreplacement (str or Optional[float]) – The value that missing or unconvertable data should have\ndate_extraction (Optional[str]) – Must be specified when parent_name is a date column (and left None otherwise).\nSpecifies which value from a date should be extracted. See the list of values in\ndatarobot.enums.DATE_EXTRACTION\nmax_wait (Optional[int]) – The maximum amount of time to wait for DataRobot to finish processing the new column.\nThis process can take more time with more data to process. If this operation times\nout, an AsyncTimeoutError will occur. DataRobot continues the processing and the\nnew column may successfully be constructed.\nReturns:\nThe data of the new Feature\nReturn type:\nFeature\nRaises:\nAsyncFailureError – If any of the responses from the server are unexpected\nAsyncProcessUnsuccessfulError – If the job being waited for has failed or has been cancelled\nAsyncTimeoutError – If the resource did not resolve in time\nget_featurelist_by_name(name)\nCreates a new featurelist\nParameters:\nname (Optional[str]) – The name of the Project’s featurelist to get.\nReturns:\nfeaturelist found by name, optional\nReturn type:\nFeaturelist\nExamples\nproject = Project.get('5223deadbeefdeadbeef0101')\nfeaturelist = project.get_featurelist_by_name(\"Raw Features\")\ncreate_featurelist(name=None, features=None, starting_featurelist=None, starting_featurelist_id=None, starting_featurelist_name=None, features_to_include=None, features_to_exclude=None)\nCreates a new featurelist\nParameters:\nname (Optional[str]) – The name to give to this new featurelist. Names must be unique, so\nan error will be returned from the server if this name has already\nbeen used in this project.  We dynamically create a name if none is\nprovided.\nfeatures (list of Optional[str]) – The names of the features. Each feature must exist in the project\nalready.\nstarting_featurelist (Featurelist, optional) – The featurelist to use as the basis when creating a new featurelist.\nstarting_featurelist.features will be read to get the list of features\nthat we will manipulate.\nstarting_featurelist_id (Optional[str]) – The featurelist ID used instead of passing an object instance.\nstarting_featurelist_name (Optional[str]) – The featurelist name like “Informative Features” to find a featurelist\nvia the API, and use to fetch features.\nfeatures_to_include (list of Optional[str]) – The list of the feature names to include in new featurelist. Throws an\nerror if an item in this list is not in the featurelist that was passed,\nor that was retrieved from the API. If nothing is passed, all features\nare included from the starting featurelist.\nfeatures_to_exclude (list of Optional[str]) – The list of the feature names to exclude in the new featurelist. Throws\nan error if an item in this list is not in the featurelist that was\npassed, also throws an error if a feature is in this list as well as\nfeatures_to_include. Method cannot use both at the same time.\nReturns:\nnewly created featurelist\nReturn type:\nFeaturelist\nRaises:\nDuplicateFeaturesError – Raised if features variable contains duplicate features\nInvalidUsageError – Raised method is called with incompatible arguments\nExamples\nproject = Project.get('5223deadbeefdeadbeef0101')\nflists = project.get_featurelists()\n# Create a new featurelist using a subset of features from an\n# existing featurelist\nflist = flists[0]\nfeatures = flist.features[::2]  # Half of the features\nnew_flist = project.create_featurelist(\nname='Feature Subset',\nfeatures=features,\n)\nproject = Project.get('5223deadbeefdeadbeef0101')\n# Create a new featurelist using a subset of features from an\n# existing featurelist by using features_to_exclude param\nnew_flist = project.create_featurelist(\nname='Feature Subset of Existing Featurelist',\nstarting_featurelist_name=\"Informative Features\",\nfeatures_to_exclude=[\"metformin\", \"weight\", \"age\"],\n)\ncreate_modeling_featurelist(name, features, skip_datetime_partition_column=False)\nCreate a new modeling featurelist\nModeling featurelists can only be created after the target and partitioning options have\nbeen set for a project.  In time series projects, these are the featurelists that can be\nused for modeling; in other projects, they behave the same as regular featurelists.\nSee the time series documentation for more information.\nParameters:\nname (str) – the name of the modeling featurelist to create.  Names must be unique within the\nproject, or the server will return an error.\nfeatures (List[str]) – the names of the features to include in the modeling featurelist.  Each feature must\nbe a modeling feature.\nskip_datetime_partition_column (boolean, optional) – False by default. If True, featurelist will not contain datetime partition column.\nUse to create monotonic feature lists in Time Series projects. Setting makes no difference for\nnot Time Series projects. Monotonic featurelists can not be used for modeling.\nReturns:\nfeaturelist – the newly created featurelist\nReturn type:\nModelingFeaturelist\nExamples\nproject = Project.get('1234deadbeeffeeddead4321')\nmodeling_features = project.get_modeling_features()\nselected_features = [feat.name for feat in modeling_features][:5]  # select first five\nnew_flist = project.create_modeling_featurelist('Model This', selected_features)\nget_metrics(feature_name)\nGet the metrics recommended for modeling on the given feature.\nParameters:\nfeature_name (str) – The name of the feature to query regarding which metrics are\nrecommended for modeling.\nReturns:\nfeature_name (str) – The name of the feature that was looked up\navailable_metrics (List[str]) – An array of strings representing the appropriate metrics.  If the feature\ncannot be selected as the target, then this array will be empty.\nmetric_details (list of dict) – The list of metricDetails objects\nmetric_name: strName of the metric\nsupports_timeseries: booleanThis metric is valid for timeseries\nsupports_multiclass: booleanThis metric is valid for multiclass classification\nsupports_binary: booleanThis metric is valid for binary classification\nsupports_regression: booleanThis metric is valid for regression\nascending: booleanShould the metric be sorted in ascending order\nget_status()\nQuery the server for project status.\nReturns:\nstatus – Contains:\nautopilot_done : a boolean.\nstage : a short string indicating which stage the project\nis in.\nstage_description : a description of what stage means.\nReturn type:\ndict\nExamples\n{\"autopilot_done\": False,\n\"stage\": \"modeling\",\n\"stage_description\": \"Ready for modeling\"}\npause_autopilot()\nPause autopilot, which stops processing the next jobs in the queue.\nReturns:\npaused – Whether the command was acknowledged\nReturn type:\nboolean\nunpause_autopilot()\nUnpause autopilot, which restarts processing the next jobs in the queue.\nReturns:\nunpaused – Whether the command was acknowledged.\nReturn type:\nboolean\nstart_autopilot(featurelist_id, mode='quick', blend_best_models=False, scoring_code_only=False, prepare_model_for_deployment=True, consider_blenders_in_recommendation=False, run_leakage_removed_feature_list=True, autopilot_cluster_list=None)\nStart Autopilot on provided featurelist with the specified Autopilot settings,\nhalting the current Autopilot run.\nOnly one autopilot can be running at the time.\nThat’s why any ongoing autopilot on a different featurelist will\nbe halted - modeling jobs in queue would not\nbe affected but new jobs would not be added to queue by\nthe halted autopilot.\nParameters:\nfeaturelist_id (str) – Identifier of featurelist that should be used for autopilot\nmode (Optional[str]) – The Autopilot mode to run. You can use AUTOPILOT_MODE enum to choose between\nAUTOPILOT_MODE.FULL_AUTO\nAUTOPILOT_MODE.QUICK\nAUTOPILOT_MODE.COMPREHENSIVE\nIf unspecified, AUTOPILOT_MODE.QUICK is used.\nblend_best_models (Optional[bool]) – Blend best models during Autopilot run. This option is not supported in SHAP-only ‘\n‘mode.\nscoring_code_only (Optional[bool]) – Keep only models that can be converted to scorable java code during Autopilot run.\nprepare_model_for_deployment (Optional[bool]) – Prepare model for deployment during Autopilot run. The preparation includes creating\nreduced feature list models, retraining best model on higher sample size,\ncomputing insights and assigning “RECOMMENDED FOR DEPLOYMENT” label.\nconsider_blenders_in_recommendation (Optional[bool]) – Include blenders when selecting a model to prepare for deployment in an Autopilot Run.\nThis option is not supported in SHAP-only mode or for multilabel projects.\nrun_leakage_removed_feature_list (Optional[bool]) – Run Autopilot on Leakage Removed feature list (if exists).\nautopilot_cluster_list (list of Optional[int]) – (New in v2.27) A list of integers, where each value will be used as the number of\nclusters in Autopilot model(s) for unsupervised clustering projects. Cannot be specified\nunless project unsupervisedMode is true and unsupervisedType is set to ‘clustering’.\nRaises:\nAppPlatformError – Raised project’s target was not selected or the settings for Autopilot are invalid\nfor the project project.\nReturn type:\nNone\ntrain(trainable, sample_pct=None, featurelist_id=None, source_project_id=None, scoring_type=None, training_row_count=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>, n_clusters=None)\nSubmit a job to the queue to train a model.\nEither sample_pct or training_row_count can be used to specify the amount of data to\nuse, but not both.  If neither are specified, a default of the maximum amount of data that\ncan safely be used to train any blueprint without going into the validation data will be\nselected.\nIn smart-sampled projects, sample_pct and training_row_count are assumed to be in terms\nof rows of the minority class.\nNotes\nIf the project uses datetime partitioning, use\nProject.train_datetime instead.\nParameters:\ntrainable (str or Blueprint) – For str, this is assumed to be a blueprint_id. If no\nsource_project_id is provided, the project_id will be assumed\nto be the project that this instance represents.\nOtherwise, for a Blueprint, it contains the\nblueprint_id and source_project_id that we want\nto use. featurelist_id will assume the default for this project\nif not provided, and sample_pct will default to using the maximum\ntraining value allowed for this project’s partition setup.\nsource_project_id will be ignored if a\nBlueprint instance is used for this parameter\nsample_pct (Optional[float]) – The amount of data to use for training, as a percentage of the project dataset from 0\nto 100.\nfeaturelist_id (Optional[str]) – The identifier of the featurelist to use. If not defined, the\ndefault for this project is used.\nsource_project_id (Optional[str]) – Which project created this blueprint_id. If None, it defaults\nto looking in this project. Note that you must have read\npermissions in this project.\nscoring_type (Optional[str]) – Either validation or crossValidation (also dr.SCORING_TYPE.validation\nor dr.SCORING_TYPE.cross_validation). validation is available for every\npartitioning type, and indicates that the default model validation should be\nused for the project.\nIf the project uses a form of cross-validation partitioning,\ncrossValidation can also be used to indicate\nthat all of the available training/validation combinations\nshould be used to evaluate the model.\ntraining_row_count (Optional[int]) – The number of rows to use to train the requested model.\nmonotonic_increasing_featurelist_id (Optional[str]) – (new in version 2.11) the id of the featurelist that defines the set of features with\na monotonically increasing relationship to the target. Passing None disables\nincreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (Optional[str]) – (new in version 2.11) the id of the featurelist that defines the set of features with\na monotonically decreasing relationship to the target. Passing None disables\ndecreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nn_clusters (Optional[int]) – (new in version 2.27) Number of clusters to use in an unsupervised clustering model.\nThis parameter is used only for unsupervised clustering models that don’t automatically\ndetermine the number of clusters.\nReturns:\nmodel_job_id – id of created job, can be used as parameter to ModelJob.get\nmethod or wait_for_async_model_creation function\nReturn type:\nstr\nExamples\nUse a Blueprint instance:\nblueprint = project.get_blueprints()[0]\nmodel_job_id = project.train(blueprint, training_row_count=project.max_train_rows)\nUse a blueprint_id, which is a string. In the first case, it is\nassumed that the blueprint was created by this project. If you are\nusing a blueprint used by another project, you will need to pass the\nid of that other project as well.\nblueprint_id = 'e1c7fc29ba2e612a72272324b8a842af'\nproject.train(blueprint, training_row_count=project.max_train_rows)\nanother_project.train(blueprint, source_project_id=project.id)\nYou can also easily use this interface to train a new model using the data from\nan existing model:\nmodel = project.get_models()[0]\nmodel_job_id = project.train(model.blueprint.id,\nsample_pct=100)\ntrain_datetime(blueprint_id, featurelist_id=None, training_row_count=None, training_duration=None, source_project_id=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>, use_project_settings=False, sampling_method=None, n_clusters=None)\nCreate a new model in a datetime partitioned project\nIf the project is not datetime partitioned, an error will occur.\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nParameters:\nblueprint_id (str) – the blueprint to use to train the model\nfeaturelist_id (Optional[str]) – the featurelist to use to train the model.  If not specified, the project default will\nbe used.\ntraining_row_count (Optional[int]) – the number of rows of data that should be used to train the model.  If specified,\nneither training_duration nor use_project_settings may be specified.\ntraining_duration (Optional[str]) – a duration string specifying what time range the data used to train the model should\nspan.  If specified, neither training_row_count nor use_project_settings may be\nspecified.\nsampling_method (Optional[str]) – (New in version v2.23) defines the way training data is selected. Can be either\nrandom or latest.  In combination with training_row_count defines how rows\nare selected from backtest (latest by default).  When training data is defined using\ntime range (training_duration or use_project_settings) this setting changes the\nway time_window_sample_pct is applied (random by default).  Applicable to OTV\nprojects only.\nuse_project_settings (Optional[bool]) – (New in version v2.20) defaults to False. If True, indicates that the custom\nbacktest partitioning settings specified by the user will be used to train the model and\nevaluate backtest scores. If specified, neither training_row_count nor\ntraining_duration may be specified.\nsource_project_id (Optional[str]) – the id of the project this blueprint comes from, if not this project.  If left\nunspecified, the blueprint must belong to this project.\nmonotonic_increasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically increasing relationship to the target.\nPassing None disables increasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nmonotonic_decreasing_featurelist_id (Optional[str]) – (New in version v2.18) optional, the id of the featurelist that defines\nthe set of features with a monotonically decreasing relationship to the target.\nPassing None disables decreasing monotonicity constraint. Default\n(dr.enums.MONOTONICITY_FEATURELIST_DEFAULT) is the one specified by the blueprint.\nn_clusters (Optional[int]) – The number of clusters to use in the specified unsupervised clustering model.\nONLY VALID IN UNSUPERVISED CLUSTERING PROJECTS\nReturns:\njob – the created job to build the model\nReturn type:\nModelJob\nblend(model_ids, blender_method)\nSubmit a job for creating blender model. Upon success, the new job will\nbe added to the end of the queue.\nParameters:\nmodel_ids (List[str]) – List of model ids that will be used to create blender. These models should have\ncompleted validation stage without errors, and can’t be blenders or DataRobot Prime\nblender_method (str) – Chosen blend method, one from datarobot.enums.BLENDER_METHOD. If this is a time\nseries project, only methods in datarobot.enums.TS_BLENDER_METHOD are allowed.\nReturns:\nmodel_job – New ModelJob instance for the blender creation job in queue.\nReturn type:\nModelJob\nSee also\ndatarobot.models.Project.check_blendableto confirm if models can be blended\ncheck_blendable(model_ids, blender_method)\nCheck if the specified models can be successfully blended\nParameters:\nmodel_ids (List[str]) – List of model ids that will be used to create blender. These models should have\ncompleted validation stage without errors, and can’t be blenders or DataRobot Prime\nblender_method (str) – Chosen blend method, one from datarobot.enums.BLENDER_METHOD. If this is a time\nseries project, only methods in datarobot.enums.TS_BLENDER_METHOD are allowed.\nReturn type:\nEligibilityResult\nstart_prepare_model_for_deployment(model_id)\nPrepare a specific model for deployment.\nThe requested model will be trained on the maximum autopilot size then go through the\nrecommendation stages. For datetime partitioned projects, this includes the feature impact\nstage, retraining on a reduced feature list, and retraining the best of the reduced\nfeature list model and the max autopilot original model on recent data. For non-datetime\npartitioned projects, this includes the feature impact stage, retraining on a reduced\nfeature list, retraining the best of the reduced feature list model and the max autopilot\noriginal model up to the holdout size, then retraining the up-to-the holdout model on the\nfull dataset.\nParameters:\nmodel_id (str) – The model to prepare for deployment.\nReturn type:\nNone\nget_all_jobs(status=None)\nGet a list of jobs\nThis will give Jobs representing any type of job, including modeling or predict jobs.\nParameters:\nstatus (QUEUE_STATUS enum, optional) – If called with QUEUE_STATUS.INPROGRESS, will return the jobs\nthat are currently running.\nIf called with QUEUE_STATUS.QUEUE, will return the jobs that\nare waiting to be run.\nIf called with QUEUE_STATUS.ERROR, will return the jobs that\nhave errored.\nIf no value is provided, will return all jobs currently running\nor waiting to be run.\nReturns:\njobs – Each is an instance of Job\nReturn type:\nlist\nget_blenders()\nGet a list of blender models.\nReturns:\nlist of all blender models in project.\nReturn type:\nlist of BlenderModel\nget_frozen_models()\nGet a list of frozen models\nReturns:\nlist of all frozen models in project.\nReturn type:\nlist of FrozenModel\nget_combined_models()\nGet a list of models in segmented project.\nReturns:\nlist of all combined models in segmented project.\nReturn type:\nlist of CombinedModel\nget_active_combined_model()\nRetrieve currently active combined model in segmented project.\nReturns:\ncurrently active combined model in segmented project.\nReturn type:\nCombinedModel\nget_segments_models(combined_model_id=None)\nRetrieve a list of all models belonging to the segments/child projects\nof the segmented project.\nParameters:\ncombined_model_id (Optional[str]) – Id of the combined model to get segments for. If there is only a single\ncombined model it can be retrieved automatically, but this must be\nspecified when there are > 1 combined models.\nReturns:\nsegments_models – A list of dictionaries containing all of the segments/child projects,\neach with a list of their models ordered by metric from best to worst.\nReturn type:\nlist(dict)\nget_model_jobs(status=None)\nGet a list of modeling jobs\nParameters:\nstatus (QUEUE_STATUS enum, optional) – If called with QUEUE_STATUS.INPROGRESS, will return the modeling jobs\nthat are currently running.\nIf called with QUEUE_STATUS.QUEUE, will return the modeling jobs that\nare waiting to be run.\nIf called with QUEUE_STATUS.ERROR, will return the modeling jobs that\nhave errored.\nIf no value is provided, will return all modeling jobs currently running\nor waiting to be run.\nReturns:\njobs – Each is an instance of ModelJob\nReturn type:\nlist\nget_predict_jobs(status=None)\nGet a list of prediction jobs\nParameters:\nstatus (QUEUE_STATUS enum, optional) – If called with QUEUE_STATUS.INPROGRESS, will return the prediction jobs\nthat are currently running.\nIf called with QUEUE_STATUS.QUEUE, will return the prediction jobs that\nare waiting to be run.\nIf called with QUEUE_STATUS.ERROR, will return the prediction jobs that\nhave errored.\nIf called without a status, will return all prediction jobs currently running\nor waiting to be run.\nReturns:\njobs – Each is an instance of PredictJob\nReturn type:\nlist\nwait_for_autopilot(check_interval=20.0, timeout=86400, verbosity=1)\nBlocks until autopilot is finished. This will raise an exception if the autopilot\nmode is changed from AUTOPILOT_MODE.FULL_AUTO.\nIt makes API calls to sync the project state with the server and to look at\nwhich jobs are enqueued.\nParameters:\ncheck_interval (float or int) – The maximum time (in seconds) to wait between checks for whether autopilot is finished\ntimeout (float or int or None) – After this long (in seconds), we give up. If None, never timeout.\nverbosity (Union[int, Enum]) – This should be VERBOSITY_LEVEL.SILENT or VERBOSITY_LEVEL.VERBOSE.\nFor VERBOSITY_LEVEL.SILENT, nothing will be displayed about progress.\nFor VERBOSITY_LEVEL.VERBOSE, the number of jobs in progress or queued is shown.\nNote that new jobs are added to the queue along the way.\nRaises:\nAsyncTimeoutError – If autopilot does not finished in the amount of time specified\nRuntimeError – If a condition is detected that indicates that autopilot will not complete\non its own\nReturn type:\nNone\nrename(project_name)\nUpdate the name of the project.\nParameters:\nproject_name (str) – The new name\nReturn type:\nNone\nset_project_description(project_description)\nSet or Update the project description.\nParameters:\nproject_description (str) – The new description for this project.\nReturn type:\nNone\nunlock_holdout()\nUnlock the holdout for this project.\nThis will cause subsequent queries of the models of this project to\ncontain the metric values for the holdout set, if it exists.\nTake care, as this cannot be undone. Remember that best practice is to\nselect a model before analyzing the model performance on the holdout set\nReturn type:\nNone\nset_worker_count(worker_count)\nSets the number of workers allocated to this project.\nNote that this value is limited to the number allowed by your account.\nLowering the number will not stop currently running jobs, but will\ncause the queue to wait for the appropriate number of jobs to finish\nbefore attempting to run more jobs.\nParameters:\nworker_count (int) – The number of concurrent workers to request from the pool of workers.\n(New in version v2.14) Setting this to -1 will update the number of workers to the\nmaximum available to your account.\nReturn type:\nNone\nset_advanced_options(advanced_options=None, **kwargs)\nUpdate the advanced options of this project.\nNotes\nProject options will not be stored at the database level, so the options\nset via this method will only be attached to a project instance for the lifetime of a\nclient session (if you quit your session and reopen a new one before running autopilot,\nthe advanced options will be lost).\nEither accepts an AdvancedOptions object to replace all advanced options or individual keyword\narguments. This is an inplace update, not a new object. The options set will only remain for the\nlife of this project instance within a given session.\nParameters:\nadvanced_options (AdvancedOptions, optional) – AdvancedOptions instance as an alternative to passing individual parameters.\nweights (string, optional) – The name of a column indicating the weight of each row\nresponse_cap (float in [0.5, 1), optional) – Quantile of the response distribution to use for response capping.\nblueprint_threshold (Optional[int]) – Number of hours models are permitted to run before being excluded from later autopilot\nstages\nMinimum 1\nseed (Optional[int]) – a seed to use for randomization\nsmart_downsampled (Optional[bool]) – whether to use smart downsampling to throw away excess rows of the majority class.  Only\napplicable to classification and zero-boosted regression projects.\nmajority_downsampling_rate (Optional[float]) – The percentage between 0 and 100 of the majority rows that should be kept.  Specify only if\nusing smart downsampling.  May not cause the majority class to become smaller than the\nminority class.\noffset (list of Optional[str]) – (New in version v2.6) the list of the names of the columns containing the offset\nof each row\nexposure (string, optional) – (New in version v2.6) the name of a column containing the exposure of each row\naccuracy_optimized_mb (Optional[bool]) – (New in version v2.6) Include additional, longer-running models that will be run by the\nautopilot and available to run manually.\nevents_count (string, optional) – (New in version v2.8) the name of a column specifying events count.\nmonotonic_increasing_featurelist_id (string, optional) – (new in version 2.11) the id of the featurelist that defines the set of features\nwith a monotonically increasing relationship to the target. If None,\nno such constraints are enforced. When specified, this will set a default for the project\nthat can be overridden at model submission time if desired.\nmonotonic_decreasing_featurelist_id (string, optional) – (new in version 2.11) the id of the featurelist that defines the set of features\nwith a monotonically decreasing relationship to the target. If None,\nno such constraints are enforced. When specified, this will set a default for the project\nthat can be overridden at model submission time if desired.\nonly_include_monotonic_blueprints (Optional[bool]) – (new in version 2.11) when true, only blueprints that support enforcing\nmonotonic constraints will be available in the project or selected for the autopilot.\nallowed_pairwise_interaction_groups (list of tuple, optional) – (New in version v2.19) For GA2M models - specify groups of columns for which pairwise\ninteractions will be allowed. E.g. if set to [(A, B, C), (C, D)] then GA2M models will\nallow interactions between columns A x B, B x C, A x C, C x D. All others (A x D, B x D) will\nnot be considered.\nblend_best_models (Optional[bool]) – (New in version v2.19) blend best models during Autopilot run\nscoring_code_only (Optional[bool]) – (New in version v2.19) Keep only models that can be converted to scorable java code\nduring Autopilot run\nshap_only_mode (Optional[bool]) – (New in version v2.21) Keep only models that support SHAP values during Autopilot run. Use\nSHAP-based insights wherever possible. Defaults to False.\nprepare_model_for_deployment (Optional[bool]) – (New in version v2.19) Prepare model for deployment during Autopilot run.\nThe preparation includes creating reduced feature list models, retraining best model\non higher sample size, computing insights and assigning “RECOMMENDED FOR DEPLOYMENT” label.\nconsider_blenders_in_recommendation (Optional[bool]) – (New in version 2.22.0) Include blenders when selecting a model to prepare for\ndeployment in an Autopilot Run. Defaults to False.\nmin_secondary_validation_model_count (Optional[int]) – (New in version v2.19) Compute “All backtest” scores (datetime models) or cross validation\nscores for the specified number of highest ranking models on the Leaderboard,\nif over the Autopilot default.\nautopilot_data_sampling_method (Optional[str]) – (New in version v2.23) one of datarobot.enums.DATETIME_AUTOPILOT_DATA_SAMPLING_METHOD.\nApplicable for OTV projects only, defines if autopilot uses “random” or “latest” sampling\nwhen iteratively building models on various training samples. Defaults to “random” for\nduration-based projects and to “latest” for row-based projects.\nrun_leakage_removed_feature_list (Optional[bool]) – (New in version v2.23) Run Autopilot on Leakage Removed feature list (if exists).\nautopilot_with_feature_discovery (Optional[bool].) – (New in version v2.23) If true, autopilot will run on a feature list that includes features\nfound via search for interactions.\nfeature_discovery_supervised_feature_reduction (Optional[bool]) – (New in version v2.23) Run supervised feature reduction for feature discovery projects.\nexponentially_weighted_moving_alpha (Optional[float]) – (New in version v2.26) defaults to None, value between 0 and 1 (inclusive), indicates\nalpha parameter used in exponentially weighted moving average within feature derivation\nwindow.\nexternal_time_series_baseline_dataset_id (Optional[str].) – (New in version v2.26) If provided, will generate metrics scaled by external model\npredictions metric for time series projects. The external predictions catalog\nmust be validated before autopilot starts, see\nProject.validate_external_time_series_baseline and\nexternal baseline predictions documentation\nfor further explanation.\nuse_supervised_feature_reduction (bool, default True optional) – Time Series only. When true, during feature generation DataRobot runs a supervised\nalgorithm to retain only qualifying features. Setting to false can\nseverely impact autopilot duration, especially for datasets with many features.\nprimary_location_column (Optional[str].) – The name of primary location column.\nprotected_features (list of Optional[str].) – (New in version v2.24) A list of project features to mark as protected for\nBias and Fairness testing calculations. Max number of protected features allowed is 10.\npreferable_target_value (Optional[str].) – (New in version v2.24) A target value that should be treated as a favorable outcome\nfor the prediction. For example, if we want to check gender discrimination for\ngiving a loan and our target is named is_bad, then the positive outcome for\nthe prediction would be No, which means that the loan is good and that’s\nwhat we treat as a favorable result for the loaner.\nfairness_metrics_set (Optional[str].) – (New in version v2.24) Metric to use for calculating fairness.\nCan be one of proportionalParity, equalParity, predictionBalance,\ntrueFavorableAndUnfavorableRateParity or\nfavorableAndUnfavorablePredictiveValueParity.\nUsed and required only if Bias & Fairness in AutoML feature is enabled.\nfairness_threshold (Optional[str].) – (New in version v2.24) Threshold value for the fairness metric.\nCan be in a range of [0.0, 1.0]. If the relative (i.e. normalized) fairness\nscore is below the threshold, then the user will see a visual indication on the\nbias_mitigation_feature_name (Optional[str]) – The feature from protected features that will be used in a bias mitigation task to\nmitigate bias\nbias_mitigation_technique (Optional[str]) – One of datarobot.enums.BiasMitigationTechnique\nOptions:\n- ‘preprocessingReweighing’\n- ‘postProcessingRejectionOptionBasedClassification’\nThe technique by which we’ll mitigate bias, which will inform which bias mitigation task\nwe insert into blueprints\ninclude_bias_mitigation_feature_as_predictor_variable (Optional[bool]) – Whether we should also use the mitigation feature as in input to the modeler just like\nany other categorical used for training, i.e. do we want the model to “train on” this\nfeature in addition to using it for bias mitigation\nseries_id (string, optional) – (New in version v3.6) The name of a column containing the series ID for each row.\nforecast_distance (string, optional) – (New in version v3.6) The name of a column containing the forecast distance for each row.\nforecast_offsets (list of Optional[str]) – (New in version v3.6) The list of the names of the columns containing the forecast offsets\nfor each row.\nincremental_learning_only_mode (Optional[bool]) – (New in version v3.4) Keep only models that support incremental learning during Autopilot run.\nincremental_learning_on_best_model (Optional[bool]) – (New in version v3.4) Run incremental learning on the best model during Autopilot run.\nchunk_definition_id (string, optional) – (New in version v3.4) Unique definition for chunks needed to run automated incremental learning.\nincremental_learning_early_stopping_rounds (Optional[int]) – (New in version v3.4) Early stopping rounds used in the automated incremental learning service.\nnumber_of_incremental_learning_iterations_before_best_model_selection (Optional[int] = None) – (New in version v3.6) Number of iterations top 5 models complete prior to best model selection.\nThe minimum is 1, which means no additional iterations after the first iteration (initial model)\nwill be run. The maximum is 10.\nReturn type:\nNone\nlist_advanced_options()\nView the advanced options that have been set on a project instance.\nIncludes those that haven’t been set (with value of None).\nReturn type:\ndict of advanced options and their values\nset_partitioning_method(cv_method=None, validation_type=None, seed=0, reps=None, user_partition_col=None, training_level=None, validation_level=None, holdout_level=None, cv_holdout_level=None, validation_pct=None, holdout_pct=None, partition_key_cols=None, partitioning_method=None)\nConfigures the partitioning method for this project.\nIf this project does not already have a partitioning method set, creates\na new configuration based on provided args.\nIf the partitioning_method arg is set, that configuration will instead be used.\nNotes\nThis is an inplace update, not a new object. The options set will only remain for the\nlife of this project instance within a given session. You must still call set_target\nto make this change permanent for the project. Calling refresh without first calling\nset_target will invalidate this configuration. Similarly, calling get to retrieve a\nsecond copy of the project will not include this configuration.\nAdded in version v3.0.\nParameters:\ncv_method (str) – The partitioning method used. Supported values can be found in datarobot.enums.CV_METHOD.\nvalidation_type (str) – May be “CV” (K-fold cross-validation) or “TVH” (Training, validation, and holdout).\nseed (int) – A seed to use for randomization.\nreps (int) – Number of cross validation folds to use.\nuser_partition_col (str) – The name of the column containing the partition assignments.\ntraining_level (Union[str,int]) – The value of the partition column indicating a row is part of the training set.\nvalidation_level (Union[str,int]) – The value of the partition column indicating a row is part of the validation set.\nholdout_level (Union[str,int]) – The value of the partition column indicating a row is part of the holdout set (use\nNone if you want no holdout set).\ncv_holdout_level (Union[str,int]) – The value of the partition column indicating a row is part of the holdout set.\nvalidation_pct (int) – The desired percentage of dataset to assign to validation set.\nholdout_pct (int) – The desired percentage of dataset to assign to holdout set.\npartition_key_cols (list) – A list containing a single string, where the string is the name of the column whose\nvalues should remain together in partitioning.\npartitioning_method (PartitioningMethod, optional) – An instance of datarobot.helpers.partitioning_methods.PartitioningMethod that will\nbe used instead of creating a new instance from the other args.\nRaises:\nTypeError – If cv_method or validation_type are not set and partitioning_method is not set.\nInvalidUsageError – If invoked after project.set_target or project.start, or\nif invoked with the wrong combination of args for a given partitioning method.\nReturns:\nproject – The instance with updated attributes.\nReturn type:\nProject\nget_uri()\nReturns:\nurl – Permanent static hyperlink to a project leaderboard.\nReturn type:\nstr\nget_rating_table_models()\nGet a list of models with a rating table\nReturns:\nlist of all models with a rating table in project.\nReturn type:\nlist of RatingTableModel\nget_rating_tables()\nGet a list of rating tables\nReturns:\nlist of rating tables in project.\nReturn type:\nlist of RatingTable\nget_access_list()\nRetrieve users who have access to this project and their access levels\nAdded in version v2.15.\nReturn type:\nlist of SharingAccess\nshare(access_list, send_notification=None, include_feature_discovery_entities=None)\nModify the ability of users to access this project\nAdded in version v2.15.\nParameters:\naccess_list (list of SharingAccess) – the modifications to make.\nsend_notification (boolean, default None) – (New in version v2.21) optional, whether or not an email notification should be sent,\ndefault to None\ninclude_feature_discovery_entities (boolean, default None) – (New in version v2.21) optional (default: None), whether or not to share all the\nrelated entities i.e., datasets for a project with Feature Discovery enabled\nReturn type:\nNone\nRaises:\ndatarobot.ClientError : – if you do not have permission to share this project, if the user you’re sharing with\ndoesn’t exist, if the same user appears multiple times in the access_list, or if these\nchanges would leave the project without an owner\nExamples\nTransfer access to the project from old_user@datarobot.com to new_user@datarobot.com\nimport datarobot as dr\nnew_access = dr.SharingAccess(new_user@datarobot.com,\ndr.enums.SHARING_ROLE.OWNER, can_share=True)\naccess_list = [dr.SharingAccess(old_user@datarobot.com, None), new_access]\ndr.Project.get('my-project-id').share(access_list)\nbatch_features_type_transform(parent_names, variable_type, prefix=None, suffix=None, max_wait=600)\nCreate new features by transforming the type of existing ones.\nAdded in version v2.17.\nNotes\nThe following transformations are only supported in batch mode:\nText to categorical or numeric\nCategorical to text or numeric\nNumeric to categorical\nSee {ref}`here <type-transform-considerations>` for special considerations when casting\nnumeric to categorical.\nDate to categorical or numeric transformations are not currently supported for batch\nmode but can be performed individually using create_type_transform_feature.\nParameters:\nparent_names (list[str]) – The list of variable names to be transformed.\nvariable_type (str) – The type new columns should have. Can be one of ‘categorical’, ‘categoricalInt’,\n‘numeric’, and ‘text’ - supported values can be found in\ndatarobot.enums.VARIABLE_TYPE_TRANSFORM.\nprefix (Optional[str]) –\nNotes\nEither prefix, suffix, or both must be provided.\nThe string that will preface all feature names. At least one of prefix and\nsuffix must be specified.\nsuffix (Optional[str]) –\nNotes\nEither prefix, suffix, or both must be provided.\nThe string that will be appended at the end to all feature names. At least one of\nprefix and suffix must be specified.\nmax_wait (Optional[int]) – The maximum amount of time to wait for DataRobot to finish processing the new column.\nThis process can take more time with more data to process. If this operation times\nout, an AsyncTimeoutError will occur. DataRobot continues the processing and the\nnew column may successfully be constructed.\nReturns:\nall features for this project after transformation.\nReturn type:\nlist of Features\nRaises:\nTypeError: – If parent_names is not a list.\nValueError – If value of variable_type is not from datarobot.enums.VARIABLE_TYPE_TRANSFORM.\nAsyncFailureError – If any of the responses from the server are unexpected.\nAsyncProcessUnsuccessfulError – If the job being waited for has failed or has been cancelled.\nAsyncTimeoutError – If the resource did not resolve in time.\nclone_project(new_project_name=None, max_wait=600)\nCreate a fresh (post-EDA1) copy of this project that is ready for setting\ntargets and modeling options.\nParameters:\nnew_project_name (Optional[str]) – The desired name of the new project. If omitted, the API will default to\n‘Copy of <original project>’\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered\nunsuccessful\nReturn type:\ndatarobot.models.Project\ncreate_interaction_feature(name, features, separator, max_wait=600)\nCreate a new interaction feature by combining two categorical ones.\nAdded in version v2.21.\nParameters:\nname (str) – The name of final Interaction Feature\nfeatures (list(str)) – List of two categorical feature names\nseparator (str) – The character used to join the two data values, one of these ` + - / | & . _ , `\nmax_wait (Optional[int]) – Time in seconds after which project creation is considered unsuccessful.\nReturns:\nThe data of the new Interaction feature\nReturn type:\ndatarobot.models.InteractionFeature\nRaises:\nClientError – If requested Interaction feature can not be created. Possible reasons for example are:\n* one of features either does not exist or is of unsupported type\n* feature with requested name already exists\n* invalid separator character submitted.\nAsyncFailureError – If any of the responses from the server are unexpected\nAsyncProcessUnsuccessfulError – If the job being waited for has failed or has been cancelled\nAsyncTimeoutError – If the resource did not resolve in time\nget_relationships_configuration()\nGet the relationships configuration for a given project\nAdded in version v2.21.\nReturns:\nrelationships_configuration – relationships configuration applied to project\nReturn type:\nRelationshipsConfiguration\ndownload_feature_discovery_dataset(file_name, pred_dataset_id=None)\nDownload Feature discovery training or prediction dataset\nParameters:\nfile_name (str) – File path where dataset will be saved.\npred_dataset_id (Optional[str]) – ID of the prediction dataset\nReturn type:\nNone\ndownload_feature_discovery_recipe_sqls(file_name, model_id=None, max_wait=600)\nExport and download Feature discovery recipe SQL statements\n.. versionadded:: v2.25\nParameters:\nfile_name (str) – File path where dataset will be saved.\nmodel_id (Optional[str]) – ID of the model to export SQL for.\nIf specified, QL to generate only features used by the model will be exported.\nIf not specified, SQL to generate all features will be exported.\nmax_wait (Optional[int]) – Time in seconds after which export is considered unsuccessful.\nRaises:\nClientError – If requested SQL cannot be exported. Possible reason is the feature is not\navailable to user.\nAsyncFailureError – If any of the responses from the server are unexpected.\nAsyncProcessUnsuccessfulError – If the job being waited for has failed or has been cancelled.\nAsyncTimeoutError – If the resource did not resolve in time.\nReturn type:\nNone\nvalidate_external_time_series_baseline(catalog_version_id, target, datetime_partitioning, max_wait=600)\nValidate external baseline prediction catalog.\nThe forecast windows settings, validation and holdout duration specified in the\ndatetime specification must be consistent with project settings as these parameters\nare used to check whether the specified catalog version id has been validated or not.\nSee external baseline predictions documentation\nfor example usage.\nParameters:\ncatalog_version_id (str) – Id of the catalog version for validating external baseline predictions.\ntarget (str) – The name of the target column.\ndatetime_partitioning (DatetimePartitioning object) – Instance of the DatetimePartitioning defined in\ndatarobot.helpers.partitioning_methods.\nAttributes of the object used to check the validation are:\ndatetime_partition_column\nforecast_window_start\nforecast_window_end\nholdout_start_date\nholdout_end_date\nbacktests\nmultiseries_id_columns\nIf the above attributes are different from the project settings, the catalog version\nwill not pass the validation check in the autopilot.\nmax_wait (Optional[int]) – The maximum number of seconds to wait for the catalog version to be validated before\nraising an error.\nReturns:\nexternal_baseline_validation_info – Validation result of the specified catalog version.\nReturn type:\nExternalBaselineValidationInfo\nRaises:\nAsyncTimeoutError – Raised if the catalog version validation took more time than specified\nby the max_wait parameter.\ndownload_multicategorical_data_format_errors(file_name)\nDownload multicategorical data format errors to the CSV file. If any format errors\nwhere detected in potentially multicategorical features the resulting file will contain\nat max 10 entries. CSV file content contains feature name, dataset index in which the\nerror was detected, row value and type of error detected. In case that there were no\nerrors or none of the features where potentially multicategorical the CSV file will be\nempty containing only the header.\nParameters:\nfile_name (str) – File path where CSV file will be saved.\nReturn type:\nNone\nget_multiseries_names()\nFor a multiseries timeseries project it returns all distinct entries in the\nmultiseries column. For a non timeseries project it will just return an empty list.\nReturns:\nmultiseries_names – List of all distinct entries in the multiseries column\nReturn type:\nList[str]\nrestart_segment(segment)\nRestart single segment in a segmented project.\nAdded in version v2.28.\nSegment restart is allowed only for segments that haven’t reached modeling phase.\nRestart will permanently remove previous project and trigger set up of a new one\nfor particular segment.\nParameters:\nsegment (str) – Segment to restart\nget_bias_mitigated_models(parent_model_id=None, offset=0, limit=100)\nList the child models with bias mitigation applied\nAdded in version v2.29.\nParameters:\nparent_model_id (Optional[str]) – Filter by parent models\noffset (Optional[int]) – Number of items to skip.\nlimit (Optional[int]) – Number of items to return.\nReturns:\nmodels\nReturn type:\nlist of dict\napply_bias_mitigation(bias_mitigation_parent_leaderboard_id, bias_mitigation_feature_name, bias_mitigation_technique, include_bias_mitigation_feature_as_predictor_variable)\nApply bias mitigation to an existing model by training a version of that model but with\nbias mitigation applied.\nAn error will be returned if the model does not support bias mitigation with the technique\nrequested.\nAdded in version v2.29.\nParameters:\nbias_mitigation_parent_leaderboard_id (str) – The leaderboard id of the model to apply bias mitigation to\nbias_mitigation_feature_name (str) – The feature name of the protected features that will be used in a bias mitigation task to\nattempt to mitigate bias\nbias_mitigation_technique (Optional[str]) – One of datarobot.enums.BiasMitigationTechnique\nOptions:\n- ‘preprocessingReweighing’\n- ‘postProcessingRejectionOptionBasedClassification’\nThe technique by which we’ll mitigate bias, which will inform which bias mitigation task\nwe insert into blueprints\ninclude_bias_mitigation_feature_as_predictor_variable (bool) – Whether we should also use the mitigation feature as in input to the modeler just like\nany other categorical used for training, i.e. do we want the model to “train on” this\nfeature in addition to using it for bias mitigation\nReturns:\nthe job of the model with bias mitigation applied that was just submitted for training\nReturn type:\nModelJob\nrequest_bias_mitigation_feature_info(bias_mitigation_feature_name)\nRequest a compute job for bias mitigation feature info for a given feature, which will\ninclude\n- if there are any rare classes\n- if there are any combinations of the target values and the feature values that never occur\nin the same row\n- if the feature has a high number of missing values.\nNote that this feature check is dependent on the current target selected for the project.\nAdded in version v2.29.\nParameters:\nbias_mitigation_feature_name (str) – The feature name of the protected features that will be used in a bias mitigation task to\nattempt to mitigate bias\nReturns:\nBias mitigation feature info model for the requested feature\nReturn type:\nBiasMitigationFeatureInfo\nget_bias_mitigation_feature_info(bias_mitigation_feature_name)\nGet the computed bias mitigation feature info for a given feature, which will include\n- if there are any rare classes\n- if there are any combinations of the target values and the feature values that never occur\nin the same row\n- if the feature has a high number of missing values.\nNote that this feature check is dependent on the current target selected for the project.\nIf this info has not already been computed, this will raise a 404 error.\nAdded in version v2.29.\nParameters:\nbias_mitigation_feature_name (str) – The feature name of the protected features that will be used in a bias mitigation task to\nattempt to mitigate bias\nReturns:\nBias mitigation feature info model for the requested feature\nReturn type:\nBiasMitigationFeatureInfo\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nset_datetime_partitioning(datetime_partition_spec=None, **kwargs)\nSet the datetime partitioning method for a time series project by either passing in\na DatetimePartitioningSpecification instance or any individual attributes of that class.\nUpdates self.partitioning_method if already set previously (does not replace it).\nThis is an alternative to passing a specification to\nProject.analyze_and_model via the\npartitioning_method parameter. To see the\nfull partitioning based on the project dataset, use\nDatetimePartitioning.generate.\nAdded in version v3.0.\nParameters:\ndatetime_partition_spec (DatetimePartitioningSpecification) – DatetimePartitioningSpecification,\noptional\nThe customizable aspects of datetime partitioning for a time series project. An alternative\nto passing individual settings (attributes of the DatetimePartitioningSpecification class).\nReturns:\nFull partitioning including user-specified attributes as well as those determined by DR\nbased on the dataset.\nReturn type:\nDatetimePartitioning\nlist_datetime_partition_spec()\nList datetime partitioning settings.\nThis method makes an API call to retrieve settings from the DB if project is in the modeling\nstage, i.e. if analyze_and_model (autopilot) has already been called.\nIf analyze_and_model has not yet been called, this method will instead simply print\nsettings from project.partitioning_method.\nAdded in version v3.0.\nReturn type:\nDatetimePartitioningSpecification or None\nclass datarobot.helpers.eligibility_result.EligibilityResult\nRepresents whether a particular operation is supported\nFor instance, a function to check whether a set of models can be blended can return an\nEligibilityResult specifying whether or not blending is supported and why it may not be\nsupported.\nVariables:\nsupported (bool) – whether the operation this result represents is supported\nreason (str) – why the operation is or is not supported\ncontext (str) – what operation isn’t supported\nAdvanced options\nclass datarobot.helpers.AdvancedOptions\nUsed when setting the target of a project to set advanced options of modeling process.\nParameters:\nweights (Optional[str]) – The name of a column indicating the weight of each row\nresponse_cap (Optional[bool] or Optional[float in [0.5, 1)]) – Defaults to none here, but server defaults to False.\nIf specified, it is the quantile of the response distribution to use for response capping.\nblueprint_threshold (Optional[int]) – Number of hours models are permitted to run before being excluded from later autopilot\nstages\nMinimum 1\nseed (Optional[int]) – a seed to use for randomization\nsmart_downsampled (Optional[bool]) – whether to use smart downsampling to throw away excess rows of the majority class.  Only\napplicable to classification and zero-boosted regression projects.\nmajority_downsampling_rate (Optional[float]) – the percentage between 0 and 100 of the majority rows that should be kept.  Specify only if\nusing smart downsampling.  May not cause the majority class to become smaller than the\nminority class.\noffset (list of Optional[str]) – (New in version v2.6) the list of the names of the columns containing the offset\nof each row\nexposure (Optional[str]) – (New in version v2.6) the name of a column containing the exposure of each row\naccuracy_optimized_mb (Optional[bool]) – (New in version v2.6) Include additional, longer-running models that will be run by the\nautopilot and available to run manually.\nscaleout_modeling_mode (Optional[str]) – (Deprecated in 2.28. Will be removed in 2.30) DataRobot no longer supports scaleout models.\nPlease remove any usage of this parameter as it will be removed from the API soon.\nevents_count (Optional[str]) – (New in version v2.8) the name of a column specifying events count.\nmonotonic_increasing_featurelist_id (Optional[str]) – (new in version 2.11) the id of the featurelist that defines the set of features\nwith a monotonically increasing relationship to the target. If None,\nno such constraints are enforced. When specified, this will set a default for the project\nthat can be overridden at model submission time if desired.\nmonotonic_decreasing_featurelist_id (Optional[str]) – (new in version 2.11) the id of the featurelist that defines the set of features\nwith a monotonically decreasing relationship to the target. If None,\nno such constraints are enforced. When specified, this will set a default for the project\nthat can be overridden at model submission time if desired.\nonly_include_monotonic_blueprints (Optional[bool]) – (new in version 2.11) when true, only blueprints that support enforcing\nmonotonic constraints will be available in the project or selected for the autopilot.\nallowed_pairwise_interaction_groups (Optional[List[Tuple[str, ]]]) – (New in version v2.19) For GA2M models - specify groups of columns for which pairwise\ninteractions will be allowed. E.g. if set to [(A, B, C), (C, D)] then GA2M models will\nallow interactions between columns A x B, B x C, A x C, C x D. All others (A x D, B x D) will\nnot be considered.\nblend_best_models (Optional[bool]) – (New in version v2.19) blend best models during Autopilot run.\nscoring_code_only (Optional[bool]) – (New in version v2.19) Keep only models that can be converted to scorable java code\nduring Autopilot run\nshap_only_mode (Optional[bool]) – (New in version v2.21) Keep only models that support SHAP values during Autopilot run. Use\nSHAP-based insights wherever possible. Defaults to False.\nprepare_model_for_deployment (Optional[bool]) – (New in version v2.19) Prepare model for deployment during Autopilot run.\nThe preparation includes creating reduced feature list models, retraining best model\non higher sample size, computing insights and assigning “RECOMMENDED FOR DEPLOYMENT” label.\nconsider_blenders_in_recommendation (Optional[bool]) – (New in version 2.22.0) Include blenders when selecting a model to prepare for\ndeployment in an Autopilot Run. Defaults to False.\nmin_secondary_validation_model_count (Optional[int]) – (New in version v2.19) Compute “All backtest” scores (datetime models) or cross validation\nscores for the specified number of the highest ranking models on the Leaderboard,\nif over the Autopilot default.\nautopilot_data_sampling_method (Optional[str]) – (New in version v2.23) one of datarobot.enums.DATETIME_AUTOPILOT_DATA_SAMPLING_METHOD.\nApplicable for OTV projects only, defines if autopilot uses “random” or “latest” sampling\nwhen iteratively building models on various training samples. Defaults to “random” for\nduration-based projects and to “latest” for row-based projects.\nrun_leakage_removed_feature_list (Optional[bool]) – (New in version v2.23) Run Autopilot on Leakage Removed feature list (if exists).\nautopilot_with_feature_discovery (Optional[bool]) – default = False\n(New in version v2.23) If true, autopilot will run on a feature list that includes features\nfound via search for interactions.\nfeature_discovery_supervised_feature_reduction (Optional[bool]) – (New in version v2.23) Run supervised feature reduction for feature discovery projects.\nexponentially_weighted_moving_alpha (Optional[float]) – (New in version v2.26) defaults to None, value between 0 and 1 (inclusive), indicates\nalpha parameter used in exponentially weighted moving average within feature derivation\nwindow.\nexternal_time_series_baseline_dataset_id (Optional[str]) – (New in version v2.26) If provided, will generate metrics scaled by external model\npredictions metric for time series projects. The external predictions catalog\nmust be validated before autopilot starts, see\nProject.validate_external_time_series_baseline and\nexternal baseline predictions documentation\nfor further explanation.\nuse_supervised_feature_reduction (Optional[bool]) – defaults to True,\nTime Series only. When true, during feature generation DataRobot runs a supervised\nalgorithm to retain only qualifying features. Setting to false can\nseverely impact autopilot duration, especially for datasets with many features.\nprimary_location_column (Optional[str].) – The name of primary location column.\nprotected_features (list of Optional[str].) – (New in version v2.24) A list of project features to mark as protected for\nBias and Fairness testing calculations. Max number of protected features allowed is 10.\npreferable_target_value (Optional[str].) – (New in version v2.24) A target value that should be treated as a favorable outcome\nfor the prediction. For example, if we want to check gender discrimination for\ngiving a loan and our target is named is_bad, then the positive outcome for\nthe prediction would be No, which means that the loan is good and that’s\nwhat we treat as a favorable result for the loaner.\nfairness_metrics_set (Optional[str].) – (New in version v2.24) Metric to use for calculating fairness.\nCan be one of proportionalParity, equalParity, predictionBalance,\ntrueFavorableAndUnfavorableRateParity or\nfavorableAndUnfavorablePredictiveValueParity.\nUsed and required only if Bias & Fairness in AutoML feature is enabled.\nfairness_threshold (Optional[str].) – (New in version v2.24) Threshold value for the fairness metric.\nCan be in a range of [0.0, 1.0]. If the relative (i.e. normalized) fairness\nscore is below the threshold, then the user will see a visual indication on the\nbias_mitigation_feature_name (Optional[str]) – The feature from protected features that will be used in a bias mitigation task to\nmitigate bias\nbias_mitigation_technique (Optional[str]) – One of datarobot.enums.BiasMitigationTechnique\nOptions:\n- ‘preprocessingReweighing’\n- ‘postProcessingRejectionOptionBasedClassification’\nThe technique by which we’ll mitigate bias, which will inform which bias mitigation task\nwe insert into blueprints\ninclude_bias_mitigation_feature_as_predictor_variable (Optional[bool]) – Whether we should also use the mitigation feature as in input to the modeler just like\nany other categorical used for training, i.e. do we want the model to “train on” this\nfeature in addition to using it for bias mitigation\ndefault_monotonic_increasing_featurelist_id (Optional[str]) – Returned from server on Project GET request - not able to be updated by user\ndefault_monotonic_decreasing_featurelist_id (Optional[str]) – Returned from server on Project GET request - not able to be updated by user\nmodel_group_id (Optional[str] = None) – (New in version v3.3) The name of a column containing the model group id for each row.\nmodel_regime_id (Optional[str] = None) – (New in version v3.3) The name of a column containing the model regime id for each row.\nmodel_baselines (Optional[List[str]] = None) – (New in version v3.3) The list of the names of the columns containing the model baselines\nseries_id (Optional[str] = None) – (New in version v3.6) The name of a column containing the series id for each row.\nforecast_distance (Optional[str] = None) – (New in version v3.6) The name of a column containing the forecast distance for each row.\nforecast_offsets (Optional[List[str]] = None) – (New in version v3.6) The list of the names of the columns containing the forecast offsets\nfor each row.\nincremental_learning_only_mode (Optional[bool] = None) – (New in version v3.4) Keep only models that support incremental learning during Autopilot run.\nincremental_learning_on_best_model (Optional[bool] = None) – (New in version v3.4) Run incremental learning on the best model during Autopilot run.\nchunk_definition_id (Optional[str]) – (New in version v3.4) Unique definition for chunks needed to run automated incremental learning.\nincremental_learning_early_stopping_rounds (Optional[int] = None) – (New in version v3.4) Early stopping rounds used in the automated incremental learning service.\nnumber_of_incremental_learning_iterations_before_best_model_selection (Optional[int] = None) – (New in version v3.6) Number of iterations top 5 models complete prior to best model selection.\nThe minimum is 1, which means no additional iterations after the first iteration (initial model) will be run.\nThe maximum is 10.\nfeature_engineering_prediction_point (Optional[str] = None) – (New in version v3.7) The date column to be used as the prediction point for time-based feature engineering.\nExamples\nimport datarobot as dr\nadvanced_options = dr.AdvancedOptions(\nweights='weights_column',\noffset=['offset_column'],\nexposure='exposure_column',\nresponse_cap=0.7,\nblueprint_threshold=2,\nsmart_downsampled=True, majority_downsampling_rate=75.0)\nget(_AdvancedOptions__key, _AdvancedOptions__default=None)\nReturn the value for key if key is in the dictionary, else default.\nReturn type:\nOptional[Any]\npop(_AdvancedOptions__key)\nIf the key is not found, return the default if given; otherwise,\nraise a KeyError.\nReturn type:\nOptional[Any]\nupdate_individual_options(**kwargs)\nUpdate individual attributes of an instance of\nAdvancedOptions.\nReturn type:\nNone\nPartitioning\nclass datarobot.RandomCV\nA partition in which observations are randomly assigned to cross-validation groups\nand the holdout set.\nParameters:\nholdout_pct (int) – the desired percentage of dataset to assign to holdout set\nreps (int) – number of cross validation folds to use\nseed (int) – a seed to use for randomization\nclass datarobot.StratifiedCV\nA partition in which observations are randomly assigned to cross-validation groups\nand the holdout set, preserving in each group the same ratio of positive to negative cases as in\nthe original data.\nParameters:\nholdout_pct (int) – the desired percentage of dataset to assign to holdout set\nreps (int) – number of cross validation folds to use\nseed (int) – a seed to use for randomization\nclass datarobot.GroupCV\nA partition in which one column is specified, and rows sharing a common value\nfor that column are guaranteed to stay together in the partitioning into cross-validation\ngroups and the holdout set.\nParameters:\nholdout_pct (int) – the desired percentage of dataset to assign to holdout set\nreps (int) – number of cross validation folds to use\npartition_key_cols (list) – a list containing a single string, where the string is the name of the column whose\nvalues should remain together in partitioning\nseed (int) – a seed to use for randomization\nclass datarobot.UserCV\nA partition where the cross-validation folds and the holdout set are specified by\nthe user.\nParameters:\nuser_partition_col (string) – the name of the column containing the partition assignments\ncv_holdout_level – the value of the partition column indicating a row is part of the holdout set\nseed (int) – a seed to use for randomization\nclass datarobot.RandomTVH\nSpecifies a partitioning method in which rows are randomly assigned to training, validation,\nand holdout.\nParameters:\nholdout_pct (int) – the desired percentage of dataset to assign to holdout set\nvalidation_pct (int) – the desired percentage of dataset to assign to validation set\nseed (int) – a seed to use for randomization\nclass datarobot.UserTVH\nSpecifies a partitioning method in which rows are assigned by the user to training,\nvalidation, and holdout sets.\nParameters:\nuser_partition_col (string) – the name of the column containing the partition assignments\ntraining_level – the value of the partition column indicating a row is part of the training set\nvalidation_level – the value of the partition column indicating a row is part of the validation set\nholdout_level – the value of the partition column indicating a row is part of the holdout set (use\nNone if you want no holdout set)\nseed (int) – a seed to use for randomization\nclass datarobot.StratifiedTVH\nA partition in which observations are randomly assigned to train, validation, and\nholdout sets, preserving in each group the same ratio of positive to negative cases as in the\noriginal data.\nParameters:\nholdout_pct (int) – the desired percentage of dataset to assign to holdout set\nvalidation_pct (int) – the desired percentage of dataset to assign to validation set\nseed (int) – a seed to use for randomization\nclass datarobot.GroupTVH\nA partition in which one column is specified, and rows sharing a common value\nfor that column are guaranteed to stay together in the partitioning into the training,\nvalidation, and holdout sets.\nParameters:\nholdout_pct (int) – the desired percentage of dataset to assign to holdout set\nvalidation_pct (int) – the desired percentage of dataset to assign to validation set\npartition_key_cols (list) – a list containing a single string, where the string is the name of the column whose\nvalues should remain together in partitioning\nseed (int) – a seed to use for randomization\nclass datarobot.DatetimePartitioningSpecification\nUniquely defines a DatetimePartitioning for some project\nIncludes only the attributes of DatetimePartitioning that are directly controllable by users,\nnot those determined by the DataRobot application based on the project dataset and the\nuser-controlled settings.\nThis is the specification that should be passed to Project.analyze_and_model via the partitioning_method parameter. To see the\nfull partitioning based on the project dataset, use DatetimePartitioning.generate.\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nNote that either (holdout_start_date, holdout_duration) or (holdout_start_date,\nholdout_end_date) can be used to specify holdout partitioning settings.\nVariables:\ndatetime_partition_column (str) – the name of the column whose values as dates are used to assign a row\nto a particular partition\nautopilot_data_selection_method (str) – one of datarobot.enums.DATETIME_AUTOPILOT_DATA_SELECTION_METHOD.  Whether models created\nby the autopilot should use “rowCount” or “duration” as their data_selection_method.\nvalidation_duration (str or None) – the default validation_duration for the backtests\nholdout_start_date (datetime.datetime or None) – The start date of holdout scoring data.  If holdout_start_date is specified,\neither holdout_duration or holdout_end_date must also be specified. If\ndisable_holdout is set to True, holdout_start_date, holdout_duration, and\nholdout_end_date may not be specified.\nholdout_duration (str or None) – The duration of the holdout scoring data.  If holdout_duration is specified,\nholdout_start_date must also be specified.  If disable_holdout is set to True,\nholdout_duration, holdout_start_date, and holdout_end_date may not be specified.\nholdout_end_date (datetime.datetime or None) – The end date of holdout scoring data.  If holdout_end_date is specified,\nholdout_start_date must also be specified.  If disable_holdout is set to True,\nholdout_end_date, holdout_start_date, and holdout_duration may not be specified.\ndisable_holdout (bool or None) – (New in version v2.8) Whether to suppress allocating a holdout fold.\nIf set to True, holdout_start_date, holdout_duration, and holdout_end_date\nmay not be specified.\ngap_duration (str or None) – The duration of the gap between training and holdout scoring data\nnumber_of_backtests (int or None) – the number of backtests to  use\nbacktests (list of BacktestSpecification) – the exact specification of backtests to use.  The indices of the specified backtests should\nrange from 0 to number_of_backtests - 1.  If any backtest is left unspecified, a default\nconfiguration will be chosen.\nuse_time_series (bool) – (New in version v2.8) Whether to create a time series project (if True) or an OTV\nproject which uses datetime partitioning (if False).  The default behavior is to create\nan OTV project.\ndefault_to_known_in_advance (bool) – (New in version v2.11) Optional, default False. Used for time series projects only. Sets\nwhether all features default to being treated as known in advance. Known in advance features\nare expected to be known for dates in the future when making predictions, e.g., “is this a\nholiday?”. Individual features can be set to a value different than the default using the\nfeature_settings parameter.\ndefault_to_do_not_derive (bool) – (New in v2.17) Optional, default False. Used for time series projects only. Sets whether\nall features default to being treated as do-not-derive features, excluding them from feature\nderivation. Individual features can be set to a value different than the default by using\nthe feature_settings parameter.\nfeature_derivation_window_start (int or None) – (New in version v2.8) Only used for time series projects. Offset into the past to define how\nfar back relative to the forecast point the feature derivation window should start.\nExpressed in terms of the windows_basis_unit and should be negative value or zero.\nfeature_derivation_window_end (int or None) – (New in version v2.8) Only used for time series projects. Offset into the past to define how\nfar back relative to the forecast point the feature derivation window should end.  Expressed\nin terms of the windows_basis_unit and should be a negative value or zero.\nfeature_settings (list of FeatureSettings) – (New in version v2.9) Optional, a list specifying per feature settings, can be\nleft unspecified.\nforecast_window_start (int or None) – (New in version v2.8) Only used for time series projects. Offset into the future to define\nhow far forward relative to the forecast point the forecast window should start.  Expressed\nin terms of the windows_basis_unit.\nforecast_window_end (int or None) – (New in version v2.8) Only used for time series projects. Offset into the future to define\nhow far forward relative to the forecast point the forecast window should end.  Expressed\nin terms of the windows_basis_unit.\nwindows_basis_unit (string, optional) – (New in version v2.14) Only used for time series projects. Indicates which unit is\na basis for feature derivation window and forecast window. Valid options are detected time\nunit (one of the datarobot.enums.TIME_UNITS) or “ROW”.\nIf omitted, the default value is the detected time unit.\ntreat_as_exponential (string, optional) – (New in version v2.9) defaults to “auto”. Used to specify whether to treat data\nas exponential trend and apply transformations like log-transform. Use values from the\ndatarobot.enums.TREAT_AS_EXPONENTIAL enum.\ndifferencing_method (string, optional) – (New in version v2.9) defaults to “auto”. Used to specify which differencing method to\napply of case if data is stationary. Use values from\ndatarobot.enums.DIFFERENCING_METHOD enum.\nperiodicities (list of Periodicity, optional) – (New in version v2.9) a list of datarobot.Periodicity. Periodicities units\nshould be “ROW”, if the windows_basis_unit is “ROW”.\nmultiseries_id_columns (List[str] or null) – (New in version v2.11) a list of the names of multiseries id columns to define series\nwithin the training data.  Currently only one multiseries id column is supported.\nuse_cross_series_features (bool) – (New in version v2.14) Whether to use cross series features.\naggregation_type (Optional[str]) – (New in version v2.14) The aggregation type to apply when creating\ncross series features. Optional, must be one of “total” or “average”.\ncross_series_group_by_columns (list of Optional[str]) – (New in version v2.15) List of columns (currently of length 1).\nOptional setting that indicates how to further split series into\nrelated groups. For example, if every series is sales of an individual product, the series\ngroup-by could be the product category with values like “men’s clothing”,\n“sports equipment”, etc.. Can only be used in a multiseries project with\nuse_cross_series_features set to True.\ncalendar_id (Optional[str]) – (New in version v2.15) The id of the CalendarFile to\nuse with this project.\nunsupervised_mode (Optional[bool]) – (New in version v2.20) defaults to False, indicates whether partitioning should be\nconstructed for the unsupervised project.\nmodel_splits (Optional[int]) – (New in version v2.21) Sets the cap on the number of jobs per model used when\nbuilding models to control number of jobs in the queue. Higher number of model splits\nwill allow for less downsampling leading to the use of more post-processed data.\nallow_partial_history_time_series_predictions (Optional[bool]) – (New in version v2.24) Whether to allow time series models to make predictions using\npartial historical data.\nunsupervised_type (Optional[str]) – (New in version v3.2) The unsupervised project type, only valid if unsupervised_mode is\nTrue. Use values from datarobot.enums.UnsupervisedTypeEnum enum.\nIf not specified then the project defaults to ‘anomaly’ when unsupervised_mode is True.\ncollect_payload()\nSet up the dict that should be sent to the server when setting the target\nReturns:\npartitioning_spec\nReturn type:\ndict\nprep_payload(project_id, max_wait=600)\nRun any necessary validation and prep of the payload, including async operations\nMainly used for the datetime partitioning spec but implemented in general for consistency\nReturn type:\nNone\nupdate(**kwargs)\nUpdate this instance, matching attributes to kwargs\nMainly used for the datetime partitioning spec but implemented in general for consistency\nReturn type:\nNone\nclass datarobot.BacktestSpecification\nUniquely defines a Backtest used in a DatetimePartitioning\nIncludes only the attributes of a backtest directly controllable by users.  The other attributes\nare assigned by the DataRobot application based on the project dataset and the user-controlled\nsettings.\nThere are two ways to specify an individual backtest:\nOption 1: Use index, gap_duration, validation_start_date, and\nvalidation_duration. All durations should be specified with a duration string such as those\nreturned by the partitioning_methods.construct_duration_string helper method.\nimport datarobot as dr\npartitioning_spec = dr.DatetimePartitioningSpecification(\nbacktests=[\n# modify the first backtest using option 1\ndr.BacktestSpecification(\nindex=0,\ngap_duration=dr.partitioning_methods.construct_duration_string(),\nvalidation_start_date=datetime(year=2010, month=1, day=1),\nvalidation_duration=dr.partitioning_methods.construct_duration_string(years=1),\n)\n],\n# other partitioning settings...\n)\nOption 2 (New in version v2.20): Use index, primary_training_start_date,\nprimary_training_end_date, validation_start_date, and validation_end_date. In this\ncase, note that setting primary_training_end_date and validation_start_date to the same\ntimestamp will result with no gap being created.\nimport datarobot as dr\npartitioning_spec = dr.DatetimePartitioningSpecification(\nbacktests=[\n# modify the first backtest using option 2\ndr.BacktestSpecification(\nindex=0,\nprimary_training_start_date=datetime(year=2005, month=1, day=1),\nprimary_training_end_date=datetime(year=2010, month=1, day=1),\nvalidation_start_date=datetime(year=2010, month=1, day=1),\nvalidation_end_date=datetime(year=2011, month=1, day=1),\n)\n],\n# other partitioning settings...\n)\nAll durations should be specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nindex (int) – the index of the backtest to update\ngap_duration (str) – a duration string specifying the desired duration of the gap between\ntraining and validation scoring data for the backtest\nvalidation_start_date (datetime.datetime) – the desired start date of the validation scoring data for this backtest\nvalidation_duration (str) – a duration string specifying the desired duration of the validation\nscoring data for this backtest\nvalidation_end_date (datetime.datetime) – the desired end date of the validation scoring data for this backtest\nprimary_training_start_date (datetime.datetime) – the desired start date of the training partition for this backtest\nprimary_training_end_date (datetime.datetime) – the desired end date of the training partition for this backtest\nclass datarobot.FeatureSettings\nPer feature settings\nVariables:\nfeature_name (string) – name of the feature\nknown_in_advance (bool) – (New in version v2.11) Optional, for time series projects\nonly. Sets whether the feature is known in advance, i.e., values for future dates are known\nat prediction time. If not specified, the feature uses the value from the\ndefault_to_known_in_advance flag.\ndo_not_derive (bool) – (New in v2.17) Optional, for time series projects only.\nSets whether the feature is excluded from feature derivation. If not\nspecified, the feature uses the value from the default_to_do_not_derive flag.\ncollect_payload(use_a_priori=False)\nParameters:\nuse_a_priori (bool) – Switch to using the older a_priori key name instead of known_in_advance.\nDefault False\nReturn type:\nBacktestSpecification dictionary representation\nclass datarobot.Periodicity\nPeriodicity configuration\nParameters:\ntime_steps (int) – Time step value\ntime_unit (string) – Time step unit, valid options are values from datarobot.enums.TIME_UNITS\nExamples\nfrom datarobot as dr\nperiodicities = [\ndr.Periodicity(time_steps=10, time_unit=dr.enums.TIME_UNITS.HOUR),\ndr.Periodicity(time_steps=600, time_unit=dr.enums.TIME_UNITS.MINUTE)]\nspec = dr.DatetimePartitioningSpecification(\n# ...\nperiodicities=periodicities\n)\nclass datarobot.DatetimePartitioning\nFull partitioning of a project for datetime partitioning.\nTo instantiate, use\nDatetimePartitioning.get(project_id).\nIncludes both the attributes specified by the user, as well as those determined by the DataRobot\napplication based on the project dataset.  In order to use a partitioning to set the target,\ncall to_specification and pass the\nresulting\nDatetimePartitioningSpecification to\nProject.analyze_and_model via the partitioning_method\nparameter.\nThe available training data corresponds to all the data available for training, while the\nprimary training data corresponds to the data that can be used to train while ensuring that all\nbacktests are available.  If a model is trained with more data than is available in the primary\ntraining data, then all backtests may not have scores available.\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nproject_id (str) – the id of the project this partitioning applies to\ndatetime_partitioning_id (str or None) – the id of the datetime partitioning it is an optimized partitioning\ndatetime_partition_column (str) – the name of the column whose values as dates are used to assign a row\nto a particular partition\ndate_format (str) – the format (e.g. “%Y-%m-%d %H:%M:%S”) by which the partition column was interpreted\n(compatible with strftime)\nautopilot_data_selection_method (str) – one of datarobot.enums.DATETIME_AUTOPILOT_DATA_SELECTION_METHOD.  Whether models created\nby the autopilot use “rowCount” or “duration” as their data_selection_method.\nvalidation_duration (str or None) – the validation duration specified when initializing the partitioning - not directly\nsignificant if the backtests have been modified, but used as the default validation_duration\nfor the backtests. Can be absent if this is a time series project with an irregular primary\ndate/time feature.\navailable_training_start_date (datetime.datetime) – The start date of the available training data for scoring the holdout\navailable_training_duration (str) – The duration of the available training data for scoring the holdout\navailable_training_row_count (int or None) – The number of rows in the available training data for scoring the holdout.  Only available\nwhen retrieving the partitioning after setting the target.\navailable_training_end_date (datetime.datetime) – The end date of the available training data for scoring the holdout\nprimary_training_start_date (datetime.datetime or None) – The start date of primary training data for scoring the holdout.\nUnavailable when the holdout fold is disabled.\nprimary_training_duration (str) – The duration of the primary training data for scoring the holdout\nprimary_training_row_count (int or None) – The number of rows in the primary training data for scoring the holdout.  Only available\nwhen retrieving the partitioning after setting the target.\nprimary_training_end_date (datetime.datetime or None) – The end date of the primary training data for scoring the holdout.\nUnavailable when the holdout fold is disabled.\ngap_start_date (datetime.datetime or None) – The start date of the gap between training and holdout scoring data.\nUnavailable when the holdout fold is disabled.\ngap_duration (str) – The duration of the gap between training and holdout scoring data\ngap_row_count (int or None) – The number of rows in the gap between training and holdout scoring data.  Only available\nwhen retrieving the partitioning after setting the target.\ngap_end_date (datetime.datetime or None) – The end date of the gap between training and holdout scoring data.\nUnavailable when the holdout fold is disabled.\ndisable_holdout (bool or None) – Whether to suppress allocating a holdout fold.\nIf set to True, holdout_start_date, holdout_duration, and holdout_end_date\nmay not be specified.\nholdout_start_date (datetime.datetime or None) – The start date of holdout scoring data.\nUnavailable when the holdout fold is disabled.\nholdout_duration (str) – The duration of the holdout scoring data\nholdout_row_count (int or None) – The number of rows in the holdout scoring data.  Only available when retrieving the\npartitioning after setting the target.\nholdout_end_date (datetime.datetime or None) – The end date of the holdout scoring data. Unavailable when the holdout fold is disabled.\nnumber_of_backtests (int) – the number of backtests used.\nbacktests (list of Backtest) – the configured backtests.\ntotal_row_count (int) – the number of rows in the project dataset.  Only available when retrieving the partitioning\nafter setting the target.\nuse_time_series (bool) – (New in version v2.8) Whether to create a time series project (if True) or an OTV\nproject which uses datetime partitioning (if False).  The default behavior is to create\nan OTV project.\ndefault_to_known_in_advance (bool) – (New in version v2.11) Optional, default False. Used for time series projects only. Sets\nwhether all features default to being treated as known in advance. Known in advance features\nare expected to be known for dates in the future when making predictions, e.g., “is this a\nholiday?”. Individual features can be set to a value different from the default using the\nfeature_settings parameter.\ndefault_to_do_not_derive (bool) – (New in v2.17) Optional, default False. Used for time series projects only. Sets whether\nall features default to being treated as do-not-derive features, excluding them from feature\nderivation. Individual features can be set to a value different from the default by using\nthe feature_settings parameter.\nfeature_derivation_window_start (int or None) – (New in version v2.8) Only used for time series projects. Offset into the past to define\nhow far back relative to the forecast point the feature derivation window should start.\nExpressed in terms of the windows_basis_unit.\nfeature_derivation_window_end (int or None) – (New in version v2.8) Only used for time series projects. Offset into the past to define how\nfar back relative to the forecast point the feature derivation window should end. Expressed\nin terms of the windows_basis_unit.\nfeature_settings (list of FeatureSettings) – (New in version v2.9) Optional, a list specifying per feature settings, can be\nleft unspecified.\nforecast_window_start (int or None) – (New in version v2.8) Only used for time series projects. Offset into the future to define\nhow far forward relative to the forecast point the forecast window should start. Expressed\nin terms of the windows_basis_unit.\nforecast_window_end (int or None) – (New in version v2.8) Only used for time series projects. Offset into the future to define\nhow far forward relative to the forecast point the forecast window should end. Expressed in\nterms of the windows_basis_unit.\nwindows_basis_unit (string, optional) – (New in version v2.14) Only used for time series projects. Indicates which unit is\na basis for feature derivation window and forecast window. Valid options are detected time\nunit (one of the datarobot.enums.TIME_UNITS) or “ROW”.\nIf omitted, the default value is detected time unit.\ntreat_as_exponential (string, optional) – (New in version v2.9) defaults to “auto”. Used to specify whether to treat data\nas exponential trend and apply transformations like log-transform. Use values from the\ndatarobot.enums.TREAT_AS_EXPONENTIAL enum.\ndifferencing_method (string, optional) – (New in version v2.9) defaults to “auto”. Used to specify which differencing method to\napply of case if data is stationary. Use values from the\ndatarobot.enums.DIFFERENCING_METHOD enum.\nperiodicities (list of Periodicity, optional) – (New in version v2.9) a list of datarobot.Periodicity. Periodicities units\nshould be “ROW”, if the windows_basis_unit is “ROW”.\nmultiseries_id_columns (List[str] or null) – (New in version v2.11) a list of the names of multiseries id columns to define series\nwithin the training data.  Currently only one multiseries id column is supported.\nnumber_of_known_in_advance_features (int) – (New in version v2.14) Number of features that are marked as known in advance.\nnumber_of_do_not_derive_features (int) – (New in v2.17) Number of features that are excluded from derivation.\nuse_cross_series_features (bool) – (New in version v2.14) Whether to use cross series features.\naggregation_type (Optional[str]) – (New in version v2.14) The aggregation type to apply when creating cross series\nfeatures. Optional, must be one of “total” or “average”.\ncross_series_group_by_columns (list of Optional[str]) – (New in version v2.15) List of columns (currently of length 1).\nOptional setting that indicates how to further split series into\nrelated groups. For example, if every series is sales of an individual product, the series\ngroup-by could be the product category with values like “men’s clothing”,\n“sports equipment”, etc.. Can only be used in a multiseries project with\nuse_cross_series_features set to True.\ncalendar_id (Optional[str]) – (New in version v2.15) Only available for time series projects. The id of the\nCalendarFile to use with this project.\ncalendar_name (Optional[str]) – (New in version v2.17) Only available for time series projects. The name of the\nCalendarFile used with this project.\nmodel_splits (Optional[int]) – (New in version v2.21) Sets the cap on the number of jobs per model used when\nbuilding models to control number of jobs in the queue. Higher number of model splits\nwill allow for less downsampling leading to the use of more post-processed data.\nallow_partial_history_time_series_predictions (Optional[bool]) – (New in version v2.24) Whether to allow time series models to make predictions using\npartial historical data.\nunsupervised_mode (Optional[bool]) – (New in version v3.1) Whether the date/time partitioning is for an unsupervised project\nunsupervised_type (Optional[str]) – (New in version v3.2) The unsupervised project type, only valid if unsupervised_mode is\nTrue. Use values from datarobot.enums.UnsupervisedTypeEnum enum.\nIf not specified then the project defaults to ‘anomaly’ when unsupervised_mode is True.\nclassmethod generate(project_id, spec, max_wait=600, target=None)\nPreview the full partitioning determined by a DatetimePartitioningSpecification\nBased on the project dataset and the partitioning specification, inspect the full\npartitioning that would be used if the same specification were passed into\nProject.analyze_and_model.\nParameters:\nproject_id (str) – the id of the project\nspec (DatetimePartitioningSpec) – the desired partitioning\nmax_wait (Optional[int]) – For some settings (e.g. generating a partitioning preview for a multiseries project for\nthe first time), an asynchronous task must be run to analyze the dataset.  max_wait\ngoverns the maximum time (in seconds) to wait before giving up.  In all non-multiseries\nprojects, this is unused.\ntarget (Optional[str]) – the name of the target column. For unsupervised projects target may be None. Providing\na target will ensure that partitions are correctly optimized for your dataset.\nReturns:\nthe full generated partitioning\nReturn type:\nDatetimePartitioning\nclassmethod get(project_id)\nRetrieve the DatetimePartitioning from a project\nOnly available if the project has already set the target as a datetime project.\nParameters:\nproject_id (str) – the id of the project to retrieve partitioning for\nReturns:\nDatetimePartitioning\nReturn type:\nthe full partitioning for the project\nclassmethod generate_optimized(project_id, spec, target, max_wait=600)\nPreview the full partitioning determined by a DatetimePartitioningSpecification\nBased on the project dataset and the partitioning specification, inspect the full\npartitioning that would be used if the same specification were passed into\nProject.analyze_and_model.\nParameters:\nproject_id (str) – the id of the project\nspec (DatetimePartitioningSpecification) – the desired partitioning\ntarget (str) – the name of the target column. For unsupervised projects target may be None.\nmax_wait (Optional[int]) – Governs the maximum time (in seconds) to wait before giving up.\nReturns:\nthe full generated partitioning\nReturn type:\nDatetimePartitioning\nclassmethod get_optimized(project_id, datetime_partitioning_id)\nRetrieve an Optimized DatetimePartitioning from a project for the specified\ndatetime_partitioning_id. A datetime_partitioning_id is created by using the\ngenerate_optimized function.\nParameters:\nproject_id (str) – the id of the project to retrieve partitioning for\ndatetime_partitioning_id (ObjectId) – the ObjectId associated with the project to retrieve from Mongo\nReturns:\nDatetimePartitioning\nReturn type:\nthe full partitioning for the project\nclassmethod feature_log_list(project_id, offset=None, limit=None)\nRetrieve the feature derivation log content and log length for a time series project.\nThe Time Series Feature Log provides details about the feature generation process for a\ntime series project. It includes information about which features are generated and their\npriority, as well as the detected properties of the time series data such as whether the\nseries is stationary, and periodicities detected.\nThis route is only supported for time series projects that have finished partitioning.\nThe feature derivation log will include information about:\nDetected stationarity of the series:\ne.g. ‘Series detected as non-stationary’\nDetected presence of multiplicative trend in the series:\ne.g. ‘Multiplicative trend detected’\nDetected presence of multiplicative trend in the series:\ne.g.  ‘Detected periodicities: 7 day’\nMaximum number of feature to be generated:\ne.g. ‘Maximum number of feature to be generated is 1440’\nWindow sizes used in rolling statistics / lag extractors\ne.g. ‘The window sizes chosen to be: 2 months\n(because the time step is 1 month and Feature Derivation Window is 2 months)’\nFeatures that are specified as known-in-advance\ne.g. ‘Variables treated as apriori: holiday’\nDetails about why certain variables are transformed in the input data\ne.g. ‘Generating variable “y (log)” from “y” because multiplicative trend\nis detected’\nDetails about features generated as timeseries features, and their priority\ne.g. ‘Generating feature “date (actual)” from “date” (priority: 1)’\nParameters:\nproject_id (str) – project id to retrieve a feature derivation log for.\noffset (int) – optional, defaults is 0, this many results will be skipped.\nlimit (int) – optional, defaults to 100, at most this many results are returned. To specify\nno limit, use 0. The default may change without notice.\nclassmethod feature_log_retrieve(project_id)\nRetrieve the feature derivation log content and log length for a time series project.\nThe Time Series Feature Log provides details about the feature generation process for a\ntime series project. It includes information about which features are generated and their\npriority, as well as the detected properties of the time series data such as whether the\nseries is stationary, and periodicities detected.\nThis route is only supported for time series projects that have finished partitioning.\nThe feature derivation log will include information about:\nDetected stationarity of the series:\ne.g. ‘Series detected as non-stationary’\nDetected presence of multiplicative trend in the series:\ne.g. ‘Multiplicative trend detected’\nDetected presence of multiplicative trend in the series:\ne.g.  ‘Detected periodicities: 7 day’\nMaximum number of feature to be generated:\ne.g. ‘Maximum number of feature to be generated is 1440’\nWindow sizes used in rolling statistics / lag extractors\ne.g. ‘The window sizes chosen to be: 2 months\n(because the time step is 1 month and Feature Derivation Window is 2 months)’\nFeatures that are specified as known-in-advance\ne.g. ‘Variables treated as apriori: holiday’\nDetails about why certain variables are transformed in the input data\ne.g. ‘Generating variable “y (log)” from “y” because multiplicative trend\nis detected’\nDetails about features generated as timeseries features, and their priority\ne.g. ‘Generating feature “date (actual)” from “date” (priority: 1)’\nParameters:\nproject_id (str) – project id to retrieve a feature derivation log for.\nReturn type:\nstr\nto_specification(use_holdout_start_end_format=False, use_backtest_start_end_format=False)\nRender the DatetimePartitioning as a DatetimePartitioningSpecification\nThe resulting specification can be used when setting the target, and contains only the\nattributes directly controllable by users.\nParameters:\nuse_holdout_start_end_format (Optional[bool]) – Defaults to False. If True, will use holdout_end_date when configuring the\nholdout partition. If False, will use holdout_duration instead.\nuse_backtest_start_end_format (Optional[bool]) – Defaults to False. If False, will use a duration-based approach for specifying\nbacktests (gap_duration, validation_start_date, and validation_duration).\nIf True, will use a start/end date approach for specifying\nbacktests (primary_training_start_date, primary_training_end_date,\nvalidation_start_date, validation_end_date).\nIn contrast, projects created in the Web UI will use the start/end date approach for specifying\nbacktests. Set this parameter to True to mirror the behavior in the Web UI.\nReturns:\nthe specification for this partitioning\nReturn type:\nDatetimePartitioningSpecification\nto_dataframe()\nRender the partitioning settings as a dataframe for convenience of display\nExcludes project_id, datetime_partition_column, date_format,\nautopilot_data_selection_method, validation_duration,\nand number_of_backtests, as well as the row count information, if present.\nAlso excludes the time series specific parameters for use_time_series,\ndefault_to_known_in_advance, default_to_do_not_derive, and defining the feature\nderivation and forecast windows.\nReturn type:\nDataFrame\nclassmethod datetime_partitioning_log_retrieve(project_id, datetime_partitioning_id)\nRetrieve the datetime partitioning log content for an optimized datetime partitioning.\nThe datetime partitioning log provides details about the partitioning process for an OTV\nor time series project.\nParameters:\nproject_id (str) – The project ID of the project associated with the datetime partitioning.\ndatetime_partitioning_id (str) – id of the optimized datetime partitioning\nReturn type:\nAny\nclassmethod datetime_partitioning_log_list(project_id, datetime_partitioning_id, offset=None, limit=None)\nRetrieve the datetime partitioning log content and log length for an optimized\ndatetime partitioning.\nThe Datetime Partitioning Log provides details about the partitioning process for an OTV\nor Time Series project.\nParameters:\nproject_id (str) – project id of the project associated with the datetime partitioning.\ndatetime_partitioning_id (str) – id of the optimized datetime partitioning\noffset (int or None) – optional, defaults is 0, this many results will be skipped.\nlimit (int or None) – optional, defaults to 100, at most this many results are returned. To specify\nno limit, use 0. The default may change without notice.\nReturn type:\nAny\nclassmethod get_input_data(project_id, datetime_partitioning_id)\nRetrieve the input used to create an optimized DatetimePartitioning from a project for\nthe specified datetime_partitioning_id. A datetime_partitioning_id is created by using the\ngenerate_optimized function.\nParameters:\nproject_id (str) – The ID of the project to retrieve partitioning for.\ndatetime_partitioning_id (ObjectId) – The ObjectId associated with the project to retrieve from Mongo.\nReturns:\nDatetimePartitioningInput\nReturn type:\nThe input to optimized datetime partitioning.\nclass datarobot.helpers.partitioning_methods.DatetimePartitioningId\nDefines a DatetimePartitioningId used for datetime partitioning.\nThis class only includes the datetime_partitioning_id that identifies a previously\noptimized datetime partitioning and the project_id for the associated project.\nThis is the specification that should be passed to Project.analyze_and_model via the partitioning_method parameter. To see\nthe full partitioning use DatetimePartitioning.get_optimized.\nVariables:\ndatetime_partitioning_id (str) – The ID of the datetime partitioning to use.\nproject_id (str) – The ID of the project that the datetime partitioning is associated with.\ncollect_payload()\nSet up the dict that should be sent to the server when setting the target\nReturns:\npartitioning_spec\nReturn type:\ndict\nprep_payload(project_id, max_wait=600)\nRun any necessary validation and prep of the payload, including async operations\nMainly used for the datetime partitioning spec but implemented in general for consistency\nReturn type:\nNone\nupdate(**kwargs)\nUpdate this instance, matching attributes to kwargs\nMainly used for the datetime partitioning spec but implemented in general for consistency\nReturn type:\nNoReturn\nclass datarobot.helpers.partitioning_methods.Backtest\nA backtest used to evaluate models trained in a datetime partitioned project\nWhen setting up a datetime partitioning project, backtests are specified by a\nBacktestSpecification.\nThe available training data corresponds to all the data available for training, while the\nprimary training data corresponds to the data that can be used to train while ensuring that all\nbacktests are available.  If a model is trained with more data than is available in the primary\ntraining data, then all backtests may not have scores available.\nAll durations are specified with a duration string such as those returned\nby the partitioning_methods.construct_duration_string helper method.\nPlease see datetime partitioned project documentation\nfor more information on duration strings.\nVariables:\nindex (int) – the index of the backtest\navailable_training_start_date (datetime.datetime) – the start date of the available training data for this backtest\navailable_training_duration (str) – the duration of available training data for this backtest\navailable_training_row_count (int or None) – the number of rows of available training data for this backtest.  Only available when\nretrieving from a project where the target is set.\navailable_training_end_date (datetime.datetime) – the end date of the available training data for this backtest\nprimary_training_start_date (datetime.datetime) – the start date of the primary training data for this backtest\nprimary_training_duration (str) – the duration of the primary training data for this backtest\nprimary_training_row_count (int or None) – the number of rows of primary training data for this backtest.  Only available when\nretrieving from a project where the target is set.\nprimary_training_end_date (datetime.datetime) – the end date of the primary training data for this backtest\ngap_start_date (datetime.datetime) – the start date of the gap between training and validation scoring data for this backtest\ngap_duration (str) – the duration of the gap between training and validation scoring data for this backtest\ngap_row_count (int or None) – the number of rows in the gap between training and validation scoring data for this\nbacktest.  Only available when retrieving from a project where the target is set.\ngap_end_date (datetime.datetime) – the end date of the gap between training and validation scoring data for this backtest\nvalidation_start_date (datetime.datetime) – the start date of the validation scoring data for this backtest\nvalidation_duration (str) – the duration of the validation scoring data for this backtest\nvalidation_row_count (int or None) – the number of rows of validation scoring data for this backtest.  Only available when\nretrieving from a project where the target is set.\nvalidation_end_date (datetime.datetime) – the end date of the validation scoring data for this backtest\ntotal_row_count (int or None) – the number of rows in this backtest.  Only available when retrieving from a project where\nthe target is set.\nto_specification(use_start_end_format=False)\nRender this backtest as a\nBacktestSpecification.\nThe resulting specification includes only the attributes users can directly control, not\nthose indirectly determined by the project dataset.\nParameters:\nuse_start_end_format (bool) – Default False. If False, will use a duration-based approach for specifying\nbacktests (gap_duration, validation_start_date, and validation_duration).\nIf True, will use a start/end date approach for specifying\nbacktests (primary_training_start_date, primary_training_end_date,\nvalidation_start_date, validation_end_date).\nIn contrast, projects created in the Web UI will use the start/end date approach for specifying\nbacktests. Set this parameter to True to mirror the behavior in the Web UI.\nReturns:\nthe specification for this backtest\nReturn type:\nBacktestSpecification\nto_dataframe()\nRender this backtest as a dataframe for convenience of display\nReturns:\nbacktest_partitioning – the backtest attributes, formatted into a dataframe\nReturn type:\npandas.Dataframe\nclass datarobot.helpers.partitioning_methods.FeatureSettingsPayload\ndatarobot.helpers.partitioning_methods.construct_duration_string(years=0, months=0, days=0, hours=0, minutes=0, seconds=0)\nConstruct a valid string representing a duration in accordance with ISO8601\nA duration of six months, 3 days, and 12 hours could be represented as P6M3DT12H.\nParameters:\nyears (int) – the number of years in the duration\nmonths (int) – the number of months in the duration\ndays (int) – the number of days in the duration\nhours (int) – the number of hours in the duration\nminutes (int) – the number of minutes in the duration\nseconds (int) – the number of seconds in the duration\nReturns:\nduration_string – The duration string, specified compatibly with ISO8601\nReturn type:\nstr\nStatus check job\nclass datarobot.models.StatusCheckJob\nTracks asynchronous task status\nVariables:\njob_id (str) – The ID of the status the job belongs to.\nwait_for_completion(max_wait=600)\nWaits for job to complete.\nParameters:\nmax_wait (Optional[int]) – How long to wait for the job to finish. If the time expires, DataRobot returns the current status.\nReturns:\nstatus – Returns the current status of the job.\nReturn type:\nJobStatusResult\nget_status()\nRetrieve JobStatusResult object with the latest job status data from the server.\nReturn type:\nJobStatusResult\nget_result_when_complete(max_wait=600)\nWait for the job to complete, then attempt to convert the resulting json into an object of type\nself.resource_type\n:rtype: A newly created resource of type self.resource_type\nclass datarobot.models.JobStatusResult\nJobStatusResult(status, status_id, completed_resource_url, message)\nstatus: Optional[str]\nAlias for field number 0\nstatus_id: Optional[str]\nAlias for field number 1\ncompleted_resource_url: Optional[str]\nAlias for field number 2\nmessage: Optional[str]\nAlias for field number 3\nSegmented modeling\nAPI Reference for entities used in Segmented Modeling. See dedicated User Guide for examples.\nclass datarobot.CombinedModel\nA model from a segmented project. Combination of ordinary models in child segments projects.\nVariables:\nid (str) – the id of the model\nproject_id (str) – the id of the project the model belongs to\nsegmentation_task_id (str) – the id of a segmentation task used in this model\nis_active_combined_model (bool) – flag indicating if this is the active combined model in segmented project\nclassmethod get(project_id, combined_model_id)\nRetrieve combined model\nParameters:\nproject_id (str) – The project’s id.\ncombined_model_id (str) – Id of the combined model.\nReturns:\nThe queried combined model.\nReturn type:\nCombinedModel\nclassmethod set_segment_champion(project_id, model_id, clone=False)\nUpdate a segment champion in a combined model by setting the model_id\nthat belongs to the child project_id as the champion.\nParameters:\nproject_id (str) – The project id for the child model that contains the model id.\nmodel_id (str) – Id of the model to mark as the champion\nclone (bool) – (New in version v2.29) optional, defaults to False.\nDefines if combined model has to be cloned prior to setting champion\n(champion will be set for new combined model if yes).\nReturns:\ncombined_model_id – Id of the combined model that was updated\nReturn type:\nstr\nget_segments_info()\nRetrieve Combined Model segments info\nReturns:\nList of segments\nReturn type:\nlist[SegmentInfo]\nget_segments_as_dataframe(encoding='utf-8')\nRetrieve Combine Models segments as a DataFrame.\nParameters:\nencoding (Optional[str]) – A string representing the encoding to use in the output csv file.\nDefaults to ‘utf-8’.\nReturns:\nCombined model segments\nReturn type:\nDataFrame\nget_segments_as_csv(filename, encoding='utf-8')\nSave the Combine Models segments to a csv.\nParameters:\nfilename (str or file object) – The path or file object to save the data to.\nencoding (Optional[str]) – A string representing the encoding to use in the output csv file.\nDefaults to ‘utf-8’.\nReturn type:\nNone\ntrain(sample_pct=None, featurelist_id=None, scoring_type=None, training_row_count=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>)\nInherited from Model - CombinedModels cannot be retrained directly\nReturn type:\nNoReturn\ntrain_datetime(featurelist_id=None, training_row_count=None, training_duration=None, time_window_sample_pct=None, monotonic_increasing_featurelist_id=<object object>, monotonic_decreasing_featurelist_id=<object object>, use_project_settings=False, sampling_method=None, n_clusters=None)\nInherited from Model - CombinedModels cannot be retrained directly\nReturn type:\nNoReturn\nretrain(sample_pct=None, featurelist_id=None, training_row_count=None, n_clusters=None)\nInherited from Model - CombinedModels cannot be retrained directly\nReturn type:\nNoReturn\nrequest_frozen_model(sample_pct=None, training_row_count=None)\nInherited from Model - CombinedModels cannot be retrained as frozen\nReturn type:\nNoReturn\nrequest_frozen_datetime_model(training_row_count=None, training_duration=None, training_start_date=None, training_end_date=None, time_window_sample_pct=None, sampling_method=None)\nInherited from Model - CombinedModels cannot be retrained as frozen\nReturn type:\nNoReturn\ncross_validate()\nInherited from Model - CombinedModels cannot request cross validation\nReturn type:\nNoReturn\nclass datarobot.SegmentationTask\nA Segmentation Task is used for segmenting an existing project into multiple child\nprojects. Each child project (or segment) will be a separate autopilot run. Currently\nonly user defined segmentation is supported.\nExample for creating a new SegmentationTask for Time Series segmentation with a\nuser defined id column:\nfrom datarobot import SegmentationTask\n# Create the SegmentationTask\nsegmentation_task_results = SegmentationTask.create(\nproject_id=project.id,\ntarget=target,\nuse_time_series=True,\ndatetime_partition_column=datetime_partition_column,\nmultiseries_id_columns=[multiseries_id_column],\nuser_defined_segment_id_columns=[user_defined_segment_id_column]\n)\n# Retrieve the completed SegmentationTask object from the job results\nsegmentation_task = segmentation_task_results['completedJobs'][0]\nVariables:\nid (bson.ObjectId) – The id of the segmentation task.\nproject_id (bson.ObjectId) – The associated id of the parent project.\ntype (str) – What type of job the segmentation task is associated with, e.g. auto_ml or auto_ts.\ncreated (datetime.datetime) – The date this segmentation task was created.\nsegments_count (int) – The number of segments the segmentation task generated.\nsegments (list[str]) – The segment names that the segmentation task generated.\nmetadata (dict) – List of features that help to identify the parameters used by the segmentation task.\ndata (dict) – Optional parameters that are associated with enabled metadata for the segmentation task.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nSegmentationTask\ncollect_payload()\nConvert the record to a dictionary\nReturn type:\nDict[str, str]\nclassmethod create(project_id, target, use_time_series=False, datetime_partition_column=None, multiseries_id_columns=None, user_defined_segment_id_columns=None, max_wait=600, model_package_id=None)\nCreates segmentation tasks for the project based on the defined parameters.\nParameters:\nproject_id (str) – The associated id of the parent project.\ntarget (str) – The column that represents the target in the dataset.\nuse_time_series (bool) – Whether AutoTS or AutoML segmentations should be generated.\ndatetime_partition_column (str or null) – Required for Time Series.\nThe name of the column whose values as dates are used to assign a row\nto a particular partition.\nmultiseries_id_columns (List[str] or null) – Required for Time Series.\nA list of the names of multiseries id columns to define series within the training\ndata. Currently only one multiseries id column is supported.\nuser_defined_segment_id_columns (List[str] or null) – Required when using a column for segmentation.\nA list of the segment id columns to use to define what columns are used to manually\nsegment data. Currently only one user defined segment id column is supported.\nmodel_package_id (str) – Required when using automated segmentation.\nThe associated id of the model in the DataRobot Model Registry that will be used to\nperform automated segmentation on a dataset.\nmax_wait (integer) – The number of seconds to wait\nReturns:\nsegmentation_tasks – Dictionary containing the numberOfJobs, completedJobs, and failedJobs. completedJobs\nis a list of SegmentationTask objects, while failed jobs is a list of dictionaries\nindicating problems with submitted tasks.\nReturn type:\ndict\nclassmethod list(project_id)\nList all of the segmentation tasks that have been created for a specific project_id.\nParameters:\nproject_id (str) – The id of the parent project\nReturns:\nsegmentation_tasks – List of instances with initialized data.\nReturn type:\nlist of SegmentationTask\nclassmethod get(project_id, segmentation_task_id)\nRetrieve information for a single segmentation task associated with a project_id.\nParameters:\nproject_id (str) – The id of the parent project\nsegmentation_task_id (str) – The id of the segmentation task\nReturns:\nsegmentation_task – Instance with initialized data.\nReturn type:\nSegmentationTask\nclass datarobot.SegmentInfo\nA SegmentInfo is an object containing information about the combined model segments\nVariables:\nproject_id (str) – The associated id of the child project.\nsegment (str) – the name of the segment\nproject_stage (str) – A description of the current stage of the project\nproject_status_error (str) – Project status error message.\nautopilot_done (bool) – Is autopilot done for the project.\nmodel_count (int) – Count of trained models in project.\nmodel_id (str) – ID of segment champion model.\nclassmethod list(project_id, model_id)\nList all of the segments that have been created for a specific project_id.\nParameters:\nproject_id (str) – The id of the parent project\nReturns:\nsegments – List of instances with initialized data.\nReturn type:\nlist of datarobot.models.segmentation.SegmentInfo\nclass datarobot.models.segmentation.SegmentationTask\nA Segmentation Task is used for segmenting an existing project into multiple child\nprojects. Each child project (or segment) will be a separate autopilot run. Currently\nonly user defined segmentation is supported.\nExample for creating a new SegmentationTask for Time Series segmentation with a\nuser defined id column:\nfrom datarobot import SegmentationTask\n# Create the SegmentationTask\nsegmentation_task_results = SegmentationTask.create(\nproject_id=project.id,\ntarget=target,\nuse_time_series=True,\ndatetime_partition_column=datetime_partition_column,\nmultiseries_id_columns=[multiseries_id_column],\nuser_defined_segment_id_columns=[user_defined_segment_id_column]\n)\n# Retrieve the completed SegmentationTask object from the job results\nsegmentation_task = segmentation_task_results['completedJobs'][0]\nVariables:\nid (bson.ObjectId) – The id of the segmentation task.\nproject_id (bson.ObjectId) – The associated id of the parent project.\ntype (str) – What type of job the segmentation task is associated with, e.g. auto_ml or auto_ts.\ncreated (datetime.datetime) – The date this segmentation task was created.\nsegments_count (int) – The number of segments the segmentation task generated.\nsegments (list[str]) – The segment names that the segmentation task generated.\nmetadata (dict) – List of features that help to identify the parameters used by the segmentation task.\ndata (dict) – Optional parameters that are associated with enabled metadata for the segmentation task.\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nSegmentationTask\ncollect_payload()\nConvert the record to a dictionary\nReturn type:\nDict[str, str]\nclassmethod create(project_id, target, use_time_series=False, datetime_partition_column=None, multiseries_id_columns=None, user_defined_segment_id_columns=None, max_wait=600, model_package_id=None)\nCreates segmentation tasks for the project based on the defined parameters.\nParameters:\nproject_id (str) – The associated id of the parent project.\ntarget (str) – The column that represents the target in the dataset.\nuse_time_series (bool) – Whether AutoTS or AutoML segmentations should be generated.\ndatetime_partition_column (str or null) – Required for Time Series.\nThe name of the column whose values as dates are used to assign a row\nto a particular partition.\nmultiseries_id_columns (List[str] or null) – Required for Time Series.\nA list of the names of multiseries id columns to define series within the training\ndata. Currently only one multiseries id column is supported.\nuser_defined_segment_id_columns (List[str] or null) – Required when using a column for segmentation.\nA list of the segment id columns to use to define what columns are used to manually\nsegment data. Currently only one user defined segment id column is supported.\nmodel_package_id (str) – Required when using automated segmentation.\nThe associated id of the model in the DataRobot Model Registry that will be used to\nperform automated segmentation on a dataset.\nmax_wait (integer) – The number of seconds to wait\nReturns:\nsegmentation_tasks – Dictionary containing the numberOfJobs, completedJobs, and failedJobs. completedJobs\nis a list of SegmentationTask objects, while failed jobs is a list of dictionaries\nindicating problems with submitted tasks.\nReturn type:\ndict\nclassmethod list(project_id)\nList all of the segmentation tasks that have been created for a specific project_id.\nParameters:\nproject_id (str) – The id of the parent project\nReturns:\nsegmentation_tasks – List of instances with initialized data.\nReturn type:\nlist of SegmentationTask\nclassmethod get(project_id, segmentation_task_id)\nRetrieve information for a single segmentation task associated with a project_id.\nParameters:\nproject_id (str) – The id of the parent project\nsegmentation_task_id (str) – The id of the segmentation task\nReturns:\nsegmentation_task – Instance with initialized data.\nReturn type:\nSegmentationTask\nclass datarobot.models.segmentation.SegmentationTaskCreatedResponse\nExternal baseline validation\nclass datarobot.models.external_baseline_validation.ExternalBaselineValidationInfo\nAn object containing information about external time series baseline predictions\nvalidation results.\nVariables:\nbaseline_validation_job_id (str) – the identifier of the baseline validation job\nproject_id (str) – the identifier of the project\ncatalog_version_id (str) – the identifier of the catalog version used in the validation job\ntarget (str) – the name of the target feature\ndatetime_partition_column (str) – the name of the column whose values as dates are used to assign a row\nto a particular partition\nis_external_baseline_dataset_valid (bool) – whether the external baseline dataset passes the validation check\nmultiseries_id_columns (List[str] or null) – a list of the names of multiseries id columns to define series\nwithin the training data.  Currently only one multiseries id column is supported.\nholdout_start_date (str or None) – the start date of holdout scoring data\nholdout_end_date (str or None) – the end date of holdout scoring data\nbacktests (list of dicts containing validation_start_date and validation_end_date or None) – the configured backtests of the time series project\nforecast_window_start (int) – offset into the future to define how far forward relative to the forecast point the\nforecast window should start.\nforecast_window_end (int) – offset into the future to define how far forward relative to the forecast point the\nforecast window should end.\nmessage (str or None) – the description of the issue with external baseline validation job\nclassmethod get(project_id, validation_job_id)\nGet information about external baseline validation job\nParameters:\nproject_id (string) – the identifier of the project\nvalidation_job_id (string) – the identifier of the external baseline validation job\nReturns:\ninfo – information about external baseline validation job\nReturn type:\nExternalBaselineValidationInfo\nCalendar file\nclass datarobot.CalendarFile\nRepresents the data for a calendar file.\nFor more information about calendar files, see the\ncalendar documentation.\nVariables:\nid (str) – The id of the calendar file.\ncalendar_start_date (str) – The earliest date in the calendar.\ncalendar_end_date (str) – The last date in the calendar.\ncreated (str) – The date this calendar was created, i.e. uploaded to DR.\nname (str) – The name of the calendar.\nnum_event_types (int) – The number of different event types.\nnum_events (int) – The number of events this calendar has.\nproject_ids (list of strings) – A list containing the projectIds of the projects using this calendar.\nmultiseries_id_columns (List[str] or None) – A list of columns in calendar which uniquely identify events for different series.\nCurrently, only one column is supported.\nIf multiseries id columns are not provided, calendar is considered to be single series.\nrole (str) – The access role the user has for this calendar.\nclassmethod create(file_path, calendar_name=None, multiseries_id_columns=None)\nCreates a calendar using the given file. For information about calendar files, see the\ncalendar documentation\nThe provided file must be a CSV in the format:\nDate,   Event,          Series ID,    Event Duration\n<date>, <event_type>,   <series id>,  <event duration>\n<date>, <event_type>,              ,  <event duration>\nA header row is required, and the “Series ID” and “Event Duration” columns are optional.\nOnce the CalendarFile has been created, pass its ID with\nthe DatetimePartitioningSpecification\nwhen setting the target for a time series project in order to use it.\nParameters:\nfile_path (string) – A string representing a path to a local csv file.\ncalendar_name (string, optional) – A name to assign to the calendar. Defaults to the name of the file if not provided.\nmultiseries_id_columns (List[str] or None) – A list of the names of multiseries id columns to define which series an event\nbelongs to. Currently only one multiseries id column is supported.\nReturns:\ncalendar_file – Instance with initialized data.\nReturn type:\nCalendarFile\nRaises:\nAsyncProcessUnsuccessfulError – Raised if there was an error processing the provided calendar file.\nExamples\n# Creating a calendar with a specified name\ncal = dr.CalendarFile.create('/home/calendars/somecalendar.csv',\ncalendar_name='Some Calendar Name')\ncal.id\n>>> 5c1d4904211c0a061bc93013\ncal.name\n>>> Some Calendar Name\n# Creating a calendar without specifying a name\ncal = dr.CalendarFile.create('/home/calendars/somecalendar.csv')\ncal.id\n>>> 5c1d4904211c0a061bc93012\ncal.name\n>>> somecalendar.csv\n# Creating a calendar with multiseries id columns\ncal = dr.CalendarFile.create('/home/calendars/somemultiseriescalendar.csv',\ncalendar_name='Some Multiseries Calendar Name',\nmultiseries_id_columns=['series_id'])\ncal.id\n>>> 5da9bb21962d746f97e4daee\ncal.name\n>>> Some Multiseries Calendar Name\ncal.multiseries_id_columns\n>>> ['series_id']\nclassmethod create_calendar_from_dataset(dataset_id, dataset_version_id=None, calendar_name=None, multiseries_id_columns=None, delete_on_error=False)\nCreates a calendar using the given dataset. For information about calendar files, see the\ncalendar documentation\nThe provided dataset have the following format:\nDate,   Event,          Series ID,    Event Duration\n<date>, <event_type>,   <series id>,  <event duration>\n<date>, <event_type>,              ,  <event duration>\nThe “Series ID” and “Event Duration” columns are optional.\nOnce the CalendarFile has been created, pass its ID with\nthe DatetimePartitioningSpecification\nwhen setting the target for a time series project in order to use it.\nParameters:\ndataset_id (string) – The identifier of the dataset from which to create the calendar.\ndataset_version_id (string, optional) – The identifier of the dataset version from which to create the calendar.\ncalendar_name (string, optional) – A name to assign to the calendar. Defaults to the name of the dataset if not provided.\nmultiseries_id_columns (list of Optional[str]) – A list of the names of multiseries id columns to define which series an event\nbelongs to. Currently only one multiseries id column is supported.\ndelete_on_error (boolean, optional) – Whether delete calendar file from Catalog if it’s not valid.\nReturns:\ncalendar_file – Instance with initialized data.\nReturn type:\nCalendarFile\nRaises:\nAsyncProcessUnsuccessfulError – Raised if there was an error processing the provided calendar file.\nExamples\n# Creating a calendar from a dataset\ndataset = dr.Dataset.create_from_file('/home/calendars/somecalendar.csv')\ncal = dr.CalendarFile.create_calendar_from_dataset(\ndataset.id, calendar_name='Some Calendar Name'\n)\ncal.id\n>>> 5c1d4904211c0a061bc93013\ncal.name\n>>> Some Calendar Name\n# Creating a calendar from a new dataset version\nnew_dataset_version = dr.Dataset.create_version_from_file(\ndataset.id, '/home/calendars/anothercalendar.csv'\n)\ncal = dr.CalendarFile.create(\nnew_dataset_version.id, dataset_version_id=new_dataset_version.version_id\n)\ncal.id\n>>> 5c1d4904211c0a061bc93012\ncal.name\n>>> anothercalendar.csv\nclassmethod create_calendar_from_country_code(country_code, start_date, end_date)\nGenerates a calendar based on the provided country code and dataset start date and end\ndates. The provided country code should be uppercase and 2-3 characters long. See\nCalendarFile.get_allowed_country_codes for a list of allowed country codes.\nParameters:\ncountry_code (string) – The country code for the country to use for generating the calendar.\nstart_date (datetime.datetime) – The earliest date to include in the generated calendar.\nend_date (datetime.datetime) – The latest date to include in the generated calendar.\nReturns:\ncalendar_file – Instance with initialized data.\nReturn type:\nCalendarFile\nclassmethod get_allowed_country_codes(offset=None, limit=None)\nRetrieves the list of allowed country codes that can be used for generating the preloaded\ncalendars.\nParameters:\noffset (int) – Optional, defaults to 0. This many results will be skipped.\nlimit (int) – Optional, defaults to 100, maximum 1000. At most this many results are returned.\nReturns:\nA list dicts, each of which represents an allowed country codes. Each item has the\nfollowing structure:\nname : (str) The name of the country.\ncode : (str) The code for this country. This is the value that should be supplied\nto CalendarFile.create_calendar_from_country_code.\nReturn type:\nlist\nclassmethod get(calendar_id)\nGets the details of a calendar, given the id.\nParameters:\ncalendar_id (str) – The identifier of the calendar.\nReturns:\ncalendar_file – The requested calendar.\nReturn type:\nCalendarFile\nRaises:\nDataError – Raised if the calendar_id is invalid, i.e. the specified CalendarFile does not exist.\nExamples\ncal = dr.CalendarFile.get(some_calendar_id)\ncal.id\n>>> some_calendar_id\nclassmethod list(project_id=None, batch_size=None)\nGets the details of all calendars this user has view access for.\nParameters:\nproject_id (Optional[str]) – If provided, will filter for calendars associated only with the specified project.\nbatch_size (Optional[int]) – The number of calendars to retrieve in a single API call. If specified, the client may\nmake multiple calls to retrieve the full list of calendars. If not specified, an\nappropriate default will be chosen by the server.\nReturns:\ncalendar_list – A list of CalendarFile objects.\nReturn type:\nlist of CalendarFile\nExamples\ncalendars = dr.CalendarFile.list()\nlen(calendars)\n>>> 10\nclassmethod delete(calendar_id)\nDeletes the calendar specified by calendar_id.\nParameters:\ncalendar_id (str) – The id of the calendar to delete.\nThe requester must have OWNER access for this calendar.\nRaises:\nClientError – Raised if an invalid calendar_id is provided.\nReturn type:\nNone\nExamples\n# Deleting with a valid calendar_id\nstatus_code = dr.CalendarFile.delete(some_calendar_id)\nstatus_code\n>>> 204\ndr.CalendarFile.get(some_calendar_id)\n>>> ClientError: Item not found\nclassmethod update_name(calendar_id, new_calendar_name)\nChanges the name of the specified calendar to the specified name.\nThe requester must have at least READ_WRITE permissions on the calendar.\nParameters:\ncalendar_id (str) – The id of the calendar to update.\nnew_calendar_name (str) – The new name to set for the specified calendar.\nReturns:\nstatus_code – 200 for success\nReturn type:\nint\nRaises:\nClientError – Raised if an invalid calendar_id is provided.\nExamples\nresponse = dr.CalendarFile.update_name(some_calendar_id, some_new_name)\nresponse\n>>> 200\ncal = dr.CalendarFile.get(some_calendar_id)\ncal.name\n>>> some_new_name\nclassmethod share(calendar_id, access_list)\nShares the calendar with the specified users, assigning the specified roles.\nParameters:\ncalendar_id (str) – The id of the calendar to update\naccess_list (List[SharingAccess]) – A list of dr.SharingAccess objects. Specify None for the role to delete a user’s\naccess from the specified CalendarFile. For more information on specific access levels,\nsee the sharing documentation.\nReturns:\nstatus_code – 200 for success\nReturn type:\nint\nRaises:\nClientError – Raised if unable to update permissions for a user.\nAssertionError – Raised if access_list is invalid.\nExamples\n# assuming some_user is a valid user, share this calendar with some_user\nsharing_list = [dr.SharingAccess(some_user_username,\ndr.enums.SHARING_ROLE.READ_WRITE)]\nresponse = dr.CalendarFile.share(some_calendar_id, sharing_list)\nresponse.status_code\n>>> 200\n# delete some_user from this calendar, assuming they have access of some kind already\ndelete_sharing_list = [dr.SharingAccess(some_user_username,\nNone)]\nresponse = dr.CalendarFile.share(some_calendar_id, delete_sharing_list)\nresponse.status_code\n>>> 200\n# Attempt to add an invalid user to a calendar\ninvalid_sharing_list = [dr.SharingAccess(invalid_username,\ndr.enums.SHARING_ROLE.READ_WRITE)]\ndr.CalendarFile.share(some_calendar_id, invalid_sharing_list)\n>>> ClientError: Unable to update access for this calendar\nclassmethod get_access_list(calendar_id, batch_size=None)\nRetrieve a list of users that have access to this calendar.\nParameters:\ncalendar_id (str) – The id of the calendar to retrieve the access list for.\nbatch_size (Optional[int]) – The number of access records to retrieve in a single API call. If specified, the client\nmay make multiple calls to retrieve the full list of calendars. If not specified, an\nappropriate default will be chosen by the server.\nReturns:\naccess_control_list – A list of SharingAccess objects.\nReturn type:\nlist of SharingAccess\nRaises:\nClientError – Raised if user does not have access to calendar or calendar does not exist.\nclass datarobot.models.calendar_file.CountryCode",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/projects.html",
      "tags": [
        "api-reference",
        "advanced",
        "beginner",
        "documentation",
        "example"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/projects.html",
        "content_length": 187862
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.external_baseline_validation",
        "dr.calendarfile.create_calendar_from_dataset",
        "project.partition",
        "datarobot.enums.ts_blender_method",
        "dr.blueprint.get",
        "datarobot.helpers.partitioning_methods",
        "project.get_dataset",
        "dr.scoring_type.validation",
        "project.set_datetime_partitioning",
        "dr.calendarfile.update_name",
        "datarobot.enums.differencing_method",
        "datarobot.enums.time_units",
        "datarobot.helpers.advancedoptions",
        "dr.project.get",
        "datarobot.enums.cv_method",
        "dr.dataset.create_version_from_file",
        "datarobot.enums.variable_type_transform",
        "datarobot.helpers.classmappingaggregationsettings",
        "project.get_featurelists",
        "datarobot.models.interactionfeature",
        "project.list",
        "dr.calendarfile.list",
        "dr.enums.sharing_role",
        "dr.calendarfile.delete",
        "project.get",
        "project.get_metrics",
        "datarobot.enums.blender_method",
        "project.train",
        "datarobot.helpers.eligibility_result",
        "project.validate_external_time_series_baseline",
        "project.set_target",
        "datarobot.enums.unsupervisedtypeenum",
        "datarobot.models.dataset",
        "project.analyze_and_modelthe",
        "model.get",
        "dr.calendarfile.get",
        "dr.model.get",
        "datarobot.models.statuscheckjob",
        "dr.enums.time_units",
        "datarobot.models.jobstatusresult",
        "datarobot.enums.date_extraction",
        "datarobot.enums.datetime_autopilot_data_selection_method",
        "project.get_blueprints",
        "project.id",
        "model.prediction_threshold",
        "dr.calendarfile.share",
        "project.get_modeling_features",
        "project.create",
        "project.get_top_model",
        "project.create_from_hdfs",
        "project.train_datetime",
        "project.set_partitioning_method",
        "model.is_n_clusters_dynamically_determined",
        "project.create_featurelist",
        "datarobot.models.segmentation",
        "dr.calendarfile.create",
        "model.prediction_threshold_read_only",
        "datarobot.models.project",
        "dr.scoring_type.cross_validation",
        "project.startcombines",
        "project.get_models",
        "project.get_featurelist_by_name",
        "dr.partitioning_methods.construct_duration_string",
        "project.create_modeling_featurelist",
        "project.start",
        "dr.dataset.create_from_file",
        "model.has_empty_clusters",
        "datarobot.enums.treat_as_exponential",
        "datarobot.enums.datetime_autopilot_data_sampling_method",
        "datarobot.models.calendar_file",
        "model.blueprint",
        "project.check_blendableto",
        "dr.enums.monotonicity_featurelist_default",
        "project.max_train_rows",
        "datarobot.enums.project_stage",
        "project.analyze_and_model",
        "datarobot.enums.biasmitigationtechnique",
        "project.partitioning_method"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_391291747923503236",
      "title": "Python API client user guide",
      "content": "Python API client user guide\nTopic\nDescription\nAdministration\nHow to manage DataRobot Self-Managed AI Platform deployments.\nData\nHow to manage data for machine learning, including importing and transforming data, and connecting to data sources.\nMLOps\nHow to deploy, monitor, manage, and govern your models in production.\nModeling\nHow to set modeling parameters before building, use the modeling workflow, and manage models and projects.\nPredictions\nHow to get predictions (“scoring”) on new data from a model.\nUse Cases\nHow to use Use Cases to group everything related to solving a specific business problem.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/index.html",
      "tags": [
        "api_reference",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/index.html",
        "content_length": 609
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.75,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_2536030574731124412",
      "title": "Data exports",
      "content": "Data exports\nUse deployment data export to retrieve the data sent for predictions along with the associated predictions.\nPrediction data export\nUse the following commands to manage prediction data exports:\nCreate a prediction data export\nTo create a prediction data export, use PredictionDataExport.create, defining the time window to include in the export\nusing the start and end parameters, as shown in the following example:\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import PredictionDataExport\nnow=datetime.now()\nprediction_data_export = PredictionDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0', start=now - timedelta(days=7), end=now)\nSpecify the model ID for export, otherwise the champion model ID is used by default:\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import PredictionDataExport\nnow=datetime.now()\nprediction_data_export = PredictionDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0',\nmodel_id='6444482e5583f6ee2e572265',\nstart=now - timedelta(days=7),\nend=now\n)\nFor deployments in batch mode, provide batch IDs to export prediction data for those batches:\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import PredictionDataExport\nnow=datetime.now()\nprediction_data_export = PredictionDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0',\nmodel_id='6444482e5583f6ee2e572265',\nstart=now - timedelta(days=7),\nend=now,\nbatch_ids=['6572db2c9f9d4ad3b9de33d0', '6572db2c9f9d4ad3b9de33d0']\n)\nThe start and end of the export can be defined as a datetime or string type.\nList prediction data exports\nTo list prediction data exports, use PredictionDataExport.list, as in the following example:\nfrom datarobot.models.deployment import PredictionDataExport\nprediction_data_exports = PredictionDataExport.list(deployment_id='5c939e08962d741e34f609f0', limit=0)\nprediction_data_exports\n>>> [PredictionDataExport('65fbe59aaa3f847bd5acc75b'),\nPredictionDataExport('65fbe59aaa3f847bd5acc75c'),\nPredictionDataExport('65fbe59aaa3f847bd5acc75a')]\nTo list all prediction data exports, set the limit to 0.\nAdjust additional parameters to filter the data as needed:\nfrom datarobot.enums import ExportStatus\nfrom datarobot.models.deployment import PredictionDataExport\nprediction_data_exports = PredictionDataExport.list(deployment_id='5c939e08962d741e34f609f0', limit=100, offset=100)\n# use additional filters\nprediction_data_exports = PredictionDataExport.list(\ndeployment_id='5c939e08962d741e34f609f0',\nmodel_id=\"6444482e5583f6ee2e572265\",\nbatch=False,\nstatus=ExportStatus.FAILED\n)\nRetrieve a prediction data export\nTo get a prediction data export by identifier, use PredictionDataExport.get, as in the following example:\nfrom datarobot.models.deployment import PredictionDataExport\nprediction_data_export = PredictionDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fbe59aaa3f847bd5acc75b'\n)\nprediction_data_exports\n>>> PredictionDataExport('65fbe59aaa3f847bd5acc75b')\nFetch prediction export datasets\nTo return data from a prediction export as dr.Dataset, use fetch_data method, as in the following example:\nfrom datarobot.models.deployment import PredictionDataExport\nprediction_data_export = PredictionDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fbe59aaa3f847bd5acc75b'\n)\nprediction_datasets = prediction_data_export.fetch_data()\nprediction_datasets\n>>> [Dataset(name='Deployment prediction data', id='65f240b0e37a9f1a104bf450')]\nprediction_dataset = prediction_datasets[0]\ndf = prediction_dataset.get_as_dataframe()\ndf.head(2)\n>>>    DR_RESERVED_PREDICTION_TIMESTAMP  ...    upstream_x_datarobot_version\n0  2024-03-13 23:00:38.998000+00:00  ...               predictionapi/X/X\n1  2024-03-13 23:00:38.998000+00:00  ...               predictionapi/X/X\nThis method can return a list of datasets; however, usually it returns one dataset . There are cases, like time series,\nwhen more than one element is returned. The obtained dataset (or datasets) can be transformed into, for example, a pandas DataFrame.\nActuals data export\nUse the following commands to manage actuals data exports:\nCreate actuals data export\nTo create actuals data export, use ActualsDataExport.create, defining the time window to include in the export\nusing the start and end parameters, as shown in the following example:\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import ActualsDataExport\nnow=datetime.now()\nactuals_data_export = ActualsDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0', start=now - timedelta(days=7), end=now\n)\nSpecify the model ID for export, otherwise the champion model ID is used by default:\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import ActualsDataExport\nnow=datetime.now()\nactuals_data_export = ActualsDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0',\nmodel_id=\"6444482e5583f6ee2e572265\",\nstart=now - timedelta(days=7),\nend=now,\n)\nTo export only actuals that are matched to predictions, set only_matched_predictions to True;\nby default all available actuals are exported.\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import ActualsDataExport\nnow=datetime.now()\nactuals_data_export = ActualsDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0',\nonly_matched_predictions=True,\nstart=now - timedelta(days=7),\nend=now,\n)\nThe start and end of the export can be defined as a datetime or string type.\nList actuals data exports\nTo list actuals data exports, use ActualsDataExport.list, as in the following example:\nfrom datarobot.models.deployment import ActualsDataExport\nactuals_data_exports = ActualsDataExport.list(deployment_id='5c939e08962d741e34f609f0', limit=0)\nactuals_data_exports\n>>> [ActualsDataExport('660456a332d0081029ee5031'),\nActualsDataExport('660456a332d0081029ee5032'),\nActualsDataExport('660456a332d0081029ee5033')]\nTo list all actuals data exports, set the limit to 0.\nAdjust additional parameters to filter the data as needed:\nfrom datarobot.enums import ExportStatus\nfrom datarobot.models.deployment import ActualsDataExport\n# use additional filters\nactuals_data_exports = ActualsDataExport.list(\ndeployment_id='5c939e08962d741e34f609f0',\noffset=500,\nlimit=50,\nstatus=ExportStatus.SUCCEEDED\n)\nRetrieve actuals data export\nTo get actuals data export by identifier, use ActualsDataExport.get, as in the following example:\nfrom datarobot.models.deployment import ActualsDataExport\nactuals_data_export = ActualsDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='660456a332d0081029ee4031'\n)\nactuals_data_export\n>>> ActualsDataExport('660456a332d0081029ee4031')\nFetch actuals export datasets\nTo return data from actuals export as dr.Dataset, use fetch_data method, as in the following example:\nfrom datarobot.models.deployment import ActualsDataExport\nactuals_data_export = ActualsDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='660456a332d0081029ee4031'\n)\nactuals_datasets = actuals_data_export.fetch_data()\nactuals_datasets\n>>> [Dataset(name='Deployment prediction data', id='65f240b0e37a9f1a104bf450')]\nactuals_dataset = actuals_datasets[0]\ndf = actuals_dataset.get_as_dataframe()\ndf.head(2)\n>>>    association_id                  timestamp  actuals  predictions\n0               1  2024-03-20 15:00:00+00:00     21.0    18.125388\n1              10  2024-03-20 15:00:00+00:00     12.0    22.805252\nThis method may return a list of datasets; however, it usually returns one dataset.\nThe obtained dataset (or datasets) can be transformed into, for example, a pandas DataFrame.\nTraining data export\nUse the following commands to manage training data exports:\nCreate training data export\nTo create training data export, use TrainingDataExport.create and define the deployment ID, as shown in the following example:\nfrom datarobot.models.deployment import TrainingDataExport\ndataset_id = TrainingDataExport.create(deployment_id='5c939e08962d741e34f609f0')\nSpecify the model ID for export, otherwise the champion model ID is used by default:\nfrom datarobot.models.deployment import TrainingDataExport\ndataset_id = TrainingDataExport.create(\ndeployment_id='5c939e08962d741e34f609f0', model_id='6444482e5583f6ee2e572265')\ndataset_id\n>>> 65fb0c25019ca3333bbb4c10\nThis method returns the ID of the dataset that contains the training data. This dataset is saved in the AI Catalog.\nList training data exports\nTo list training data exports, use TrainingDataExport.list, as in the following example:\nfrom datarobot.models.deployment import TrainingDataExport\ntraining_data_exports = TrainingDataExport.list(deployment_id='5c939e08962d741e34f609f0')\ntraining_data_exports\n>>> [TrainingDataExport('6565fbf2356124f1daa3acc522')]\nRetrieve training data export\nTo get training data export by identifier, use TrainingDataExport.get, as in the following example:\nfrom datarobot.models.deployment import ActualsDataExport\ntraining_data_export = TrainingDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='65fbf2356124f1daa3acc522'\n)\ntraining_data_export\n>>> TrainingDataExport('6565fbf2356124f1daa3acc522')\nFetch training export dataset\nTo return data from the training export as dr.Dataset, use fetch_data, as in the following example:\nfrom datarobot.models.deployment import TrainingDataExport\ntraining_data_export = TrainingDataExport.get(\ndeployment_id='5c939e08962d741e34f609f0', export_id='660456a332d0081029ee4031'\n)\ntraining_dataset = training_data_export.fetch_data()\ntraining_dataset\n>>> [Dataset(name='training-data-10k_diabetes.csv', id='65fb0c25019ca3333bbb4c10')]\ndf = training_dataset.get_as_dataframe()\ndf.head(2)\n>>> acetohexamide  time_in_hospital  ... number_outpatient payer_code\n0            No                 1  ...                 0         YY\n1            No                 2  ...                 0         XX\nThis method returns a single training dataset. The obtained dataset can be transformed into, for example, a pandas DataFrame.\nData quality export\nThe data-quality exports provide feedback on LLM deployments. It is intended to be used in conjunction with custom-metrics for prompt monitoring.\nUse the following commands to manage data-quality exports:\nData quality export list\nTo list data quality exports, use DataQualityExport.list, as in the following example:\nfrom datetime import datetime, timedelta\nfrom datarobot.models.deployment import DataQualityExport\nnow=datetime.now()\ndata_quality_exports = DataQualityExport.list(\ndeployment_id='66903c40f18e6ec90fd7c8c7',\nstart=now - timedelta(days=1),\nend=now,\n)\ndata_quality_exports\n>>> [DataQualityExport(6447ca39c6a04df6b5b0ed19c6101e3c),\n...\nDataQualityExport(0ff46fd3636545a9ac3e15ee1dbd8638)]\ndata_quality_deports[0].metrics\n>>> [{'id': '669688f90a23524131e2d301', 'name': 'metric 3', 'value': None},\n{'id': '669688e633ae1ffce40eb2f8', 'name': 'metric 2', 'value': 45.0},\n{'id': '669688d282c9384ab8068a6c', 'name': 'metric 1', 'value': 178.0}]\nThe start and end of the export can be defined as a datetime or string type. And, there are many options for filtering and ordering the data.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/data_exports.html",
      "tags": [
        "advanced",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/data_exports.html",
        "content_length": 11127
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.deployment"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-7163502634158214068",
      "title": "Custom Metrics",
      "content": "Custom Metrics\nCustom metrics are used to compute and monitor user-defined metrics.\nManage custom metrics\nUse the following commands to manage custom metrics:\nCreate custom metric\nTo create a custom metric, use CustomMetric.create, as shown in the following example:\nFill in all metric required custom metric fields:\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import CustomMetricAggregationType, CustomMetricDirectionality\ncustom_metric = CustomMetric.create(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nname=\"My custom metric\",\nunits=\"x\",\nis_model_specific=True,\naggregation_type=CustomMetricAggregationType.AVERAGE,\ndirectionality=CustomMetricDirectionality.HIGHER_IS_BETTER,\n)\nSet the baseline value during metric creation:\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import (\nCustomMetricAggregationType,\nCustomMetricDirectionality,\nCustomMetricBucketTimeStep,\n)\ncustom_metric = CustomMetric.create(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nname=\"My custom metric 2\",\nunits=\"y\",\nbaseline_value=12,\nis_model_specific=True,\naggregation_type=CustomMetricAggregationType.AVERAGE,\ndirectionality=CustomMetricDirectionality.HIGHER_IS_BETTER,\ntime_step=CustomMetricBucketTimeStep.HOUR,\n)\nDefine the names of the columns that will be used when submitting values from a dataset:\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import CustomMetricAggregationType, CustomMetricDirectionality\ncustom_metric = CustomMetric.create(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nname=\"My custom metric 3\",\nunits=\"z\",\nbaseline_value=1000,\nis_model_specific=False,\naggregation_type=CustomMetricAggregationType.SUM,\ndirectionality=CustomMetricDirectionality.LOWER_IS_BETTER,\ntimestamp_column_name=\"My Timestamp column\",\ntimestamp_format=\"%d/%m/%y\",\nvalue_column_name=\"My Value column\",\nsample_count_column_name=\"My Sample Count column\",\n)\nFor batches:\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import CustomMetricAggregationType, CustomMetricDirectionality\ncustom_metric = CustomMetric.create(\ndeployment_id=\"5c939e08962d741e34f609f0\",\nname=\"My custom metric 4\",\nunits=\"z\",\nbaseline_value=1000,\nis_model_specific=False,\naggregation_type=CustomMetricAggregationType.SUM,\ndirectionality=CustomMetricDirectionality.LOWER_IS_BETTER,\nbatch_column_name=\"My Batch column\",\n)\nList custom metrics\nTo list all custom metrics available for a given deployment, use CustomMetric.list, as in the following example:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metrics = CustomMetric.list(deployment_id=\"5c939e08962d741e34f609f0\")\ncustom_metrics\n>>> [CustomMetric('66015bdda7ba87e66baa09ee' | 'My custom metric 2'),\nCustomMetric('66015bdc5f850c5df3aa09f0' | 'My custom metric')]\nRetrieve custom metrics\nTo get a custom metric by unique identifier, use CustomMetric.get, as in the following example:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\ncustom_metric\n>>> CustomMetric('66015bdc5f850c5df3aa09f0' | 'My custom metric')\nUpdate custom metrics\nTo get a custom metric by unique identifier and update it, use CustomMetric.get() and then update(), as in the following example:\nfrom datarobot.models.deployment import CustomMetric\nfrom datarobot.enums import CustomMetricAggregationType, CustomMetricDirectionality\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\ncustom_metric.update(\nname=\"Updated custom metric\",\nunits=\"foo\",\nbaseline_value=-12,\naggregation_type=CustomMetricAggregationType.SUM,\ndirectionality=CustomMetricDirectionality.LOWER_IS_BETTER,\n)\nUnset custom metric baseline\nTo reset the current metric baseline, use a separate method unset_baseline(), as in the following example:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\ncustom_metric.baseline_values\n>>> [{'value': -12.0}]\ncustom_metric.unset_baseline()\ncustom_metric.baseline_values\n>>> []\nDelete custom metrics\nTo delete a custom metric by unique identifier, use CustomMetric.delete, as in the following example:\nfrom datarobot.models.deployment import CustomMetric\nCustomMetric.delete(deployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\nSubmit custom metric values\nUse the following commands to submit custom metric values:\nSubmit values from JSON\nTo submit aggregated custom metric values from JSON, use submit_values method, as shown in the following example:\nSubmit data in the form of a list of dictionaries:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\ndata = [{'value': 12, 'sample_size': 3, 'timestamp': '2024-03-15T18:00:00'},\n{'value': 11, 'sample_size': 5, 'timestamp': '2024-03-15T17:00:00'},\n{'value': 14, 'sample_size': 3, 'timestamp': '2024-03-15T16:00:00'}]\ncustom_metric.submit_values(data=data)\n# data witch association IDs\ndata = [{'value': 15, 'sample_size': 2, 'timestamp': '2024-03-15T21:00:00', 'association_id': '65f44d04dbe192b552e752aa'},\n{'value': 13, 'sample_size': 6, 'timestamp': '2024-03-15T20:00:00', 'association_id': '65f44d04dbe192b552e753bb'},\n{'value': 17, 'sample_size': 2, 'timestamp': '2024-03-15T19:00:00', 'association_id': '65f44d04dbe192b552e754cc'}]\ncustom_metric.submit_values(data=data)\nSubmit data in the form of pandas DataFrame:\nfrom datetime import datetime\nimport pandas as pd\nfrom datarobot.models.deployment import CustomMetric\ndf = pd.DataFrame(\ndata={\n\"timestamp\": [\ndatetime(year=2024, month=3, day=10),\ndatetime(year=2024, month=3, day=11),\ndatetime(year=2024, month=3, day=12),\ndatetime(year=2024, month=3, day=13),\ndatetime(year=2024, month=3, day=14),\ndatetime(year=2024, month=3, day=15),\n],\n\"value\": [28, 34, 29, 1, 2, 13],\n\"sample_size\": [1, 2, 3, 4, 1, 2],\n}\n)\ncustom_metric.submit_values(data=df)\nFor deployment-specific metrics, do not provide model information.\nFor model specific metrics set model_package_id or model_id:\ncustom_metric.submit_values(data=data, model_package_id=\"6421df32525c58cc6f991f25\")\ncustom_metric.submit_values(data=data, model_id=\"6444482e5583f6ee2e572265\")\nUse a dry run for test uploads (without saving metric data on the DR side), this option is disabled by default:\ncustom_metric.submit_values(data=data, dry_run=True)\nTo send data for a given segment, it must be specified as follow, more than one segment can be specified:\nsegments = [{\"name\": \"custom_seg\", \"value\": \"baz\"}]\ncustom_metric.submit_values(data=data, segments=segments)\nBatch mode  requires specifying batch IDs, for batches always specify a model by model_package_id or model_id:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f600e1\", custom_metric_id=\"65f17bdcd2d66683cdfc2224\")\ndata = [{'value': 12, 'sample_size': 3, 'batch': '65f44c93fedc5de16b673aaa'},\n{'value': 11, 'sample_size': 5, 'batch': '65f44c93fedc5de16b673bbb'},\n{'value': 14, 'sample_size': 3, 'batch': '65f44c93fedc5de16b673ccc'}]\ncustom_metric.submit_values(data=data, model_package_id=\"6421df32525c58cc6f991f25\")\nSubmit a single value\nTo report a single metric value at the current moment, use submit_single_value method, as in the following example:\nFor deployment-specific metrics:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\ncustom_metric.submit_single_value(value=16)\nFor model specific metrics set model_package_id or model_id:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\ncustom_metric.submit_single_value(value=16, model_package_id=\"6421df32525c58cc6f991f25\")\ncustom_metric.submit_single_value(value=16, model_id=\"6444482e5583f6ee2e572265\")\nDry run and segments work analogously to reporting aggregated metric values:\ncustom_metric.submit_single_value(value=16, dry_run=True)\nsegments = [{\"name\": \"custom_seg\", \"value\": \"boo\"}]\ncustom_metric.submit_single_value(value=16, segments=segments)\nThe sent value timestamp indicates the time the request was sent, the number of samples values is always 1.\nThis method does not support batch submissions.\nSubmit values from a dataset\nTo report aggregated custom metrics values from dataset (AI catalog), use submit_values_from_catalog method, as in the following example:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\n# for deployment specific metrics\ncustom_metric.submit_values_from_catalog(dataset_id=\"61093144cabd630828bca321\")\n# for model specific metrics set model_package_id or model_id\ncustom_metric.submit_values_from_catalog(\ndataset_id=\"61093144cabd630828bca321\",\nmodel_package_id=\"6421df32525c58cc6f991f25\"\n)\nFor segmented analysis define the name of the column in the dataset and the segment it corresponds to:\nsegments = [{\"name\": \"custom_seg\", \"column\": \"column_with_segment_values\"}]\ncustom_metric.submit_values_from_catalog(\ndataset_id=\"61093144cabd630828bca321\",\nmodel_package_id=\"6421df32525c58cc6f991f25\",\nsegments=segments\n)\nFor batches, specify batch IDs in the dataset or send the entire dataset for a single batch ID:\ncustom_metric.submit_values_from_catalog(\ndataset_id=\"61093144cabd630828bca432\",\nmodel_package_id=\"6421df32525c58cc6f991f25\",\nbatch_id=\"65f7f71198c2f234b4cb2f7d\"\n)\nThe names of the columns in the dataset should correspond to the names of the columns that were defined in\nthe custom metric. In addition, the format of the timestamps should also be the same as defined in the metric.\nIf the sample size is not specified, it is treated as a 1 sample by default.\nThe following is an example of the shape of a dataset saved in the AI catalog:\ntimestamp\nsample_size\nvalue\n12/12/22\n1\n22\n13/12/22\n2\n23\n14/12/22\n3\n24\n15/12/22\n4\n25\nSample dataset for batches:\nbatch\nsample_size\nvalue\n6572db2c9f9d4ad3b9de33d0\n1\n22\n6572db2c9f9d4ad3b9de33d0\n2\n23\n6572db319f9d4ad3b9de33d9\n3\n24\n6572db319f9d4ad3b9de33d9\n4\n25\nRetrieve custom metric values over time\nUse the following commands to retrieve custom metric values:\nRetrieve values over a time period\nTo retrieve values of a custom metric over a time period, use get_values_over_time, as in the following example:\nfrom datetime import datetime, timedelta\nfrom datarobot.enums import BUCKET_SIZE\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\nnow = datetime.now()\n# specify the time window and bucket size by which results are grouped, the default bucket is 7 days\nvalues_over_time = custom_metric.get_values_over_time(\nstart=now - timedelta(days=2), end=now, bucket_size=BUCKET_SIZE.P1D)\nvalues_over_time\n>>> CustomMetricValuesOverTime('2024-03-21 20:15:00+00:00'- '2024-03-23 20:15:00+00:00')\"\nvalues_over_time.bucket_values\n>>>{datetime.datetime(2024, 3, 22, 10, 0, tzinfo=tzutc()): 1.0,\n>>> datetime.datetime(2024, 3, 22, 11, 0, tzinfo=tzutc()): 123.0}}\nvalues_over_time.bucket_sample_sizes\n>>>{datetime.datetime(2024, 3, 22, 10, 0, tzinfo=tzutc()): 1,\n>>> datetime.datetime(2024, 3, 22, 11, 0, tzinfo=tzutc()): 1}}\nvalues_over_time.get_buckets_as_dataframe()\n>>>                        start                       end  value  sample_size\n>>> 0  2024-03-21 00:00:00+00:00 2024-03-22 00:00:00+00:00    1.0            1\n>>> 1  2024-03-22 00:00:00+00:00 2024-03-23 00:00:00+00:00  123.0            1\nFor model specific metrics set model_package_id or model_id:\nvalues_over_time = custom_metric.get_values_over_time(\nstart=now - timedelta(days=1), end=now, model_package_id=\"6421df32525c58cc6f991f25\")\nvalues_over_time = custom_metric.get_values_over_time(\nstart=now - timedelta(days=1), end=now, model_id=\"6444482e5583f6ee2e572265\")\nTo retrieve values for a specific segment, specify the segment name and its value:\nvalues_over_time = custom_metric.get_values_over_time(\nstart=now - timedelta(days=1), end=now, segment_attribute=\"custom_seg\", segment_value=\"val_1\")\nRetrieve a summary over a time period\nTo retrieve summary of a custom metric over a time period, use get_summary method, as in the following example:\nfrom datetime import datetime, timedelta\nfrom datarobot.enums import BUCKET_SIZE\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\", custom_metric_id=\"65f17bdcd2d66683cdfc1113\")\nnow = datetime.now()\n# specify the time window\nsummary = custom_metric.get_summary(start=now - timedelta(days=7), end=now)\nprint(summary)\n>> \"CustomMetricSummary(2024-03-15 15:52:13.392178+00:00 - 2024-03-22 15:52:13.392168+00:00:\n{'id': '65fd9b1c0c1a840bc6751ce0', 'name': 'My custom metric', 'value': 215.0, 'sample_count': 13,\n'baseline_value': 12.0, 'percent_change': 24.02})\"\nFor model specific metrics set model_package_id or model_id:\nsummary = custom_metric.get_summary(\nstart=now - timedelta(days=7), end=now, model_package_id=\"6421df32525c58cc6f991f25\")\nsummary = custom_metric.get_summary(\nstart=now - timedelta(days=7), end=now, model_id=\"6444482e5583f6ee2e572265\")\nTo retrieve summary for a specific segment, specify the segment name and its value:\nsummary = custom_metric.get_summary(\nstart=now - timedelta(days=7), end=now, segment_attribute=\"custom_seg\", segment_value=\"val_1\")\nRetrieve values over batch\nTo retrieve values of a custom metric over batch, use get_values_over_batch, as in the following example:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\n# all batch metrics all model specific\nvalues_over_batch = custom_metric.get_values_over_batch(model_package_id='6421df32525c58cc6f991f25')\nvalues_over_batch.bucket_values\n>>> {'6572db2c9f9d4ad3b9de33d0': 35.0, '6572db2c9f9d4ad3b9de44e1': 105.0}\nvalues_over_batch.bucket_sample_sizes\n>>> {'6572db2c9f9d4ad3b9de33d0': 6, '6572db2c9f9d4ad3b9de44e1': 8}\nvalues_over_batch.get_buckets_as_dataframe()\n>>>                    batch_id                     batch_name  value  sample_size\n>>> 0  6572db2c9f9d4ad3b9de33d0  Batch 1 - 03/26/2024 13:04:46   35.0            6\n>>> 1  6572db2c9f9d4ad3b9de44e1  Batch 2 - 03/26/2024 13:06:04  105.0            8\nFor specific batches, set batch_ids:\nvalues_over_batch = custom_metric.get_values_over_batch(\nmodel_package_id='6421df32525c58cc6f991f25', batch_ids=[\"65f44c93fedc5de16b673aaa\", \"65f44c93fedc5de16b673bbb\"])\nTo retrieve values for a specific segment, specify the segment name and its value:\nvalues_over_batch = custom_metric.get_values_over_batch(\nmodel_package_id='6421df32525c58cc6f991f25', segment_attribute=\"custom_seg\", segment_value=\"val_1\")\nRetrieve a summary over batch\nTo retrieve summary of a custom metric over batch, use get_summary, as in the following example:\nfrom datarobot.models.deployment import CustomMetric\ncustom_metric = CustomMetric.get(\ndeployment_id=\"5c939e08962d741e34f609f0\",\ncustom_metric_id=\"65f17bdcd2d66683cdfc1113\"\n)\n# all batch metrics all model specific\nbatch_summary = custom_metric.get_batch_summary(model_package_id='6421df32525c58cc6f991f25')\nprint(batch_summary)\n>> CustomMetricBatchSummary({'id': '6605396413434b3a7b74342c', 'name': 'batch metric', 'value': 41.25,\n'sample_count': 28, 'baseline_value': 123.0, 'percent_change': -66.46})\nFor specific batches, set batch_ids:\nbatch_summary = custom_metric.get_batch_summary(\nmodel_package_id='6421df32525c58cc6f991f25', batch_ids=[\"65f44c93fedc5de16b673aaa\", \"65f44c93fedc5de16b673bbb\"])\nTo retrieve values for a specific segment, specify the segment name and its value:\nbatch_summary = custom_metric.get_batch_summary(\nmodel_package_id='6421df32525c58cc6f991f25', segment_attribute=\"custom_seg\", segment_value=\"val_1\")",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/custom_metrics.html",
      "tags": [
        "advanced",
        "api_reference",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/custom_metrics.html",
        "content_length": 16247
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.deployment"
      ],
      "complexity_score": 0.7999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_6461900687136724405",
      "title": "Custom Models",
      "content": "Custom Models\nCustom models provide users the ability to run arbitrary modeling code in an environment defined by the user.\nManage Execution Environments\nExecution Environment defines the runtime environment for custom models.\nExecution Environment Version is a revision of Execution Environment with an actual runtime definition.\nPlease refer to DataRobot User Models (https://github.com/datarobot/datarobot-user-models) for sample\nenvironments.\nCreate Execution Environment\nTo create an Execution Environment run:\nimport datarobot as dr\nexecution_environment = dr.ExecutionEnvironment.create(\nname=\"Python3 PyTorch Environment\",\ndescription=\"This environment contains Python3 pytorch library.\",\n)\nexecution_environment.id\n>>> '5b6b2315ca36c0108fc5d41b'\nCreate an execution environment version from Docker Context or Docker URI\nYou can create an Execution Environment Version using either a Docker image URI, a Docker context, or both.\nIf you provide both, the environment version is built from the image URI,\nwhile the context is uploaded for informational purposes and can be later downloaded.\nThe execution environment versions offer two creation modes: synchronous and asynchronous.\nSynchronous way means that program execution will be blocked until an Execution Environment Version\ncreation process is finished with either success or failure:\nimport datarobot as dr\n# use execution_environment created earlier\nenvironment_version = dr.ExecutionEnvironmentVersion.create(\nexecution_environment.id,\ndocker_context_path=\"datarobot-user-models/public_dropin_environments/python3_pytorch\",\nmax_wait=3600,  # 1 hour timeout\n)\nenvironment_version.id\n>>> '5eb538959bc057003b487b2d'\nenvironment_version.build_status\n>>> 'success'\nAsynchronous way means that program execution will be not blocked, but an Execution Environment Version\ncreated will not be ready to be used for some time, until its creation process is finished.\nIn such case, it will be required to manually call refresh()\nfor the Execution Environment Version and check if its build_status is “success”.\nTo create an Execution Environment Version without blocking a program, set max_wait to None:\nimport datarobot as dr\n# use execution_environment created earlier\n# create environment version from docker context\nenvironment_version = dr.ExecutionEnvironmentVersion.create(\nexecution_environment.id,\ndocker_context_path=\"datarobot-user-models/public_dropin_environments/python3_pytorch\",\nmax_wait=None,  # set None to not block execution on this method\n)\nenvironment_version.id\n>>> '5eb538959bc057003b487b2d'\nenvironment_version.build_status\n>>> 'processing'\n# after some time\nenvironment_version.refresh()\nenvironment_version.build_status\n>>> 'success'\n# now create anoter environment version from docker image URI\nenvironment_version = dr.ExecutionEnvironmentVersion.create(\nexecution_environment.id,\ndocker_image_uri=\"test_org/test_repo:test_tag\",\nmax_wait=None,  # set None to not block execution on this method\n)\nenvironment_version.id\n>>> '5eb538959bc057003b4943d2'\nenvironment_version.build_status\n>>> 'success'\nenvironment_version.docker_image_uri\n'test_org/test_repo:test_tag'\nIf your environment requires additional metadata to be supplied for models using it, you can create an\nenvironment with additional metadata keys. Custom model versions that use this environment must specify\nvalues for these keys before they can be used to run tests or make deployments.  The values will be baked in\nas environment variables with field_name as the environment variable name.\nimport datarobot as dr\nfrom datarobot.models.execution_environment import RequiredMetadataKey\nexecution_environment = dr.ExecutionEnvironment.create(\nname=\"Python3 PyTorch Environment\",\ndescription=\"This environment contains Python3 pytorch library.\",\nrequired_metadata_keys=[\nRequiredMetadataKey(field_name=\"MY_VAR\", display_name=\"A value needed by hte environment\")\n],\n)\nmodel_version = dr.CustomModelVersion.create_clean(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nfolder_path=custom_model_folder,\nrequired_metadata={\"MY_VAR\": \"a value\"}\n)\nList Execution Environments\nUse the following command to list execution environments available to the user.\nimport datarobot as dr\nexecution_environments = dr.ExecutionEnvironment.list()\nexecution_environments\n>>> [ExecutionEnvironment('[DataRobot] Python 3 PyTorch Drop-In'), ExecutionEnvironment('[DataRobot] Java Drop-In')]\nenvironment_versions = dr.ExecutionEnvironmentVersion.list(execution_environment.id)\nenvironment_versions\n>>> [ExecutionEnvironmentVersion('v1')]\nRefer to ExecutionEnvironment for properties of the execution environment object and\nExecutionEnvironmentVersion for properties of the execution environment object version.\nYou can also filter the execution environments that are returned by passing a string as search_for parameter -\nonly the execution environments that contain the passed string in name or description will be returned.\nimport datarobot as dr\nexecution_environments = dr.ExecutionEnvironment.list(search_for='java')\nexecution_environments\n>>> [ExecutionEnvironment('[DataRobot] Java Drop-In')]\nExecution environment versions can be filtered by build status.\nimport datarobot as dr\nenvironment_versions = dr.ExecutionEnvironmentVersion.list(\nexecution_environment.id, dr.EXECUTION_ENVIRONMENT_VERSION_BUILD_STATUS.PROCESSING\n)\nenvironment_versions\n>>> [ExecutionEnvironmentVersion('v1')]\nRetrieve Execution Environment\nTo retrieve an execution environment and an execution environment version by identifier,\nrather than list all available ones, do the following:\nimport datarobot as dr\nexecution_environment = dr.ExecutionEnvironment.get(execution_environment_id='5506fcd38bd88f5953219da0')\nexecution_environment\n>>> ExecutionEnvironment('[DataRobot] Python 3 PyTorch Drop-In')\nenvironment_version = dr.ExecutionEnvironmentVersion.get(\nexecution_environment_id=execution_environment.id, version_id='5eb538959bc057003b487b2d')\nenvironment_version\n>>> ExecutionEnvironmentVersion('v1')\nUpdate Execution Environment\nTo update name and/or description of the execution environment run:\nimport datarobot as dr\nexecution_environment = dr.ExecutionEnvironment.get(execution_environment_id='5506fcd38bd88f5953219da0')\nexecution_environment.update(name='new name', description='new description')\nDelete Execution Environment\nTo delete the execution environment and execution environment version, use the following commands.\nimport datarobot as dr\nexecution_environment = dr.ExecutionEnvironment.get(execution_environment_id='5506fcd38bd88f5953219da0')\nexecution_environment.delete()\nGet Execution Environment build log\nTo get execution environment version build log run:\nimport datarobot as dr\nenvironment_version = dr.ExecutionEnvironmentVersion.get(\nexecution_environment_id='5506fcd38bd88f5953219da0', version_id='5eb538959bc057003b487b2d')\nlog, error = environment_version.get_build_log()\nManage Custom Models\nCustom Inference Model is user-defined modeling code that supports making predictions against it.\nCustom Inference Model supports regression and binary classification target types.\nTo upload actual modeling code Custom Model Version must be created for a custom model.\nPlease see Custom Model Version documentation.\nCreate Custom Inference Model\nTo create a regression Custom Inference Model run:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 PyTorch Custom Model',\ntarget_type=dr.TARGET_TYPE.REGRESSION,\ntarget_name='MEDV',\ndescription='This is a Python3-based custom model. It has a simple PyTorch model built on boston housing',\nlanguage='python'\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\nWhen creating a binary classification Custom Inference Model,\npositive_class_label and negative_class_label must be set:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 PyTorch Custom Model',\ntarget_type=dr.TARGET_TYPE.BINARY,\ntarget_name='readmitted',\npositive_class_label='False',\nnegative_class_label='True',\ndescription='This is a Python3-based custom model. It has a simple PyTorch model built on 10k_diabetes dataset',\nlanguage='Python 3'\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\nWhen creating a multiclass classification Custom Inference Model,\nclass_labels must be provided:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 PyTorch Custom Model',\ntarget_type=dr.TARGET_TYPE.MULTICLASS,\ntarget_name='readmitted',\nclass_labels=['hot dog', 'burrito', 'hoagie', 'reuben'],\ndescription='This is a Python3-based custom model. It has a simple PyTorch model built on sandwich dataset',\nlanguage='Python 3'\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\nFor convenience when there are many class labels, multiclass labels can also be provided as a file.\nThe file should have all the class labels separated by newline:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 PyTorch Custom Model',\ntarget_type=dr.TARGET_TYPE.MULTICLASS,\ntarget_name='readmitted',\nclass_labels_file='/path/to/classlabels.txt',\ndescription='This is a Python3-based custom model. It has a simple PyTorch model built on sandwich dataset',\nlanguage='Python 3'\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\nFor unstructured model target_name parameter is optional and is ignored if provided.\nTo create an unstructured Custom Inference Model run:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 Unstructured Custom Model',\ntarget_type=dr.TARGET_TYPE.UNSTRUCTURED,\ndescription='This is a Python3-based unstructured model',\nlanguage='python'\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\nFor anomaly detection models, the target_name parameter is also optional and is ignored if provided.\nTo create an anomaly Custom Inference Model run:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 Unstructured Custom Model',\ntarget_type=dr.TARGET_TYPE.ANOMALY,\ndescription='This is a Python3-based anomaly detection model',\nlanguage='python'\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\nTo create a Custom Inference Model with specific k8s resources:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 PyTorch Custom Model',\ntarget_type=dr.TARGET_TYPE.BINARY,\ntarget_name='readmitted',\npositive_class_label='False',\nnegative_class_label='True',\ndescription='This is a Python3-based custom model. It has a simple PyTorch model built on 10k_diabetes dataset',\nlanguage='Python 3',\nmaximum_memory=512*1024*1024,\n)\nCustom Inference Model k8s resources are optional and unless specifically provided, the configured defaults\nwill be used.\nTo create a Custom Inference Model enabling training data assignment on the model version level,\nprovide the is_training_data_for_versions_permanently_enabled=True parameter.\nFor more information, refer to the Custom model version creation with training data documentation.\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.create(\nname='Python 3 PyTorch Custom Model',\ntarget_type=dr.TARGET_TYPE.REGRESSION,\ntarget_name='MEDV',\ndescription='This is a Python3-based custom model. It has a simple PyTorch model built on boston housing',\nlanguage='python',\nis_training_data_for_versions_permanently_enabled=True\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\nList Custom Inference Models\nUse the following command to list Custom Inference Models available to the user:\nimport datarobot as dr\ndr.CustomInferenceModel.list()\n>>> [CustomInferenceModel('my model 2'), CustomInferenceModel('my model 1')]\n# use these parameters to filter results:\ndr.CustomInferenceModel.list(\nis_deployed=True,  # set to return only deployed models\norder_by='-updated',  # set to define order of returned results\nsearch_for='model 1',  # return only models containing 'model 1' in name or description\n)\n>>> CustomInferenceModel('my model 1')\nPlease refer to list() for detailed parameter description.\nRetrieve Custom Inference Model\nTo retrieve a specific Custom Inference Model, run:\nimport datarobot as dr\ndr.CustomInferenceModel.get('5ebe95044024035cc6a65602')\n>>> CustomInferenceModel('my model 1')\nUpdate Custom Model\nTo update Custom Inference Model properties execute the following:\nimport datarobot as dr\ncustom_model = dr.CustomInferenceModel.get('5ebe95044024035cc6a65602')\ncustom_model.update(\nname='new name',\ndescription='new description',\n)\nPlease, refer to update() for the full list of properties that can be updated.\nDownload latest revision of Custom Inference Model\nTo download content of the latest Custom Model Version of CustomInferenceModel as a ZIP archive:\nimport datarobot as dr\npath_to_download = '/home/user/Documents/myModel.zip'\ncustom_model = dr.CustomInferenceModel.get('5ebe96b84024035cc6a6560b')\ncustom_model.download_latest_version(path_to_download)\nAssign training data to a custom inference model\nThis example assigns training data on the model level. To assign training data on the model version level, see the Custom model version creation with training data documentation.\nTo assign training data to custom inference model, run:\nimport datarobot as dr\npath_to_dataset = '/home/user/Documents/trainingDataset.csv'\ndataset = dr.Dataset.create_from_file(file_path=path_to_dataset)\ncustom_model = dr.CustomInferenceModel.get('5ebe96b84024035cc6a6560b')\ncustom_model.assign_training_data(dataset.id)\nTo assign training data without blocking a program, set max_wait to None:\nimport datarobot as dr\npath_to_dataset = '/home/user/Documents/trainingDataset.csv'\ndataset = dr.Dataset.create_from_file(file_path=path_to_dataset)\ncustom_model = dr.CustomInferenceModel.get('5ebe96b84024035cc6a6560b')\ncustom_model.assign_training_data(\ndataset.id,\nmax_wait=None\n)\ncustom_model.training_data_assignment_in_progress\n>>> True\n# after some time\ncustom_model.refresh()\ncustom_model.training_data_assignment_in_progress\n>>> False\nNote: training data must be assigned to retrieve feature impact from a custom model version.\nSee the Custom Model Version documentation.\nManage Custom Model Versions\nModeling code for Custom Inference Models can be uploaded by creating a Custom Model Version.\nWhen creating a Custom Model Version, the version must be associated with a base execution\nenvironment.  If the base environment supports additional model dependencies\n(R or Python environments) and the Custom Model Version\ncontains a valid requirements.txt file, the model version will run in an environment based on\nthe base environment with the additional dependencies installed.\nCreate Custom Model Version\nUpload actual custom model content by creating a clean Custom Model Version:\nimport os\nimport datarobot as dr\ncustom_model_folder = \"datarobot-user-models/model_templates/python3_pytorch\"\n# add files from the folder to the custom model\nmodel_version = dr.CustomModelVersion.create_clean(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nfolder_path=custom_model_folder,\n)\ncustom_model.id\n>>> '5b6b2315ca36c0108fc5d41b'\n# or add a list of files to the custom model\nmodel_version_2 = dr.CustomModelVersion.create_clean(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nfiles=[(os.path.join(custom_model_folder, 'custom.py'), 'custom.py')],\n)\n# and/or set k8s resources to the custom model\nmodel_version_3 = dr.CustomModelVersion.create_clean(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nfiles=[(os.path.join(custom_model_folder, 'custom.py'), 'custom.py')],\nnetwork_egress_policy=dr.NETWORK_EGRESS_POLICY.PUBLIC,\nmaximum_memory=512*1024*1024,\nreplicas=1,\n)\nTo create a new Custom Model Version from a previous one, with just some files added or removed, do the following:\nimport os\nimport datarobot as dr\ncustom_model_folder = \"datarobot-user-models/model_templates/python3_pytorch\"\nfile_to_delete = model_version_2.items[0].id\nmodel_version_3 = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nfiles=[(os.path.join(custom_model_folder, 'custom.py'), 'custom.py')],\nfiles_to_delete=[file_to_delete],\n)\nPlease refer to CustomModelFileItem for description of custom model file properties.\nSpecify a custom environment version when creating a custom model version.\nBy default a version of the same environment does not change between consecutive model versions.\nThis behavior can be overridden:\nimport os\nimport datarobot as dr\ncustom_model_folder = \"datarobot-user-models/model_templates/python3_pytorch\"\n# create a clean version and specify an explicit environment version.\nmodel_version = dr.CustomModelVersion.create_clean(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nbase_environment_version_id=\"642209acc5638929a9b8dc3d\",\nfolder_path=custom_model_folder,\n)\n# create a version from a previous one, specify an explicit environment version.\nmodel_version_2 = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nbase_environment_version_id=\"660186775d016eabb290aee9\",\n)\nTo create a new Custom Model Version from a previous one, with just new k8s resources values, do the following:\nimport os\nimport datarobot as dr\ncustom_model_folder = \"datarobot-user-models/model_templates/python3_pytorch\"\nfile_to_delete = model_version_2.items[0].id\nmodel_version_3 = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\nmaximum_memory=1024*1024*1024,\n)\nCreate a custom model version with training data\nModel version creation allows to provide training (and holdout) data information.\nEvery custom model has to be explicitly switched to allow training data assignment for model versions.\nNote that the training data assignment differs for structured and unstructured models, and should be handled differently.\nEnable training data assignment for custom model versions\nBy default, custom model training data is assigned on the model level; for more information, see the Custom model training data assignment documentation.\nWhen training data is assigned to a model, the same training data is used for every model version. This method of training data assignment is deprecated and scheduled for removal; however, to avoid introducing issues for existing models, you must individually convert existing models to perform training data assignment by model version. This change is permanent and can not be undone.\nBecause the conversion process is irreversible, it is highly recommended that you do not convert critical models to the new training data assignment method. Instead, you should duplicate the existing model and test the new method.\nTo permanently enable a training data assignment on the model version level for the specified model, do the following:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ncustom_model = dr.CustomInferenceModel.get(custom_model_id)\ncustom_model.update(is_training_data_for_versions_permanently_enabled=True)\ncustom_model.is_training_data_for_versions_permanently_enabled  # True\nAssign training data for structured models\nAssign training data for structured models, you can provide the parameters training_dataset_id and partition_column.\nTraining data assignment is performed asynchronously,\nso you can create a version in a blocking or non-blocking way (see examples).\nCreate a structured model version with blocking (default max_wait=600) and wait for the training data assignment result.\nIf the training data assignment fails:\na datarobot.errors.TrainingDataAssignmentError exception is raised. The exception contains the custom model ID, the custom model version ID, the failure message.\na new custom model version is still created and can be fetched for further processing, but it’s not possible to create a model package from it or deploy it.\nimport datarobot as dr\nfrom datarobot.errors import TrainingDataAssignmentError\ndr.Client(token=my_token, endpoint=endpoint)\ntry:\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\n)\nexcept TrainingDataAssignmentError as e:\nprint(e)\nFetching model version in the case of the assignment error, example 1:\nimport datarobot as dr\nfrom datarobot.errors import TrainingDataAssignmentError\ndr.Client(token=my_token, endpoint=endpoint)\ntry:\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\n)\nexcept TrainingDataAssignmentError as e:\nversion = CustomModelVersion.get(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\ncustom_model_version_id=e.custom_model_version_id,\n)\nprint(version.training_data.dataset_id)\nprint(version.training_data.dataset_version_id)\nprint(version.training_data.dataset_name)\nprint(version.training_data.assignment_error)\nFetching model version in the case of the assignment error, example 2:\nimport datarobot as dr\nfrom datarobot.errors import TrainingDataAssignmentError\ndr.Client(token=my_token, endpoint=endpoint)\ncustom_model = dr.CustomInferenceModel.get(\"6444482e5583f6ee2e572265\")\ntry:\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\n)\nexcept TrainingDataAssignmentError as e:\npass\ncustom_model.refresh()\nversion = custom_model.latest_version\nprint(version.training_data.dataset_id)\nprint(version.training_data.dataset_version_id)\nprint(version.training_data.dataset_name)\nprint(version.training_data.assignment_error)\nCreate a structured model version with a non-blocking (set max_wat=None) training data assignment.\nIn this case, it is the user’s responsibility to poll for version.training_data.assignment_in_progress.\nOnce the assignment is finished, check for errors if version.training_data.assignment_in_progress==False. If\nversion.training_data.assignment_error is None, then there is no error.\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\nmax_wait=None,\n)\nwhile version.training_data.assignment_in_progress:\ntime.sleep(10)\nversion.refresh()\nif version.training_data.assignment_error:\nprint(version.training_data.assignment_error[\"message\"])\nAssign training data for unstructured models\nFor unstructured models: you can provide the parameters training_dataset_id and holdout_dataset_id.\nThe training data assignment is performed synchronously and the max_wait parameter is ignored.\nThe example below shows how to create an unstructured model version with training and holdout data.\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\ntraining_dataset_id=\"6421f2149a4f9b1bec6ad6dd\",\nholdout_dataset_id=\"6421f2149a4f9b1bec6ad6ef\",\n)\nif version.training_data.assignment_error:\nprint(version.training_data.assignment_error[\"message\"])\nRemove training data\nBy default, training and holdout data are copied to a new model version from the previous model version.\nIf you don’t want to keep training and holdout data for the new version, set keep_training_holdout_data to False.\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\nversion = dr.CustomModelVersion.create_from_previous(\ncustom_model_id=\"6444482e5583f6ee2e572265\",\nbase_environment_id=\"642209acc563893014a41e24\",\nkeep_training_holdout_data=False,\n)\nList Custom Model Versions\nUse the following command to list Custom Model Versions available to the user:\nimport datarobot as dr\ndr.CustomModelVersion.list(custom_model.id)\n>>> [CustomModelVersion('v2.0'), CustomModelVersion('v1.0')]\nRetrieve Custom Model Version\nTo retrieve a specific Custom Model Version, run:\nimport datarobot as dr\ndr.CustomModelVersion.get(custom_model.id, custom_model_version_id='5ebe96b84024035cc6a6560b')\n>>> CustomModelVersion('v2.0')\nUpdate Custom Model Version\nTo update Custom Model Version description execute the following:\nimport datarobot as dr\ncustom_model_version = dr.CustomModelVersion.get(\ncustom_model.id,\ncustom_model_version_id='5ebe96b84024035cc6a6560b',\n)\ncustom_model_version.update(description='new description')\ncustom_model_version.description\n>>> 'new description'\nDownload Custom Model Version\nDownload content of the Custom Model Version as a ZIP archive:\nimport datarobot as dr\npath_to_download = '/home/user/Documents/myModel.zip'\ncustom_model_version = dr.CustomModelVersion.get(\ncustom_model.id,\ncustom_model_version_id='5ebe96b84024035cc6a6560b',\n)\ncustom_model_version.download(path_to_download)\nStart Custom Model Inference Legacy Conversion\nCustom model version may include SAS files, with a main program entrypoint. In order to be able\nto use this model it is required to run a conversion. The conversion can later be fetched and\nexamined by reading the conversion print-outs.\nBy default, a conversion is initiated in a non-blocking mode. If a max_wait parameter\nis provided, than the call is blocked until the conversion is completed. The results can\nthan be read by fetching the conversion entity.\nimport datarobot as dr\n# Read a custom model version\ncustom_model_version = dr.CustomModelVersion.get(model_id, model_version_id)\n# Find the main program item ID\nmain_program_item_id = None\nfor item in cm_ver.items:\nif item.file_name.lower().endswith('.sas'):\nmain_program_item_id = item.id\n# Execute the conversion\nif async:\n# This is a non-blocking call\nconversion_id = dr.models.CustomModelVersionConversion.run_conversion(\ncustom_model_version.custom_model_id,\ncustom_model_version.id,\nmain_program_item_id,\n)\nelse:\n# This call is blocked until a completion or a timeout\nconversion_id = dr.models.CustomModelVersionConversion.run_conversion(\ncustom_model_version.custom_model_id,\ncustom_model_version.id,\nmain_program_item_id,\nmax_wait=60,\n)\nMonitor Custom Model Inference Legacy Conversion Process\nIf a custom model version conversion was initiated in a non-blocking mode, it is possible\nto monitor the progress as follows:\nimport datarobot as dr\nwhile True:\nconversion = dr.models.CustomModelVersionConversion.get(\ncustom_model_id, custom_model_version_id, conversion_id,\n)\nif conversion.conversion_in_progress:\nlogging.info('Conversion is in progress...')\ntime.sleep(1)\nelse:\nif conversion.conversion_succeeded:\nlogging.info('Conversion succeeded')\nelse:\nlogging.error(f'Conversion failed!\\n{conversion.log_message}')\nbreak\nStop a Custom Model Inference Legacy Conversion\nIt is possible to stop a custom model version conversion that is in progress. The call is\nnon-blocking and you may keep monitoring the conversion progress (see above) until is it completed.\nimport datarobot as dr\ndr.models.CustomModelVersionConversion.stop_conversion(\ncustom_model_id, custom_model_version_id, conversion_id,\n)\nCalculate Custom ModelVersion feature impact\nTo trigger calculation of custom model version Feature Impact, training data must be assigned to a custom inference model.\nPlease refer to the custom inference model documentation.\nIf training data is assigned, run the following to trigger the calculation of the feature impact:\nimport datarobot as dr\nversion = dr.CustomModelVersion.get(custom_model.id, custom_model_version_id='5ebe96b84024035cc6a6560b')\nversion.calculate_feature_impact()\nTo trigger calculating feature impact without blocking a program, set max_wait to None:\nimport datarobot as dr\nversion = dr.CustomModelVersion.get(custom_model.id, custom_model_version_id='5ebe96b84024035cc6a6560b')\nversion.calculate_feature_impact(max_wait=None)\nRetrieve Custom Inference Image feature impact\nTo retrieve Custom Model Version feature impact, it must be calculated beforehand.\nPlease refer to Custom Inference Image feature impact documentation.\nRun the following to get feature impact:\nimport datarobot as dr\nversion = dr.CustomModelVersion.get(custom_model.id, custom_model_version_id='5ebe96b84024035cc6a6560b')\nversion.get_feature_impact()\n>>> [{'featureName': 'B', 'impactNormalized': 1.0, 'impactUnnormalized': 1.1085356209402688, 'redundantWith': 'B'}...]\nPreparing a Custom Model Version for Use\nIf your custom model version has dependencies, a dependency build must be completed before the model\ncan be used.  The dependency build installs your model’s dependencies into the base environment\nassociated with the model version.\nStarting the Dependency Build\nTo start the Custom Model Version Dependency Build, run:\nimport datarobot as dr\nbuild_info = dr.CustomModelVersionDependencyBuild.start_build(\ncustom_model_id=custom_model.id,\ncustom_model_version_id=model_version.id,\nmax_wait=3600,  # 1 hour timeout\n)\nbuild_info.build_status\n>>> 'success'\nTo start Custom Model Version Dependency Build without blocking a program until the test finishes,\nset max_wait to None:\nimport datarobot as dr\nbuild_info = dr.CustomModelVersionDependencyBuild.start_build(\ncustom_model_id=custom_model.id,\ncustom_model_version_id=model_version.id,\nmax_wait=None,\n)\nbuild_info.build_status\n>>> 'submitted'\n# after some time\nbuild_info.refresh()\nbuild_info.build_status\n>>> 'success'\nIn case the build fails, or you are just curious, do the following to retrieve the build log once complete:\nprint(build_info.get_log())\nTo cancel a Custom Model Version Dependency Build, simply run:\nbuild_info.cancel()\nManage Custom Model Tests\nA Custom Model Test represents testing performed on custom models.\nCreate Custom Model Test\nTo create Custom Model Test, run:\nimport datarobot as dr\npath_to_dataset = '/home/user/Documents/testDataset.csv'\ndataset = dr.Dataset.create_from_file(file_path=path_to_dataset)\ncustom_model_test = dr.CustomModelTest.create(\ncustom_model_id=custom_model.id,\ncustom_model_version_id=model_version.id,\ndataset_id=dataset.id,\nmax_wait=3600,  # 1 hour timeout\n)\ncustom_model_test.overall_status\n>>> 'succeeded'\nor, with k8s resources:\nimport datarobot as dr\npath_to_dataset = '/home/user/Documents/testDataset.csv'\ndataset = dr.Dataset.create_from_file(file_path=path_to_dataset)\ncustom_model_test = dr.CustomModelTest.create(\ncustom_model_id=custom_model.id,\ncustom_model_version_id=model_version.id,\ndataset_id=dataset.id,\nmax_wait=3600,  # 1 hour timeout\nmaximum_memory=1024*1024*1024,\n)\ncustom_model_test.overall_status\n>>> 'succeeded'\nTo start Custom Model Test without blocking a program until the test finishes, set max_wait to None:\nimport datarobot as dr\npath_to_dataset = '/home/user/Documents/testDataset.csv'\ndataset = dr.Dataset.create_from_file(file_path=path_to_dataset)\ncustom_model_test = dr.CustomModelTest.create(\ncustom_model_id=custom_model.id,\ncustom_model_version_id=model_version.id,\ndataset_id=dataset.id,\nmax_wait=None,\n)\ncustom_model_test.overall_status\n>>> 'in_progress'\n# after some time\ncustom_model_test.refresh()\ncustom_model_test.overall_status\n>>> 'succeeded'\nRunning a Custom Model Test uses the Custom Model Version’s base image with its dependencies installed as an execution\nenvironment. To start Custom Model Test using an execution environment “as-is”, without the model’s\ndependencies installed, supply an environment ID and (optionally) and environment version ID:\nimport datarobot as dr\npath_to_dataset = '/home/user/Documents/testDataset.csv'\ndataset = dr.Dataset.create_from_file(file_path=path_to_dataset)\ncustom_model_test = dr.CustomModelTest.create(\ncustom_model_id=custom_model.id,\ncustom_model_version_id=model_version.id,\ndataset_id=dataset.id,\nmax_wait=3600,  # 1 hour timeout\n)\ncustom_model_test.overall_status\n>>> 'succeeded'\nIn case a test fails, do the following to examine details of the failure:\nfor name, test in custom_model_test.detailed_status.items():\nprint('Test: {}'.format(name))\nprint('Status: {}'.format(test['status']))\nprint('Message: {}'.format(test['message']))\nprint(custom_model_test.get_log())\nTo cancel a Custom Model Test, simply run:\ncustom_model_test.cancel()\nTo start Custom Model Test for an unstructured custom model, dataset details should not be provided:\nimport datarobot as dr\ncustom_model_test = dr.CustomModelTest.create(\ncustom_model_id=custom_model.id,\ncustom_model_version_id=model_version.id,\n)\nList Custom Model Tests\nUse the following command to list Custom Model Tests available to the user:\nimport datarobot as dr\ndr.CustomModelTest.list(custom_model_id=custom_model.id)\n>>> [CustomModelTest('5ec262604024031bed5aaa16')]\nRetrieve Custom Model Test\nTo retrieve a specific Custom Model Test, run:\nimport datarobot as dr\ndr.CustomModelTest.get(custom_model_test_id='5ec262604024031bed5aaa16')\n>>> CustomModelTest('5ec262604024031bed5aaa16')",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/custom_model.html",
      "tags": [
        "api-reference",
        "api_reference",
        "advanced",
        "beginner",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/custom_model.html",
        "content_length": 33084
      },
      "code_examples": [],
      "api_methods": [
        "model.download_latest_version",
        "dr.custommodelversion.create_clean",
        "dr.target_type.binary",
        "dr.custominferencemodel.create",
        "dr.executionenvironment.list",
        "dr.custommodeltest.get",
        "dr.dataset.create_from_file",
        "dr.custommodelversion.list",
        "dr.custommodelversiondependencybuild.start_build",
        "model.id",
        "dr.custommodelversion.create_from_previous",
        "dr.target_type.anomaly",
        "model.is_training_data_for_versions_permanently_enabled",
        "model.assign_training_data",
        "datarobot.errors.trainingdataassignmenterror",
        "dr.executionenvironment.get",
        "model.training_data_assignment_in_progress",
        "dr.executionenvironmentversion.get",
        "dr.custominferencemodel.list",
        "model.create",
        "dr.target_type.regression",
        "dr.execution_environment_version_build_status.processing",
        "dr.executionenvironment.create",
        "dr.custominferencemodel.get",
        "dr.custommodelversion.get",
        "datarobot.models.execution_environment",
        "dr.executionenvironmentversion.create",
        "dr.target_type.multiclass",
        "dr.custommodeltest.create",
        "model.latest_version",
        "model.get",
        "dr.network_egress_policy.public",
        "model.update",
        "dr.custommodeltest.list",
        "dr.models.custommodelversionconversion",
        "model.list",
        "model.refresh",
        "dr.executionenvironmentversion.list",
        "dr.target_type.unstructured",
        "model.zip"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_2923987144741071635",
      "title": "Custom Metrics",
      "content": "Custom Metrics\nHosted custom metrics run user-provided code on DataRobot infrastructure to calculate\nyour organization’s customized metrics.\nDataRobot provides a variety of templates for common metrics. These metrics can be used as is,\nor as a starting point for user-provided metrics.\nIn this tutorial, you create a hosted custom metric using the Python SDK.\nPrerequisites\nBefore you start, import all objects used in this tutorial and initialize the DataRobot client:\nimport datarobot as dr\nfrom datarobot.enums import HostedCustomMetricsTemplateMetricTypeQueryParams\nfrom datarobot.models.deployment.custom_metrics import HostedCustomMetricTemplate, HostedCustomMetric, \\\nHostedCustomMetricBlueprint, CustomMetric, MetricTimestampSpoofing, ValueField, SampleCountField, BatchField\nfrom datarobot.models.registry import JobRun\nfrom datarobot.models.registry.job import Job\nfrom datarobot import Deployment\nfrom datarobot.models.runtime_parameters import RuntimeParameterValue\nfrom datarobot.models.types import Schedule\ndr.Client(token=\"<DataRobot API Token>\", endpoint=\"<DataRobot URL>\")\ngen_ai_deployment_1 = Deployment.get('<Deployment Id>')\nList hosted custom metrics templates\nBefore creating a hosted custom metric from a template,\nretrieve the LLM metric template to use as the basis of the new metric.\nTo do this, specify the metric_type and,\nbecause the deployments are LLM models handling Japanese text,\nsearch for the specific metric by name, limiting the search to 1 result.\nStore the result in templates for the next step.\ntemplates = HostedCustomMetricTemplate.list(\nsearch=\"[JP] Character Count\",\nmetric_type=HostedCustomMetricsTemplateMetricTypeQueryParams.LLM,\nlimit=1,\noffset=0,\n)\nCreate hosted custom metric in single step\nAfter locating the custom metric template, create the hosted custom metric from that template.\nThis method is a shortcut, combining two steps to create the new custom metric from the retrieved template:\n1. Create a custom job for a hosted custom metric from the template retrieved in the previous step (stored in templates).\n2. Connect the hosted custom metric job to the deployment defined during the prerequisites step (stored in  gen_ai_deployment_1).\nBecause we are creating 2 objects, specify both the job name and custom metric name\nin addition to the template and deployment IDs. Additionally, define the job schedule\nand the runtime parameter overrides for the deployment.\nhosted_custom_metric = HostedCustomMetric.create_from_template(\ntemplate_id=templates[0].id,\ndeployment_id=gen_ai_deployment_1.id,\njob_name=\"Hosted Custom Metric Character Count\",\ncustom_metric_name=\"Character Count\",\njob_description=\"Hosted Custom Metric\",\ncustom_metric_description=\"LLM Character Count\",\nbaseline_value=10,\ntimestamp=MetricTimestampSpoofing(\ncolumn_name=\"timestamp\",\ntime_format=\"%Y-%m-%d %H:%M:%S\",\n),\nvalue = ValueField(column_name=\"value\"),\nsample_count=SampleCountField(column_name='Sample Count'),\nbatch=BatchField(column_name='Batch'),\nschedule=Schedule(\nday_of_week=[0],\nhour=['*'],\nminute=['*'],\nday_of_month=[12],\nmonth=[1],\n),\nparameter_overrides=[RuntimeParameterValue(field_name='DRY_RUN', value=\"0\", type=\"string\")]\n)\nOnce we have created the hosted custom metric, you can initiate the manual run.\njob_run = JobRun.create(\njob_id=hosted_custom_metric.custom_job_id\nruntime_parameter_values=[\nRuntimeParameterValue(field_name='DRY_RUN', value=\"1\", type=\"string\"),\nRuntimeParameterValue(field_name='DEPLOYMENT_ID', value=gen_ai_deployment_1.id, type=\"deployment\"),\nRuntimeParameterValue(field_name='CUSTOM_METRIC_ID', value=hosted_custom_metric.id, type=\"customMetric\"),\n]\n)\nprint(job_run.status)\nCreate hosted custom metric in two steps\nYou can also perform these steps manually, in sequence. This is useful if you want\nto edit the custom metric blueprint before attaching the custom job to the deployment.\nWhen you attach the job to the deployment, most settings are\ncopied from blueprint (unless you provide an override) .\nTo create the hosted custom metric manually, first,\ncreate a custom job from the template retrieved earlier (stored in templates).\njob = Job.create_from_custom_metric_gallery_template(\ntemplate_id=templates[0].id,\nname=\"Job created from template\",\ndescription=\"Job created from template\"\n)\nNext, retrieve the default blueprint, provided by the template, and edit it.\nblueprint = HostedCustomMetricBlueprint.get(job.id)\nprint(f\"Original directionality: {blueprint.directionality}\")\nThen, update the parameters of the custom metric in the blueprint.\nupdated_blueprint = blueprint.update(\ndirectionality='lowerIsBetter',\nunits='characters',\ntype='gauge',\ntime_step='hour',\nis_model_specific=False\n)\nprint(f\"Updated directionality: {updated_blueprint.directionality}\")\nNow create the hosted custom metric. As in the shortcut method, you can provide the job schedule,\nruntime parameter overrides, and custom metric parameters specific to this deployment.\nanother_hosted_custom_metric = HostedCustomMetric.create_from_custom_job(\ncustom_job_id=job.id,\ndeployment_id=gen_ai_deployment_1.id,\nname=\"Custom metric created in 2 steps\",\n)\nAfter creating and configuring the metric, verify that the changes to the blueprint are reflected in the custom metric.\nanother_custom_metric = CustomMetric.get(custom_metric_id=another_hosted_custom_metric.id, deployment_id=gen_ai_deployment_1.id)\nprint(f\"Directionality of another custom metric: {another_custom_metric.directionality}\")\nFinally, create a manual job run for the custom metric job.\njob_run = JobRun.create(\njob_id=job.id,\nruntime_parameter_values=[\nRuntimeParameterValue(field_name='DRY_RUN', value=\"1\", type=\"string\"),\nRuntimeParameterValue(field_name='DEPLOYMENT_ID', value=gen_ai_deployment_1.id, type=\"deployment\"),\nRuntimeParameterValue(field_name='CUSTOM_METRIC_ID', value=another_hosted_custom_metric.id, type=\"customMetric\"),\n]\n)\nprint(job_run.status)\nList Hosted Custom Metrics\nTo list all hosted custom metrics associated with a custom job, use the following code:\nhosted_custom_metrics = HostedCustomMetric.list(deployment_id=hosted_custom_metric.custom_job_id)\nfor metric in hosted_custom_metrics:\nprint(metric.name)\nDelete Hosted Custom Metrics\nIn addition, you can delete the hosted custom metric, removing the custom metric from deployment\nbut keeping the job, allowing you to create the metric for another deployment.\nhosted_custom_metric.delete()\nanother_hosted_custom_metric.delete()\nIf necessary, you can delete the entire custom job. If there are any custom metrics associated with that job,\nthey are also deleted.\njob.delete()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/hosted_custom_metrics.html",
      "tags": [
        "api-reference",
        "api_reference",
        "advanced",
        "example",
        "tutorial"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/hosted_custom_metrics.html",
        "content_length": 6579
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.models.types",
        "deployment.get",
        "datarobot.models.registry",
        "datarobot.models.runtime_parameters",
        "datarobot.models.deployment",
        "deployment.custom_metrics"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_1963043167995012538",
      "title": "Deployments",
      "content": "Deployments\nDeployment is the central hub for users to deploy, manage and monitor their models.\nManage Deployments\nThe following commands can be used to manage deployments.\nCreate a Deployment\nA new deployment can be created from:\nDataRobot model - use create_from_registered_model_version(). Please refer to the Model Registry documentation that describes how to create a registered model version.\nWhen creating a new deployment, a DataRobot registered_model_version_id (also known as model_package_id) and label must be provided.\nA description can be optionally provided to document the purpose of the deployment.\nThe default prediction server is used when making predictions against the deployment,\nand is a requirement for creating a deployment on DataRobot cloud.\nFor on-prem installations, a user must not provide a default prediction server\nand a pre-configured prediction server will be used instead.\nRefer to datarobot.PredictionServer.list for more information on retrieving available prediction servers.\nimport datarobot as dr\nproject = dr.Project.get('6527eb38b9e5dead5fc12491')\nmodel = project.get_models()[0]\nprediction_server = dr.PredictionServer.list()[0]\nregistered_model_version = dr.RegisteredModelVersion.create_for_leaderboard_item(\nmodel_id=model.id,\nname=\"Name of the version(aka model package)\",\nregistered_model_name='Name of the registered model unique across the org '\n)\ndeployment = dr.Deployment.create_from_registered_model_version(\nregistered_model_version.id, label='New Deployment', description='A new deployment',\ndefault_prediction_server_id=prediction_server.id)\n>>> Deployment('New Deployment')\nList Deployments\nUse the following command to list deployments a user can view.\nimport datarobot as dr\ndeployments = dr.Deployment.list()\ndeployments\n>>> [Deployment('New Deployment'), Deployment('Previous Deployment')]\nRefer to Deployment for properties of the deployment object.\nYou can also filter the deployments that are returned by passing an instance of the\nDeploymentListFilters class to the filters keyword argument.\nimport datarobot as dr\nfilters = dr.models.deployment.DeploymentListFilters(\nrole='OWNER',\naccuracy_health=dr.enums.DEPLOYMENT_ACCURACY_HEALTH_STATUS.FAILING\n)\ndeployments = dr.Deployment.list(filters=filters)\ndeployments\n>>> [Deployment('Deployment Owned by Me w/ Failing Accuracy 1'), Deployment('Deployment Owned by Me w/ Failing Accuracy 2')]\nRetrieve a Deployment\nIt is possible to retrieve a single deployment with its identifier,\nrather than list all deployments.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.id\n>>> '5c939e08962d741e34f609f0'\ndeployment.label\n>>> 'New Deployment'\nRefer to Deployment for properties of the deployment object.\nUpdate a Deployment\nDeployment’s label and description can be updated.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update(label='new label')\nDelete a Deployment\nTo mark a deployment as deleted, use the following command.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.delete()\nActivate or deactivate a Deployment\nTo activate a deployment, use the following command.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.activate()\ndeployment.status\n>>> 'active'\nTo deactivate a deployment, use the following command.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.deactivate()\ndeployment.status\n>>> 'inactive'\nMake batch predictions with a deployment\nDataRobot provides a small utility function to make batch predictions using a deployment: Deployment.predict_batch.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\n# To note: `source` can be a file path, a file, or a pandas DataFrame\nprediction_results_as_dataframe = deployment.predict_batch(\nsource=\"./my_local_file.csv\",\n)\nModel Replacement\nA deployment’s model can be replaced effortlessly with zero interruption of\npredictions.\nModel replacement is an asynchronous process, which means some\npreparatory work may be performed after the initial request is completed.\nPredictions made against this deployment will start\nusing the new model as soon as the request is completed.\nThere will be no interruption for predictions throughout the process.\nThe replace_model() function won’t return until the\nasynchronous process is fully finished.\nAlongside the identifier of the new model, a reason is also required.\nThe reason is stored in model history of the deployment for bookkeeping purpose.\nAn enum MODEL_REPLACEMENT_REASON is provided for convenience, all possible values are documented below:\nMODEL_REPLACEMENT_REASON.ACCURACY\nMODEL_REPLACEMENT_REASON.DATA_DRIFT\nMODEL_REPLACEMENT_REASON.ERRORS\nMODEL_REPLACEMENT_REASON.SCHEDULED_REFRESH\nMODEL_REPLACEMENT_REASON.SCORING_SPEED\nMODEL_REPLACEMENT_REASON.OTHER\nHere is an example of model replacement:\nimport datarobot as dr\nfrom datarobot.enums import MODEL_REPLACEMENT_REASON\nproject = dr.Project.get('5cc899abc191a20104ff446a')\nmodel = project.get_models()[0]\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.model['id'], deployment.model['type']\n>>> ('5c0a979859b00004ba52e431', 'Decision Tree Classifier (Gini)')\ndeployment.replace_model('5c0a969859b00004ba52e41b', MODEL_REPLACEMENT_REASON.ACCURACY)\ndeployment.model['id'], deployment.model['type']\n>>> ('5c0a969859b00004ba52e41b', 'Support Vector Classifier (Linear Kernel)')\nValidation\nBefore initiating the model replacement request, it is usually a good idea to use\nthe validate_replacement_model() function to validate if the new model can be used as a replacement.\nThe validate_replacement_model() function returns the validation status, a message and a checks dictionary.\nIf the status is ‘passing’ or ‘warning’, use replace_model() to perform model the replacement.\nIf status is ‘failing’, refer to the checks dict for more details on why the new model cannot be used as a replacement.\nimport datarobot as dr\nproject = dr.Project.get('5cc899abc191a20104ff446a')\nmodel = project.get_models()[0]\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nstatus, message, checks = deployment.validate_replacement_model(new_model_id=model.id)\nstatus\n>>> 'passing'\n# `checks` can be inspected for detail, showing two examples here:\nchecks['target']\n>>> {'status': 'passing', 'message': 'Target is compatible.'}\nchecks['permission']\n>>> {'status': 'passing', 'message': 'User has permission to replace model.'}\nMonitoring\nDeployment monitoring can be categorized into several area of concerns:\nService Stats & Service Stats Over Time\nAccuracy & Accuracy Over Time\nWith a Deployment object, get functions are provided to allow querying of the monitoring data.\nAlternatively, it is also possible to retrieve monitoring data directly using a deployment ID. For example:\nfrom datarobot.models import Deployment, ServiceStats\ndeployment_id = '5c939e08962d741e34f609f0'\n# call `get` functions on a `Deployment` object\ndeployment = Deployment.get(deployment_id)\nservice_stats = deployment.get_service_stats()\n# directly fetch without a `Deployment` object\nservice_stats = ServiceStats.get(deployment_id)\nWhen querying monitoring data, a start and end time can be optionally provided, will accept either a datetime object or a string.\nNote that only top of the hour datetimes are accepted, for example: 2019-08-01T00:00:00Z.\nBy default, the end time of the query will be the next top of the hour, the start time will be 7 days before the end time.\nIn the over time variants, an optional bucket_size can be provided to specify the resolution of time buckets.\nFor example, if start time is 2019-08-01T00:00:00Z, end time is 2019-08-02T00:00:00Z and bucket_size is T1H,\nthen 24 time buckets will be generated, each providing data calculated over one hour.\nUse construct_duration_string() to help construct a bucket size string.\nNote\nThe minimum bucket size is one hour.\nService Stats\nService stats are metrics tracking deployment utilization and how well deployments respond to prediction requests.\nUse SERVICE_STAT_METRIC.ALL to retrieve a list of supported metrics.\nServiceStats retrieves values for all service stats metrics;\nServiceStatsOverTime can be used to fetch how one single metric changes over time.\nfrom datetime import datetime\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nservice_stats = deployment.get_service_stats(\nstart_time=datetime(2019, 8, 1, hour=15),\nend_time=datetime(2019, 8, 8, hour=15)\n)\nservice_stats[SERVICE_STAT_METRIC.TOTAL_PREDICTIONS]\n>>> 12597\ntotal_predictions = deployment.get_service_stats_over_time(\nstart_time=datetime(2019, 8, 1, hour=15),\nend_time=datetime(2019, 8, 8, hour=15),\nbucket_size=construct_duration_string(days=1),\nmetric=SERVICE_STAT_METRIC.TOTAL_PREDICTIONS\n)\ntotal_predictions.bucket_values\n>>> OrderedDict([(datetime.datetime(2019, 8, 1, 15, 0, tzinfo=tzutc()), 1610),\n(datetime.datetime(2019, 8, 2, 15, 0, tzinfo=tzutc()), 2249),\n(datetime.datetime(2019, 8, 3, 15, 0, tzinfo=tzutc()), 254),\n(datetime.datetime(2019, 8, 4, 15, 0, tzinfo=tzutc()), 943),\n(datetime.datetime(2019, 8, 5, 15, 0, tzinfo=tzutc()), 1967),\n(datetime.datetime(2019, 8, 6, 15, 0, tzinfo=tzutc()), 2810),\n(datetime.datetime(2019, 8, 7, 15, 0, tzinfo=tzutc()), 2775)])\nData Drift\nData drift describe how much the distribution of target or a feature has changed comparing to the training data.\nDeployment’s target drift and feature drift can be retrieved separately using datarobot.models.deployment.TargetDrift and datarobot.models.deployment.FeatureDrift.\nUse DATA_DRIFT_METRIC.ALL to retrieve a list of supported metrics.\nfrom datetime import datetime\nfrom datarobot.enums import DATA_DRIFT_METRIC\nfrom datarobot.models import Deployment, FeatureDrift\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ntarget_drift = deployment.get_target_drift(\nstart_time=datetime(2019, 8, 1, hour=15),\nend_time=datetime(2019, 8, 8, hour=15)\n)\ntarget_drift.drift_score\n>>> 0.00408514\nfeature_drift_data = FeatureDrift.list(\ndeployment_id='5c939e08962d741e34f609f0',\nstart_time=datetime(2019, 8, 1, hour=15),\nend_time=datetime(2019, 8, 8, hour=15),\nmetric=DATA_DRIFT_METRIC.HELLINGER\n)\nfeature_drift = feature_drift_data[0]\nfeature_drift.name\n>>> 'age'\nfeature_drift.drift_score\n>>> 4.16981594\nPredictions Over Time\nPredictions over time gives insight on how deployment’s prediction response has changed over time.\nDifferent data can be retrieved in each bucket, depending on deployment’s target type:\nrow_count: number of rows in the bucket, available for all target types\nmean_predicted_value: mean of predicted value for all rows in the bucket, available for regression target type\nmean_probabilities: mean of predicted probability for each class, available for binary or multiclass classification target types\nclass_distribution: count and percent of predicted class labels, available for binary or multiclass classification target types\npercentiles: 10th and 90th percentile of predicted value or positive class probability, available for regression and binary target type\nfrom datetime import datetime\nfrom datarobot.enums import BUCKET_SIZE\nfrom datarobot.models import Deployment\n# deployment with regression target type\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\npredictions_over_time = deployment.get_predictions_over_time(\nstart_time=datetime(2023, 4, 1),\nend_time=datetime(2023, 4, 30),\nbucket_size=BUCKET_SIZE.P1D,\n)\npredicted = [bucket['mean_predicted_value'] for bucket in predictions_over_time.buckets]\npredicted\n>>> [0.3772, 0.6642, ...., 0.7937]\n# deployment with binary target type\ndeployment = Deployment.get(deployment_id='62fff28a0f5fee488587ce92')\npredictions_over_time = deployment.get_predictions_over_time(\nstart_time=datetime(2023, 4, 1),\nend_time=datetime(2023, 4, 22),\nbucket_size=BUCKET_SIZE.P7D,\n)\npredicted = [\n{item['class_name']: item['value'] for item in bucket['mean_probabilities']}.get('True')\nfor bucket in predictions_over_time.buckets\n]\npredicted\n>>> [0.3955, 0.4274, None]\nAccuracy\nA collection of metrics are provided to measure the accuracy of a deployment’s predictions.\nFor deployments with classification model, use ACCURACY_METRIC.ALL_CLASSIFICATION for all supported metrics;\nin the case of deployment with regression model, use ACCURACY_METRIC.ALL_REGRESSION instead.\nSimilarly with Service Stats, Accuracy and AccuracyOverTime\nare provided to retrieve all default accuracy metrics and how one single metric change over time.\nfrom datetime import datetime\nfrom datarobot.enums import ACCURACY_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\naccuracy = deployment.get_accuracy(\nstart_time=datetime(2019, 8, 1, hour=15),\nend_time=datetime(2019, 8, 1, 15, 0)\n)\naccuracy[ACCURACY_METRIC.RMSE]\n>>> 943.225\nrmse = deployment.get_accuracy_over_time(\nstart_time=datetime(2019, 8, 1),\nend_time=datetime(2019, 8, 3),\nbucket_size=construct_duration_string(days=1),\nmetric=ACCURACY_METRIC.RMSE\n)\nrmse.bucket_values\n>>> OrderedDict([(datetime.datetime(2019, 8, 1, 15, 0, tzinfo=tzutc()), 1777.190657),\n(datetime.datetime(2019, 8, 2, 15, 0, tzinfo=tzutc()), 1613.140772)])\nIt is also possible to retrieve how multiple metrics changes over the same period of time,\nenabling easier side by side comparison across different metrics.\nfrom datarobot.enums import ACCURACY_METRIC\nfrom datarobot.models import Deployment\naccuracy_over_time = AccuracyOverTime.get_as_dataframe(\nram_app.id, [ACCURACY_METRIC.RMSE, ACCURACY_METRIC.GAMMA_DEVIANCE, ACCURACY_METRIC.MAD])\nPredictions vs Actuals Over Time\nPredictions vs actuals over time can be used to analyze how deployment’s predictions compare against actuals.\nDifferent data can be retrieved in each bucket, depending on deployment’s target type:\nrow_count_total: The number of rows with or without actual in the bucket, available for all target types.\nrow_count_with_actual: The number of rows with actuals in the bucket, available for all target types.\nmean_predicted_value: The mean of the predicted value for all rows match with an actual in the bucket, available for the regression target type.\nmean_actual_value: The mean of the actual value for all rows in the bucket. Available for the regression target type.\npredicted_class_distribution: The count and percent of predicted class labels. Available for binary and multiclass classification target types.\nactual_class_distribution: The count and percent of actual class labels. Available for binary or multiclass classification target types.\nfrom datetime import datetime\nfrom datarobot.enums import BUCKET_SIZE\nfrom datarobot.models import Deployment\n# deployment with regression target type\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\npredictions_over_time = deployment.get_predictions_vs_actuals_over_time(\nstart_time=datetime(2023, 4, 1),\nend_time=datetime(2023, 4, 30),\nbucket_size=BUCKET_SIZE.P1D,\n)\npredicted = [bucket['mean_actual_value'] for bucket in predictions_over_time.buckets]\npredicted\n>>> [0.2806, 0.9170, ...., 0.0314]\n# deployment with binary target type\ndeployment = Deployment.get(deployment_id='62fff28a0f5fee488587ce92')\npredictions_over_time = deployment.get_predictions_vs_actuals_over_time(\nstart_time=datetime(2023, 4, 1),\nend_time=datetime(2023, 4, 22),\nbucket_size=BUCKET_SIZE.P7D,\n)\npredicted = [\n{item['class_name']: item['value'] for item in bucket['mean_predicted_value']}.get('True')\nfor bucket in predictions_over_time.buckets\n]\npredicted\n>>> [0.5822, 0.6305, None]\nDelete Data\nMonitoring data accumulated on a deployment can be deleted using delete_monitoring_data().\nA start and end timestamp could be provided to limit data deletion to certain time period.\nWarning\nMonitoring data is not recoverable once deleted.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.delete_monitoring_data(model_id=deployment.model['id'])\nList deployment prediction data exports\nPrediction data exports for a deployment can be retrieved using list_prediction_data_exports().\nfrom datarobot.enums import ExportStatus\nfrom datarobot.models import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nprediction_data_exports = deployment.list_prediction_data_exports(limit=0)\nprediction_data_exports\n>>> [PredictionDataExport('65fbe59aaa3f847bd5acc75b'),\nPredictionDataExport('65fbe59aaa3f847bd5acc75c'),\nPredictionDataExport('65fbe59aaa3f847bd5acc75a')]\nTo list all prediction data exports, set the limit to 0.\nAdjust additional parameters to filter the data as needed:\nfrom datarobot.enums import ExportStatus\nfrom datarobot.models import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nprediction_data_exports = deployment.list_prediction_data_exports(\nmodel_id=\"6444482e5583f6ee2e572265\",\nbatch=False,\nstatus=ExportStatus.SUCCEEDED,\nlimit=100,\noffset=50,\n)\nList deployment actuals data exports\nActuals data exports for a deployment can be retrieved using list_actuals_data_exports().\nfrom datarobot.enums import ExportStatus\nfrom datarobot.models import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nactuals_data_exports = deployment.list_actuals_data_exports(limit=0)\nactuals_data_exports\n>>> [ActualsDataExport('660456a332d0081029ee5031'),\nActualsDataExport('660456a332d0081029ee5032'),\nActualsDataExport('660456a332d0081029ee5033')]\nTo list all actuals data exports, set the limit to 0.\nAdjust additional parameters to filter the data as needed:\nfrom datarobot.enums import ExportStatus\nfrom datarobot.models import Deployment\ndeployment = Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nactuals_data_exports = deployment.list_actuals_data_exports(\ndeployment_id='5c939e08962d741e34f609f0',\noffset=500,\nlimit=50,\nstatus=ExportStatus.SUCCEEDED\n)\nList deployment training data exports\nTo retrieve successful training data exports for a deployment, use list_training_data_exports().\nfrom datarobot.models.deployment import TrainingDataExport\ntraining_data_exports = TrainingDataExport.list(deployment_id='5c939e08962d741e34f609f0')\ntraining_data_exports\n>>> [TrainingDataExport('6565fbf2356124f1daa3acc522')]\nList deployment data quality exports\nTo retrieve successful data quality exports for a deployment, use list_data_quality_exports().\nfrom datarobot.models import Deployment\ndeployment = Deployment.get('66903c40f18e6ec90fd7c8c7')\ndata_quality_exports = deployment.list_data_quality_exports(start='2024-07-01', end='2024-08-01')\ndata_quality_exports\n>>> [DataQualityExport(6447ca39c6a04df6b5b0ed19c6101e3c),\n...\nDataQualityExport(0ff46fd3636545a9ac3e15ee1dbd8638)]\nThere are many filtering and sorting options available.\nSegment Analysis\nSegment analysis is a deployment utility that filters service stats, data drift, and accuracy statistics into unique segment attributes and values.\nUse get_segment_attributes() to retrieve segment analysis data.\nUse get_segment_values() to retrieve segment value data.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsegment_attributes_service_health = deployment.get_segment_attributes(DEPLOYMENT_MONITORING_TYPE.SERVICE_HEALTH)\n>>>['DataRobot-Consumer', 'DataRobot-Host-IP', 'DataRobot-Remote-IP']\nsegment_attributes_data_drift = deployment.get_segment_attributes(DEPLOYMENT_MONITORING_TYPE.DATA_DRIFT)\n>>>['DataRobot-Consumer', 'attribute_1', 'attribute_2']\nsegment_values = deployment.get_segment_values(segmentAttribute=ReservedSegmentAttributes.CONSUMER)\n>>>['DataRobot-Consumer', '[email protected]']\nChallengers\nChallenger models can be used to compare the currently deployed model (the “champion” model) to another model.\nThe following functions can be used to manage deployment’s challenger models:\nList: list_challengers() or list().\nCreate: create().\nGet: get().\nUpdate: update().\nDelete: delete().\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nchallenger = deployment.list_challengers()[-1]\nchallenger.update(name='New Challenger Name')\nchallenger.name\n>>> 'New Challenger Name'\nSettings\nUse get_challenger_models_settings()\nand update_challenger_models_settings()\nto retrieve and update challenger model settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_challenger_models_settings(challenger_models_enabled=True)\nsettings = deployment.get_challenger_models_settings()\nsettings\n>>> {'enabled': True}\nUse get_challenger_replay_settings()\nand update_challenger_replay_settings()\nto retrieve and update challenger replay settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_challenger_replay_settings(enabled=True)\nsettings = deployment.get_challenger_replay_settings()\nsettings['enabled']\n>>> True\nSettings\nDrift Tracking Settings\nDrift tracking is used to help analyze and monitor the performance of a model after it is deployed.\nWhen the model of a deployment is replaced drift tracking status will not be altered.\nUse get_drift_tracking_settings() to retrieve the current tracking status for target drift and feature drift.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsettings = deployment.get_drift_tracking_settings()\nsettings\n>>> {'target_drift': {'enabled': True}, 'feature_drift': {'enabled': True}}\nUse update_drift_tracking_settings() to update target drift and feature drift tracking status.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_drift_tracking_settings(target_drift_enabled=True, feature_drift_enabled=True)\nAssociation ID Settings\nAssociation ID is used to identify predictions, so that when actuals are acquired, accuracy can be calculated.\nUse get_association_id_settings() to retrieve current association ID settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsettings = deployment.get_association_id_settings()\nsettings\n>>> {'column_names': ['application_id'], 'required_in_prediction_requests': True}\nUse update_association_id_settings() to update association ID settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_association_id_settings(column_names=['application_id'], required_in_prediction_requests=True)\nPredictions By Forecast Date\nForecast date setting for the deployment.\nUse get_predictions_by_forecast_date_settings() to retrieve current predictions by forecast date settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsettings = deployment.get_predictions_by_forecast_date_settings()\nsettings\n>>> {'enabled': False, 'column_name': 'date (actual)', 'datetime_format': '%Y-%m-%d'}\nUse update_predictions_by_forecast_date_settings() to update predictions by forecast date settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_predictions_by_forecast_date_settings(\nenable_predictions_by_forecast_date=True,\nforecast_date_column_name='date (actual)',\nforecast_date_format='%Y-%m-%d')\nHealth Settings\nHealth settings APIs can be used to customize definitions for deployment health status.\nUse get_health_settings() to retrieve current health settings,\nand get_default_health_settings() to retrieve default health settings.\nTo perform updates, use update_health_settings().\nimport datarobot as dr\n# get current data drift threshold\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsettings = deployment.get_health_settings()\nsettings['data_drift']['drift_threshold']\n>>> 0.15\n# update accuracy health metric\nsettings['accuracy']['metric'] = 'AUC'\nsettings = deployment.update_health_settings(accuracy=settings['accuracy'])\nsettings['accuracy']['metric']\n>>> 'AUC'\n# set accuracy health metric to default\ndefault_settings = deployment.get_default_health_settings()\nsettings = deployment.update_health_settings(accuracy=default_settings['accuracy'])\nsettings['accuracy']['metric']\n>>> 'LogLoss'\nSegment Analysis Settings\nSegment analysis is a deployment utility that filters data drift and accuracy statistics into unique segment attributes and values.\nUse get_segment_analysis_settings() to retrieve current segment analysis settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsettings = deployment.get_segment_analysis_settings()\nsettings\n>>> {'enabled': False, 'attributes': []}\nUse update_segment_analysis_settings() to update segment analysis settings. Any categorical column can be a segment attribute.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_segment_analysis_settings(\nsegment_analysis_enabled=True,\nsegment_analysis_attributes=[\"country_code\", \"is_customer\"])\nPredictions Data Collection Settings\nPredictions Data Collection configures whether prediction requests and results should be saved to\nPredictions Data Storage.\nUse get_predictions_data_collection_settings() to retrieve current\nsettings of predictions data collection.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsettings = deployment.get_predictions_data_collection_settings()\nsettings\n>>> {'enabled': True}\nUse update_predictions_data_collection_settings() to update predictions data\ncollection settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_predictions_data_collection_settings(enabled=True)\nPrediction Warning Settings\nPrediction Warning is used to enable Humble AI for a deployment which determines if a\nmodel is misbehaving when a prediction goes outside of the calculated boundaries.\nUse get_prediction_warning_settings() to retrieve the current prediction warning settings.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nsettings = deployment.get_prediction_warning_settings()\nsettings\n>>> { {'enabled': True}, 'custom_boundaries': {'upper': 1337, 'lower': 0} }\nUse update_prediction_warning_settings() to update current prediction warning settings.\nimport datarobot as dr\n# Set custom boundaries\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\ndeployment.update_prediction_warning_settings(\nprediction_warning_enabled=True,\nuse_default_boundaries=False,\nlower_boundary=1337,\nupper_boundary=2000,\n)\n# Reset boundaries\ndeployment.update_prediction_warning_settings(\nprediction_warning_enabled=True,\nuse_default_boundaries=True,\n)\nSecondary Dataset Config Settings\nThe secondary dataset config for a deployed Feature discovery model can be replaced and retrieved.\nSecondary dataset config is used to specify which secondary datasets to use during\nprediction for a given deployment.\nUse update_secondary_dataset_config() to update the secondary dataset config.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nconfig = deployment.update_secondary_dataset_config(secondary_dataset_config_id='5f48cb94408673683eca0fab')\nconfig\n>>> '5f48cb94408673683eca0fab'\nUse get_secondary_dataset_config() to get the secondary dataset config.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nconfig = deployment.get_secondary_dataset_config()\nconfig\n>>> '5f48cb94408673683eca0fab'\nShare deployments\nYou can grant or revoke other users’ access to a deployment.\nAccess levels\nFor deployments, there are 3 access levels:\nOWNER - Allows all actions on a deployment.\nUSER - Can see the deployment in the DataRobot UI and see the prediction statistics of the deployment, but cannot edit or delete the deployment.\nCONSUMER - Can only make predictions on the deployment. Cannot see the deployment in the DataRobot UI or retrieve prediction statistics for the deployment in the API.\nSharing\nUse list_shared_roles() to get a list of users, groups, and organizations that\ncurrently have a role on the project. Each role will be returned as a datarobot.models.deployment.DeploymentSharedRole.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nroles = deployment.list_shared_roles()\n[role.to_dict() for role in roles]\n>>> [{'role': 'OWNER', 'id': '5c939e08962d741e34f609f0', 'share_recipient_type': 'user', 'name': '[email protected]'},\n{'role': 'USER', 'id': '5c939e08962d741e34f609f1', 'share_recipient_type': 'group', 'name': 'Example Group'},\n{'role': 'CONSUMER', 'id': '5c939e08962d741e34f609f2', 'share_recipient_type': 'organization', 'name': 'Example Org'}]\nUse update_shared_roles() to grant and revoke roles on the deployment. This function\ntakes a list of datarobot.models.deployment.DeploymentGrantSharedRoleWithId and\ndatarobot.models.deployment.DeploymentGrantSharedRoleWithUsername objects and updates roles accordingly.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\nroles = deployment.list_shared_roles()\n[role.to_dict() for role in roles]\n>>> [{'role': 'OWNER', 'id': '5c939e08962d741e34f609f0', 'share_recipient_type': 'user', 'name': '[email protected]'}]\nnew_role = DeploymentGrantSharedRoleWithUsername(username='[email protected]', role='OWNER')\nresponse = deployment.update_shared_roles([new_role])\nresponse.status_code\n>>> 204\nroles = deployment.list_shared_roles()\n[role.to_dict() for role in roles]\n>>> [{'role': 'OWNER', 'id': '5c939e08962d741e34f609f0', 'share_recipient_type': 'user', 'name': '[email protected]'},\n{'role': 'OWNER', 'id': '5c939e08962d741e34f609f1', 'share_recipient_type': 'user', 'name': '[email protected]'}]\nrevoke_role =  DeploymentGrantSharedRoleWithUsername(username='[email protected]', role='NO_ROLE')\nresponse = deployment.update_shared_roles([revoke_role])\nresponse.status_code\n>>> 204\nroles = deployment.list_shared_roles()\n[role.to_dict() for role in roles]\n>>> [{'role': 'OWNER', 'id': '5c939e08962d741e34f609f0', 'share_recipient_type': 'user', 'name': '[email protected]'}]",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/deployment.html",
      "tags": [
        "advanced",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/deployment.html",
        "content_length": 30749
      },
      "code_examples": [],
      "api_methods": [
        "deployment.get",
        "deployment.get_predictions_over_time",
        "dr.models.deployment",
        "deployment.create_from_registered_model_version",
        "deployment.validate_replacement_model",
        "deployment.list_actuals_data_exports",
        "deployment.get_drift_tracking_settings",
        "deployment.list",
        "deployment.update_drift_tracking_settings",
        "deployment.get_predictions_by_forecast_date_settings",
        "deployment.get_secondary_dataset_config",
        "deployment.get_default_health_settings",
        "deployment.get_prediction_warning_settings",
        "deployment.targetdrift",
        "deployment.update_shared_roles",
        "dr.deployment.get",
        "deployment.label",
        "deployment.replace_model",
        "deployment.get_accuracy",
        "deployment.update_challenger_replay_settings",
        "model.id",
        "dr.predictionserver.list",
        "deployment.model",
        "deployment.update_predictions_data_collection_settings",
        "deployment.list_shared_roles",
        "deployment.deactivate",
        "deployment.deploymentlistfilters",
        "dr.deployment.create_from_registered_model_version",
        "deployment.get_service_stats",
        "deployment.get_segment_attributes",
        "deployment.update_challenger_models_settings",
        "deployment.update_segment_analysis_settings",
        "deployment.update_health_settings",
        "project.get",
        "deployment.update_association_id_settings",
        "deployment.get_segment_analysis_settings",
        "datarobot.helpers.partitioning_methods",
        "deployment.deploymentgrantsharedrolewithusername",
        "deployment.get_challenger_models_settings",
        "deployment.delete",
        "deployment.id",
        "deployment.deploymentsharedrole",
        "datarobot.predictionserver.list",
        "deployment.list_data_quality_exports",
        "deployment.get_predictions_vs_actuals_over_time",
        "deployment.get_challenger_replay_settings",
        "deployment.update_predictions_by_forecast_date_settings",
        "deployment.get_accuracy_over_time",
        "deployment.get_service_stats_over_time",
        "deployment.get_health_settings",
        "deployment.get_association_id_settings",
        "deployment.list_challengers",
        "deployment.get_predictions_data_collection_settings",
        "deployment.delete_monitoring_data",
        "deployment.get_segment_values",
        "deployment.update_prediction_warning_settings",
        "deployment.activate",
        "deployment.deploymentgrantsharedrolewithid",
        "deployment.status",
        "deployment.update_secondary_dataset_config",
        "dr.deployment.list",
        "deployment.get_target_drift",
        "dr.project.get",
        "deployment.featuredrift",
        "datarobot.models.deployment",
        "deployment.list_prediction_data_exports",
        "project.get_models",
        "deployment.predict_batch",
        "dr.enums.deployment_accuracy_health_status",
        "deployment.update",
        "dr.registeredmodelversion.create_for_leaderboard_item"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_2315698481677914652",
      "title": "MLOps",
      "content": "MLOps\nDataRobot MLOps provides a central hub to deploy, monitor, manage, and govern all your models in production.",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/index.html",
        "content_length": 115
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.4,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_1460931525737780675",
      "title": "Jobs",
      "content": "Jobs\nJobs allow you to run your code as jobs on the DataRobot platform to implement various workloads (tests, metrics etc).\nManage jobs\nUse the following commands to manage jobs:\nCreate job\nTo create a job, use dr.registry.Job.create, as shown in the following example:\nimport os\nimport datarobot as dr\n# add files content using `file_data` argument\njob = dr.registry.Job.create(\n\"my-job\",\nenvironment_id=\"65c4f3ed001d3e27a382608f\",\nfile_data={\"run.sh\": \"echo 'hello world'\"},\n)\n# or add files from the folder\njob_folder = \"my-folder/files\"\njob_2 = dr.registry.Job.create(\n\"my-job\",\nenvironment_id=\"65c4f3ed001d3e27a382608f\",\nfolder_path=job_folder,\n)\n# or add files as a list of individual file paths\njob_3 = dr.registry.Job.create(\n\"my-job\",\nenvironment_id=\"65c4f3ed001d3e27a382608f\",\nfiles=[(os.path.join(job_folder, 'run.sh'), 'run.sh')],\n)\n# if the files should be added to the root of the job filesystem with\n# with the same names as on the local file system, the above can be simplified to the following:\njob_4 = dr.registry.Job.create(\n\"my-job\",\nenvironment_id=\"65c4f3ed001d3e27a382608f\",\nfiles=[os.path.join(job_folder, 'run.sh')],\n)\n# or a job can be created without the files,\n# and the files can be added later using the `update` method\njob_5 = dr.registry.Job.create(\"my-job\")\nCreate hosted custom metric job from template\nTo create a hosted custom metric job from gallery template,\nuse dr.registry.Job.create_from_custom_metric_gallery_template, as shown in the following example:\nimport datarobot as dr\ntemplates = dr.models.deployment.custom_metrics.HostedCustomMetricTemplate.list()\ntemplate_id = templates[0].id\njob = dr.registry.Job.create_from_custom_metric_gallery_template(\ntemplate_id = template_id,\nname = \"my-job\",\n)\nList jobs\nTo list all jobs available to the current user, use dr.registry.Job.list, as in the following example:\nimport datarobot as dr\njobs = dr.registry.Job.list()\njobs\n>>> [Job('my-job')]\nRetrieve jobs\nTo get a job by unique identifier, use dr.registry.Job.get, as in the following example:\nimport datarobot as dr\njob = dr.registry.Job.get(\"65f4453e6ea907cb0405ff7f\")\njob\n>>> Job('my-job')\nUpdate jobs\nTo get a job by unique identifier and update it, use dr.registry.Job.get() and then update(), as in the following example:\nimport datarobot as dr\njob = dr.registry.Job.get(\"65f4453e6ea907cb0405ff7f\")\njob.update(\nenvironment_id=\"65c4f3ed001d3e27a382608f\",\ndescription=\"My Job\",\nfolder_path=job_folder,\nfile_data={\"README.md\": \"My README file\"},\n)\nDelete jobs\nTo get a job by unique identifier and delete it, use dr.registry.Job.get() and then delete(), as in the following example:\nimport datarobot as dr\njob = dr.registry.Job.get(\"65f4453e6ea907cb0405ff7f\")\njob.delete()\nManage job runs\nUse the following commands to manage job runs:\nCreate job runs\nTo create a job run, use dr.registry.JobRun.create, as shown in the following example:\nimport datarobot as dr\nimport time\njob_id = \"65f4453e6ea907cb0405ff7f\"\n# block until job run is finished\njob_run = dr.registry.JobRun.create(job_id)\n# or run job without blocking the thread, and check the job run status manually\njob_run = dr.registry.JobRun.create(job_id, max_wait=None)\nwhile job_run.status == dr.registry.JobRunStatus.RUNNING:\ntime.sleep(1)\njob_run.refresh()\nList job runs\nTo list all job runs, use dr.registry.JobRun.list, as in the following example:\nimport datarobot as dr\njob_id = \"65f4453e6ea907cb0405ff7f\"\njob_runs = dr.registry.JobRun.list(job_id)\njob_runs\n>>> [JobRun('65f856957d897d46b0e54b37'),\nJobRun('65f8567f7d897d46b0e54b32'),\nJobRun('65f856617d897d46b0e54b2d')]\nRetrieve job runs\nTo get a job run with an identifier, use dr.registry.JobRun.get, as in the following example:\nimport datarobot as dr\njob_id = \"65f4453e6ea907cb0405ff7f\"\njob_run = dr.registry.JobRun.get(job_id, \"65f856957d897d46b0e54b37\")\njob_run\n>>> JobRun('65f856957d897d46b0e54b37')\nUpdate job runs\nTo get a job run by unique identifier and update it, use dr.registry.JobRun.get() and then update(), as in the following example:\nimport datarobot as dr\njob_id = \"65f4453e6ea907cb0405ff7f\"\njob_run = dr.registry.JobRun.get(job_id, \"65f856957d897d46b0e54b37\")\njob_run.update(description=\"The description of this job run\")\nCancel a job run\nTo get a running job run by identifier and cancel it, use dr.registry.JobRun.get() and then cancel(), as in the following example:\nimport datarobot as dr\njob_id = \"65f4453e6ea907cb0405ff7f\"\njob_run = dr.registry.JobRun.get(job_id, \"65f856957d897d46b0e54b37\")\njob_run.cancel()\nRetrieve job run logs\nTo get job run logs, use dr.registry.JobRun.get_logs, as in the following example:\nimport datarobot as dr\njob_id = \"65f4453e6ea907cb0405ff7f\"\njob_run = dr.registry.JobRun.get(job_id, \"65f856957d897d46b0e54b37\")\njob_run.get_logs()\n>>> 2024-03-18T16:06:46.044946476Z Some log output\nDelete job run logs\nTo delete job run logs, use dr.registry.JobRun.delete_logs, as in the following example:\nimport datarobot as dr\njob_id = \"65f4453e6ea907cb0405ff7f\"\njob_run = dr.registry.JobRun.get(job_id, \"65f856957d897d46b0e54b37\")\njob_run.delete_logs()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/jobs.html",
      "tags": [
        "advanced",
        "api_reference",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/jobs.html",
        "content_length": 5073
      },
      "code_examples": [],
      "api_methods": [
        "dr.models.deployment",
        "dr.registry.jobrun",
        "deployment.custom_metrics",
        "dr.registry.job",
        "dr.registry.jobrunstatus"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-3425707393473805894",
      "title": "Key Values",
      "content": "Key Values\nKey values associated with a DataRobot model, deployment, job or other DataRobot entities are key-value pairs containing information about the related entity. Each key-value pair has the following:\nName: The unique and descriptive name of the key (for the model package or version).\nValue type: The data type of the value associated with the key. The possible types are string, numeric, boolean, URL, image, dataset, pickle, binary, JSON, or YAML.\nCategory: The type of model information provided by the key value. The possible types are training parameter, metric, tag, artifact, and runtime parameter.\nValue: The stored data or file.\nYou can include string, numeric, boolean, image, and dataset key values in custom compliance documentation templates.\nIn addition, with key values for registered models, when you generate compliance documentation for a model package and reference a supported key value in the template, DataRobot inserts the matching values from the associated model package.\nManage Key Values\nUse the following commands to manage key values:\nCreate a Key Value\nTo create a key value, use dr.KeyValue.create, as shown in the following example:\nimport datarobot as dr\nregistered_model_id = \"65ccb597732422fa2297199e\"\nkey_value = dr.KeyValue.create(\nregistered_model_id,\ndr.KeyValueEntityType.REGISTERED_MODEL,\n\"my-kv-name\",\ndr.KeyValueCategory.TAG,\ndr.KeyValueType.STRING,\n\"tag-name\",\n)\nkey_value.id\n>>> '65f32822be17d11dec9ebdfb'\nList Key Values\nTo list all key values available to the current user, use dr.KeyValue.list, as in the following example:\nimport datarobot as dr\nregistered_model_id = \"65ccb597732422fa2297199e\"\nkey_values = dr.KeyValue.list(registered_model_id, dr.KeyValueEntityType.REGISTERED_MODEL)\nkey_values\n>>> [KeyValue('my-kv-name')]\nRetrieve Key Value\nTo get a key value by unique identifier, use dr.KeyValue.get, as in the following example:\nimport datarobot as dr\nkey_value = dr.KeyValue.get(\"65f32822be17d11dec9ebdfb\")\nkey_value\n>>> KeyValue('my-kv-name')\nFind Key Value By Name\nTo find a key value by name, use dr.KeyValue.find. Provide the entity ID, entity type, and key value name, as in the following example:\nimport datarobot as dr\nkey_value = dr.KeyValue.find(\"65f32822be17d11dec9ebdfb\", dr.KeyValueEntityType.REGISTERED_MODEL, \"my-kv-name\")\nkey_value\n>>> KeyValue('my-kv-name')\nUpdate Key Value\nTo get a key value by unique identifier and update it, use dr.KeyValue.get() and then update(), as in the following example:\nimport datarobot as dr\nkey_value = dr.KeyValue.get(\"65f32822be17d11dec9ebdfb\")\nkey_value.update(value=4.7)\nkey_value.update(value_type=dr.KeyValueType.STRING, value=\"abc\")\nkey_value.update(name=\"new-kv-name\")\nGet Key Value Data\nTo get the value from a key value, use dr.KeyValue.get_value(). Provide the key value ID,  as in the following example:\nimport datarobot as dr\nkey_value = dr.KeyValue.get(\"65f32822be17d11dec9ebdfb\")\nkey_value.update(value=4.7)\nkey_value.get_value()\n>>> 4.7\nkey_value.update(value_type=dr.KeyValueType.STRING, value=\"abc\")\nkey_value.get_value()\n>>> \"abc\"\nkey_value.update(value_type=dr.KeyValueType.BOOLEAN, value=True)\nkey_value.get_value()\n>>> True\nDelete Key Value\nTo get a key value by unique identifier and delete it, use dr.KeyValue.get() and then delete(), as in the following example:\nimport datarobot as dr\nkey_value = dr.KeyValue.get(\"65f32822be17d11dec9ebdfb\")\nkey_value.delete()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/key_values.html",
      "tags": [
        "advanced",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/key_values.html",
        "content_length": 3408
      },
      "code_examples": [],
      "api_methods": [
        "dr.keyvaluecategory.tag",
        "dr.keyvalue.list",
        "dr.keyvaluetype.string",
        "dr.keyvalue.get",
        "dr.keyvalue.find",
        "dr.keyvalueentitytype.registered_model",
        "dr.keyvalue.create",
        "dr.keyvaluetype.boolean",
        "dr.keyvalue.get_value"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_1028390852561926403",
      "title": "Model Registry",
      "content": "Model Registry\nRegistered models are generic containers that group multiple versions of models which can be deployed, used as a challenger model, or replace a deployed model.\nEach registered model can have multiple versions. Each version can be created from a DataRobot model, custom model, or external model.\nRegistered models can have versions of different types (leaderboard, custom, or external) simultaneously as long as they have same target properties and time series settings where applicable.\nCreate registered model & version\nThe following command can be used to either create a registered model from or add version to the existing model.\nLEADERBOARD_MODEL_ID = \"650c2372c538ffa4480567d1\"\n# passing registered_model_name creates new version\nfirst_version = dr.RegisteredModelVersion.create_for_leaderboard_item(\nmodel_id=LEADERBOARD_MODEL_ID,\nname=\"Name of the version(aka model package)\",\nregistered_model_name='DEMO 3: Name of the registered model unique across the org '\n)\n# add custom model as a version\n# passing registered_model_id adds version to existing registered model\nCUSTOM_MODEL_VERSION_ID = \"619679c86c1abbc2bd628ed1\"\nsecond_version_from_custom = dr.RegisteredModelVersion.create_for_custom_model_version(\ncustom_model_version_id=CUSTOM_MODEL_VERSION_ID,\nregistered_model_id=first_version.registered_model_id,\nname=\"Another Name of the version(aka model package)\",\n)\n# add external model as a version\nsecond_version_from_external = dr.RegisteredModelVersion.create_for_external(\nname='Another name',\ntarget={'name': 'Target', 'type': 'Regression'},\nregistered_model_id=first_version.registered_model_id,\n)\nList and filter registered models\nUse the following command to list registered models.\nYou can filter the registered models that are returned by passing an instance of the\nRegisteredModelListFilters class to the filters keyword argument.\nYou can also filter the registered model versions that are returned by passing an instance of the\nRegisteredModelVersionsListFilters class to the filters keyword argument.\ndemo_registered_models = dr.RegisteredModel.list(search=\"DEMO\")\nregistered_model_filters = dr.models.model_registry.RegisteredModelListFilters(\ncreated_at_start=datetime.datetime(2020, 1, 1),\ncreated_at_end=datetime.datetime(2024, 1, 2),\nmodified_at_start=datetime.datetime(2020, 1, 1),\nmodified_at_end=datetime.datetime(2024, 1, 2),\ntarget_name='readmitted',\ntarget_type='Binary',\ncreated_by='[email protected]',\ncompatible_with_model_package_id='650a9f57d3f427ce1cc64747',\nprediction_threshold=0.5,\nimported=False,\nfor_challenger=False,\n)\nregistered_models = dr.RegisteredModel.list(filters=registered_model_filters, search=\"10k\")\nregistered_model = registered_models[0]\nversions = registered_model.list_versions()\n# similarily to registered models, versions also support fine-grain filtering and search\nfilters = dr.models.model_registry.RegisteredModelVersionsListFilters(\ntarget_name='readmitted',\n)\nversions_with_search = registered_model.list_versions(search=\"Elastic\", filters=filters)\nArchive, update and share registered models\nUse the following command to archive registered models. Archiving registered models archives all the versions of the registered model.\nREGISTERED_MODEL_ID = \"651bd2317aed25ed7d4bca7f\"\ndr.RegisteredModel.archive(REGISTERED_MODEL_ID)\nUse the following command to update registered models.\nREGISTERED_MODEL_ID = \"651bd2317aed25ed7d4bca7f\"\ndr.RegisteredModel.update(REGISTERED_MODEL_ID, name=\"New name\")\nThe following commands can be used to share registered models with other users or groups or retrieve existing roles on the deployment.\nregistered_model = dr.RegisteredModel.get('645b62d5373ed49b485d73e9')\n# EXISTING ROLES\nroles = registered_model.get_shared_roles()\nrole = dr.SharingRole(\nshare_recipient_type=\"user\",\nid='5ca19879a950d002c61ea3e7',\nrole=\"USER\",\n)\nregistered_model.share([role])\nList deployments associated with a registered model\nUse the following command to list deployments associated with registered model. The deployment is considered associated if one of the versions of the registered model is either a champion or a challenger model.\nmodel_with_deployments = dr.RegisteredModel.get('65035d911e9ff5b07f00f2ea')\n# we can list deployments associated with this registered model. Method is searchable and paginated.\nmodel_associated_deployments = model_with_deployments.list_associated_deployments()\n# we can also list deployments associated with specific version of the registered model\nversion = model_with_deployments.list_versions()[1]\nversion.list_associated_deployments()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/model_registry.html",
      "tags": [
        "advanced",
        "api_reference",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/mlops/model_registry.html",
        "content_length": 4580
      },
      "code_examples": [],
      "api_methods": [
        "dr.registeredmodel.update",
        "model.get",
        "dr.registeredmodel.list",
        "model.list_versions",
        "model.archive",
        "dr.registeredmodelversion.create_for_external",
        "dr.registeredmodelversion.create_for_custom_model_version",
        "model.update",
        "dr.registeredmodel.get",
        "model.get_shared_roles",
        "dr.registeredmodel.archive",
        "model.list",
        "model.share",
        "dr.models.model_registry",
        "dr.registeredmodelversion.create_for_leaderboard_item"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-5719394240225732009",
      "title": "Blueprints",
      "content": "Blueprints\nBlueprints are a set of computation paths that a dataset passes through before producing\npredictions from data. A blueprint can be trained on a dataset to generate a model.\nTo modify blueprints using Python, reference the Blueprint Workshop documentation.\nThe following code block summarizes the interactions available for blueprints.\n# Get the set of blueprints recommended by datarobot\nimport datarobot as dr\nmy_projects = dr.Project.list()\nproject = my_projects[0]\nmenu = project.get_blueprints()\nfirst_blueprint = menu[0]\nproject.train(first_blueprint)\nList blueprints\nWhen you upload a file to a project and set a target, you receive a set of recommended blueprints that are appropriate for the task at hand.\nUse get_blueprints to get the list of blueprints recommended for a project:\nproject = dr.Project.get('5506fcd38bd88f5953219da0')\nmenu = project.get_blueprints()\nblueprint = menu[0]\nGet a blueprint\nIf you already have a blueprint_id from a model you can retrieve the blueprint directly.\nproject_id = '5506fcd38bd88f5953219da0'\nproject = dr.Project.get(project_id)\nmodels = project.get_models()\nmodel = models[0]\nblueprint = Blueprint.get(project_id, model.blueprint_id)\nGet a blueprint chart\nYou can retrieve charts for all blueprints that are either from a blueprint menu or are already used in a model. You can also get a blueprint’s representation in Graphviz DOT format to render it into the format you need.\nproject_id = '5506fcd38bd88f5953219da0'\nblueprint_id = '4321fcd38bd88f595321554223'\nbp_chart = BlueprintChart.get(project_id, blueprint_id)\nprint(bp_chart.to_graphviz())\nGet blueprint documentation\nYou can retrieve documentation for tasks used in a blueprint. The documentation contains information about\nthe task, its parameters, and links and references to additional sources. All documents are instances of the BlueprintTaskDocument class.\nproject_id = '5506fcd38bd88f5953219da0'\nblueprint_id = '4321fcd38bd88f595321554223'\nbp = Blueprint.get(project_id, blueprint_id)\ndocs = bp.get_documents()\nprint(docs[0].task)\n>>> Average Blend\nprint(docs[0].links[0]['url'])\n>>> https://en.wikipedia.org/wiki/Ensemble_learning\nBlueprint attributes\nThe Blueprint class holds the data required to use the blueprint\nfor modeling. This includes the blueprint_id and project_id.\nThere are also two attributes that help distinguish blueprints: model_type\nand processes.\nprint(blueprint.id)\n>>> u'8956e1aeecffa0fa6db2b84640fb3848'\nprint(blueprint.project_id)\n>>> u5506fcd38bd88f5953219da0'\nprint(blueprint.model_type)\n>>> Logistic Regression\nprint(blueprint.processes)\n>>> [u'One-Hot Encoding',\nu'Missing Values Imputed',\nu'Standardize',\nu'Logistic Regression']\nBuild a model from a blueprint\nYou can also use a blueprint to train a model. The model is trained on the associated project’s dataset by default.\nNote that Project.train is used for non-datetime partitioned projects.\nProject.train_datetime should be used for datetime partitioned\nprojects.\nmodel_job_id = project.train(blueprint)\n# For datetime partitioned projects\nmodel_job = project.train_datetime(blueprint.id)\nBoth Project.train and Project.train_datetime\nwill put a new modeling job into the queue. However, note that Project.train returns the ID of the created\nModelJob, while Project.train_datetime returns the ModelJob object itself.\nYou can pass a ModelJob ID to wait_for_async_model_creation function,\nwhich polls the async model creation status and returns the newly created model when it’s finished.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/blueprint.html",
      "tags": [
        "api_reference",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/blueprint.html",
        "content_length": 3504
      },
      "code_examples": [],
      "api_methods": [
        "dr.project.list",
        "project.list",
        "project.train_datetime",
        "model.blueprint_id",
        "dr.project.get",
        "project.get",
        "project.get_models",
        "project.train",
        "project.get_blueprints"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-7569489407199234628",
      "title": "Modeling",
      "content": "Modeling\nThe Modeling section provides information to help you easily navigate the process of building, understanding, and analyzing models.",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/index.html",
        "content_length": 141
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_942446055156654160",
      "title": "External Testset",
      "content": "External Testset\nTesting with external datasets allows better evaluation model performance, you can compute metric\nscores and insights on external test dataset to ensure consistent performance prior to deployment.\nNote\nNot available for Time series models.\nRequesting External Scores and Insights\nTo compute scores and insights on a dataset\nUpload a prediction dataset that contains the target column PredictionDataset.contains_target_values == True.\nDataset should be in the same structure as the original project.\nimport datarobot as dr\n# Upload dataset\nproject = dr.Project(project_id)\ndataset = project.upload_dataset('./test_set.csv')\ndataset.contains_target_values\n>>>True\n# request external test to compute metric scores and insights on dataset\n# select model using project.get_models()\nexternal_test_job = model.request_external_test(dataset.id)\n# once job is complete, scores and insights are ready for retrieving\nexternal_test_job.wait_for_completion()\nRetrieving External Metric Scores and Insights\nAfter completion of external test job, metric scores and insights for external testsets will be ready.\nNote\nPlease check PredictionDataset.data_quality_warnings for dataset warnings.\nInsights are not available if dataset is too small (less than 10 rows).\nROC curve cannot be calculated if dataset has only one class in target column\nRetrieving External Metric Scores\nimport datarobot as dr\n# retrieving list of external metric scores on multiple datasets\nmetric_scores_list = dr.ExternalScores.list(project_id, model_id)\n# retrieving external metric scores on one dataset\nmetric_scores = dr.ExternalScores.get(project_id, model_id, dataset_id)\nRetrieving External Lift Chart\nimport datarobot as dr\n# retrieving list of lift charts on multiple datasets\nlift_list = dr.ExternalLiftChart.list(project_id, model_id)\n# retrieving one lift chart for dataset\nlift = dr.ExternalLiftChart.get(project_id, model_id, dataset_id)\nRetrieving External Multiclass Lift Chart\nLift chart for Multiclass models only\nimport datarobot as dr\n# retrieving list of lift charts on multiple datasets\nlift_list = ExternalMulticlassLiftChart.list(project_id, model_id)\n# retrieving one lift chart for dataset and a target class\nlift = ExternalMulticlassLiftChart.get(project_id, model_id, dataset_id, target_class)\nRetrieving External ROC Curve\nAvailable for Binary classification models only\nimport datarobot as dr\n# retrieving list of roc curves on multiple datasets\nroc_list = ExternalRocCurve.list(project_id, model_id)\n# retrieving one ROC curve for dataset\nroc = ExternalRocCurve.get(project_id, model_id, dataset_id)\nRetrieving Multiclass Confusion Matrix\nAvailable for Multiclass classification models only\nimport datarobot as dr\n# retrieving list of confusion charts on multiple datasets\nconfusion_list = ExternalConfusionChart.list(project_id, model_id)\n# retrieving one confusion chart for dataset\nconfusion = ExternalConfusionChart.get(project_id, model_id, dataset_id)\nRetrieving Residuals Chart\nAvailable for Regression models only\nimport datarobot as dr\n# retrieving list of residuals charts on multiple datasets\nresiduals_list = ExternalResidualsChart.list(project_id, model_id)\n# retrieving one residuals chart for dataset\nresiduals = ExternalResidualsChart.get(project_id, model_id, dataset_id)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/external_testset.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/external_testset.html",
        "content_length": 3304
      },
      "code_examples": [],
      "api_methods": [
        "model.request_external_test",
        "dr.externalscores.get",
        "dr.externalliftchart.get",
        "project.upload_dataset",
        "dr.externalscores.list",
        "dr.externalliftchart.list",
        "project.get_models"
      ],
      "complexity_score": 0.7,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-5264933675966343511",
      "title": "Automated Documentation",
      "content": "Automated Documentation\nDataRobot can generate Automated Documentation about various entities within the platform, such\nas specific models or projects. These reports can be downloaded and shared to help with\nregulatory compliance as well as to provide a general understanding of the AI lifecycle.\nCheck Available Document Types\nAutomated Documentation is available behind different feature flags set up according to your POC\nsettings or subscription plan. MODEL_COMPLIANCE documentation is a premium add-on DataRobot\nproduct, while AUTOPILOT_SUMMARY report is available behind an optional feature flag for\nSelf-Service and other platforms.\nimport datarobot as dr\n# Connect to your DataRobot platform with your token\ndr.Client(token=my_token, endpoint=endpoint)\noptions = dr.AutomatedDocument.list_available_document_types()\nIn response, you get a data dictionary with a list of document types that are available for\ngeneration with your account.\nGenerate Automated Documents\nNow that you know which documents you can generate, create one with AutomatedDocument .generate method. Note that for AUTOPILOT_SUMMARY report, you need to assign a project ID\nto the entity_id parameter, while MODEL_COMPLIANCE expects an ID of a model with the\nentity_id parameter.\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc_type = \"AUTOPILOT_SUMMARY\"\nentity_id = \"5e8b6a34d2426053ab9a39ed\"  #  This is an ID of a project\nfile_format=\"docx\"\ndoc = dr.AutomatedDocument(document_type=doc_type, entity_id=entity_id, output_format=file_format)\ndoc.generate()\nYou can specify other attributes. For example, filepath presets the file location and name to\nuse when downloading the document. Please see the API Reference for more details.\nDownload Automated Documents\nIf you followed the steps above to generate an automated document, you can use the\nAutomatedDocument.download method right away to get the document.\ndoc.filepath = \"Users/jeremy/DR_project_docs/autopilot_report_staff_2021.docx\"\ndoc.download()\nYou can set a desired filepath (that includes the future file’s name) before you download a\ndocument. Otherwise, it will be automatically downloaded to the directory from which you launched\nyour script.\nPlease note that to download the document, you need its ID. When you generate a document with the\nPython client, the ID is set automatically without your interference. However, if the document\nhas already been generated from the application interface (or REST API) and you want to download\nit using the Python client, you need to provide the ID of the document you want to download:\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc_id = \"604f81f0f3d6397d250c35bc\"\npath = \"Users/jeremy/DR_project_docs/xgb_model_doc_staff_project_2021.docx\"\ndoc = dr.AutomatedDocument(id=doc_id, filepath=path)\ndoc.download()\nList Previously Generated Automated Documents\nYou can retrieve information about previously generated documents available for your account. The\ninformation includes document ID and type, ID of the entity it was generated for, time of\ncreation, and other information. Documents are sorted by creation time  – created_at key –\nfrom most recent to oldest.\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndocs = dr.AutomatedDocument.list_generated_documents()\nThis returns list of AutomatedDocument objects. You can request a list of specific documents.\nFor example, get a list of all MODEL_COMPLIANCE documents:\nmodel_docs = dr.AutomatedDocument.list_generated_documents(document_types=[\"MODEL_COMPLIANCE\"])\nOr get a list of documents created for specific entities:\notv_project_reports = dr.AutomatedDocument.list_generated_documents(\nentity_ids=[\"604f81f0f3d6397d250c35bc\", \"5ed60de32f18d97d250c3db5\"]\n)\nFor more information about all query options, see AutomatedDocument .list_generated_documents in the API Reference.\nDelete Automated Documents\nTo delete a document from the DataRobot application, use the AutomatedDocument.delete method.\nimport datarobot as dr\ndr.Client(token=my_token, endpoint=endpoint)\ndoc = dr.AutomatedDocument(id=\"604f81f0f3d6397d250c35bc\")\ndoc.delete()\nAll locally saved automated documents will remain intact.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/automated_documentation.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/automated_documentation.html",
        "content_length": 4202
      },
      "code_examples": [],
      "api_methods": [
        "dr.automateddocument.list_available_document_types",
        "dr.automateddocument.list_generated_documents"
      ],
      "complexity_score": 0.5,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_458197644820139742",
      "title": "Model Insights",
      "content": "Model Insights\nThe Modeling section provides information to help you easily navigate the process of building, understanding, and analyzing models.",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/index.html",
        "content_length": 147
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_6702977768587188152",
      "title": "Rating Table",
      "content": "Rating Table\nA rating table is an exportable csv representation of a Generalized Additive Model. They contain\ninformation about the features and coefficients used to make predictions. Users can influence\npredictions by downloading and editing values in a rating table, then re-uploading the table and\nusing it to create a new model.\nSee the page about interpreting Generalized Additive Models’ output in the DataRobot user guide for\nmore details on how to interpret and edit rating tables.\nDownload A Rating Table\nYou can retrieve a rating table from the list of rating tables in a project:\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nproject = dr.Project.get(project_id)\nrating_tables = project.get_rating_tables()\nrating_table = rating_tables[0]\nOr you can retrieve a rating table from a specific model. The model must already exist:\nimport datarobot as dr\nfrom datarobot.models import RatingTableModel, RatingTable\nproject_id = '5506fcd38bd88f5953219da0'\nproject = dr.Project.get(project_id)\n# Get model from list of models with a rating table\nrating_table_models = project.get_rating_table_models()\nrating_table_model = rating_table_models[0]\n# Or retrieve model by id. The model must have a rating table.\nmodel_id = '5506fcd98bd88f1641a720a3'\nrating_table_model = dr.RatingTableModel.get(project=project_id, model_id=model_id)\n# Then retrieve the rating table from the model\nrating_table_id = rating_table_model.rating_table_id\nrating_table = dr.RatingTable.get(projcet_id, rating_table_id)\nThen you can download the contents of the rating table:\nrating_table.download('./my_rating_table.csv')\nUploading A Rating Table\nAfter you’ve retrieved the rating table CSV and made the necessary edits, you\ncan re-upload the CSV so you can create a new model from it:\njob = dr.RatingTable.create(project_id, model_id, './my_rating_table.csv')\nnew_rating_table = job.get_result_when_complete()\njob = new_rating_table.create_model()\nmodel = job.get_result_when_complete()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/rating_table.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/rating_table.html",
        "content_length": 1986
      },
      "code_examples": [],
      "api_methods": [
        "model.get",
        "dr.ratingtablemodel.get",
        "project.get_rating_tables",
        "project.get_rating_table_models",
        "dr.ratingtable.create",
        "dr.project.get",
        "model.rating_table_id",
        "project.get",
        "dr.ratingtable.get"
      ],
      "complexity_score": 0.75,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_2297011524141119725",
      "title": "Prediction Explanations",
      "content": "Prediction Explanations\nTo compute prediction explanations you need to have feature impact\ncomputed for a model, and predictions for an uploaded dataset\ncomputed with a selected model.\nComputing prediction explanations is a resource-intensive task, but you can configure it with\nmaximum explanations per row and prediction value thresholds to speed up the process.\nQuick Reference\nimport datarobot as dr\n# Get project\nmy_projects = dr.Project.list()\nproject = my_projects[0]\n# Get model\nmodels = project.get_models()\nmodel = models[0]\n# Compute feature impact\nfeature_impacts = model.get_or_request_feature_impact()\n# Upload dataset\ndataset = project.upload_dataset('./data_to_predict.csv')\n# Compute predictions\npredict_job = model.request_predictions(dataset.id)\npredict_job.wait_for_completion()\n# Initialize prediction explanations\npei_job = dr.PredictionExplanationsInitialization.create(project.id, model.id)\npei_job.wait_for_completion()\n# Compute prediction explanations with default parameters\npe_job = dr.PredictionExplanations.create(project.id, model.id, dataset.id)\npe = pe_job.get_result_when_complete()\n# Iterate through predictions with prediction explanations\nfor row in pe.get_rows():\nprint(row.prediction)\nprint(row.prediction_explanations)\n# download to a CSV file\npe.download_to_csv('prediction_explanations.csv')\nList Prediction Explanations\nYou can use the PredictionExplanations.list() method to return a list of prediction\nexplanations computed for a project’s models:\nimport datarobot as dr\nprediction_explanations = dr.PredictionExplanations.list('58591727100d2b57196701b3')\nprint(prediction_explanations)\n>>> [PredictionExplanations(id=585967e7100d2b6afc93b13b,\nproject_id=58591727100d2b57196701b3,\nmodel_id=585932c5100d2b7c298b8acf),\nPredictionExplanations(id=58596bc2100d2b639329eae4,\nproject_id=58591727100d2b57196701b3,\nmodel_id=585932c5100d2b7c298b8ac5),\nPredictionExplanations(id=58763db4100d2b66759cc187,\nproject_id=58591727100d2b57196701b3,\nmodel_id=585932c5100d2b7c298b8ac5),\n...]\npe = prediction_explanations[0]\npe.project_id\n>>> u'58591727100d2b57196701b3'\npe.model_id\n>>> u'585932c5100d2b7c298b8acf'\nYou can pass following parameters to filter the result:\nmodel_id – str, used to filter returned prediction explanations by model_id.\nlimit – int, limit for number of items returned, default: no limit.\noffset – int, number of items to skip, default: 0.\nList Prediction Explanations Example:\nproject_id = '58591727100d2b57196701b3'\nmodel_id = '585932c5100d2b7c298b8acf'\ndr.PredictionExplanations.list(project_id, model_id=model_id, limit=20, offset=100)\nInitialize Prediction Explanations\nIn order to compute prediction explanations you have to initialize it for a particular model.\ndr.PredictionExplanationsInitialization.create(project_id, model_id)\nCompute Prediction Explanations on New Data\nIf all prerequisites are in place, you can compute prediction explanations in the following way:\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\ndataset_id = '5506fcd98bd88a8142b725c8'\npe_job = dr.PredictionExplanations.create(project_id, model_id, dataset_id,\nmax_explanations=2, threshold_low=0.2, threshold_high=0.8)\npe = pe_job.get_result_when_complete()\nWhere:\nmax_explanations are the maximum number of prediction explanations to compute for each row.\nthreshold_low and threshold_high are thresholds for the value of the prediction of the\nrow. Prediction explanations will be computed for a row if the row’s prediction value is higher\nthan threshold_high or lower than threshold_low. If no thresholds are specified,\nprediction explanations will be computed for all rows.\nCompute Prediction Explanations on Training Data\nTo compute Prediction Explanations on training data, use the code snippet below.\nThe prerequisites are generally the same as computing on newly uploaded data:\nFeature Impact calculations must have completed.\nPrediction Explanations must be initialized.\nPredictions on training data must first be computed for the model.\nThe dataset_id parameter is the ID of the feature list that was used to train the model.\nimport datarobot as dr\nproject_id = '67771742b4d4cf44277b1ff0'\nmodel_id = '677859cfeaea57c1bc9a150a'\nmodel = dr.Model.get(project_id, model_id)\ndataset_id = model.featurelist_id\n# Request feature impact if not done yet.\nmodel.request_feature_impact()\n# Request training predictions for the model if not done yet.\n# The subset 'all' includes training, validation, and holdout data.\nmodel.request_training_predictions(dr.enums.DATA_SUBSET.ALL)\n# Initialize explanations.\ndr.PredictionExplanationsInitialization.create(project_id, model_id)\n# Request and download explanations for the full training data.\npe_job = dr.PredictionExplanations.create_on_training_data(project_id, model_id, dataset_id)\nresult = pe_job.get_result_when_done()\ndf = result.get_all_as_dataframe()\nRetrieving Prediction Explanations\nYou have three options for retrieving prediction explanations.\nNote\nPredictionExplanations.get_all_as_dataframe() and\nPredictionExplanations.download_to_csv() reformat\nprediction explanations to match the schema of CSV file downloaded from UI (RowId,\nPrediction, Explanation 1 Strength, Explanation 1 Feature, Explanation 1 Value, …,\nExplanation N Strength, Explanation N Feature, Explanation N Value)\nGet prediction explanations rows one by one as\nPredictionExplanationsRow\nobjects:\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nprediction_explanations_id = '5506fcd98bd88f1641a720a3'\npe = dr.PredictionExplanations.get(project_id, prediction_explanations_id)\nfor row in pe.get_rows():\nprint(row.prediction_explanations)\nGet all rows as pandas.DataFrame:\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nprediction_explanations_id = '5506fcd98bd88f1641a720a3'\npe = dr.PredictionExplanations.get(project_id, prediction_explanations_id)\nprediction_explanations_df = pe.get_all_as_dataframe()\nDownload all rows to a file as CSV document:\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nprediction_explanations_id = '5506fcd98bd88f1641a720a3'\npe = dr.PredictionExplanations.get(project_id, prediction_explanations_id)\npe.download_to_csv('prediction_explanations.csv')\nAdjusted Predictions In Prediction Explanations\nIn some projects such as insurance projects, the prediction adjusted by exposure is more useful\ncompared with raw prediction. For example, the raw prediction (e.g. claim counts) is divided by\nexposure (e.g. time) in the project with exposure column. The adjusted prediction provides insights\nwith regard to the predicted claim counts per unit of time. To include that information, set\nexclude_adjusted_predictions to False in correspondent method calls.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nprediction_explanations_id = '5506fcd98bd88f1641a720a3'\npe = dr.PredictionExplanations.get(project_id, prediction_explanations_id)\npe.download_to_csv('prediction_explanations.csv', exclude_adjusted_predictions=False)\nprediction_explanations_df = pe.get_all_as_dataframe(exclude_adjusted_predictions=False)\nMulticlass/Clustering Prediction Explanation Modes\nWhen calculating prediction explanations for the multiclass or clustering model you need to specify\nwhich classes should be explained in each row. By default we only explain the predicted class but\nit can be set with the mode parameter of PredictionExplanations.create\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\ndataset_id = '5506fcd98bd88a8142b725c8'\n# Explain predicted and second-best class results in each row\npe_job = dr.PredictionExplanations.create(project_id, model_id, dataset_id,\nmode=dr.models.TopPredictionsMode(2))\npe = pe_job.get_result_when_complete()\n# Explain results for classes \"setosa\" and \"versicolor\" in each row\npe_job = dr.PredictionExplanations.create(project_id, model_id, dataset_id,\nmode=dr.models.ClassListMode([\"setosa\", \"versicolor\"]))\npe = pe_job.get_result_when_complete()\nSHAP based prediction explanations\nThere are two types of SHAP prediction explanations available, universal SHAP explanations and\nmodel-specific SHAP explanations. All models support universal SHAP explanations, which use the\npermutation based explainer algorithm. Selected models support SHAP explanations such as the tree-based\nexplainer or kernel explainer.\nUniversal SHAP explanations can be computed and retrieved very simply and do not require any pre-requisites. They\ncan be computed for any available partition, and can be restricted to specific data slices.\nimport datarobot as dr\nfrom datarobot.insights import ShapMatrix\nproject_id = '5ea6d3354cfad121cf33a5e1'\nmodel_id = '5ea6d38b4cfad121cf33a60d'\nproject = dr.Project.get(project_id)\nmodel = dr.Model.get(project=project_id, model_id=model_id)\n# Additional parameters can be passed to specify the partition,\n# data slice, and other parametrers.\nshap_insight = ShapMatrix.create(model_id)\n# Get all computed SHAP matrices\nall_shap_insights = ShapMatrix.list(model_id)\n# Retrieve the SHAP matrix as a numpy array\nmatrix = shap_insight.matrix\n# Retrieve the SHAP matrix columns\ncolumns = shap_insight.columns\n# Retrieve the SHAP base value for additivity checks\nbase_value = shap_insight.base_value\nYou can request model-specific SHAP based prediction explanations using previously uploaded scoring dataset for\nsupported models. Unlike for XEMP prediction explanations you do not need to have\nfeature impact computed for a model, and predictions for an\nuploaded dataset. See datarobot.models.ShapMatrix.create() reference\nfor a description of the types of parameters that can be passed in.\nimport datarobot as dr\nproject_id = '5ea6d3354cfad121cf33a5e1'\nmodel_id = '5ea6d38b4cfad121cf33a60d'\nproject = dr.Project.get(project_id)\nmodel = dr.Model.get(project=project_id, model_id=model_id)\n# check if model supports SHAP\nmodel_capabilities = model.get_supported_capabilities()\nprint(model_capabilities.get('supportsShap'))\n>>> True\n# upload dataset to generate prediction explanations\ndataset_from_path = project.upload_dataset('./data_to_predict.csv')\nshap_matrix_job = ShapMatrix.create(project_id=project_id, model_id=model_id, dataset_id=dataset_from_path.id)\nshap_matrix_job\n>>> Job(shapMatrix, status=inprogress)\n# wait for job to finish\nshap_matrix = shap_matrix_job.get_result_when_complete()\nshap_matrix\n>>> ShapMatrix(id='5ea84b624cfad1361c53f65d', project_id='5ea6d3354cfad121cf33a5e1', model_id='5ea6d38b4cfad121cf33a60d', dataset_id='5ea84b464cfad1361c53f655')\n# retrieve SHAP matrix as pandas.DataFrame\ndf = shap_matrix.get_as_dataframe()\n# list as available SHAP matrices for a project\nshap_matrices = dr.ShapMatrix.list(project_id)\nshap_matrices\n>>> [ShapMatrix(id='5ea84b624cfad1361c53f65d', project_id='5ea6d3354cfad121cf33a5e1', model_id='5ea6d38b4cfad121cf33a60d', dataset_id='5ea84b464cfad1361c53f655')]\nshap_matrix = shap_matrices[0]\n# retrieve SHAP matrix as pandas.DataFrame\ndf = shap_matrix.get_as_dataframe()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/prediction_explanations.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/prediction_explanations.html",
        "content_length": 11025
      },
      "code_examples": [],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "datarobot.models.shapmatrix",
        "dr.enums.data_subset",
        "model.request_feature_impact",
        "model.request_predictions",
        "model.request_training_predictions",
        "model.id",
        "dr.predictionexplanationsinitialization.create",
        "model.featurelist_id",
        "dr.predictionexplanations.get",
        "project.list",
        "project.id",
        "model.get_supported_capabilities",
        "project.get",
        "project.upload_dataset",
        "dr.project.list",
        "dr.models.classlistmode",
        "dr.shapmatrix.list",
        "model.get",
        "dr.predictionexplanations.list",
        "dr.predictionexplanations.create",
        "dr.model.get",
        "dr.project.get",
        "dr.models.toppredictionsmode",
        "project.get_models",
        "dr.predictionexplanations.create_on_training_data"
      ],
      "complexity_score": 0.95,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_7347395233266486141",
      "title": "SHAP insights",
      "content": "SHAP insights\nSHAP is an open-source method for explaining the predictions from machine learning models.\n(You can find more information about SHAP at its repository on GitHub: https://github.com/slundberg/shap)\nDataRobot supports SHAP computations for all regression and binary classification blueprints. You can\ncompute three different insights:\n“SHAP matrix”: Raw SHAP values for each feature column and each row.\n“SHAP impact”: Overall importance for each feature column across all rows, based on aggregated\nSHAP matrix values.\n“SHAP preview”: SHAP values for the most important features in each row, presented with the values\nof the features in that row.\nThe following example code assumes that you have a trained model object called model.\nimport datarobot as dr\nfrom datarobot.insights.shap_matrix import ShapMatrix\nfrom datarobot.insights.shap_impact import ShapImpact\nfrom datarobot.insights.shap_preview import ShapPreview\nmodel_id = model.id  # or model_id = 'YOUR_MODEL_ID'\n# request SHAP Matrix, and wait for it to complete\nresult = ShapMatrix.create(entity_id=model_id)  # default source is 'validation'\n# view the properties of the SHAP Matrix\nprint(result.columns)\n>>> ['AUCGUART', 'Color', 'Make', ...\nprint(result.matrix)\n>>> [[ 1.22604372e-02  1.98424454e-01  2.23308013e-01  ...] ... ]\n# request SHAP Matrix on a different partition, and return immediately with job reference\njob = ShapMatrix.compute(entity_id=model_id, source='holdout')\n# wait for the job to complete\nresult = job.get_result_when_complete()\nprint(result.columns)\n>>> ['AUCGUART', 'Color', 'Make', ...\nprint(result.matrix)\n>>> [[-0.11443075 -0.01130723  0.22330801 ... ] ... ]\n# request SHAP Impact; only works for training currently\njob = ShapImpact.compute(entity_id=model_id, source='training', row_count=100)\nresult = job.get_result_when_complete()\n# Impacts are listed as [feature_name, normalized_impact, unnormalized_impact]\nprint(result.shap_impacts)\n>>> [['AUCGUART', 0.07989059458051094, 0.022147886593333888], ...]\n# list all matrices computed for this model, including each partition\nmatrix_list = ShapMatrix.list(entity_id=model_id)\nprint(matrix_list)\n>>> [<datarobot.insights.shap_matrix.ShapMatrix object at 0x114e52090>, ...]\nprint([(matrix_obj, matrix_obj.source) for matrix_obj in matrix_list])\n>>> [(<datarobot.insights.shap_matrix.ShapMatrix object at 0x114e52090>, 'validation'), ... ]\n# upload a file to the AI Catalog\ndataset = dr.Dataset.upload(\"./path/to/dataset.csv\")\n# request explanations for that file in the \"preview\" format\njob = ShapPreview.compute(entity_id=model_id, source='externalTestSet', external_dataset_id=dataset.id)\nresult = job.get_result_when_complete()\nprint(result.previews[0])\n>>> {'row_index': 0,\n>>> 'prediction_value': 0.3024851286385187,\n>>>  'preview_values': [{'feature_rank': 1,\n>>>    'feature_name': 'BYRNO',\n>>>    'feature_value': '21973',\n>>>    'shap_value': 0.22025144078391848,\n>>>    'has_text_explanations': False,\n>>>    'text_explanations': []},\n>>> ... }\nSHAP insights for custom models\nYou can compute SHAP insights for custom models, not just native DataRobot models. To do this, first\ncomplete the following setup:\nCreate a custom model version with an execution environment and a training dataset; note the version ID.\nRegister the custom model version as a registered model.\nInitialize the registered model for insights, using the AutomatedDocument.initialize_model_compliance method.\nAt this point, the model is ready for SHAP insights computation. Once these steps are completed for\na given registered model version, they do not have to be repeated.\nAs an example, the code snippet below outlines the preparation steps and then requests a ShapMatrix\ncomputation on an external dataset via the AI Catalog. It assumes that you have a Scoring Code file,\nmodel.jar, for the custom model, which you will run using the Java drop-in execution environment,\nas well as a training dataset called training.csv.\nimport datarobot as dr\nfrom datarobot.insights.shap_matrix import ShapMatrix\n# 1: create a custom model version with an execution environment and a training dataset, and note the version id\nmodel_args = {\n\"target_type\": dr.TARGET_TYPE.REGRESSION,\n\"target_name\": \"time_in_hospital\",\n\"language\": \"java\",\n}\ntraining_dataset = dr.Dataset.create_from_file(file_path=\"path/to/training.csv\")\nexecution_environment = dr.ExecutionEnvironment.list(search_for=\"java\")[0]\ncustom_model = dr.CustomInferenceModel.create(\nname=\"model.jar\",\n**model_args,\n)\ncustom_model_version = dr.CustomModelVersion.create_clean(\ncustom_model_id=custom_model.id,\nbase_environment_id=execution_environment.id,\ntraining_dataset_id=dataset.id,\nfiles=[(\"path/to/model.jar\", \"model.jar\")],\n)\ncustom_model_version_id = custom_model_version.id\n# 2. register the custom model version as a registered model\nregistered_model = dr.RegisteredModelVersion.create_for_custom_model_version(\ncustom_model_version_id=custom_model_version.id, name=model_name, registered_model_name=model_name\n)\n# 3. initialize the registered model for insights\nautodocs = dr.AutomatedDocument(\nentity_id=registered_model.id,\ndocument_type=\"MODEL_COMPLIANCE\",\n)\nautodocs.initialize_model_compliance()\nassert autodocs.is_model_compliance_initialized[0]\n# Add the scoring dataset to the AI catalog\nscoring_dataset = dr.Dataset.create_from_file(file_path=\"path/to/scoring_dataset.csv\")\n# Request the ShapMatrix computation, and retrieve results when it finishes\njob = ShapMatrix.compute(\nentity_id=custom_model_version_id,\nsource='externalTestSet',\nexternal_dataset_id=scoring_dataset.id,\nentity_type=\"customModel\",\n)\nresult = job.get_result_when_complete()\nprint(result.columns)\n>>> ['AUCGUART', 'Color', 'Make', ...\nprint(result.matrix)\n>>> [[ 1.22604372e-02  1.98424454e-01  2.23308013e-01  ...] ... ]",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/shap_insights.html",
      "tags": [
        "advanced",
        "api_reference",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/insights/shap_insights.html",
        "content_length": 5800
      },
      "code_examples": [],
      "api_methods": [
        "model.id",
        "datarobot.insights.shap_preview",
        "model.jar",
        "model.create",
        "dr.custommodelversion.create_clean",
        "dr.target_type.regression",
        "dr.custominferencemodel.create",
        "datarobot.insights.shap_matrix",
        "dr.executionenvironment.list",
        "dr.registeredmodelversion.create_for_custom_model_version",
        "dr.dataset.upload",
        "datarobot.insights.shap_impact",
        "dr.dataset.create_from_file"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-311456067831281746",
      "title": "Jobs",
      "content": "Jobs\nThe Job class is a generic representation of jobs running\nin a project’s queue.  Many tasks for modeling, such as creating a new model or\ncomputing Feature Impact for a model, use a job to track the worker usage and progress of\nthe associated task.\nCheck the contents of the queue\nTo see what jobs running or waiting in the queue for a project, use the Project.get_all_jobs\nmethod.\nfrom datarobot.enums import QUEUE_STATUS\njobs_list = project.get_all_jobs()  # gives all jobs queued or inprogress\njobs_by_type = {}\nfor job in jobs_list:\nif job.job_type not in jobs_by_type:\njobs_by_type[job.job_type] = [0, 0]\nif job.status == QUEUE_STATUS.QUEUE:\njobs_by_type[job.job_type][0] += 1\nelse:\njobs_by_type[job.job_type][1] += 1\nfor type in jobs_by_type:\n(num_queued, num_inprogress) = jobs_by_type[type]\nprint('{} jobs: {} queued, {} inprogress'.format(type, num_queued, num_inprogress))\nCancel a job\nIf a job is taking too long to run or no longer necessary, it can be cancelled from the\nJob object.\nfrom datarobot.enums import QUEUE_STATUS\nproject.pause_autopilot()\nbad_jobs = project.get_all_jobs(status=QUEUE_STATUS.QUEUE)\nfor job in bad_jobs:\njob.cancel()\nproject.unpause_autopilot()\nRetrieve results from a job\nYou can retrieve the results of a job once it is complete. Note that the type of the returned\nobject varies depending on the job_type. All return types\nare documented in Job.get_result.\nfrom datarobot.enums import JOB_TYPE\ntime_to_wait = 60 * 60  # how long to wait for the job to finish (in seconds) - i.e. an hour\nassert my_job.job_type == JOB_TYPE.MODEL\nmy_model = my_job.get_result_when_complete(max_wait=time_to_wait)\nModel jobs\nModel creation is an asynchronous process. This means that when explicitly invoking\nnew model creation (with project.train or model.train for example), all you get\nis the ID of the process responsible for model creation. With this ID, you can\nget info about the model that is being created—or the model itself, once\nthe creation process is finished—by using the ModelJob class.\nGet an existing model job\nTo retrieve existing model jobs, use the ModelJob.get method.\nFor this, you need the ID of the project from which the model was built and the ID of the model job.\nThe model job is useful if you want to know the parameters for a model’s creation (automatically chosen by the API backend)\nbefore the actual model was created.\nIf the model is already created, ModelJob.get will raise the PendingJobFinished exception.\nimport time\nimport datarobot as dr\nblueprint_id = '5506fcd38bd88f5953219da0'\nmodel_job_id = project.train(blueprint_id)\nmodel_job = dr.ModelJob.get(project_id=project.id,\nmodel_job_id=model_job_id)\nmodel_job.sample_pct\n>>> 64.0\n# wait for model to be created (in a very inefficient way)\ntime.sleep(10 * 60)\nmodel_job = dr.ModelJob.get(project_id=project.id,\nmodel_job_id=model_job_id)\n>>> datarobot.errors.PendingJobFinished\n# get the job attached to the model\nmodel_job.model\n>>> Model('5d518cd3962d741512605e2b')\nGet a created model\nAfter a model is created, you can use ModelJob.get_model to get the newly-created model.\nimport datarobot as dr\nmodel = dr.ModelJob.get_model(project_id=project.id,\nmodel_job_id=model_job_id)\nAsync model creation\nIf you want to get the created model after getting the model job ID, you\ncan use the wait_for_async_model_creation function.\nIt will poll for the status of the model creation process until it’s finished, and\nthen return the newly-created model. Note the differences below between datetime partitioned projects and\nnon-datetime partitioned projects.\nfrom datarobot.models.modeljob import wait_for_async_model_creation\n# Used during training based on blueprint\nmodel_job_id = project.train(blueprint, sample_pct=33)\nnew_model = wait_for_async_model_creation(\nproject_id=project.id,\nmodel_job_id=model_job_id,\n)\n# Used during training based on existing model\nmodel_job_id = existing_model.train(sample_pct=33)\nnew_model = wait_for_async_model_creation(\nproject_id=existing_model.project_id,\nmodel_job_id=model_job_id,\n)\n# For datetime-partitioned projects, use project.train_datetime. Note that train_datetime returns a model job instead\n# of just an ID.\nmodel_job = project.train_datetime(blueprint)\nnew_model = wait_for_async_model_creation(\nproject_id=project.id,\nmodel_job_id=model_job.id\n)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/job.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/job.html",
        "content_length": 4317
      },
      "code_examples": [],
      "api_methods": [
        "project.pause_autopilot",
        "project.get_all_jobs",
        "dr.modeljob.get_model",
        "datarobot.models.modeljob",
        "project.id",
        "project.train_datetime",
        "model.project_id",
        "datarobot.errors.pendingjobfinished",
        "dr.modeljob.get",
        "model.train",
        "project.train",
        "project.unpause_autopilot"
      ],
      "complexity_score": 0.75,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2451710316858968405",
      "title": "Model recommendation",
      "content": "Model recommendation\nDuring Autopilot, DataRobot recommends a model for deployment based on its accuracy and complexity.\nWhen running Autopilot in Full or Comprehensive mode, DataRobot uses the following deployment preparation process:\nFirst, DataRobot calculates Feature Impact for the selected model and uses it to generate a reduced feature list.\nNext, DataRobot retrains the selected model on the reduced feature list. If the new model performs better than the original model, DataRobot uses the new model for the next stage. Otherwise, the original model is used.\nDataRobot then retrains the selected model at an up-to-holdout sample size (typically 80%). As long as the sample is under the frozen threshold (1.5GB), the stage is not frozen.\nFinally, DataRobot retrains the selected model as a frozen run (hyperparameters are not changed from the up-to-holdout run) using a 100% sample size and selects it as Recommended for Deployment.\nNote\nThe higher sample size DataRobot uses in Step 3 is either:\nUp to holdout if the training sample size does not exceed the maximum Autopilot size threshold: sample size is the training set plus the validation set (for TVH) or 5-folds (for CV). In this case, DataRobot compares retrained and original models on the holdout score.\nUp to validation if the training sample size does exceed the maximum Autopilot size threshold: sample size is the training set (for TVH) or 4-folds (for CV). In this case, DataRobot compares retrained and original models on the validation score.\nDataRobot gives one model the Recommended for Deployment* badge. This is the most accurate individual, non-blender model on the Leaderboard. After completing the steps described above, it will receive the Prepared for Deployment badge.\nRetrieve all recommendations\nThe following code will return all models recommended for the project.\nimport datarobot as dr\nrecommendations = dr.ModelRecommendation.get_all(project_id)\nRetrieve a default recommendation\nIf you are unsure about the tradeoffs between the various types of recommendations, DataRobot can make this choice\nfor you. The following route will return the “Recommended for Deployment” model to use for predictions for the project.\nimport datarobot as dr\nrecommendation = dr.ModelRecommendation.get(project_id)\nRetrieve a specific recommendation\nIf you know which recommendation you want to use, you can select a specific recommendation using the\nfollowing code.\nimport datarobot as dr\nrecommendation_type = dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT\nrecommendations = dr.ModelRecommendation.get(project_id, recommendation_type)\nGet recommended model\nYou can use method get_model() of a recommendation object to retrieve a recommended model.\nimport datarobot as dr\nrecommendation = dr.ModelRecommendation.get(project_id)\nrecommended_model = recommendation.get_model()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/model_recommendation.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/model_recommendation.html",
        "content_length": 2865
      },
      "code_examples": [],
      "api_methods": [
        "dr.modelrecommendation.get",
        "dr.modelrecommendation.get_all",
        "dr.enums.recommended_model_type"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-5098807429791113861",
      "title": "Models",
      "content": "Models\nWhen a blueprint has been trained on a specific dataset at a specified sample\nsize, the result is a model. Models can be inspected to analyze their accuracy.\nStart training a model\nTo start training a model, use the Project.train method with\na blueprint object:\nimport datarobot as dr\nproject = dr.Project.get('5506fcd38bd88f5953219da0')\nblueprints = project.get_blueprints()\nmodel_job_id = project.train(blueprints[0].id)\nFor a datetime partitioned project (see the specialized workflows section), use\nProject.train_datetime:\nimport datarobot as dr\nproject = dr.Project.get('5506fcd38bd88f5953219da0')\nblueprints = project.get_blueprints()\nmodel_job_id = project.train_datetime(blueprints[0].id)\nList finished models\nYou can use the Project.get_models method to\nreturn a list of the project models that have finished training:\nimport datarobot as dr\nproject = dr.Project.get('5506fcd38bd88f5953219da0')\nmodels = project.get_models()\nprint(models[:5])\n>>> [Model(Decision Tree Classifier (Gini)),\nModel(Auto-tuned K-Nearest Neighbors Classifier (Minkowski Distance)),\nModel(Gradient Boosted Trees Classifier (R)),\nModel(Gradient Boosted Trees Classifier),\nModel(Logistic Regression)]\nmodel = models[0]\nproject.id\n>>> u'5506fcd38bd88f5953219da0'\nmodel.id\n>>> u'5506fcd98bd88f1641a720a3'\nYou can pass following parameters to change the result:\nsearch_params - A dict. Used to filter returned projects. Currently, you can query models by name, sample_pct, and is_starred.\norder_by — A str or list. If passed, returned models are ordered by this attribute(s). You can sort by the metric and sample_pct attributes.\nIf the sort attribute is preceded by a hyphen, models will be sorted in descending\norder, otherwise, in ascending order. Multiple sort attributes can be included as a comma-delimited string or in a list,\ne.g., order_by='sample_pct,-metric' or order_by=['sample_pct', '-metric']. Using metric to sort will result\nin models being sorted according to their validation score by how well they did according to the project metric.\nwith_metric – A str. If not set as None, the returned models will only have scores for this metric. Otherwise, all the metrics are returned.\nReview an example of listing models below.\nimport datarobot as dr\ndr.Project('5506fcd38bd88f5953219da0').get_models(order_by=['sample_pct', '-metric'])\n# Getting models that contain \"Ridge\" in name\n# and with sample_pct more than 64\ndr.Project('5506fcd38bd88f5953219da0').get_models(\nsearch_params={\n'sample_pct__gt': 64,\n'name': \"Ridge\"\n})\n# Getting models marked as starred\ndr.Project('5506fcd38bd88f5953219da0').get_models(\nsearch_params={\n'is_starred': True\n})\nRetrieve a known model\nIf you know the model_id and project_id values of a model, you can\nretrieve it directly:\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nmodel = dr.Model.get(project=project_id,\nmodel_id=model_id)\nYou can also use an instance of Project as the parameter for\nModel.get.\nmodel = dr.Model.get(project=project,\nmodel_id=model_id)\nRetrieve the highest scoring model for a given metric\nYou can retrieve the highest scoring model for a project based on a metric of your\nchoice.\nIf you decide not to pass a metric to this method or if you pass the default project metric (\nthe value of the metric attribute of your project instance), the result of\nProject.recommended_model is returned.\nimport datarobot as dr\nproject = dr.Project.get('5506fcd38bd88f5953219da0')\ntop_model_r_squared = project.get_top_model(metric=\"R Squared\")\nTrain a model on a different sample size\nOne of the key insights into a model and the data behind it is how its\nperformance varies with more training data.\nIn Autopilot, DataRobot runs at several sample sizes by default,\nbut you can also create a job that will run at a specific sample size,\nor specify a feature list that should be used for training the new model.\nThe Model.train method of a Model instance will\nput a new modeling job into the queue and return the ID of the created\nModelJob.\nYou can pass the model job ID to the wait_for_async_model_creation function,\nwhich polls the async model creation status and returns the newly-created model when it’s finished.\nimport datarobot as dr\nmodel_job_id = model.train(sample_pct=33)\n# Retrain a model on a custom featurelist using cross validation.\n# Note that you can specify a custom value for `sample_pct`.\nmodel_job_id = model.train(\nsample_pct=55,\nfeaturelist_id=custom_featurelist.id,\nscoring_type=dr.SCORING_TYPE.cross_validation,\n)\nCross-validating a model\nBy default, models are evaluated on the first validation partition. To start\ncross-validation, use Model.cross_validate:\nimport datarobot as dr\nmodel_job_id = model.cross_validate()\nFor a :doc:datetime partitioned project , backtesting is\nthe only cross-validation method supported. To run backtesting for a datetime model, use the\nDatetimeModel.score_backtests method:\nimport datarobot as dr\n# `model` here must be an instance of `dr.DatetimeModel`.\nmodel_job_id = model.score_backtests()\nFind the features used\nBecause each project can have many associated feature lists, it is\nimportant to know which features a model requires in order to run. This helps ensure that the necessary features are provided when generating predictions.\nfeature_names = model.get_features_used()\nprint(feature_names)\n>>> ['MonthlyIncome',\n'VisitsLast8Weeks',\n'Age']\nFeature Impact\nFeature Impact measures how much worse a model’s error score would be if DataRobot made predictions\nafter randomly shuffling a particular column (a technique sometimes called\nPermutation Importance).\nThe following example code snippet shows how a feature list with just the features with the highest\nfeature impact could be created.\nimport datarobot as dr\nmax_num_features = 10\ntime_to_wait_for_impact = 4 * 60  # seconds\nfeature_impacts = model.get_or_request_feature_impact(time_to_wait_for_impact)\nfeature_impacts.sort(key=lambda x: x['impactNormalized'], reverse=True)\nfinal_names = [f['featureName'] for f in feature_impacts[:max_num_features]]\nproject.create_featurelist('highest_impact', final_names)\nFor datetime-aware models, Feature Impact can be calculated for any backtest and holdout.\nimport datarobot as dr\ndatetime_model = dr.Model.get(project=project_id, model_id=model_id)\nfeature_impacts = datetime_model.get_or_request_feature_impact(backtest=1, with_metadata=True)\nFeature Effects\nFeature Effects helps to understand how changing a single feature affects the target while holding all other\nfeatures constant. Feature Effects provides partial dependence plot and prediction vs accuracy plot data.\nimport datarobot as dr\nfeature_effects = model.get_or_request_feature_effect(source='validation')\nFor multiclass models use request_feature_effects_multiclass and get_feature_effects_multiclass or\nget_or_request_feature_effects_multiclass methods.\nimport datarobot as dr\nfeature_effects = model.get_feature_effect(source='validation')\nPredict new data\nAfter creating models, you can use them to generate predictions on new data.\nSee the predictions documentation for further information on how to request predictions\nfrom a model.\nModel IDs vs. blueprint IDs\nEach model has both a model_id and a blueprint_id.\nA model is the result of training a blueprint on a dataset at a specified\nsample percentage. The blueprint_id is used to keep track of which\nblueprint was used to train the model, while the model_id is used to\nlocate the trained model in the system.\nModel parameters\nSome models can have parameters that provide data needed to reproduce their predictions.\nFor additional usage information see Coefficients.\nimport datarobot as dr\nmodel = dr.Model.get(project=project, model_id=model_id)\nmp = model.get_parameters()\nprint(mp.derived_features)\n>>> [{\n'coefficient': -0.015,\n'originalFeature': u'A1Cresult',\n'derivedFeature': u'A1Cresult->7',\n'type': u'CAT',\n'transformations': [{'name': u'One-hot', 'value': u\"'>7'\"}]\n}]\nCreate a blender model\nYou can blend multiple models; in many cases, the resulting blender model is more accurate\nthan the parent models. To do so, you need to select parent models and a blender method from\ndatarobot.enums.BLENDER_METHOD. If this is a time series project, only methods in\ndatarobot.enums.TS_BLENDER_METHOD are allowed.\nBe aware that the tradeoff for better prediction accuracy is bigger resource consumption\nand slower predictions.\nimport datarobot as dr\npr = dr.Project.get(pid)\nmodels = pr.get_models()\nparent_models = [model.id for model in models[:2]]\npr.blend(parent_models, dr.enums.BLENDER_METHOD.AVERAGE)\nLift chart retrieval\nYou can use the Model methods get_lift_chart and get_all_lift_charts to retrieve\nlift chart data. The first will get it from specific source (validation data, cross validation, or\nunlocked Holdout) and the second will list all available data.\nFor multiclass models, you can get a list of per-class lift charts using the Model method get_multiclass_lift_chart.\nROC curve retrieval\nSame as with the lift chart, you can use Model methods get_roc_curve and\nget_all_roc_curves to retrieve ROC curve data. More information about working with ROC\ncurves can be found in ROC curve.\nResiduals chart retrieval\nJust as with the lift and ROC charts, you can use Model methods get_residuals_chart and\nget_all_residuals_charts to retrieve residuals chart data. The first will get it from a\nspecific source (validation data, cross-validation data, or unlocked Holdout). The second\nretrieves all available data.\nWord cloud\nIf your dataset contains text columns, DataRobot can create text processing models that will\ncontain word cloud insight data. An example of such a model is any “Auto-Tuned Word N-Gram Text\nModeler” model. You can use the {meth}`Model.get_word_cloud <datarobot.models.Model.get_word_cloud> method to retrieve those insights — it\nprovides up to the 200 most important ngrams in the model and coefficients corresponding to their influence.\nScoring Code\nA subset of models support code generation. For each of those models, you can download\na JAR file with Scoring Code to make predictions locally using\nmodel.download_scoring_code. For details on how to do so, see Scoring Code. Optionally, you can download source code in Java to see\nwhat calculations those models do internally.\nBe aware that the source code JAR isn’t compiled so it cannot be used for making predictions.\nGet a model blueprint chart\nFor any model, you can retrieve its blueprint chart. You can also get its representation in graphviz DOT format to render it into the format you need.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nmodel = dr.Model.get(project=project_id,\nmodel_id=model_id)\nbp_chart = model.get_model_blueprint_chart()\nprint(bp_chart.to_graphviz())\nGet a model missing values report\nFor the majority of models, you can retrieve their missing values reports on training data\nper each numeric and categorical feature. Model needs to have at least one of the supported tasks\nin the blueprint in order to have a missing values report (blenders are not supported).\nReport is gathered for Numerical Imputation tasks and Categorical converters like Ordinal Encoding,\nOne-Hot Encoding, etc.\nMissing values report is available to users with access to full blueprint docs.\nA report is collected for those features which are considered eligible by a given blueprint task.\nFor instance, a categorical feature with a lot of unique values may not be considered as eligible in\nthe One-Hot encoding task.\nPlease refer to Missing report attributes description\nfor report interpretation.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nmodel = dr.Model.get(project=project_id, model_id=model_id)\nmissing_reports_per_feature = model.get_missing_report_info()\nfor report_per_feature in missing_reports_per_feature:\nprint(report_per_feature)\nConsider the following example of a Decision Tree Classifier (Gini) blueprint chart representation. A summary of the results is outlined below.\nprint(blueprint_chart.to_graphviz())\n>>> digraph \"Blueprint Chart\" {\ngraph [rankdir=LR]\n0 [label=\"Data\"]\n-2 [label=\"Numeric Variables\"]\n2 [label=\"Missing Values Imputed\"]\n3 [label=\"Decision Tree Classifier (Gini)\"]\n4 [label=\"Prediction\"]\n-1 [label=\"Categorical Variables\"]\n1 [label=\"Ordinal encoding of categorical variables\"]\n0 -> -2\n-2 -> 2\n2 -> 3\n3 -> 4\n0 -> -1\n-1 -> 1\n1 -> 3\n}\nAnd a missing report:\nprint(report_per_feature1)\n>>> {'feature': 'Veh Year',\n'type': 'Numeric',\n'missing_count': 150,\n'missing_percentage': 50.00,\n'tasks': [\n{'id': u'2',\n'name': u'Missing Values Imputed',\n'descriptions': [u'Imputed value: 2006']\n}\n]\n}\nprint(report_per_feature2)\n>>> {'feature': 'Model',\n'type': 'Categorical',\n'missing_count': 100,\n'missing_percentage': 33.33,\n'tasks': [\n{'id': u'1',\n'name': u'Ordinal encoding of categorical variables',\n'descriptions': [u'Imputed value: -2']\n}\n]\n}\nThe numeric feature “Veh Year” has 150 missing values and, respectively, 50% in training data.\nIt was transformed by the “Missing Values Imputed” task with imputed value 2006. Task has ID 2, and its\noutput goes into Decision Tree Classifier (Gini), which can be inferred from the chart.\nThe “Model” categorical feature was transformed by “Ordinal encoding of categorical variables” task with\nimputed value -2.\nGet a blueprint’s documentation\nYou can retrieve documentation on tasks used to build a model. It will contain information about the task, its parameters and (when available) links and references to additional sources.\nAll documents are instances of BlueprintTaskDocument class.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nmodel = dr.Model.get(project=project_id,\nmodel_id=model_id)\ndocs = model.get_model_blueprint_documents()\nprint(docs[0].task)\n>>> Average Blend\nprint(docs[0].links[0]['url'])\n>>> https://en.wikipedia.org/wiki/Ensemble_learning\nRequest training predictions\nYou can request a model’s predictions for a particular subset of its training data.\nSee datarobot.models.Model.request_training_predictions() reference for all the valid subsets.\nSee training predictions reference for more details.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nmodel = dr.Model.get(project=project_id,\nmodel_id=model_id)\ntraining_predictions_job = model.request_training_predictions(dr.enums.DATA_SUBSET.HOLDOUT)\ntraining_predictions = training_predictions_job.get_result_when_complete()\nfor row in training_predictions.iterate_rows():\nprint(row.row_id, row.prediction)\nAdvanced tuning\nYou can perform advanced tuning on a model — generate a new model by taking an existing\nmodel and rerunning it with modified tuning parameters.\nThe AdvancedTuningSession class exists to track the creation of an advanced tuning model on the\nclient. It enables browsing and setting advanced tuning parameters one at a time, and\nusing human-readable parameter names rather than requiring opaque parameter IDs in all cases.\nNo information is sent to the server until the run() method is called on the\nAdvancedTuningSession.\nSee datarobot.models.Model.get_advanced_tuning_parameters() reference for a description\nof the types of parameters that can be passed in.\nAs of v2.17 of the Python client, all models other than blenders, open source, and user-created models support\nAdvanced Tuning. The use of Advanced Tuning via the API for non-Eureqa models is in beta, but is enabled\nby default for all users.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nmodel = dr.Model.get(project=project_id,\nmodel_id=model_id)\ntune = model.start_advanced_tuning_session()\n# Get available task names,\n# and available parameter names for a task name that exists on this model\ntune.get_task_names()\ntune.get_parameter_names('Eureqa Generalized Additive Model Classifier (3000 Generations)')\ntune.set_parameter(\ntask_name='Eureqa Generalized Additive Model Classifier (3000 Generations)',\nparameter_name='EUREQA_building_block__sine',\nvalue=1)\njob = tune.run()\nSHAP Feature Impact\nSHAP Feature Impact is computed by calculating the SHAP values on a sample of training data and then taking\nthe mean absolute value for each column. A larger value of impact indicates a more important feature.\nSee datarobot.models.ShapImpact.create() reference for a description of the types of parameters\nthat can be passed in.\nimport datarobot as dr\nproject_id = '5ec3d6884cfad17cd8c0ed62'\nmodel_id = '5ec3d6f44cfad17cd8c0ed78'\nshap_impact_job = dr.ShapImpact.create(project_id=project_id, model_id=model_id)\nshap_impact = shap_impact_job.get_result_when_complete()\nprint(shap_impact)\n>>> [ShapImpact(count=36)]\nprint(shap_impact.shap_impacts[:1])\n>>> [{'feature_name': 'number_inpatient', 'impact_normalized': 1.0, 'impact_unnormalized': 0.07670175497683789}]\nshap_impact = dr.ShapImpact.get(project_id=project_id, model_id=model_id)\nprint(shap_impact.shap_impacts[:1])\n>>> [{'feature_name': 'number_inpatient', 'impact_normalized': 1.0, 'impact_unnormalized': 0.07670175497683789}]\nNumber of iterations trained\nEarly-stopping models will train a subset of max estimators/iterations that are defined in advanced tuning.\nThis method allows the user to retrieve the actual number of estimators that were trained by an early-stopping\ntree-based model (currently the only model type supported). The method returns the projectId, modelId, and\na list of dictionaries containing the number of iterations trained for each model stage. In the case of single-stage models,\nthis dictionary will contain only one entry.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nmodel = dr.Model.get(project=project_id,\nmodel_id=model_id)\nnum_iterations = model.get_num_iterations_trained()\nprint(num_iterations)\n>>> {\"projectId\": \"5506fcd38bd88f5953219da0\", \"modelId\": \"5506fcd98bd88f1641a720a3\", \"data\" [{\"stage\": \"FREQ\", \"numIterations\":250}, {\"stage\":\"SEV\", \"numIterations\":50}]}",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/model.html",
      "tags": [
        "advanced",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/model.html",
        "content_length": 18123
      },
      "code_examples": [],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "dr.enums.data_subset",
        "dr.enums.blender_method",
        "model.train",
        "datarobot.models.model",
        "project.get_blueprints",
        "model.get_or_request_feature_effect",
        "model.get_word_cloud",
        "datarobot.enums.ts_blender_method",
        "model.request_training_predictions",
        "model.id",
        "project.id",
        "project.get",
        "dr.shapimpact.get",
        "project.train",
        "datarobot.enums.blender_method",
        "model.get_features_used",
        "project.get_top_model",
        "model.get_model_blueprint_chart",
        "project.train_datetime",
        "model.get_parameters",
        "model.get_missing_report_info",
        "dr.shapimpact.create",
        "model.download_scoring_code",
        "model.get_advanced_tuning_parameters",
        "datarobot.models.shapimpact",
        "project.create_featurelist",
        "model.get",
        "model.score_backtests",
        "project.recommended_model",
        "model.get_num_iterations_trained",
        "model.get_model_blueprint_documents",
        "model.start_advanced_tuning_session",
        "dr.model.get",
        "dr.project.get",
        "dr.scoring_type.cross_validation",
        "project.get_models",
        "model.get_feature_effect",
        "model.cross_validate"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2695646348276466033",
      "title": "DataRobot Prime",
      "content": "DataRobot Prime\nDataRobot Prime allows the download of executable code approximating models. For more information about this feature, see the documentation within the\nDataRobot webapp. Contact your Account Executive or CFDS for information on enabling DataRobot Prime, if needed.\nApproximate a Model\nGiven a Model you wish to approximate, Model.request_approximation will start a job creating\nseveral Ruleset objects approximating the parent model.  Each of those rulesets will identify\nhow many rules were used to approximate the model, as well as the validation score\nthe approximation achieved.\nrulesets_job = model.request_approximation()\nrulesets = rulesets_job.get_result_when_complete()\nfor ruleset in rulesets:\ninfo = (ruleset.id, ruleset.rule_count, ruleset.score)\nprint('id: {}, rule_count: {}, score: {}'.format(*info))\nPrime Models vs. Models\nGiven a ruleset, you can create a model based on that ruleset.  We consider such models to be Prime\nmodels.  The PrimeModel class inherits from the Model class, so anything a Model can do,\nas PrimeModel can do as well.\nThe PrimeModel objects available within a Project can be listed by\nproject.get_prime_models, or a particular one can be retrieve via PrimeModel.get.  If a\nruleset has not yet had a model built for it, ruleset.request_model can be used to start\na job to make a PrimeModel using a particular ruleset.\nrulesets = parent_model.get_rulesets()\nselected_ruleset = sorted(rulesets, key=lambda x: x.score)[-1]\nif selected_ruleset.model_id:\nprime_model = PrimeModel.get(selected_ruleset.project_id, selected_ruleset.model_id)\nelse:\nprime_job = selected_ruleset.request_model()\nprime_model = prime_job.get_result_when_complete()\nThe PrimeModel class has two additional attributes and one additional method.  The attributes\nare ruleset, which is the Ruleset used in the PrimeModel, and parent_model_id which is\nthe id of the model it approximates.\nFinally, the new method defined is request_download_validation which is used to prepare code\ndownload for the model and is discussed later on in this document.\nRetrieving Code from a PrimeModel\nGiven a PrimeModel, you can download the code used to approximate the parent model, and view\nand execute it locally.\nThe first step is to validate the PrimeModel, which runs some basic validation of the generated\ncode, as well as preparing it for download.  We use the PrimeFile object to represent code\nthat is ready to download.  PrimeFiles can be prepared by the request_download_validation\nmethod on PrimeModel objects, and listed from a project with the get_prime_files method.\nOnce you have a PrimeFile you can check the is_valid attribute to verify the code passed\nbasic validation, and then download it to a local file with download.\nvalidation_job = prime_model.request_download_validation(enums.PRIME_LANGUAGE.PYTHON)\nprime_file = validation_job.get_result_when_complete()\nif not prime_file.is_valid:\nraise ValueError('File was not valid')\nprime_file.download('/home/myuser/drCode/primeModelCode.py')",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/prime.html",
      "tags": [
        "beginner",
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/prime.html",
        "content_length": 3018
      },
      "code_examples": [],
      "api_methods": [
        "model.get",
        "model.request_download_validation",
        "project.get_prime_models",
        "model.request_approximation",
        "model.get_rulesets"
      ],
      "complexity_score": 0.6499999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-6858506122294790164",
      "title": "Projects",
      "content": "Projects\nAll of the modeling within DataRobot happens within a project. Each project\nhas one dataset that is used as the source from which to train models.\nCreate a project\nYou can create a project from previously-created Datasets or directly from a data source.\nimport datarobot as dr\ndataset = Dataset.create_from_file(file_path='/home/user/data/last_week_data.csv')\nproject = dr.Project.create_from_dataset(dataset.id, project_name='New Project')\nThe following command creates a new project directly from a data source. You must specify a path\nto data file, file object URL (starting with http://, https://, file://, or s3://),\nraw file contents, or a pandas.DataFrame object when creating a new project.\nPath to file can be either a path to a local file or a publicly accessible URL.\nimport datarobot as dr\nproject = dr.Project.create('/home/user/data/last_week_data.csv',\nproject_name='New Project')\nYou can use the following commands to view the project ID and name:\nproject.id\n>>> u'5506fcd38bd88f5953219da0'\nproject.project_name\n>>> u'New Project'\nSelect modeling parameters\nThe final information needed to begin modeling includes the target feature, queue mode, metric for comparing models, and optional parameters such as weights, offset, exposure, and downsampling.\nTarget\nThe target must be the name of one of the columns of data uploaded to the project.\nMetric\nThe optimization metric used to compare models is an important factor in building accurate models. If a metric is not specified, the default metric recommended by DataRobot will be used. You can use the following code to view a list of valid metrics for a specified target:\ntarget_name = 'ItemsPurchased'\nproject.get_metrics(target_name)\n>>> {'available_metrics': [\n'Gini Norm',\n'Weighted Gini Norm',\n'Weighted R Squared',\n'Weighted RMSLE',\n'Weighted MAPE',\n'Weighted Gamma Deviance',\n'Gamma Deviance',\n'RMSE',\n'Weighted MAD',\n'Tweedie Deviance',\n'MAD',\n'RMSLE',\n'Weighted Tweedie Deviance',\n'Weighted RMSE',\n'MAPE',\n'Weighted Poisson Deviance',\n'R Squared',\n'Poisson Deviance'],\n'feature_name': 'SalePrice'}\nPartitioning method\nDataRobot projects always have a Holdout set used for final model validation. You can use two different approaches for testing prior to the Holdout set:\nSplit the remaining data into training and validation sets.\nCross-validation, in which the remaining data is split into a number of folds (partitions); each fold serves as a validation set, with models trained on the other folds and evaluated on that fold.\nThere are several other options you can control. To specify a partition method, create an instance of one of the Partition Classes, and pass it as the partitioning_method argument in your call to project.analyze_and_model or project.start.  As of v3.0 of the Python client, you can alternately use project.set_partitioning_method.  See here for more information on using datetime partitioning.\nSeveral partitioning methods include parameters for validation_pct and holdout_pct, specifying desired percentages for the validation and holdout sets. Note that there may be constraints that prevent the actual percentages used from exactly (or some cases, even closely) matching the requested percentages.\nQueue mode\nYou can use the API to set the DataRobot modeling process to run Autopilot in manual, quick, or comprehensive mode.\nAutopilot mode means that the modeling process will proceed completely\nautomatically, including running recommended models, running at\ndifferent sample sizes, and blending.\nManual mode means that DataRobot will populate a list of recommended models, but will not insert any of them into the queue. This mode lets you specify which models to execute before starting the modeling process.\nQuick mode means that a smaller set of blueprints is used, so Autopilot finishes faster.\nWeights\nDataRobot also supports using a weight parameter, which are often used to help compensate for rare events in data. You can specify a column name in the project dataset to be used as a weight column.\nOffsets\nStarting with Python client v2.6, DataRobot also supports using an offset parameter. Offsets are commonly used in insurance modeling to include effects that are outside of the training data due to regulatory compliance or constraints. You can specify the names of several columns in the project dataset to be used as the offset columns.\nExposure\nStarting with version v2.6, DataRobot also supports using an exposure parameter. Exposure is often used to model insurance premiums where strict proportionality of premiums to duration is required. You can specify the name of the column in the project dataset to be used as an exposure column.\nStart modeling\nOnce you have selected modeling parameters, you can use the following code structure to specify parameters and start the modeling process.\nimport datarobot as dr\nproject.analyze_and_model(target='ItemsPurchased',\nmetric='Tweedie Deviance',\nmode=dr.AUTOPILOT_MODE.FULL_AUTO)\nYou can also pass additional parameters to project.analyze_and_model to change parts of\nthe modeling process. Some of those parameters include:\nworker_count - int, sets number of workers used for modeling.\npartitioning_method - PartitioningMethod object.\npositive_class - str, float, or int; Specifies a level of the target column that should be treated as the positive class for binary classification.  May only be specified for binary classification targets.\nadvanced_options - AdvancedOptions object; Used to set advanced options of modeling process. Can alternatively call set_options on a project instance which will be used automatically if nothing is passed here.\ntarget_type - str; Overrides the automatically selected target_type. An example usage would be setting the target_type=TARGET_TYPE.MULTICLASS when you want to perform a multiclass classification task on a numeric column that has a low cardinality.\nYou can run different Autopilot modes with the mode parameter. AUTOPILOT_MODE.FULL_AUTO\nis the default, which will trigger modeling with no further actions necessary. Other accepted modes\ninclude AUTOPILOT_MODE.MANUAL for manual mode (choose your own models to run rather than use the\nDataRobot autopilot), AUTOPILOT_MODE.QUICK (run on a more limited set of models\nto get insights more quickly), and AUTOPILOT_MODE.COMPREHENSIVE (used to invest more time to find\nthe most accurate model to serve your use case).\nFor a full reference of available parameters, see Project.analyze_and_model.\nClone a project\nOnce a project has been successfully created, you may clone it using the following code structure:\nnew_project = project.clone_project(new_project_name='This is my new project')\nnew_project.project_name\n>> 'This is my new project'\nnew_project.id != project.id\n>> True\nThe new_project_name attribute is optional. If it is omitted, the default new project name will be ‘Copy of <project.name>’.\nInteract with a project\nThe following commands can be used to manage DataRobot projects.\nList projects\nReturns a list of projects associated with current API user.\nimport datarobot as dr\ndr.Project.list()\n>>> [Project(Project One), Project(Two)]\ndr.Project.list(search_params={'project_name': 'One'})\n>>> [Project(One)]\nYou can pass following parameter to change the result:\nsearch_params – dict; Used to filter returned projects. You can only query projects by project_name.\nGet an existing project\nRather than querying the full list of projects every time you need\nto interact with a project, you can retrieve its ID value and use that to reference the project.\nimport datarobot as dr\nproject = dr.Project.get(project_id='5506fcd38bd88f5953219da0')\nproject.id\n>>> '5506fcd38bd88f5953219da0'\nproject.project_name\n>>> 'Churn Projection'\nGet feature association statistics for an existing project\nYou can retrieve either feature association or correlation statistics and metadata on informative\nfeatures for a given project.\nimport datarobot as dr\nproject = dr.Project.get(project_id='5506fcd38bd88f5953219da0')\nassociation_data = project.get_associations(assoc_type='association', metric='mutualInfo')\nassociation_data.keys()\n>>> ['strengths', 'features']\nGet whether your featurelists have association statistics\nGet whether an association matrix job has been run on each of your feature lists.\nimport datarobot as dr\nproject = dr.Project.get(project_id='5506fcd38bd88f5953219da0')\nfeaturelists = project.get_association_featurelists()\nfeaturelists['featurelists'][0]\n>>> {\"featurelistId\": \"54e510ef8bd88f5aeb02a3ed\", \"hasFam\": True, \"title\": \"Informative Features\"}\nCreate association statistics for a featurelist\nGenerate the feature association statistics for all features in a feature list.\nimport datarobot as dr\nfrom datarobot.models.feature_association_matrix import FeatureAssociationMatrix\nproject = dr.Project.get(project_id='5506fcd38bd88f5953219da0')\nfeaturelist = project.get_featurelist_by_name(\"Raw Features\")\nstatus = FeatureAssociationMatrix.create(project.id, featurelist.id)\n# two ways to wait for completion\n# option 1\nstatus.wait_for_completion()\nfam = FeatureAssociationMatrix.get(project_id=project.id, featurelist_id=featurelist.id)\n# or option 2\n# fam = status.get_result_when_complete()\nGet a project’s feature list by name\nGet a feature list by name.\nimport datarobot as dr\nproject = dr.Project.get(project_id='5506fcd38bd88f5953219da0')\nfeaturelist = project.get_featurelist_by_name(\"Raw Features\")\nfeaturelist\n>>> Featurelist(Raw Features)\n# Trying to get feature list that does not exist\nfeaturelist = project.get_featurelist_by_name(\"Flying Circus\")\nfeaturelist is None\n>>> True\nCreate project feature lists\nUsing a project’s create_featurelist() method, you can create feature lists in multiple ways:\nimport datarobot as dr\nproject = dr.Project.get(project_id='5506fcd38bd88f5953219da0')\nfeaturelist_one = project.create_featurelist(\nname=\"Testing featurelist creation\",\nfeatures=[\"age\", \"weight\", \"number_diagnoses\"],\n)\nfeaturelist_one\n>>> Featurelist(Testing featurelist creation)\nfeaturelist_one.features\n>>> ['age', 'weight', 'number_diagnoses']\n# Create a feature list using another feature list as a starting point (`starting_featurelist`)\n# To Note: this example passes the `featurelist` object but you can also pass the\n# id (`starting_featurelist_id`) or the name (`starting_featurelist_name`)\nfeaturelist_two = project.create_featurelist(\nstarting_featurelist=featurelist_one,\nfeatures_to_exclude=[\"number_diagnoses\"],  # Please see docs for use of `features_to_include`\n)\nfeaturelist_two  # Note below we have an auto-generated name because we did not pass `name`\n>>> Featurelist(Testing featurelist creation - 2022-07-12)\n>>> # Note below we have a new feature list which has `\"number_diagnoses\"` excluded\nfeaturelist_two.features\n>>> ['age', 'weight']\nGet values for a pair of features in an existing project\nGet a sample of the exact values used in the feature association matrix plotting.\nimport datarobot as dr\nproject = dr.Project.get(project_id='5506fcd38bd88f5953219da0')\nfeature_values = project.get_association_matrix_details(feature1='foo', feature2='bar')\nfeature_values.keys()\n>>> ['features', 'types', 'values']\nUpdate a project\nYou can update various attributes of a project.\nTo update the name of the project:\nproject.rename(new_name)\nTo update the number of workers used by your project (this will fail if you request more workers than you have\navailable; the special value -1 will request your maximum number):\nproject.set_worker_count(num_workers)\nTo unlock the Holdout set, allowing holdout scores to be shown and models to be trained on more data:\nproject.unlock_holdout()\nTo add or change the project description:\nproject.set_project_description(project_description)\nTo add or change the project’s advanced_options:\n# Using kwargs\nproject.set_options(blend_best_models=False)\n# Using an ``AdvancedOptions`` instance\nproject.set_options(AdvancedOptions(blend_best_models=False))\nDelete a project\nUse the following command to delete a project:\nproject.delete()\nWait for Autopilot to finish\nOnce the modeling Autopilot is started, in some cases you will want to wait for Autopilot to finish:\nproject.wait_for_autopilot()\nPlay/Pause Autopilot\nIf your project is running in Autopilot, it will continually use\navailable workers, subject to the number of workers allocated to the project\nand the total number of simultaneous workers allowed according to the user\npermissions.\nTo pause a project running in Autopilot:\nproject.pause_autopilot()\nTo resume running a paused project:\nproject.unpause_autopilot()\nStart Autopilot on another feature list\nYou can start Autopilot on an existing feature list.\nimport datarobot as dr\nfeaturelist = project.create_featurelist('test', ['feature 1', 'feature 2'])\nproject.start_autopilot(featurelist.id)\n>>> True\n# Starting autopilot that is already running on the provided featurelist\nproject.start_autopilot(featurelist.id)\n>>> dr.errors.AppPlatformError\nNote\nThis method should be used on a project where the target has already been\nset. An error will be raised if autopilot is currently running on\nor has already finished running on the provided feature list.\nStart preparing a specific model for deployment\nYou can start preparing a specific model for deployment. The model will then go through the various\nrecommendation stages including retraining on a reduced feature list and retraining the model on\na higher sample size (recent data for datetime partitioned).\n# prepare a specific model for deployment and wait for the process to complete\nproject.start_prepare_model_for_deployment(model_id=model.id)\nproject.wait_for_autopilot(check_interval=5, timeout=600)\n# get the prepared model\nprepared_for_deployment_model = dr.models.ModelRecommendation.get(\nproject.id, recommendation_type=RECOMMENDED_MODEL_TYPE.PREPARED_FOR_DEPLOYMENT\n)\nprepared_for_deployment_model_id = prepared_for_deployment_model.model_id\nNote\nThis method should be used on a project where the target has already been\nset. An error will be raised if autopilot is currently running on the project or\nanother model in the project is being prepared for deployment.\nUsing credential data\nFor methods that accept credential data instead of user/password or credential ID, please see Credential Data documentation.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/project.html",
      "tags": [
        "advanced",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/project.html",
        "content_length": 14286
      },
      "code_examples": [],
      "api_methods": [
        "dr.autopilot_mode.full_auto",
        "dr.errors.appplatformerror",
        "project.wait_for_autopilot",
        "dr.project.create",
        "project.start",
        "datarobot.models.feature_association_matrix",
        "project.get_association_featurelists",
        "project.start_autopilot",
        "model.id",
        "project.project_name",
        "project.get_association_matrix_details",
        "project.start_prepare_model_for_deployment",
        "project.id",
        "project.list",
        "project.clone_project",
        "project.get",
        "project.get_metrics",
        "project.unlock_holdout",
        "project.set_worker_count",
        "project.create",
        "dr.project.list",
        "project.set_options",
        "project.delete",
        "project.set_partitioning_method",
        "project.set_project_description",
        "project.unpause_autopilot",
        "project.get_associations",
        "project.create_featurelist",
        "project.pause_autopilot",
        "dr.project.get",
        "project.name",
        "project.rename",
        "project.analyze_and_model",
        "model.model_id",
        "project.create_from_dataset",
        "dr.models.modelrecommendation",
        "project.get_featurelist_by_name",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_6791529003710435164",
      "title": "Working with binary data",
      "content": "Working with binary data\nPreparing data for training\nWorking with binary files using the DataRobot API requires prior dataset preparation in one of the\nsupported formats. See “Prepare the dataset”\nfor more detail. When the dataset is ready, you can start a project following one of the methods\ndescribed in working with Datasets and Projects.\nPreparing data for predictions\nFor project creation and a lot of the prediction options, DataRobot allows you to upload\narchives with binary files (e.g. images files). Whenever possible it is recommended to use this\noption. However, in a few cases the API routes only allow you to upload your dataset in the JSON\nor CSV format. In these cases, you can add the binary files as base64 strings to your dataset.\nProcessing images\nInstallation\nTo enable support for processing images, install the datarobot library with the images option:\npip install datarobot[images]\nThis will install all needed dependencies for image processing.\nProcessing images\nWhen working with image files, helper functions may first transform your images before encoding\ntheir binary data as base64 strings.\nSpecifically, helper functions will perform these steps:\nRetrieve binary data from the file in the specified location (local path or URL).\nResize images to the image size used by DataRobot and save them in a different format\nConvert binary data to base64-encoded strings.\nWorking with images locally and located on external servers differs only in the steps related\nto binary file retrieval. The following steps for transformation and conversion to base64-encoded\nstrings are the same.\nThis examples uses data stored in a folder structure:\n/home/user/data/predictions\n├── images\n├  ├──animal01.jpg\n├  ├──animal02.jpg\n├  ├──animal03.png\n├── data.csv\nAs an input for processing, DataRobot needs a collection of image locations. Helper functions\nwill process the images and return base64-encoded strings in the same order. The first example\nuses the contents of data.csv as an input. This file holds data needed for model predictions\nand also the image storage locations (in the “image_path” column).\nContents of data.csv:\nweight_in_grams,age_in_months,image_path\n5000,34,/home/user/data/predictions/images/animal01.jpg\n4300,56,/home/user/data/predictions/images/animal02.jpg\n4200,22,/home/user/data/predictions/images/animal03.png\nThis code snippet will read each image from the “image_path” column and store the base64-string\nwith image data in the “image_base64” column.\nimport os\nimport pandas as pd\nfrom datarobot.helpers.binary_data_utils import get_encoded_image_contents_from_paths\ndataset_dir = '/home/user/data/predictions'\nfile_in = os.path.join(dataset_dir, 'data.csv')\nfile_out = os.path.join(dataset_dir, 'out.csv')\ndf = pd.read_csv(file_in)\ndf['image_base64'] = get_encoded_image_contents_from_paths(df['image_path'])\ndf.to_csv(file_out, index=False)\nThe same helper function will work with other iterables:\nimport os\nfrom datarobot.helpers.binary_data_utils import get_encoded_image_contents_from_paths\nimages_dir = '/home/user/data/predictions/images'\nimages_absolute_paths = [\nos.path.join(images_dir, file) for file in ['animal01.jpg', 'animal02.jpg', 'animal03.png']\n]\nimages_base64 = get_encoded_image_contents_from_paths(images_absolute_paths)\nAbove examples used absolute paths. When working with relative paths, by default the helper function will resolve them relative to the script location. To override this behavior, use base_path parameter to specify the base path for relative paths.\nfrom datarobot.helpers.binary_data_utils import get_encoded_image_contents_from_paths\nimages_dir = '/home/user/data/predictions/images'\nimages_relative_paths = ['animal01.jpg', 'animal02.jpg', 'animal03.png']\nimages_base64 = get_encoded_image_contents_from_paths(\nimages_relative_paths, base_path=images_dir\n)\nThere is also one helper function to work with remote data. This function retrieves binary content\nfrom specified URLs, transforms the images, and returns base64-encoded strings (in the the same way\nas it does for images loaded from local paths).\nExample:\nimport os\nfrom datarobot.helpers.binary_data_utils import get_encoded_image_contents_from_urls\nimage_urls = [\n'https://<YOUR_SERVER_ADDRESS>/animal01.jpg',\n'https://<YOUR_SERVER_ADDRESS>/animal02.jpg',\n'https://<YOUR_SERVER_ADDRESS>/animal03.png'\n]\nimages_base64 = get_encoded_image_contents_from_urls(image_urls)\nExamples of helper functions up to this points have used default settings. If needed, the following\nfunctions allow for further customization by passing explicit parameters related to error handling,\nimage transformations, and request header customization.\nCustom image transformations\nBy default helper functions will apply transformations, which have proven good results. The default\nvalues align with the preprocessing used for images uploaded in archives for training.\nTherefore, using default values should be the first choice when preparing datasets with images\nfor predictions. However, you can also specify custom image transformation settings to override\ndefault transformations before converting data into base64 strings. To override the default\nbehavior, create an instance of the ImageOptions class and pass it as an additional parameter\nto the helper function.\nNote that there is no guarantee that images converted by DataRobot during archive dataset upload\nmatch images converted by you on a pixel level, even if the default ImageOptions are\nused. However, if you use ImageOptions, you most likely will not be able to visually identify any differences.\nExamples:\nimport os\nfrom datarobot.helpers.image_utils import ImageOptions\nfrom datarobot.helpers.binary_data_utils import get_encoded_image_contents_from_paths\nimages_dir = '/home/user/data/predictions/images'\nimages_absolute_paths = [\nos.path.join(images_dir, file) for file in ['animal01.jpg', 'animal02.jpg', 'animal03.png']\n]\n# Override the default behavior for image quality and subsampling, but the images\n# will still be resized because that's the default behavior. Note: the `keep_quality`\n# parameter for JPEG files by default preserves the quality of the original images,\n# so this behavior must be disabled to manually override the quality setting with an\n# explicit value.\nimage_options = ImageOptions(keep_quality=False, image_quality=80, image_subsampling=0)\nimages_base64 = get_encoded_image_contents_from_paths(\npaths=images_absolute_paths, image_options=image_options\n)\n# overwrite default behavior for image resizing, this will keep image aspect\n# ratio and will resize all images using specified size: width=300 and height=300.\n# Note: if image had different aspect ratio originally it will generate image\n# thumbnail, not larger than the original, that will fit in requested image size\nimage_options = ImageOptions(image_size=(300, 300))\nimages_base64 = get_encoded_image_contents_from_paths(\npaths=images_absolute_paths, image_options=image_options\n)\n# Override the default behavior for image resizing, This will force the image\n# to be resized to size: width=300 and height=300. When the image originally\n# had a different aspect ratio - than resizing it using `force_size` parameter\n# will alter its aspect ratio modifying the image (e.g. stretching)\nimage_options = ImageOptions(image_size=(300, 300), force_size=True)\nimages_base64 = get_encoded_image_contents_from_paths(\npaths=images_absolute_paths, image_options=image_options\n)\n# overwrite default behavior and retain original image sizes\nimage_options = ImageOptions(should_resize=False)\nimages_base64 = get_encoded_image_contents_from_paths(\npaths=images_absolute_paths, image_options=image_options\n)\nCustom request headers\nIf needed, you can specify custom request headers for downloading binary data.\nExample:\nimport os\nfrom datarobot.helpers.binary_data_utils import get_encoded_image_contents_from_urls\ntoken = 'Nl69vmABaEuchUsj88N0eOoH2kfUbhCCByhoFDf4whJyJINTf7NOhhPrNQKqVVJJ'\ncustom_headers = {\n'User-Agent': 'My User Agent',\n'Authorization': 'Bearer {}'.format(token)\n}\nimage_urls = [\n'https://<YOUR_SERVER_ADDRESS>/animal01.jpg',\n'https://<YOUR_SERVER_ADDRESS>/animal02.jpg',\n'https://<YOUR_SERVER_ADDRESS>/animal03.png',\n]\nimages_base64 = get_encoded_image_contents_from_urls(image_urls, custom_headers)\nHandling errors\nWhen processing multiple images, any error during processing will, by default, stop operations\n(i.e., the helper function will raise datarobot.errors.ContentRetrievalTerminatedError and\nterminate further processing). In the case of an error during content retrieval (“connectivity\nissue”, “file not found” etc), you can override this behavior by passing continue_on_error=True\nto the helper function. When specified, processing will continue. In rows where the error was\nraised, the value``None`` value will be returned instead of a base64-encoded string. This applies\nonly to errors during content retrieval, other errors will always terminate execution.\nExample:\nimport os\nfrom datarobot.helpers.binary_data_utils import get_encoded_image_contents_from_paths\nimages_dir = '/home/user/data/predictions/images'\nimages_absolute_paths = [\nos.path.join(images_dir, file) for file in ['animal01.jpg', 'missing.jpg', 'animal03.png']\n]\n# This execution will print None for missing files and base64 strings for exising files\nimages_base64 = get_encoded_image_contents_from_paths(images_absolute_paths, continue_on_error=True)\nfor value in images_base64:\nprint(value)\n# This execution will raise error during processing of missing file terminating operation\nimages_base64 = get_encoded_image_contents_from_paths(images_absolute_paths)\nProcessing other binary files\nOther binary files can be processed by dedicated functions. These functions work similarly to the\nfunctions used for images, although they do not provide functionality for any transformations.\nProcessing follows two steps instead of three:\nRetrieve binary data from the file in the specified location (local path or URL).\nConvert binary data to base64-encoded strings.\nTo process documents into base64-encoded strings use these functions:\nTo retrieve files from local paths: get_encoded_file_contents_from_paths - t\nTo retrieve files from locations specified as URLs: get_encoded_file_contents_from_urls -\nExamples:\nimport os\nfrom datarobot.helpers.binary_data_utils import get_encoded_file_contents_from_urls\ndocument_urls = [\n'https://<YOUR_SERVER_ADDRESS>/document01.pdf',\n'https://<YOUR_SERVER_ADDRESS>/missing.pdf',\n'https://<YOUR_SERVER_ADDRESS>/document03.pdf',\n]\n# this call will return base64 strings for existing documents and None for missing files\ndocuments_base64 = get_encoded_file_contents_from_urls(document_urls, continue_on_error=True)\nfor value in documents_base64:\nprint(value)\n# This execution will raise error during processing of missing file terminating operation\ndocuments_base64 = get_encoded_file_contents_from_urls(document_urls)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/binary_data.html",
      "tags": [
        "advanced",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/binary_data.html",
        "content_length": 10915
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.helpers.image_utils",
        "datarobot.errors.contentretrievalterminatederror",
        "datarobot.helpers.binary_data_utils"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_6379478301336329513",
      "title": "Composable ML",
      "content": "Composable ML\nComposable ML consists of two major components: the DataRobot Blueprint Workshop and custom tasks, detailed below.\nCustom tasks provide users the ability to train models with arbitrary code in an environment defined by the user.\nFor details on using environments, see: Manage Execution Environments.\nManage Custom Tasks\nBefore you can upload code for a custom task, you need to create the entity that holds all the\nmetadata.\nimport datarobot as dr\nfrom datarobot.enums import CUSTOM_TASK_TARGET_TYPE\ntransform = dr.CustomTask.create(\nname=\"a convenient display name\",  # required\ntarget_type=CUSTOM_TASK_TARGET_TYPE.TRANSFORM,  # required\nlanguage=\"python\",\ndescription=\"a longer description of the task\"\n)\nbinary = dr.CustomTask.create(\nname=\"this or that\",\ntarget_type=CUSTOM_TASK_TARGET_TYPE.BINARY,\n)\nA task, by itself is an empty metadata container. Before using your tasks, you need create a\nCustomTaskVersion associated with it. A task that is ready for use\nwill have a latest_version field populated with this task.\nbinary.latest_version\n>>> None\nexecution_environment = dr.ExecutionEnvironment.create(\nname=\"Python3 PyTorch Environment\",\ndescription=\"This environment contains Python3 pytorch library.\",\n)\ncustom_task_folder = \"datarobot-user-tasks/task_templates/python3_pytorch\"\ntask_version = dr.CustomTaskVersion.create_clean(\ncustom_task_id=binary.id,\nbase_environment_id=execution_environment.id,\nfolder_path=custom_task_folder,\n)\nbinary.refresh()  # In order to see the change, you need to GET it from DataRobot\nbinary.latest_version\n>>> CustomTaskVersion('v1.0')\nIf you create a new version, that will be returned as the latest_version. You can\ndownload the latest version as a zip file.\nbinary.latest_version\n>>> CustomTaskVersion('v1.0')\ncustom_task_folder = \"/home/my-user-name/tasks/my-updated-task/\"\ntask_version = dr.CustomTaskVersion.create_clean(\ncustom_task_id=binary.id,\nbase_environment_id=execution_environment.id,\nfolder_path=custom_task_folder,\n)\nbinary.refresh()\nbinary.latest_version\n>>> CustomTaskVersion('v2.0')\nbinary.download_latest_version(\"/home/my-user-name/downloads/my-task-files.zip\")\nYou can get, list, copy, exactly as you would expect. copy makes a complete copy of the\ntask: new copies of the metadata, new copies of the versions, new copies of uploaded files for the\nnew versions.\nall_tasks = CustomTask.list()\nassert {el.id for el in all_tasks} == {binary.id, transform.id}\nnew_binary = CustomTask.copy(binary.id)\nassert new_binary.latest_version.id != binary.latest_version.id\noriginal_binary = CustomTask.get(binary.id)\nassert len(CustomTask.list()) == 3\nYou can update the metadata of a task. When you do this, the object is also updated to the latest\ndata.\nassert binary.description == new_binary.description\nbinary.update(description=\"totally new description\")\nassert binary.description != new_binary.description\nassert original_binary.description != binary.description  # hasn't refreshed from the server yet\noriginal_binary.refresh()\nassert original_binary.description == binary.description\nAnd finally, you can delete only if the task is not in use by any of the following:\nTrained models\nDeployments\nBlueprints in the AI catalog\nOnce you have deleted the objects that use the task, you will be able to delete the task itself.\nManage Custom Task Versions\nCode for Custom Tasks can be uploaded by creating a Custom Task Version.\nWhen creating a Custom Task Version, the version must be associated with a base execution\nenvironment.  If the base environment supports additional task dependencies\n(R or Python environments) and the Custom Task Version\ncontains a valid requirements.txt file, the task version will run in an environment based on\nthe base environment with the additional dependencies installed.\nCreate Custom Task Version\nUpload actual custom task content by creating a clean Custom Task Version:\nimport os\nfrom datarobot.enums import CustomTaskOutboundNetworkPolicy\ncustom_task_id = binary.id\ncustom_task_folder = \"datarobot-user-tasks/task_templates/python3_pytorch\"\n# add files from the folder to the custom task\ntask_version = dr.CustomTaskVersion.create_clean(\ncustom_task_id=custom_task_id,\nbase_environment_id=execution_environment.id,\nfolder_path=custom_task_folder,\noutbound_network_policy=CustomTaskOutboundNetworkPolicy.PUBLIC,\n)\nTo create a new Custom Task Version from a previous one, with just some files added or removed, do the following:\nimport os\nimport datarobot as dr\nnew_files_folder = \"datarobot-user-tasks/task_templates/my_files_to_add_to_pytorch_task\"\nfile_to_delete = task_version.items[0].id\ntask_version_2 = dr.CustomTaskVersion.create_from_previous(\ncustom_task_id=custom_task_id,\nbase_environment_id=execution_environment.id,\nfolder_path=new_files_folder,\n)\nPlease refer to CustomTaskFileItem for description of custom task file properties.\nList Custom Task Versions\nUse the following command to list Custom Task Versions available to the user:\nimport datarobot as dr\ndr.CustomTaskVersion.list(custom_task_id)\n>>> [CustomTaskVersion('v2.0'), CustomTaskVersion('v1.0')]\nRetrieve Custom Task Version\nTo retrieve a specific Custom Task Version, run:\nimport datarobot as dr\ndr.CustomTaskVersion.get(custom_task_id, custom_task_version_id='5ebe96b84024035cc6a6560b')\n>>> CustomTaskVersion('v2.0')\nUpdate Custom Task Version\nTo update Custom Task Version description execute the following:\nimport datarobot as dr\ncustom_task_version = dr.CustomTaskVersion.get(\ncustom_task_id,\ncustom_task_version_id='5ebe96b84024035cc6a6560b',\n)\ncustom_task_version.update(description='new description')\ncustom_task_version.description\n>>> 'new description'\nDownload Custom Task Version\nDownload content of the Custom Task Version as a ZIP archive:\nimport datarobot as dr\npath_to_download = '/home/user/Documents/myTask.zip'\ncustom_task_version = dr.CustomTaskVersion.get(\ncustom_task_id,\ncustom_task_version_id='5ebe96b84024035cc6a6560b',\n)\ncustom_task_version.download(path_to_download)\nPreparing a Custom Task Version for Use\nIf your custom task version has dependencies, a dependency build must be completed before the task\ncan be used.  The dependency build installs your task’s dependencies into the base environment\nassociated with the task version.\nsee: Preparing a Custom Model Version for Use",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/custom_task.html",
      "tags": [
        "advanced",
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/custom_task.html",
        "content_length": 6302
      },
      "code_examples": [],
      "api_methods": [
        "dr.customtaskversion.create_from_previous",
        "dr.customtaskversion.get",
        "dr.customtaskversion.list",
        "dr.executionenvironment.create",
        "dr.customtaskversion.create_clean",
        "dr.customtask.create"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-8182281996139529902",
      "title": "Datetime Partitioned Projects",
      "content": "Datetime Partitioned Projects\nIf your dataset is modeling events taking place over time, datetime partitioning may be appropriate.\nDatetime partitioning ensures that when partitioning the dataset for training and validation, rows\nare ordered according to the value of the date partition feature.\nSetting Up a Datetime Partitioned Project\nAfter creating a project and before setting the target, create a\nDatetimePartitioningSpecification to define how the project should\nbe partitioned.  By passing the specification into DatetimePartitioning.generate, the full\npartitioning can be previewed before finalizing the partitioning.  After verifying that the\npartitioning is correct for the project dataset, pass the specification into Project.analyze_and_model\nvia the partitioning_method argument. Alternatively, as of v3.0, by using Project.set_datetime_partitioning(),\nthe partitioning (and individual options of the partitioning specification) can be updated (with repeated\nmethod calls) up until calling Project.analyze_and_model. Once modeling begins, the project can be used as normal.\nThe following code block shows the basic workflow for creating datetime partitioned projects.\nimport datarobot as dr\nproject = dr.Project.create('some_data.csv')\nspec = dr.DatetimePartitioningSpecification('my_date_column')\n# can customize the spec as needed\npartitioning_preview = dr.DatetimePartitioning.generate(project.id, spec)\n# the preview generated is based on the project's data\nprint(partitioning_preview.to_dataframe())\n# hmm ... I want more backtests\nspec.number_of_backtests = 5\npartitioning_preview = dr.DatetimePartitioning.generate(project.id, spec)\nprint(partitioning_preview.to_dataframe())\n# looks good\nproject.analyze_and_model('target_column')\n# As of v3.0, ``Project.set_datetime_partitioning()`` and ``Project.list_datetime_partition_spec()``\n# are available as an alternative:\n# view settings\nproject.list_datetime_partition_spec()\n# maybe I want to also disable holdout before starting modeling\nproject.set_datetime_partitioning(disable_holdout=True)\n# view settings\nproject.list_datetime_partition_spec()\n# all of the settings look good\n# don't need to pass the spec into ``analyze_and_model`` because it's already been set\nproject.analyze_and_model('target_column')\n# I can retrieve the partitioning settings after the target has been set too\npartitioning = dr.DatetimePartitioning.get(project.id)\nConfiguring Backtests\nBacktests are configurable using one of two methods:\nMethod 1:\nindex (int): The index from zero of this backtest.\ngap_duration (str): A duration string such as those returned by the partitioning_methods.construct_duration_string helper method. This represents the gap between\ntraining and validation scoring data for this backtest.\nvalidation_start_date (datetime.datetime): Represents the start date of the validation scoring data for this backtest.\nvalidation_duration (str): A duration string such as those returned by the partitioning_methods.construct_duration_string helper method. This represents the desired duration\nof the validation scoring data for this backtest.\nimport datarobot as dr\nfrom datetime import datetime\npartitioning_spec = dr.DatetimePartitioningSpecification(\nbacktests=[\n# modify the first backtest using option 1\ndr.BacktestSpecification(\nindex=0,\ngap_duration=dr.partitioning_methods.construct_duration_string(),\nvalidation_start_date=datetime(year=2010, month=1, day=1),\nvalidation_duration=dr.partitioning_methods.construct_duration_string(years=1),\n)\n],\n# other partitioning settings...\n)\nMethod 2 (New in version v2.20):\nvalidation_start_date (datetime.datetime): Represents the start date of the validation scoring data for this backtest.\nvalidation_end_date (datetime.datetime): Represents the end date of the validation scoring data for this backtest.\nprimary_training_start_date (datetime.datetime): Represents the desired start date of the training partition for this backtest.\nprimary_training_end_date (datetime.datetime): Represents the desired end date of the training partition for this backtest.\nimport datarobot as dr\nfrom datetime import datetime\npartitioning_spec = dr.DatetimePartitioningSpecification(\nbacktests=[\n# modify the first backtest using option 2\ndr.BacktestSpecification(\nindex=0,\nprimary_training_start_date=datetime(year=2005, month=1, day=1),\nprimary_training_end_date=datetime(year=2010, month=1, day=1),\nvalidation_start_date=datetime(year=2010, month=1, day=1),\nvalidation_end_date=datetime(year=2011, month=1, day=1),\n)\n],\n# other partitioning settings...\n)\nNote that Method 2 allows you to directly configure the start and end dates of each partition, including the training\npartition. The gap partition is calculated as the time between primary_training_end_date and\nvalidation_start_date. Using the same date for both primary_training_end_date and validation_start_date will\nresult in no gap being created.\nAfter configuring backtests, you can set use_project_settings to True in calls to\nModel.train_datetime. This will create models that are trained\nand validated using your custom backtest training partition start and end dates.\nModeling with a Datetime Partitioned Project\nWhile Model objects can still be used to interact with the project,\nDatetimeModel objects, which are only retrievable from datetime partitioned\nprojects, provide more information including which date ranges and how many rows are used in\ntraining and scoring the model as well as scores and statuses for individual backtests.\nThe autopilot workflow is the same as for other projects, but to manually train a model,\nProject.train_datetime and Model.train_datetime should be used in the place of\nProject.train and Model.train.  To create frozen models,\nModel.request_frozen_datetime_model should be used in place of\nDatetimeModel.request_frozen_datetime_model.  Unlike other projects, to trigger computation of\nscores for all backtests use DatetimeModel.score_backtests instead of using the scoring_type\nargument in the train methods.\nAccuracy Over Time Plots\nFor datetime partitioned model you can retrieve the Accuracy over Time plot. To do so use\nDatetimeModel.get_accuracy_over_time_plot.\nYou can also retrieve the detailed metadata using DatetimeModel.get_accuracy_over_time_plots_metadata,\nand the preview plot using DatetimeModel.get_accuracy_over_time_plot_preview.\nDates, Datetimes, and Durations\nWhen specifying a date or datetime for datetime partitioning, the client expects to receive and\nwill return a datetime.  Timezones may be specified, and will be assumed to be UTC if left\nunspecified.  All dates returned from DataRobot are in UTC with a timezone specified.\nDatetimes may include a time, or specify only a date; however, they may have a non-zero time\ncomponent only if the partition column included a time component in its date format. If the\npartition column included only dates like “24/03/2015”, then the time component of any datetimes,\nif present, must be zero.\nWhen date ranges are specified with a start and an end date, the end date is exclusive, so only\ndates earlier than the end date are included, but the start date is inclusive, so dates equal to or\nlater than the start date are included.  If the start and end date are the same, then no dates are\nincluded in the range.\nDurations are specified using a subset of ISO8601.  Durations will be of the form PnYnMnDTnHnMnS\nwhere each “n” may be replaced with an integer value.  Within the duration string,\nnY represents the number of years\nthe nM following the “P” represents the number of months\nnD represents the number of days\nnH represents the number of hours\nthe nM following the “T” represents the number of minutes\nnS represents the number of seconds\nand “P” is used to indicate that the string represents a period and “T” indicates the beginning of\nthe time component of the string.  Any section with a value of 0 may be excluded.  As with\ndatetimes, if the partition column did not include a time component in its date format, the time\ncomponent of any duration must be either unspecified or consist only of zeros.\nExample Durations:\n“P3Y6M” (three years, six months)\n“P1Y0M0DT0H0M0S” (one year)\n“P1Y5DT10H” (one year, 5 days, 10 hours)\ndatarobot.helpers.partitioning_methods.construct_duration_string is a\nhelper method that can be used to construct appropriate duration strings.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/datetime_partition.html",
      "tags": [
        "advanced",
        "api_reference",
        "beginner",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/datetime_partition.html",
        "content_length": 8355
      },
      "code_examples": [],
      "api_methods": [
        "dr.partitioning_methods.construct_duration_string",
        "dr.datetimepartitioning.generate",
        "model.get_accuracy_over_time_plots_metadata",
        "dr.project.create",
        "model.train",
        "model.request_frozen_datetime_model",
        "model.get_accuracy_over_time_plot_preview",
        "project.id",
        "datarobot.helpers.partitioning_methods",
        "project.train",
        "project.create",
        "project.train_datetime",
        "project.set_datetime_partitioning",
        "model.score_backtests",
        "model.train_datetime",
        "dr.datetimepartitioning.get",
        "project.analyze_and_model",
        "project.list_datetime_partition_spec",
        "model.get_accuracy_over_time_plot"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-3359693861278709279",
      "title": "Specialized workflows",
      "content": "Specialized workflows\nThe following sections describe alternative workflows for a variety of specialized data types.",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/index.html",
        "content_length": 117
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.15,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-3140124092250946513",
      "title": "Monotonic Constraints",
      "content": "Monotonic Constraints\nTraining with monotonic constraints allows users to force models to learn monotonic relationships with respect to some features and the target. This helps users create accurate models that comply with regulations (e.g. insurance, banking). Currently, only certain blueprints (e.g. xgboost) support this feature, and it is only supported for regression and binary classification projects. Typically working with monotonic constraints follows the following two workflows:\nWorkflow one - Running a project with default monotonic constraints\nset the target and specify default constraint lists for the project\nwhen running autopilot or manually training models without overriding constraint settings, all blueprints that support monotonic constraints will use the specified default constraint featurelists\nWorkflow two - Running a model with specific monotonic constraints\ncreate featurelists for monotonic constraints\ntrain a blueprint that supports monotonic constraints while specifying monotonic constraint featurelists\nthe specified constraints will be used, regardless of the defaults on the blueprint\nCreating featurelists\nWhen specifying monotonic constraints, users must pass a reference to a featurelist containing only the features to be constrained, one for features that should monotonically increase with the target and another for those that should monotonically decrease with the target.\nimport datarobot as dr\nproject = dr.Project.get(project_id)\nfeatures_mono_up = ['feature_0', 'feature_1']  # features that have monotonically increasing relationship with target\nfeatures_mono_down = ['feature_2', 'feature_3']  # features that have monotonically decreasing relationship with target\nflist_mono_up = project.create_featurelist(name='mono_up',\nfeatures=features_mono_up)\nflist_mono_down = project.create_featurelist(name='mono_down',\nfeatures=features_mono_down)\nSpecify default monotonic constraints for a project\nUsers can specify default monotonic constraints for the project, to ensure that autopilot models use the desired settings, and optionally to ensure that only blueprints supporting monotonic constraints appear in the project. Regardless of the defaults specified via advanced options selection, the user can override them when manually training a particular model.\nimport datarobot as dr\nfrom datarobot.enums import AUTOPILOT_MODE\nproject = dr.Project.get(project_id)\n# As of v3.0, ``Project.set_options`` may be used as an alternative to passing `advanced_options`` into ``Project.analyze_and_model``.\nproject.set_options(\nmonotonic_increasing_featurelist_id=flist_mono_up.id,\nmonotonic_decreasing_featurelist_id=flist_mono_down.id,\nonly_include_monotonic_blueprints=True\n)\nproject.analyze_and_model(target='target', mode=AUTOPILOT_MODE.FULL_AUTO)\nIf Project.set_options is not used, alternatively, an advanced options instance may be passed directly to project.analyze_and_model:\nproject.analyze_and_model(\ntarget='target',\nmode=AUTOPILOT_MODE.FULL_AUTO,\nadvanced_options=AdvancedOptions(monotonic_increasing_featurelist_id=flist_mono_up.id, monotonic_decreasing_featurelist_id=flist_mono_down.id, only_include_monotonic_blueprints=True)\n)\nRetrieve models and blueprints using monotonic constraints\nWhen retrieving models, users can inspect to see which supports monotonic constraints, and which actually enforces them. Some models will not support monotonic constraints at all, and some may support constraints but not have any constrained features specified.\nimport datarobot as dr\nproject = dr.Project.get(project_id)\nmodels = project.get_models()\n# retrieve models that support monotonic constraints\nmodels_support_mono = [model for model in models if model.supports_monotonic_constraints]\n# retrieve models that support and enforce monotonic constraints\nmodels_enforce_mono = [model for model in models\nif (model.monotonic_increasing_featurelist_id or\nmodel.monotonic_decreasing_featurelist_id)]\nWhen retrieving blueprints, users can check if they support monotonic constraints and see what default constraint lists are associated with them. The monotonic featurelist ids associated with a blueprint will be used every time it is trained, unless the user specifically overrides them at model submission time.\nimport datarobot as dr\nproject = dr.Project.get(project_id)\nblueprints = project.get_blueprints()\n# retrieve blueprints that support monotonic constraints\nblueprints_support_mono = [blueprint for blueprint in blueprints if blueprint.supports_monotonic_constraints]\n# retrieve blueprints that support and enforce monotonic constraints\nblueprints_enforce_mono = [blueprint for blueprint in blueprints\nif (blueprint.monotonic_increasing_featurelist_id or\nblueprint.monotonic_decreasing_featurelist_id)]\nTrain a model with specific monotonic constraints\nEven after specifying default settings for the project, users can override them to train a new model with different constraints, if desired.\nimport datarobot as dr\nfeatures_mono_up = ['feature_2', 'feature_3']  # features that have monotonically increasing relationship with target\nfeatures_mono_down = ['feature_0', 'feature_1']  # features that have monotonically decreasing relationship with target\nproject = dr.Project.get(project_id)\nflist_mono_up = project.create_featurelist(name='mono_up',\nfeatures=features_mono_up)\nflist_mono_down = project.create_featurelist(name='mono_down',\nfeatures=features_mono_down)\nmodel_job_id = project.train(\nblueprint,\nsample_pct=55,\nfeaturelist_id=featurelist.id,\nmonotonic_increasing_featurelist_id=flist_mono_up.id,\nmonotonic_decreasing_featurelist_id=flist_mono_down.id\n)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/monotonic_constraints.html",
      "tags": [
        "advanced",
        "api_reference",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/monotonic_constraints.html",
        "content_length": 5634
      },
      "code_examples": [],
      "api_methods": [
        "model.monotonic_decreasing_featurelist_id",
        "model.supports_monotonic_constraints",
        "project.set_options",
        "model.monotonic_increasing_featurelist_id",
        "dr.project.get",
        "project.get",
        "project.analyze_and_model",
        "project.get_models",
        "project.train",
        "project.get_blueprints",
        "project.create_featurelist"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-9050814872061524597",
      "title": "Segmented Modeling Projects",
      "content": "Segmented Modeling Projects\nMany time series multiseries projects introduce complex forecasting use cases that require using different models for subsets of series (i.e., sales of groceries and clothing can be very different). Within the segmented modeling framework, DataRobot runs multiple time series projects (one per segment / group of series), selects the best models for each segment, and then combines those models to make predictions.\nSegment\nA segment is a group of series in a multiseries project. For example, given store and country columns in dataset, you can use the former as the series identifier and the latter  as the segment identifier. For the best results, group series with similar patterns into segments (instead of random selection).\nSegmentation Task\nA segmentation task is an entity that defines how input dataset is partitioned. Currently only user-defined segmentation is supported. That is, the dataset must have a separate column that is used to identify segment (and the user must select it). All records within a series must have the same segment identifier.\nCombined Model\nA combined model in a segmented modeling project can be thought of as a meta-model made of references to the best model within each segment. While being quite different from a standard DataRobot model in its creation, its use is very much the same after the model is complete (for example, deploying or making predictions).\nThe following examples illustrate how to set up, run, and manage a segmented modeling project using the Python public API client. For details please refer to Segmented Modeling API Reference.\nStarting a Segmentation Project with a User Defined Segment ID\nTime series modeling must be enabled for your account to run segmented modeling projects.\nUse the standard method to create a DataRobot project:\nfrom datarobot import DatetimePartitioningSpecification\nfrom datarobot import enums\nfrom datarobot import Project\nfrom datarobot import SegmentationTask\nproject_name = \"Segmentation Demo with Segmentation ID\"\nproject_dataset = \"multiseries_segmentation.csv\"\nproject = Project.create(project_dataset, project_name=project_name)\ndatetime_partition_column = \"timestamp\"\nmultiseries_id_column = \"series_id\"\nuser_defined_segment_id_column = \"物类segment_id\"\ntarget = \"target\"\nCreate a simple datetime specification for a time series project:\nspec = DatetimePartitioningSpecification(\nuse_time_series=True,\ndatetime_partition_column=datetime_partition_column,\nmultiseries_id_columns=[multiseries_id_column],\n)\nCreate a segmentation task for the project:\nsegmentation_task_results = SegmentationTask.create(\nproject_id=project.id,\ntarget=target,\nuse_time_series=True,\ndatetime_partition_column=datetime_partition_column,\nmultiseries_id_columns=[multiseries_id_column],\nuser_defined_segment_id_columns=[user_defined_segment_id_column],\n)\nsegmentation_task = segmentation_task_results[\"completedJobs\"][0]\nStart a segmented project by passing the segmentation_task_id argument:\nproject.analyze_and_model(\ntarget=target,\npartitioning_method=spec,\nmode=enums.AUTOPILOT_MODE.QUICK,\nworker_count=-1,\nsegmentation_task_id=segmentation_task.id,\n)\nWorking with Combined Models\nRetrieve Combined Models:\nfrom datarobot import Project, CombinedModel\nproject_id = \"60ff165dde5f3ceacda0f2d6\"\n# Get an existing segmentation project\nproject = Project.get(segmented_project_id)\n# Retrieve list of all combined models in the project\ncombined_models = project.get_combined_models()\n# Or just an active (current) combined model\ncurrent_combined_model = project.get_active_combined_model()\nGet information about segments in the Combined Model:\nsegments_info = current_combined_model.get_segments_info()\n# Alternatively this information can be retrieved as a Pandas DataFrame\nsegments_df = current_combined_model.get_segments_as_dataframe()\n# Or even in CSV format\ncurrent_combined_model.get_segments_as_csv(\"combined_model_segments.csv\")\nEnsure Autopilot has completed for all segments:\nsegments_info = current_combined_model.get_segments_info()\nassert all(segment.autopilot_done for segment in segments_info)\nOptionally, view a list of all models associated with individual segments:\nsegments_and_child_models = project.get_segments_models(current_combined_model.id)\nSet a new champion for a segment in the Combined Model, specifying the project_id of the segmented  project and the model_id from that project:\nsegment_project_id = \"60ff165dde5f3ceacdaabcde\"\nnew_champion_id = \"60ff165dde5f3ceacdaa12f7\"\nCombinedModel.set_segment_champion(project_id=segment_project_id, model_id=new_champion_id)\nIf active Combined Model has already been deployed - changing champions is not allowed. In this case, create a copy of Combined Model, make it active, and set champion for it (deployed model remains unchanged):\nnew_combined_model = CombinedModel.set_segment_champion(project_id=segment_project_id, model_id=new_champion_id, clone=True)\nRun predictions on the Combined Model:\nprediction_dataset = \"multiseries_predictions.csv\"\n# Upload dataset\ndataset = project.upload_dataset(\nsource=prediction_dataset,\n)\n# Request predictions\npredictions_job = current_combined_model.request_predictions(\ndataset_id=dataset.id,\n)\npredictions_job.wait_for_completion()\npredictions = predictions.get_result()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/segmented_modeling.html",
      "tags": [
        "beginner",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/segmented_modeling.html",
        "content_length": 5295
      },
      "code_examples": [],
      "api_methods": [
        "model.id",
        "project.create",
        "project.get_combined_models",
        "model.set_segment_champion",
        "project.id",
        "model.get_segments_as_csv",
        "project.upload_dataset",
        "model.get_segments_info",
        "project.get",
        "model.request_predictions",
        "project.analyze_and_model",
        "project.get_active_combined_model",
        "project.get_segments_models",
        "model.get_segments_as_dataframe"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_1526068433175962318",
      "title": "Time series projects",
      "content": "Time series projects\nTime series projects, like OTV projects, use datetime partitioning, and all\nthe workflow changes that apply to other datetime partitioned projects also apply to them.\nUnlike other projects, time series projects produce different types of models which forecast\nmultiple future predictions instead of an individual prediction for each row.\nDataRobot uses a general time series framework to configure how time series features are created\nand what future values the models will output. This framework consists of a Forecast Point\n(defining a time a prediction is being made), a Feature Derivation Window (a rolling window used\nto create features), and a Forecast Window (a rolling window of future values to predict). These\ncomponents are described in more detail below.\nTime series projects will automatically transform the dataset provided in order to apply this\nframework. During the transformation, DataRobot uses the Feature Derivation Window to derive\ntime series features (such as lags and rolling statistics), and uses the Forecast Window to provide\nexamples of forecasting different distances in the future (such as time shifts).\nAfter project creation, a new dataset and a new feature list are generated and used to train\nthe models. This process is reapplied automatically at prediction time as well in order to\ngenerate future predictions based on the original data features.\nThe time_unit and time_step used to define the Feature Derivation and Forecast Windows are\ntaken from the datetime partition column, and can be retrieved for a given column in the input data\nby looking at the corresponding attributes on the datarobot.models.Feature object.\nIf windows_basis_unit is set to ROW, then Feature Derivation and Forecast Windows will be\ndefined using number of the rows.\nSetting up a time series project\nTo set up a time series project, follow the standard datetime partitioning\nworkflow and use the six new time series specific parameters on the\ndatarobot.DatetimePartitioningSpecification object:\nuse_time_seriesbool, set this to True to enable time series for the project.\ndefault_to_known_in_advancebool, set this to True to default to treating all features as known in advance, or a priori, features. Otherwise,\nthey will not be handled as known in advance features. Individual features can be set to a value\ndifferent than the default by using the featureSettings parameter. See\nthe prediction documentation for more information.\ndefault_to_do_not_derivebool, set this to True to default to excluding all features from feature derivation.  Otherwise,\nthey will not be excluded and will be included in the feature derivation process.\nIndividual features can be set to a value different than the default by using the\nfeatureSettings parameter.\nfeature_derivation_window_startint, specifies how many units of the windows_basis_unit from the forecast point into the past is the start of\nthe feature derivation window\nfeature_derivation_window_endint, specifies how many units of the windows_basis_unit from the forecast point into the past is the end of the\nfeature derivation window\nforecast_window_startint, specifies how many units of the windows_basis_unit from the forecast point into the future is the start of\nthe forecast window\nforecast_window_endint, specifies how many units of the windows_basis_unit from the forecast point into the future is the end of\nthe forecast window\nwindows_basis_unitstring, set this to ROW to define feature derivation and forecast windows in terms of the\nrows, rather than time units. If omitted, will default to the detected time unit (one of the\ndatarobot.enums.TIME_UNITS).\nfeature_settingslist of FeatureSettings specifying per feature settings, can be left unspecified\nFeature Derivation Window\nThe Feature Derivation window represents the rolling window that is used to derive\ntime series features and lags, relative to the Forecast Point. It is defined in terms of\nfeature_derivation_window_start and feature_derivation_window_end which are integer values\nrepresenting datetime offsets in terms of the time_unit (e.g. hours or days).\nThe Feature Derivation Window start and end must be less than or equal to zero, indicating they are\npositioned before the forecast point. Additionally, the window must be specified as an integer\nmultiple of the time_step which defines the expected difference in time units between rows in\nthe data.\nThe window is closed, meaning the edges are considered to be inside the window.\nForecast window\nThe Forecast Window represents the rolling window of future values to predict, relative to the\nForecast Point. It is defined in terms of the forecast_window_start and forecast_window_end,\nwhich are positive integer values indicating datetime offsets in terms of the time_unit (e.g.\nhours or days).\nThe Forecast Window start and end must be positive integers, indicating they are\npositioned after the forecast point. Additionally, the window must be specified as an integer\nmultiple of the time_step which defines the expected difference in time units between rows in\nthe data.\nThe window is closed, meaning the edges are considered to be inside the window.\nMultiseries projects\nCertain time series problems represent multiple separate series of data, e.g. “I have five different\nstores that all have different customer bases.  I want to predict how many units of a particular\nitem will sell, and account for the different behavior of each store”.  When setting up the project,\na column specifying series ids must be identified, so that each row from the same series has the\nsame value in the multiseries id column.\nUsing a multiseries id column changes which partition columns are eligible for time series, as\neach series is required to be unique and regular, instead of the entire partition column being\nrequired to have those properties.  In order to use a multiseries id column for partitioning,\na detection job must first be run to analyze the relationship between the partition and multiseries\nid columns.  If needed, it will be automatically triggered by calling\ndatarobot.models.Feature.get_multiseries_properties() on the desired partition column. The\npreviously computed multiseries properties for a particular partition column can then be accessed\nvia that method.  The computation will also be automatically triggered when calling\ndatarobot.DatetimePartitioning.generate() or datarobot.models.Project.analyze_and_model()\nwith a multiseries id column specified.\nNote that currently only one multiseries id column is supported, but all interfaces accept lists\nof id columns to ensure multiple id columns will be able to be supported in the future.\nIn order to create a multiseries project:\nSet up a datetime partitioning specification with the desired partition column and multiseries\nid columns.\n(Optionally) Use datarobot.models.Feature.get_multiseries_properties() to confirm the\ninferred time step and time unit of the partition column when used with the specified\nmultiseries id column.\n(Optionally) Specify the multiseries id column in order to preview the full datetime\npartitioning settings using datarobot.DatetimePartitioning.generate().\nSpecify the multiseries id column when sending the target and partitioning settings via\ndatarobot.models.Project.analyze_and_model().\nproject = dr.Project.create('path/to/multiseries.csv', project_name='my multiseries project')\npartitioning_spec = dr.DatetimePartitioningSpecification(\n'timestamp', use_time_series=True, multiseries_id_columns=['multiseries_id']\n)\n# manually confirm time step and time unit are as expected\ndatetime_feature = dr.Feature.get(project.id, 'timestamp')\nmultiseries_props = datetime_feature.get_multiseries_properties(['multiseries_id'])\nprint(multiseries_props)\n# manually check out the partitioning settings like feature derivation window and backtests\n# to make sure they make sense before moving on\nfull_part = dr.DatetimePartitioning.generate(project.id, partitioning_spec)\nprint(full_part.feature_derivation_window_start, full_part.feature_derivation_window_end)\nprint(full_part.to_dataframe())\n# As of v3.0, can use ``Project.set_datetime_partitioning`` instead of passing the spec into ``Project.analyze_and_model`` via ``partitioning_method``.\n# The spec options can be passed individually:\nproject.set_datetime_partitioning(use_time_series=True, datetime_partition_column='date', multiseries_id_columns=['series_id'])\n# Or the whole spec object can be passed:\nproject.set_datetime_partitioning(datetime_partitioning_spec=datetime_spec)\n# finalize the project and start the autopilot\nproject.analyze_and_model('target', partitioning_method=partitioning_spec)\nYou can also access optimized partitioning in the API where the target over time is inspected to\nensure that the default backtests cover regions of interest and adjust backtests avoid common\nproblems with missing target values or partitions with single values (e.g. zero-inflated datasets).\nIn this case you need to pass the target column when generating the partitioning specification (either\nby calling DatetimePartitioning.generate or Project.set_datetime_partitioning) and\nthen pass the full partitioning specification when starting autopilot (if Project.set_datetime_partitioning\nis not used).\nproject = dr.Project.create('path/to/multiseries.csv', project_name='my multiseries project')\npartitioning_spec = dr.DatetimePartitioningSpecification(\n'timestamp', use_time_series=True, multiseries_id_columns=['multiseries_id']\n)\n# Pass the target column to generate optimized partitions\nfull_part = dr.DatetimePartitioning.generate(project.id, partitioning_spec, 'target')\n# Or, as of v3.0, call ``Project.set_datetime_partitioning`` after specifying the project target\n# to generate optimized partitions.\nproject.target = 'target'\nproject.set_datetime_partitioning(datetime_partition_spec=partitioning_spec)\n# finalize the project and start the autopilot, passing in the full partitioning spec\n# (if ``Project.set_datetime_partitioning`` was used there is no need to pass ``partitioning_method``)\nproject.analyze_and_model('target', partitioning_method=full_part.to_specification())\nFeature settings\ndatarobot.FeatureSettings constructor receives feature_name and settings. For now\nsettings known_in_advance and do_not_derive are supported.\n# I have 10 features, 8 of them are known in advance and two are not\n# Also, I do not want to derive new features from previous_day_sales\nnot_known_in_advance_features = ['previous_day_sales', 'amount_in_stock']\ndo_not_derive_features = ['previous_day_sales']\nfeature_settings = [dr.FeatureSettings(feat_name, known_in_advance=False) for feat_name in not_known_in_advance_features]\nfeature_settings += [dr.FeatureSettings(feat_name, do_not_derive=True) for feat_name in do_not_derive_features]\nspec = dr.DatetimePartitioningSpecification(\n# ...\ndefault_to_known_in_advance=True,\nfeature_settings=feature_settings\n)\nModeling data and time series features\nIn time series projects, a new set of modeling features is created after setting the\npartitioning options.  If a featurelist is specified with the partitioning options, it will be used\nto select which features should be used to derived modeling features; if a featurelist is not\nspecified, the default featurelist will be used.\nThese features are automatically derived from those in the project’s\ndataset and are the features used for modeling - note that the Project methods\nget_featurelists and get_modeling_featurelists will return different data in time series\nprojects.  Modeling featurelists are the ones that can be used for modeling and will be accepted by\nthe backend, while regular featurelists will continue to exist but cannot be used.  Modeling\nfeatures are only accessible once the target and partitioning options have been\nset.  In projects that don’t use time series modeling, once the target has been set,\nmodeling and regular features and featurelists will behave the same.\nRestoring discarded features\ndatarobot.models.restore_discarded_features.DiscardedFeaturesInfo can be used to get and\nrestore features that have been removed by the time series feature generation and reduction functionality.\nproject = Project(project_id)\ndiscarded_feature_info = project.get_discarded_features()\nrestored_features_info = project.restore_discarded_features(discarded_features_info.features)\nMaking predictions\nPrediction datasets are uploaded as normal. However, when uploading a\nprediction dataset, a new parameter forecast_point can be specified. The forecast point of a\nprediction dataset identifies the point in time relative which predictions should be generated, and\nif one is not specified when uploading a dataset, the server will choose the most recent possible\nforecast point. The forecast window specified when setting the partitioning options for the project\ndetermines how far into the future from the forecast point predictions should be calculated.\nTo simplify the predictions process, starting in version v2.20 a forecast point or prediction start and end dates can\nbe specified when requesting predictions, instead of being specified at dataset upload. Upon uploading a dataset,\nDataRobot will calculate the range of dates available for use as a forecast point or for batch predictions. To that end,\nPredictions objects now also contain the following new fields:\nforecast_point: The default point relative to which predictions will be generated\npredictions_start_date: The start date for bulk historical predictions.\npredictions_end_date: The end date for bulk historical predictions.\nSimilar settings are provided as part of the batch prediction API\nand the real-time prediction API\nto make predictions using deployed time series models.\ndatarobot.models.BatchPredictionJob.score\nWhen setting up a time series project, input features could be identified as known-in-advance features.\nThese features are not used to generate lags, and are expected to be known for the rows in the\nforecast window at predict time (e.g. “how much money will have been spent on marketing”, “is this\na holiday”).\nEnough rows of historical data must be provided to cover the span of the effective Feature\nDerivation Window (which may be longer than the project’s Feature Derivation Window depending\non the differencing settings chosen).  The effective Feature Derivation Window of any model\ncan be checked via the effective_feature_derivation_window_start and\neffective_feature_derivation_window_end attributes of a\nDatetimeModel.\nWhen uploading datasets to a time series project, the dataset might look something like the\nfollowing, where “Time” is the datetime partition column, “Target” is the target column, and “Temp.”\nis an input feature.  If the dataset was uploaded with a forecast point of “2017-01-08” and the\neffective feature derivation window start and end for the model are -5 and -3 and the forecast\nwindow start and end were set to 1 and 3, then rows 1 through 3 are historical data, row 6 is the\nforecast point, and rows 7 though 9 are forecast rows that will have predictions when predictions\nare computed.\nRow, Time, Target, Temp.\n1, 2017-01-03, 16443, 72\n2, 2017-01-04, 3013, 72\n3, 2017-01-05, 1643, 68\n4, 2017-01-06, ,\n5, 2017-01-07, ,\n6, 2017-01-08, ,\n7, 2017-01-09, ,\n8, 2017-01-10, ,\n9, 2017-01-11, ,\nOn the other hand, if the project instead used “Holiday” as an a priori input feature, the uploaded\ndataset might look like the following:\nRow, Time, Target, Holiday\n1, 2017-01-03, 16443, TRUE\n2, 2017-01-04, 3013, FALSE\n3, 2017-01-05, 1643, FALSE\n4, 2017-01-06, , FALSE\n5, 2017-01-07, , FALSE\n6, 2017-01-08, , FALSE\n7, 2017-01-09, , TRUE\n8, 2017-01-10, , FALSE\n9, 2017-01-11, , FALSE\nCalendars\nYou can upload a calendar file containing a list of events relevant to your\ndataset. When provided, DataRobot automatically derives and creates time series features based on the calendar\nevents (e.g., time until the next event, labeling the most recent event).\nThe calendar file:\nShould span the entire training data date range, as well as all future dates in which model will be forecasting.\nMust be in csv or xlsx format with a header row.\nMust have one date column which has values in the date-only format YYY-MM-DD (i.e., no hour, month, or second).\nCan optionally include a second column that provides the event name or type.\nCan optionally include a series ID column which specifies which series an event is applicable to. This column name\nmust match the name of the column set as the series ID.\nMultiseries ID columns are used to add an ability to specify different sets of events for different series, e.g.\nholidays for different regions.\nValues of the series ID may be absent for specific events. This means that the event is valid for all series in\nproject dataset (e.g. New Year’s Day is a holiday in all series in the example below).\nIf a multiseries ID column is not provided, all listed events will be applicable to all series in the project\ndataset.\nCannot be updated in an active project. You must specify all future calendar events at project start. To update the\ncalendar file, you will have to train a new project.\nAn example of a valid calendar file:\nDate,        Name\n2019-01-01,  New Year's Day\n2019-02-14,  Valentine's Day\n2019-04-01,  April Fools\n2019-05-05,  Cinco de Mayo\n2019-07-04,  July 4th\nAn example of a valid multiseries calendar file:\nDate,        Name,                   Country\n2019-01-01,  New Year's Day,\n2019-05-27,  Memorial Day,           USA\n2019-07-04,  July 4th,               USA\n2019-11-28,  Thanksgiving,           USA\n2019-02-04,  Constitution Day,       Mexico\n2019-03-18,  Benito Juárez's birth,  Mexico\n2019-12-25,  Christmas Day,\nOnce created, a calendar can be used with a time series project by specifying the calendar_id field in the datarobot.DatetimePartitioningSpecification object for the project:\nimport datarobot as dr\n# create the project\nproject = dr.Project.create('input_data.csv')\n# create the calendar\ncalendar = dr.CalendarFile.create('calendar_file.csv')\n# specify the calendar_id in the partitioning specification\ndatetime_spec = dr.DatetimePartitioningSpecification(\nuse_time_series=True,\ndatetime_partition_column='date'\ncalendar_id=calendar.id\n)\n# As of v3.0, can use ``Project.set_datetime_partitioning`` instead of passing the spec into ``Project.analyze_and_model`` via ``partitioning_method``.\n# The spec options can be passed individually:\nproject.set_datetime_partitioning(use_time_series=True, datetime_partition_column='date', calendar_id=calendar.id)\n# Or the whole spec object can be passed:\nproject.set_datetime_partitioning(datetime_partitioning_spec=datetime_spec)\n# start the project, specifying the partitioning method (if ``Project.set_datetime_partitioning`` was used there is no need to pass ``partitioning_method``)\nproject.analyze_and_model(\ntarget='project target',\npartitioning_method=datetime_spec\n)\nAs of version v2.23 it is possible to ask DataRobot to generate a calendar file for you using\nCalendarFile.create_calendar_from_country_code.\nThis method allows you to provide a country code specifying which country’s holidays to use in generating the calendar,\nalong with a start and end date indicating the bounds of the calendar. Allowed country codes can be retrieved using\nCalendarFile.get_allowed_country_codes. See the following code block for example usage:\nimport datarobot as dr\nfrom datetime import datetime\n# create the project\nproject = dr.Project.create('input_data.csv')\n# retrieve the allowed country codes and use the first one\ncountry_code = dr.CalendarFile.get_allowed_country_codes()[0]['code']\ncalendar = dr.CalendarFile.create_calendar_from_country_code(\ncountry_code, datetime(2018, 1, 1), datetime(2018, 7, 4)\n)\n# specify the calendar_id in the partitioning specification\ndatetime_spec = dr.DatetimePartitioningSpecification(\nuse_time_series=True,\ndatetime_partition_column='date'\ncalendar_id=calendar.id\n)\n# As of v3.0, can use ``Project.set_datetime_partitioning`` instead of passing the spec into ``Project.analyze_and_model`` via ``partitioning_method``.\n# The spec options can be passed individually:\nproject.set_datetime_partitioning(use_time_series=True, datetime_partition_column='date', calendar_id=calendar.id)\n# Or the whole spec object can be passed:\nproject.set_datetime_partitioning(datetime_partitioning_spec=datetime_spec)\n# Start the project, specifying the partitioning method (if ``Project.set_datetime_partitioning`` was used there is no need to pass ``partitioning_method``)\nproject.analyze_and_model(\ntarget='project target',\npartitioning_method=datetime_spec\n)\nDatetime trend plots\nAs a version v2.25, it is possible to retrieve Datetime Trend Plots for time series models\nto estimate the accuracy of the model. This includes Accuracy over Time and Forecast vs Actual for supervised projects,\nand Anomaly over Time for unsupervised projects. You can retrieve respective plots using following methods:\nDatetimeModel.get_accuracy_over_time_plot\nDatetimeModel.get_forecast_vs_actual_plot\nDatetimeModel.get_anomaly_over_time_plot\nBy default, the plots would be automatically computed when accessed via retrieval methods. You can compute Datetime Trend Plots separately\nusing a common method DatetimeModel.compute_datetime_trend_plots.\nIn addition, you can retrieve the respective detailed metadata for each plot type:\nDatetimeModel.get_accuracy_over_time_plots_metadata\nDatetimeModel.get_forecast_vs_actual_plots_metadata\nDatetimeModel.get_anomaly_over_time_plots_metadata\nAnd the preview plots:\nDatetimeModel.get_accuracy_over_time_plot_preview\nDatetimeModel.get_forecast_vs_actual_plot_preview\nDatetimeModel.get_anomaly_over_time_plot_preview\nPrediction intervals\nFor each model, prediction intervals estimate the range of values DataRobot expects actual values of the target to fall within.\nThey are similar to a confidence interval of a prediction, but are based on the residual errors measured during the\nbacktesting for the selected model.\nNote that because calculation depends on the backtesting values, prediction intervals are not available for predictions\non models that have not had all backtests completed. To that end, note that creating a prediction with prediction intervals through the API will\nautomatically complete all backtests if they were not already completed. For start-end retrained models, the parent model will be used for backtesting.\nAdditionally, prediction intervals are not available when the number of points per forecast distance is less than 10, due to insufficient data.\nIn a prediction request, users can specify a prediction interval’s size, which specifies the desired probability of actual values\nfalling within the interval range. Larger values are less precise, but more conservative. For example, specifying a size\nof 80 will result in a lower bound of 10% and an upper bound of 90%. More generally, for a specific prediction_intervals_size,\nthe upper and lower bounds will be calculated as follows:\nprediction_interval_upper_bound = 50% + (prediction_intervals_size / 2)\nprediction_interval_lower_bound = 50% - (prediction_intervals_size / 2)\nPrediction intervals can be calculated for a DatetimeModel using the\nDatetimeModel.calculate_prediction_intervals method.\nUsers can also retrieve which intervals have already been calculated for the model using the\nDatetimeModel.get_calculated_prediction_intervals method.\nTo view prediction intervals data for a prediction, the prediction needs to have been created using the\nDatetimeModel.request_predictions method and specifying\ninclude_prediction_intervals = True. The size for the prediction interval can be specified with the prediction_intervals_size\nparameter for the same function, and will default to 80 if left unspecified. Specifying either of these fields will\nresult in prediction interval bounds being included in the retrieved prediction data for that request (see the\nPredictions class for retrieval methods). Note that if the specified interval\nsize has not already been calculated, this request will automatically calculate the specified size.\nPrediction intervals are also supported for time series model deployments, and should be specified in deployment settings\nif desired. Use Deployment.get_prediction_intervals_settings\nto retrieve current prediction intervals settings for a deployment, and Deployment.update_prediction_intervals_settings\nto update prediction intervals settings for a deployment.\nPartial history predictions\nAs of version v2.24 it is possible to ask DataRobot to allow to make predictions with incomplete historical data\nmultiseries regression projects. To make predictions in regular project user has to provide enough data for the\nfeature derivation. By setting the datetime partitioning attribute allow_partial_history_time_series_predictions\nto true (datarobot.DatetimePartitioningSpecification object),\nthe project would be created that allow to make such predictions. The number of models are significantly\nsmaller compared to regular multiseries model, but they are designed to make predictions on unseen series with\nreasonable accuracy.\nExternal baseline predictions\nAs of version v2.26  it is possible to ask DataRobot to scale accuracy metric by external predictions. Users can\nupload data into a Dataset (see Dataset documentation) and compare the external time series\npredictions with DataRobot models’ accuracy performance. To use the external predictions dataset in the autopilot,\nthe dataset must be validated first (see\nProject.validate_external_time_series_baseline).\nOnce the dataset is validated, it can be used with a time series project by specifying external_time_series_baseline_dataset_id\nfield in AdvancedOptions and passes the advanced options to the project.\nSee the following code block for example usage:\nimport datarobot as dr\nfrom datarobot.helpers import AdvancedOptions\nfrom datarobot.models import Dataset\n# create the project\nproject = dr.Project.create('input_data.csv')\n# prepare datetime partitioning for external baseline validation\ndatetime_spec = dr.DatetimePartitioningSpecification(\nuse_time_series=True,\ndatetime_partition_column='date',\nmultiseries_id_columns=['series_id'],\n)\ndatetime_partitioning = dr.DatetimePartitioning.generate(\nproject_id=project.id,\nspec=datetime_spec,\ntarget='target',\n)\n# create external baseline prediction dataset from local file\nexternal_baseline_dataset = Dataset.create_from_file(file_path='external_predictions.csv')\n# validate the external baseline prediction dataset\nvalidation_info = project.validate_external_time_series_baseline(\ncatalog_version_id=external_baseline_dataset.version_id,\ntarget='target',\ndatetime_partitioning=datetime_partitioning,\n)\nprint(\n'External baseline predictions passes validation check:',\nvalidation_info.is_external_baseline_dataset_valid\n)\n# As of v3.0, can use ``Project.set_datetime_partitioning`` instead of passing the spec into ``Project.analyze_and_model`` via ``partitioning_method``.\n# The spec options can be passed individually:\nproject.set_datetime_partitioning(use_time_series=True, datetime_partition_column='date', multiseries_id_columns=['series_id'])\n# Or the whole spec object can be passed:\nproject.set_datetime_partitioning(datetime_partitioning_spec=datetime_spec)\n# As of v3.0, add the validated dataset version id into advanced options\nproject.set_options(\nexternal_time_series_baseline_dataset_id=external_baseline_dataset.version_id\n)\n# start the project, specifying the partitioning method (if ``Project.set_datetime_partitioning`` and ``Project.set_options`` were not used)\nproject.analyze_and_model(\ntarget='target',\npartitioning_method=datetime_spec\nadvanced_options=AdvancedOptions(external_time_series_baseline_dataset_id)\n)\nTime Series Data Prep\nAs of version v2.27 it is possible to prepare a dataset for time series modeling in the AI catalog\nusing the API client. Users can upload unprepped modeling data into a Dataset\n(see Dataset documentation) and the prep the data set for time series modeling by\naggregating data to a regular time step and filling gaps via a generated Spark SQL query in the AI\ncatalog. Once the dataset is uploaded, the time series data prep query generator can be created\nusing DataEngineQueryGenerator.create.\nAs of version v3.1 convenience methods have been added to streamline the process of applying\ntime series data prep for predictions.\nSee the following code block for example usage:\nimport datarobot as dr\nfrom datarobot.models.data_engine_query_generator import (\nQueryGeneratorDataset,\nQueryGeneratorSettings,\n)\nfrom datetime import datetime\n# upload the dataset to the AI Catalog\ndataset = dr.Dataset.create_from_file('input_data.csv')\n# create a time series data prep query generator\nquery_generator_dataset = QueryGeneratorDataset(\nalias='input_data_csv',\ndataset_id=dataset.id,\ndataset_version_id=dataset.version_id,\n)\nquery_generator_settings = QueryGeneratorSettings(\ndatetime_partition_column=\"date\",\ntime_unit=\"DAY\",\ntime_step=1,\ndefault_numeric_aggregation_method=\"sum\",\ndefault_categorical_aggregation_method=\"mostFrequent\",\ntarget=\"y\",\nmultiseries_id_columns=[\"id\"],\ndefault_text_aggregation_method=\"concat\",\nstart_from_series_min_datetime=True,\nend_to_series_max_datetime=True,\n)\nquery_generator = dr.DataEngineQueryGenerator.create(\ngenerator_type='TimeSeries',\ndatasets = [query_generator_dataset],\ngenerator_settings=query_generator_settings,\n)\n# prep the training dataset\ntraining_dataset = query_generator.create_dataset()\n# create a project\nproject = dr.Project.create_from_dataset(training_dataset.id, project_name='prepped_dataset')\n# set up datetime partitioning, target, and train model(s)\npartitioning_spec = dr.DatetimePartitioningSpecification(\ndatetime_partition_column='date', use_time_series=True\n)\nproject.analyze_and_model(target='y', mode='manual', partitioning_method=partitioning_spec)\nblueprints = project.get_blueprints()\nmodel_job = project.train_datetime(blueprints[0].id)\nmodel = model_job.get_result_when_complete()\n# query generator can be retrieved from the project if necessary\n# query_generator = dr.DataEngineQueryGenerator.get(project.query_generator_id)\n# prep and upload a prediction dataset to the project\nprediction_dataset = query_generator.prepare_prediction_dataset(\n'prediction_data.csv', project.id\n)\n# make predictions within the project\n# Either forecast point or predictions start/end dates must be specified\nmodel.request_predictions(prediction_dataset.id, forecast_point=datetime(2023, 1, 1))\n# query generator can be retrieved from a deployed model via project if necessary\n# deployment = dr.Deployment.get(deployment_id)\n# project = dr.Project.get(deployment.model['project_id'])\n# query_generator = dr.DataEngineQueryGenerator.get(project.query_generator_id)\n# Deploy the model\nprediction_servers = dr.PredictionServer.list()\ndeployment = dr.Deployment.create_from_learning_model(\nmodel.id, 'prepped_deployment', default_prediction_server_id=prediction_servers[0].id\n)\n# Make batch predictions from batch prediction job, supports localFile or dataset for intake\n# and all types for output\ntimeseries_settings = {'type': 'forecast', 'forecast_point': datetime(2023, 1, 1)}\nintake_settings = {'type': 'localFile', 'file': 'prediction_data.csv'}\noutput_settings = {'type': 'localFile', 'path': 'predictions_out.csv'}\nbatch_predictions_job = dr.BatchPredictionJob.apply_time_series_data_prep_and_score(\ndeployment, intake_settings, timeseries_settings, output_settings=output_settings\n)",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/time_series.html",
      "tags": [
        "advanced",
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/time_series.html",
        "content_length": 31493
      },
      "code_examples": [],
      "api_methods": [
        "model.compute_datetime_trend_plots",
        "model.get_anomaly_over_time_plots_metadata",
        "dr.datetimepartitioning.generate",
        "model.get_forecast_vs_actual_plot_preview",
        "deployment.get_prediction_intervals_settings",
        "model.get_forecast_vs_actual_plots_metadata",
        "deployment.get",
        "project.get_discarded_features",
        "model.get_accuracy_over_time_plots_metadata",
        "dr.project.create",
        "project.query_generator_id",
        "model.get_calculated_prediction_intervals",
        "deployment.create_from_learning_model",
        "dr.dataenginequerygenerator.create",
        "model.request_predictions",
        "datarobot.models.restore_discarded_features",
        "dr.deployment.get",
        "project.get_blueprints",
        "dr.dataset.create_from_file",
        "model.id",
        "dr.predictionserver.list",
        "model.get_accuracy_over_time_plot_preview",
        "deployment.model",
        "dr.dataenginequerygenerator.get",
        "datarobot.models.data_engine_query_generator",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "datarobot.datetimepartitioning.generate",
        "model.get_forecast_vs_actual_plot",
        "dr.batchpredictionjob.apply_time_series_data_prep_and_score",
        "project.get",
        "project.validate_external_time_series_baseline",
        "project.create",
        "model.get_anomaly_over_time_plot_preview",
        "project.set_options",
        "project.train_datetime",
        "project.set_datetime_partitioning",
        "dr.feature.get",
        "datarobot.models.batchpredictionjob",
        "model.get_anomaly_over_time_plot",
        "dr.calendarfile.create_calendar_from_country_code",
        "dr.calendarfile.get_allowed_country_codes",
        "project.target",
        "deployment.update_prediction_intervals_settings",
        "datarobot.enums.time_units",
        "model.calculate_prediction_intervals",
        "project.restore_discarded_features",
        "datarobot.models.feature",
        "dr.calendarfile.create",
        "dr.project.get",
        "datarobot.models.project",
        "project.analyze_and_model",
        "model.get_accuracy_over_time_plot",
        "project.create_from_dataset",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-3169615268729324763",
      "title": "Unsupervised Projects (Anomaly Detection)",
      "content": "Unsupervised Projects (Anomaly Detection)\nWhen the data is not labelled and the problem can be interpreted either as anomaly detection or time\nseries anomaly detection, projects in unsupervised mode become useful.\nCreating Unsupervised Projects\nIn order to create an unsupervised project set unsupervised_mode to True when setting the target.\n>>> import datarobot as dr\n>>> project = Project.create('dataset.csv', project_name='unsupervised')\n>>> project.analyze_and_model(unsupervised_mode=True)\nCreating Time Series Unsupervised Projects\nTo create a time series unsupervised project pass  unsupervised_mode=True to\ndatetime partitioning creation and to project aim. The forecast window will be automatically set to nowcasting,\ni.e. forecast distance zero (FW = 0, 0).\n>>> import datarobot as dr\n>>> project = Project.create('dataset.csv', project_name='unsupervised')\n>>> spec = DatetimePartitioningSpecification('date',\n...    use_time_series=True, unsupervised_mode=True,\n...    feature_derivation_window_start=-4, feature_derivation_window_end=0)\n# this step is optional - preview the default partitioning which will be applied\n>>> partitioning_preview = DatetimePartitioning.generate(project.id, spec)\n>>> full_spec = partitioning_preview.to_specification()\n# As of v3.0, can use ``Project.set_datetime_partitioning`` and ``Project.list_datetime_partitioning_spec`` instead\n>>> project.set_datetime_partitioning(datetime_partition_spec=spec)\n>>> project.list_datetime_partitioning_spec()\n# If ``Project.set_datetime_partitioning`` was used there is no need to pass ``partitioning_method`` in ``Project.analyze_and_model``\n>>> project.analyze_and_model(unsupervised_mode=True, partitioning_method=full_spec)\nUnsupervised Project Metrics\nIn unsupervised projects, metrics are not used for the model optimization. Instead, they are used for the\npurpose of model ranking. There are two available unsupervised metrics – Synthetic AUC and\nsynthetic LogLoss – both of which are calculated on artificially-labelled validation samples.\nEstimating Accuracy of Unsupervised Anomaly Detection Datetime Partitioned Models\nFor datetime partitioned unsupervised model you can retrieve the Anomaly over Time plot. To do so use\nDatetimeModel.get_anomaly_over_time_plot.\nYou can also retrieve the detailed metadata using DatetimeModel.get_anomaly_over_time_plots_metadata,\nand the preview plot using DatetimeModel.get_anomaly_over_time_plot_preview.\nExplaining Unsupervised Time Series Anomaly Detection Models Predictions\nWithin a timeseries unsupervised project for models supporting calculation of Shapley values,\nAnomaly Assessment insight can be computed to explain anomalies.\nExample 1: computation, retrieval and deletion of the anomaly assessment insight.\n>>> import datarobot as dr\n# Initialize Anomaly Assessment for the backtest 0, training subset and series \"series1\"\n>>> model = dr.DatetimeModel.get(project_id, model_id)\n>>> anomaly_assessment_record = model.initialize_anomaly_assessment(0, \"training\", \"series1\")\n# Get available Anomaly Assessment for the project and model\n>>> all_records = model.get_anomaly_assessment_records()\n# Get most recent anomaly assessment explanations\n>>> all_records[0].get_latest_explanations()\n# Get anomaly assessment explanations in the range\n>>> all_records[0].get_explanations(start_date=\"2020-01-01\", points_count=500)\n# Get anomaly assessment predictions preview\n>>> all_records[0].get_predictions_preview()\n# Delete record\n>>> all_records[0].delete()\nExample 2: Find explanations for the anomalous regions (regions with maximum anomaly score >=0.6)\nfor the multiseries project. Leave only explanations for the rows with anomaly score >= 0.5.\n>>> def collect_explanations(model, backtest, source, series_ids):\n... for series in series_ids:\n...     try:\n...         model.initialize_anomaly_assessment(backtest, source, series)\n...      except ClientError:\n...         # when insight was already computed\n...         pass\n... records_for_series = model.get_anomaly_assessment_records(source=source, backtest=backtest, with_data_only=True, limit=0)\n... result = {}\n... for record in records_for_series:\n...     preview = record.get_predictions_preview()\n...     anomalous_regions = preview.find_anomalous_regions(max_prediction_threshold=0.6)\n...     if anomalous_regions:\n...         result[record.series_id] = record.get_explanations_data_in_regions(anomalous_regions, prediction_threshold=0.5)\n... return result\n>>> import datarobot as dr\n>>> model = dr.DatetimeModel.get(project_id, model_id)\n>>> collect_explanations(model, 0, \"validation\", series_ids)\nAssessing Unsupervised Anomaly Detection Models on External Test Set\nIn unsupervised projects, if there is some labelled data, it may be used to assess anomaly detection\nmodels by checking computed classification metrics such as AUC and LogLoss, etc. and insights such as ROC and Lift.\nSuch data is uploaded as a prediction dataset with a specified actual value column name, and, if it\nis a time series project, a prediction date range. The actual value column can contain only zeros and ones or True/False,\nand it should not have been seen during training time.\nRequesting External Scores and Insights (Time Series)\nThere are two ways to specify an actual value column and compute scores and insights:\n1. Upload a prediction dataset, specifying predictions_start_date, predictions_end_date,\nand actual_value_column, and request predictions on that dataset using a specific model.\n>>> import datarobot as dr\n# Upload dataset\n>>> project = dr.Project(project_id)\n>>> dataset = project.upload_dataset(\n...    './data_to_predict.csv',\n...    predictions_start_date=datetime(2000, 1, 1),\n...    predictions_end_date=datetime(2015, 1, 1),\n...    actual_value_column='actuals'\n...    )\n# run prediction job which also will calculate requested scores and insights.\n>>> predict_job = model.request_predictions(dataset.id)\n# prediction output will have column with actuals\n>>> result = pred_job.get_result_when_complete()\n2. Upload a prediction dataset without specifying any options, and request predictions for a specific model with\npredictions_start_date, predictions_end_date, and actual_value_column specified.\nNote, these settings cannot be changed for the dataset after making predictions.\n>>> import datarobot as dr\n# Upload dataset\n>>> project = dr.Project(project_id)\n>>> dataset = project.upload_dataset('./data_to_predict.csv')\n# Check which columns are candidates for actual value columns\n>>> dataset.detected_actual_value_columns\n[{'missing_count': 25, 'name': 'label_column'}]\n# run prediction job which also will calculate requested scores and insights.\n>>> predict_job = model.request_predictions(\n...    dataset.id,\n...    predictions_start_date=datetime(2000, 1, 1),\n...    predictions_end_date=datetime(2015, 1, 1),\n...    actual_value_column='label_column'\n...  )\n>>> result = pred_job.get_result_when_complete()\nRequesting External Scores and Insights for AutoML models\nTo compute scores and insights on an external dataset for unsupervised AutoML models (Non Time series)\nUpload a prediction dataset that contains label column(s), request compute external test on one\nof PredictionDataset.detected_actual_value_columns\nimport datarobot as dr\n# Upload dataset\nproject = dr.Project(project_id)\ndataset = project.upload_dataset('./test_set.csv')\ndataset.detected_actual_value_columns\n>>>['label_column_1', 'label_column_2']\n# request external test to compute metric scores and insights on dataset\nexternal_test_job = model.request_external_test(dataset.id, actual_value_column='label_column_1')\n# once job is complete, scores and insights are ready for retrieving\nexternal_test_job.wait_for_completion()\nRetrieving External Scores and Insights\nUpon completion of prediction, external scores and insights can be retrieved to assess model\nperformance. For unsupervised projects Lift Chart and ROC Curve are computed.\nIf the dataset is too small insights will not be computed. If the actual value column contained\nonly one class, the ROC Curve will not be computed. Information about the dataset can be retrieved\nusing PredictionDataset.get.\n>>> import datarobot as dr\n# Check which columns are candidates for actual value columns\n>>> scores_list = ExternalScores.list(project_id)\n>>> scores = ExternalScores.get(project_id, dataset_id=dataset_id, model_id=model_id)\n>>> lift_list = ExternalLiftChart.list(project_id, model_id)\n>>> roc = ExternalRocCurve.get(project_id, model, dataset_id)\n# check dataset warnings, need to be called after predictions are computed.\n>>> dataset = PredictionDataset.get(project_id, dataset_id)\n>>> dataset.data_quality_warnings\n{'single_class_actual_value_column': True,\n'insufficient_rows_for_evaluating_models': False,\n'has_kia_missing_values_in_forecast_window': False}",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/unsupervised_anomaly.html",
      "tags": [
        "advanced",
        "api_reference",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/unsupervised_anomaly.html",
        "content_length": 8820
      },
      "code_examples": [],
      "api_methods": [
        "model.get",
        "model.initialize_anomaly_assessment",
        "project.create",
        "model.get_anomaly_over_time_plots_metadata",
        "model.get_anomaly_over_time_plot_preview",
        "project.list_datetime_partitioning_spec",
        "project.id",
        "model.request_external_test",
        "project.upload_dataset",
        "model.request_predictions",
        "project.set_datetime_partitioning",
        "project.analyze_and_model",
        "model.get_anomaly_assessment_records",
        "dr.datetimemodel.get",
        "model.get_anomaly_over_time_plot"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_3750487074715601870",
      "title": "Unsupervised Projects (Clustering)",
      "content": "Unsupervised Projects (Clustering)\nUse clustering when data is not labelled and the problem can be interpreted as grouping a set of\nobjects in such a way that objects in the same group (called a cluster) are more similar\nto each other than to those in other groups (clusters). It is a common task in data exploration\nwhen finding groups and similarities is needed.\nCreating Unsupervised Projects\nTo create an unsupervised project, set unsupervised_mode to True when setting the target.\nTo specify clustering, set unsupervised_type to CLUSTERING. When setting the modeling mode\nis required, clustering supports either AUTOPILOT_MODE.COMPREHENSIVE for DataRobot-run Autopilot\nor AUTOPILOT_MODE.MANUAL for user control of which models/parameters to use.\nExample:\nfrom datarobot import Project\nfrom datarobot.enums import UnsupervisedTypeEnum\nfrom datarobot.enums import AUTOPILOT_MODE\nproject = Project.create(\"dataset.csv\", project_name=\"unsupervised clustering\")\nproject.analyze_and_model(\nunsupervised_mode=True,\nmode=AUTOPILOT_MODE.COMPREHENSIVE,\nunsupervised_type=UnsupervisedTypeEnum.CLUSTERING,\n)\nYou can optionally specify list of explicit cluster numbers. To do this, pass a list of integer\nvalues to optional autopilot_cluster_list parameter using the analyze_and_model() method.\nproject.analyze_and_model(\nunsupervised_mode=True,\nmode=AUTOPILOT_MODE.COMPREHENSIVE,\nunsupervised_type=UnsupervisedTypeEnum.CLUSTERING,\nautopilot_cluster_list=[7, 9, 11, 15, 19],\n)\nYou can also do both in one step using the Project.start() method. This method by default will\nuse AUTOPILOT_MODE.COMPREHENSIVE mode.\nfrom datarobot import Project\nfrom datarobot.enums import UnsupervisedTypeEnum\nproject = Project.start(\n\"dataset.csv\",\nunsupervised_mode=True,\nproject_name=\"unsupervised clustering project\",\nunsupervised_type=UnsupervisedTypeEnum.CLUSTERING,\n)\nUnsupervised Clustering Project Metric\nUnsupervised clustering projects use the Silhouette Score metric for model ranking (instead of\nusing it for model optimization). It measures the average similarity of objects within a cluster\nand their distance to the other objects in the other clusters.\nRetrieving information about Clusters\nIn a trained model, you can retrieve information about clusters in along with standard model\ninformation. To do this, when training completes, retrieve a model and view basic clustering\ninformation:\nn_clusters : number of clusters for model\nis_n_clusters_dynamically_determined : how clustering model picks number of clusters\nHere is a code snippet to retrieve information about the number of clusters for model:\nfrom datarobot import ClusteringModel\nmodel = ClusteringModel.get(project_id, model_id)\nprint(\"{} clusters found\".format(model.n_clusters))\nYou can retrieve more details about clusters and their data using cluster insights.\nWorking with Clusters Insights\nYou can compute insights to gain deep insights into clusters and their characteristics. This\nprocess will perform calculations and return detailed information about each feature and its\nimportance, as well as a detailed per-cluster breakdown.\nTo compute and retrieve cluster insights, use the ClusteringModel and its compute_insights\nmethod. The method starts the cluster insights compute job, waits for its completion for the number\nof seconds specified in the optional parameter max_wait (default: 600), and returns results\nwhen insights are ready.\nIf clusters are already computed,  access them using the insights property of the\nClusteringModel method.\nfrom datarobot import ClusteringModel\nmodel = ClusteringModel.get(project_id, model_id)\ninsights = model.compute_insights()\nThis call, with the specified wait_time, will run and wait for specified time:\nfrom datarobot import ClusteringModel\nmodel = ClusteringModel.get(project_id, model_id)\ninsights = model.compute_insights(max_wait=60)\nIf computation fails to finish before max_wait expires, the method will raise\nan AsyncTimeoutError. You can retrieve cluster insights after jobs computation finishes.\nTo retrieve cluster insights already computed:\nfrom datarobot import ClusteringModel\nmodel = ClusteringModel.get(project_id, model_id)\nfor insight in model.insights:\nprint(insight)\nWorking with Clusters\nBy default, DataRobot names clusters “Cluster 1”, “Cluster 2”, … , “Cluster N” .\nYou can retrieve these names and alter them according to preference. When retrieving\nclusters before computing insights, clusters will contain only names. After insight computation\ncompletes, each cluster will also hold information about the percentage of data that is represented\nby the Cluster.\nFor example:\nfrom datarobot import ClusteringModel\nmodel = ClusteringModel.get(project_id, model_id)\n# helper function\ndef print_summary(name, percent):\nif not percent:\npercent = \"?\"\nprint(\"'{}' holds {} % of data\".format(name, percent))\nfor cluster in model.clusters:\nprint_summary(cluster.name, cluster.percent)\nmodel.compute_insights()\nfor cluster in model.clusters:\nprint_summary(cluster.name, cluster.percent)\nFor a model with three clusters, the code snippet will output:\n'Cluster 1' holds ? % of data\n'Cluster 2' holds ? % of data\n'Cluster 3' holds ? % of data\n-- Cluster insights computation finished --\n'Cluster 1' holds 27.1704180064 % of data\n'Cluster 2' holds 36.9131832797 % of data\n'Cluster 3' holds 35.9163987138 % of data\nUse the following methods of ClusteringModel class to alter cluster names:\nupdate_cluster_names - changes multiple cluster names using mapping in dictionary\nupdate_cluster_name - changes one cluster name\nAfter update, each method will return a list of clusters with changed names.\nFor example:\nfrom datarobot import ClusteringModel\nmodel = ClusteringModel.get(project_id, model_id)\n# update multiple\ncluster_name_mappings = [\n(\"Cluster 1\", \"AAA\"),\n(\"Cluster 2\", \"BBB\"),\n(\"Cluster 3\", \"CCC\")\n]\nclusters = model.update_cluster_names(cluster_name_mappings)\n# update single\nclusters = model.update_cluster_name(\"CCC\", \"DDD\")\nClustering Classes Reference\nClusteringModel\nclass datarobot.models.model.ClusteringModel\nClusteringModel extends Model class.\nIt provides provides properties and methods specific to clustering projects.\ncompute_insights(max_wait=600)\nCompute and retrieve cluster insights for model. This method awaits completion of\njob computing cluster insights and returns results after it is finished. If computation\ntakes longer than specified max_wait exception will be raised.\nParameters:\nproject_id (str) – Project to start creation in.\nmodel_id (str) – Project’s model to start creation in.\nmax_wait (int) – Maximum number of seconds to wait before giving up\nReturn type:\nList of ClusterInsight\nRaises:\nClientError – Server rejected creation due to client error.\nMost likely cause is bad project_id or model_id.\nAsyncFailureError – If any of the responses from the server are unexpected\nAsyncProcessUnsuccessfulError – If the cluster insights computation has failed or was cancelled.\nAsyncTimeoutError – If the cluster insights computation did not resolve in time\nproperty insights: List[ClusterInsight]\nReturn actual list of cluster insights if already computed.\nReturn type:\nList of ClusterInsight\nproperty clusters: List[Cluster]\nReturn actual list of Clusters.\nReturn type:\nList of Cluster\nupdate_cluster_names(cluster_name_mappings)\nChange many cluster names at once based on list of name mappings.\nParameters:\ncluster_name_mappings (List of tuples) – Cluster names mapping consisting of current cluster name and old cluster name.\nExample:\ncluster_name_mappings = [\n(\"current cluster name 1\", \"new cluster name 1\"),\n(\"current cluster name 2\", \"new cluster name 2\")]\nReturn type:\nList of Cluster\nRaises:\ndatarobot.errors.ClientError – Server rejected update of cluster names.\nPossible reasons include: incorrect format of mapping, mapping introduces duplicates.\nupdate_cluster_name(current_name, new_name)\nChange cluster name from current_name to new_name.\nParameters:\ncurrent_name (str) – Current cluster name.\nnew_name (str) – New cluster name.\nReturn type:\nList of Cluster\nRaises:\ndatarobot.errors.ClientError – Server rejected update of cluster names.\nCluster\nclass datarobot.models.model.Cluster\nRepresentation of a single cluster.\nVariables:\nname (str) – Current cluster name\npercent (float) – Percent of data contained in the cluster. This value is reported after cluster insights\nare computed for the model.\nclassmethod list(project_id, model_id)\nRetrieve a list of clusters in the model.\nParameters:\nproject_id (str) – ID of the project that the model is part of.\nmodel_id (str) – ID of the model.\nReturn type:\nList of clusters\nclassmethod update_multiple_names(project_id, model_id, cluster_name_mappings)\nUpdate many clusters at once based on list of name mappings.\nParameters:\nproject_id (str) – ID of the project that the model is part of.\nmodel_id (str) – ID of the model.\ncluster_name_mappings (List of tuples) – Cluster name mappings, consisting of current and previous names for each cluster.\nExample:\ncluster_name_mappings = [\n(\"current cluster name 1\", \"new cluster name 1\"),\n(\"current cluster name 2\", \"new cluster name 2\")]\nReturn type:\nList of clusters\nRaises:\ndatarobot.errors.ClientError – Server rejected update of cluster names.\nValueError – Invalid cluster name mapping provided.\nclassmethod update_name(project_id, model_id, current_name, new_name)\nChange cluster name from current_name to new_name\nParameters:\nproject_id (str) – ID of the project that the model is part of.\nmodel_id (str) – ID of the model.\ncurrent_name (str) – Current cluster name\nnew_name (str) – New cluster name\nReturn type:\nList of Cluster\nClusterInsight\nclass datarobot.models.model.ClusterInsight\nHolds data on all insights related to feature as well as breakdown per cluster.\nParameters:\nfeature_name (str) – Name of a feature from the dataset.\nfeature_type (str) – Type of feature.\ninsights (List[ClusterInsight]) – List provides information regarding the importance of a specific feature in relation\nto each cluster. Results help understand how the model is grouping data and what each\ncluster represents.\nfeature_impact (float) – Impact of a feature ranging from 0 to 1.\nclassmethod compute(project_id, model_id, max_wait=600)\nStarts creation of cluster insights for the model and if successful, returns computed\nClusterInsights. This method allows calculation to continue for a specified time and\nif not complete, cancels the request.\nParameters:\nproject_id (str) – ID of the project to begin creation of cluster insights for.\nmodel_id (str) – ID of the project model to begin creation of cluster insights for.\nmax_wait (int) – Maximum number of seconds to wait canceling the request.\nReturn type:\nList[ClusterInsight]\nRaises:\nClientError – Server rejected creation due to client error.\nMost likely cause is bad project_id or model_id.\nAsyncFailureError – Indicates whether any of the responses from the server are unexpected.\nAsyncProcessUnsuccessfulError – Indicates whether the cluster insights computation failed or was cancelled.\nAsyncTimeoutError – Indicates whether the cluster insights computation did not resolve within the specified\ntime limit (max_wait).",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/unsupervised_clustering.html",
      "tags": [
        "api-reference",
        "api_reference",
        "advanced",
        "beginner",
        "example"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/unsupervised_clustering.html",
        "content_length": 11142
      },
      "code_examples": [],
      "api_methods": [
        "model.get",
        "project.create",
        "model.clusters",
        "model.cluster",
        "model.update_cluster_names",
        "model.n_clusters",
        "model.compute_insights",
        "model.update_cluster_name",
        "model.clusterinsight",
        "project.start",
        "project.analyze_and_model",
        "model.clusteringmodel",
        "datarobot.models.model",
        "model.insights",
        "datarobot.errors.clienterror"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-3505383925526895894",
      "title": "Predictions",
      "content": "Predictions\nThe following sections describe the components to making predictions in DataRobot:\nGenerate predictions: Initiate a prediction job with the  Model.request_predictions() method. This method can use either a training dataset or predictions dataset for scoring.\nBatch predictions: Score large sets of data with batch predictions. You can define jobs and their schedule.\nPrediction API: Use DataRobot’s Prediction API. to make predictions on both a dedicated and/or a standalone prediction server.\nScoring Code: Qualifying models allow you to export Scoring Code and use DataRobot-generated models outside of the platform",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/index.html",
        "content_length": 630
      },
      "code_examples": [],
      "api_methods": [
        "model.request_predictions"
      ],
      "complexity_score": 0.4,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-192591006234023564",
      "title": "Visual AI projects",
      "content": "Visual AI projects\nWith Visual AI, DataRobot allows you to use image data for modeling. You can create projects with one\nor multiple image features and also mix them with other DataRobot-supported feature types. You can\nfind more information about\nVisual AI\nin the Platform documentation.\nCreate a Visual AI project\nDataRobot offers you different ways to prepare your dataset and to start a Visual AI project. The\nvarious ways to do this are covered in detail in the documentation,\nPreparing the dataset.\nFor the examples given here the images are partitioned into named\ndirectories. In the following, images are partitioned into named directories, which serve as labels\nfor the project. For example, to predict on images of cat and dog breeds, labels could be\nabyssinian, american_bulldog, etc.\n/home/user/data/imagedataset\n├── abyssinian\n│   ├── abyssinian01.jpg\n│   ├── abyssinian02.jpg\n│   ├── …\n├── american_bulldog\n│   ├── american_bulldog01.jpg\n│   ├── american_bulldog02.jpg\n│   ├── …\nYou then compress the directory containing the named directories into a\nZIP file, creating the dataset used for the project.\nfrom datarobot.models import Project, Dataset\ndataset = Dataset.create_from_file(file_path='/home/user/data/imagedataset.zip')\nproject = Project.create_from_dataset(dataset.id, project_name='My Image Project')\nTarget\nSince this example uses named directories the target name must be\nclass, which will contain the name of each directory in the ZIP\nfile.\nOther parameters\nSetting modeling parameters, such as partitioning method, queue mode,\netc, functions in the same way as starting a non-image project.\nStart modeling\nOnce you have set modeling parameters, use the following code snippet\nto specify parameters and start the modeling process.\nfrom datarobot import AUTOPILOT_MODE\nproject.analyze_and_model(target='class', mode=AUTOPILOT_MODE.QUICK)\nYou can also pass optional parameters to project.analyze_and_model\nto change aspects of the modeling process. Some of those parameters\ninclude:\nworker_count – int, sets the number of workers used for modeling.\npartitioning_method – PartitioningMethod object.\nFor a full reference of available parameters, see\nProject.analyze_and_model.\nYou can use the mode parameter to set the Autopilot mode.\nAUTOPILOT_MODE.FULL_AUTO, is the default, triggers modeling\nwith no further actions necessary. Other accepted modes include\nAUTOPILOT_MODE.MANUAL for manual mode (choose your own models to run\nrather than running the full Autopilot) and AUTOPILOT_MODE.QUICK to\nrun on a more limited set of models and get insights more quickly\n(“quick run”).\nInteract with a Visual AI project\nThe following code snippets may be used to access Visual AI images and\ninsights.\nList sample images\nSample images allow you to see a subset of images, chosen by DataRobot,\nin the dataset. The returned SampleImage objects have an associated\ntarget_value that will allow you to categorize the images (abyssinian, american_bulldog, etc).\nUntil you set the target and EDA2 has finished, the target_value will be None.\nimport io\nimport PIL.Image\nfrom datarobot.models.visualai import SampleImage\ncolumn_name = \"image\"\nnumber_of_images_to_show = 5\nfor sample in SampleImage.list(project.id, column_name)[:number_of_images_to_show]:\n# Display the image in the GUI\nbio = io.BytesIO(sample.image.image_bytes)\nimg = PIL.Image.open(bio)\nimg.show()\nThe results would be images such as:\nList duplicate images\nDuplicate images, images with different names but are determined by DataRobot\nto be the same, may exist in a dataset. If this happens, the code returns\none of the images and the number of times it occurs in the dataset.\nfrom datarobot.models.visualai import DuplicateImage\ncolumn_name = \"image\"\nfor duplicate in DuplicateImage.list(project.id, column_name):\n# To show an image see the previous sample image example\nprint(f\"Image id = {duplicate.image.id} has {duplicate.count} duplicates\")\nActivation maps\nActivation maps are overlaid on the images to show which image areas are driving model prediction\ndecisions.\nDetailed explanations are available in DataRobot Platform\ndocumentation, Model insights.\nCompute activation maps\nTo begin, you must first compute activation maps. The following snippet is an example of starting\nthe computation for a Keras model in a Visual AI project. The compute method returns a URL that\ncan be used to determine when the computation completes.\nfrom datarobot.models.visualai import ImageActivationMap\nkeras_model = project.get_models(search_params={'name': 'Keras'})[0]\nstatus_url = ImageActivationMap.compute(project.id, keras_model.id)\nprint(status_url)\nList activation maps\nAfter activation maps are computed, you can download them from the\nDataRobot server. The following snippet is an example of how to get the\nactivation maps and how to plot them.\nimport PIL.Image\nfrom datarobot.models.visualai import ImageActivationMap\ncolumn_name = \"image\"\nmax_activation_maps = 5\nkeras_model = project.get_models(search_params={'name': 'Keras'})[0]\nfor activation_map in ImageActivationMap.list(project.id, keras_model.id, column_name)[:max_activation_maps]:\nbio = io.BytesIO(activation_map.overlay_image.image_bytes)\nimg = PIL.Image.open(bio)\nimg.show()\nImage embeddings\nImage embeddings allow you to get an impression on how similar two images look to a featurizer\nnetwork. The embeddings project images from their high-dimensional feature space onto a 2D plane.\nThe closer the images appear in this plane, the more similar they look to the featurizer.\nDetailed explanations are available in the DataRobot Platform documentation,\nModel insights.\nCompute image embeddings\nYou must compute image embeddings before retrieving. The following snippet\nis an example of starting the computation for a Keras model in our Visual AI project. The\ncompute method returns a URL that can be used to determine when the computation is complete.\nfrom datarobot.models.visualai import ImageEmbedding\nkeras_model = project.get_models(search_params={'name': 'Keras'})[0]\nstatus_url = ImageEmbedding.compute(project.id, keras_model.id)\nprint(status_url)\nList image embeddings\nAfter image embeddings are computed, you can download them from the\nDataRobot server. The following snippet is an example of how to get the\nembeddings for a model and plot them.\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nfrom datarobot.models.visualai import ImageEmbedding\ncolumn_name = \"image\"\nkeras_model = project.get_models(search_params={'name': 'Keras'})[0]\nzoom = 0.15\nfig, ax = plt.subplots(figsize=(15,10))\nfor image_embedding in ImageEmbedding.list(project.id, keras_model.id, column_name):\nimage_bytes = image_embedding.image.image_bytes\nx_position = image_embedding.position_x\ny_position = image_embedding.position_y\nimage = PIL.Image.open(io.BytesIO(image_bytes))\noffset_image = OffsetImage(np.array(image), zoom=zoom)\nannotation_box = AnnotationBbox(offset_image, (x_position, y_position), xycoords='data', frameon=False)\nax.add_artist(annotation_box)\nax.update_datalim([(x_position, y_position)])\nax.autoscale()\nax.grid(True)\nfig.show()\nImage augmentation\nImage Augmentation is a processing step in the DataRobot blueprint that creates new images for\ntraining by randomly transforming existing images, thereby increasing the size of\n(i.e., “augmenting”) the training data.\nDetailed explanations are available in the DataRobot Platform documentation,\nCreating augmented models.\nCreate image augmentation list\nTo create image augmentation samples, you need to provide an image augmentation list. This list holds all\ninformation required to compute image augmentation samples. The following snippet shows\nhow to create an image augmentation list. It is then used to compute image augmentation samples.\nfrom datarobot.models.visualai import ImageAugmentationList\nblur_param = {\"name\": \"maximum_filter_size\", \"currentValue\": 10}\nblur = {\"name\": \"blur\", \"params\": [blur_param]}\nflip = {\"name\": \"horizontal_flip\", \"params\": []}\nimage_augmentation_list = ImageAugmentationList.create(\nname=\"my blur and flip augmentation list\",\nproject_id=project.id,\nfeature_name=\"image\",\ntransformation_probability=0.5,\nnumber_of_new_images=5,\ntransformations=[blur, flip],\n)\nprint(image_augmentation_list)\nList image augmentation lists\nYou can retrieve all available augmentation lists for a project by project_id.\nfrom datarobot.models.visualai import ImageAugmentationList\nimage_augmentation_lists = ImageAugmentationList.list(\nproject_id=project.id\n)\nprint(image_augmentation_lists)\nCompute and retrieve image augmentation samples\nYou must compute image augmentation samples before retrieving. To compute image augmentation sample, you will need an image augmentation list. This list holds all parameters and transformation information\nneeded to compute samples. You can either create a new one or retrieve an existing one.\nThe following snippet is an example of computing and retrieving image augmentation samples. It\nuses the previous snippet that creates an image augmentation list, but instead uses it to compute and\nretrieve image augmentation samples using the compute_samples method.\nfrom datarobot.models.visualai import ImageAugmentationList, ImageAugmentationSample\nimage_augmentation_list = ImageAugmentationList.get('<image_augmentation_list_id>')\nfor sample in image_augmentation_list.compute_samples():\n# Display the image in popup widows\nbio = io.BytesIO(sample.image.image_bytes)\nimg = PIL.Image.open(bio)\nimg.show()\nList image augmentation samples\nIf image augmentation samples were already computed instead of recomputing them we can retrieve\nthe last sample that was computed for image augmentation list from DataRobot server. The following\nsnippet is an example of how to get the image augmentation samples.\nimport io\nimport PIL.Image\nfrom datarobot.models.visualai import ImageAugmentationList\nimage_augmentation_list = ImageAugmentationList.get('<image_augmentation_list_id>')\nfor sample in image_augmentation_list.retrieve_samples():\n# Display the image in popup widows\nbio = io.BytesIO(sample.image.image_bytes)\nimg = PIL.Image.open(bio)\nimg.show()\nConfigure augmentations to use during training\nIn order to automatically augment a dataset during training the DataRobot server will\nlook for an augmentation list associated with the project that has the key\ninitial_list set to True.  An augmentation list like this can be created with the\nfollowing code snippet.  If it is created for the project before autopilot is started.\nit will be used to automatically augment the images in the training dataset.\nfrom datarobot.models.visualai import ImageAugmentationList\nblur_param = {\"name\": \"maximum_filter_size\", \"currentValue\": 10}\nblur = {\"name\": \"blur\", \"params\": [blur_param]}\nflip = {\"name\": \"horizontal_flip\", \"params\": []}\ntransforms_to_apply = ImageAugmentationList.create(name=\"blur and scale\", project_id=project.id,\nfeature_name='image', transformation_probability=0.5, number_of_new_images=5,\ntransformations=[blur, flip], initial_list=True)\nDetermine available transformations for augmentations\nThe Augmentation List in the example above supports horizontal flip and blur transformations,\nbut DataRobot supports several other transformations. To retrieve the list of supported\ntransformations use the ImageAugmentationOptions object as the example below shows.\nfrom datarobot.models.visualai import ImageAugmentationOptions\noptions = ImageAugmentationOptions.get(project.id)\nConverting images to base64-encoded strings for predictions\nIf your training dataset contained images, images in the prediction dataset need to be converted\nto a base64-encoded strings so it can be fully contained in the prediction request (for example, in a CSV\nfile or JSON). For more detail, see: working with binary data\nLicense\nFor the examples here we used the\nThe Oxford-IIIT Pet Dataset licensed under\nCreative Commons Attribution-ShareAlike 4.0 International License",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/visualai.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/visualai.html",
        "content_length": 12007
      },
      "code_examples": [],
      "api_methods": [
        "model.id",
        "datarobot.models.visualai",
        "project.id",
        "project.analyze_and_model",
        "project.get_models",
        "project.create_from_dataset"
      ],
      "complexity_score": 0.95,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_994698598427164071",
      "title": "Batch Predictions",
      "content": "Batch Predictions\nThe Batch Prediction API provides a way to score large datasets using flexible options\nfor intake and output on the Prediction Servers you have already deployed.\nThe main features are:\nFlexible options for intake and output.\nStream local files and start scoring while still uploading - while simultaneously downloading the results.\nScore large datasets from and to S3.\nConnect to your database using JDBC with bidirectional streaming of scoring data and results.\nIntake and output options can be mixed and doesn’t need to match. So scoring from a JDBC source to an S3 target is also an option.\nProtection against overloading your prediction servers with the option to control the concurrency level for scoring.\nPrediction Explanations can be included (with option to add thresholds).\nPassthrough Columns are supported to correlate scored data with source data.\nPrediction Warnings can be included in the output.\nTo interact with Batch Predictions, you should use the BatchPredictionJob class.\nMake batch predictions with a deployment\nDataRobot provides a utility function to make batch predictions using a deployment: Deployment.predict_batch.\nimport datarobot as dr\ndeployment = dr.Deployment.get(deployment_id='5c939e08962d741e34f609f0')\n# To note: `source` can be a file path, a file or a pandas DataFrame\nprediction_results_as_dataframe = deployment.predict_batch(\nsource=\"./my_local_file.csv\",\n)\nScoring local CSV files\nWe provide a small utility function for scoring from/to local CSV files: BatchPredictionJob.score_to_file.\nThe first parameter can be either:\nPath to a CSV dataset\nFile-like object\nPandas DataFrame\nFor larger datasets, you should avoid using a DataFrame, as that will load\nthe entire dataset into memory. The other options don’t.\nimport datarobot as dr\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\ndr.BatchPredictionJob.score_to_file(\ndeployment_id,\n'./data_to_predict.csv',\n'./predicted.csv',\n)\nThe input file will be streamed to our API and scoring will start immediately.\nAs soon as results start coming in, we will initiate the download concurrently.\nThe entire call will block until the file has been scored.\nScoring from and to S3\nWe provide a small utility function for scoring from/to CSV files hosted on S3 BatchPredictionJob.score_s3.\nThis requires that the intake and output buckets share the same credentials (see Credentials\nand Credential.create_s3) or that their access policy is set to public:\nimport datarobot as dr\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\ncred = dr.Credential.get('5a8ac9ab07a57a0001be501f')\njob = dr.BatchPredictionJob.score_s3(\ndeployment=deployment_id,\nsource_url='s3://mybucket/data_to_predict.csv',\ndestination_url='s3://mybucket/predicted.csv',\ncredential=cred,\n)\nNote\nThe S3 output functionality has a limit of 100 GB.\nScoring from and to Azure Cloud Storage\nLike with S3, we provide the same support for Azure through the utility function BatchPredictionJob.score_azure.\nThis required that an Azure connection string has been added to the DataRobot credentials store.\n(see Credentials and Credential.create_azure)\nimport datarobot as dr\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\ncred = dr.Credential.get('5a8ac9ab07a57a0001be501f')\njob = dr.BatchPredictionJob.score_azure(\ndeployment=deployment_id,\nsource_url='https://mybucket.blob.core.windows.net/bucket/data_to_predict.csv',\ndestination_url='https://mybucket.blob.core.windows.net/results/predicted.csv',\ncredential=cred,\n)\nScoring from and to Google Cloud Platform\nLike with Azure, we provide the same support for GCP through the utility function BatchPredictionJob.score_gcp.\nThis required that an Azure connection string has been added to the DataRobot credentials store. (see Credentials and\nCredential.create_gcp)\nimport datarobot as dr\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\ncred = dr.Credential.get('5a8ac9ab07a57a0001be501f')\njob = dr.BatchPredictionJob.score_gcp(\ndeployment=deployment_id,\nsource_url='gs:/bucket/data_to_predict.csv',\ndestination_url='gs://results/predicted.csv',\ncredential=cred,\n)\nWiring a Batch Prediction Job manually\nIf you can’t use any of the utilities above, you are also free to configure\nyour job manually. This requires configuring an intake and output option:\nimport datarobot as dr\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\ndr.BatchPredictionJob.score(\ndeployment_id,\nintake_settings={\n'type': 's3',\n'url': 's3://public-bucket/data_to_predict.csv',\n'credential_id': '5a8ac9ab07a57a0001be501f',\n},\noutput_settings={\n'type': 'localFile',\n'path': './predicted.csv',\n},\n)\nCredentials may be created with Credentials API.\nSupported intake types\nThese are the supported intake types and descriptions of their configuration parameters:\nLocal file intake\nThis requires you to pass either a path to a CSV dataset, file-like object or a Pandas\nDataFrame as the file parameter:\nintake_settings={\n'type': 'localFile',\n'file': './data_to_predict.csv',\n}\nS3 CSV intake\nThis requires you to pass an S3 URL to the CSV file your scoring in the url parameter:\nintake_settings={\n'type': 's3',\n'url': 's3://public-bucket/data_to_predict.csv',\n}\nIf the bucket is not publicly accessible, you can supply AWS credentials using the three\nparameters:\naws_access_key_id\naws_secret_access_key\naws_session_token\nAnd save it to the Credential API. Here is an example:\nimport datarobot as dr\n# get to make sure it exists\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\nintake_settings={\n'type': 's3',\n'url': 's3://private-bucket/data_to_predict.csv',\n'credential_id': cred.credential_id,\n}\nJDBC intake\nThis requires you to create a DataStore and\nCredential for your database:\n# get to make sure it exists\ndatastore_id = '5a8ac9ab07a57a0001be5010'\ndata_store = dr.DataStore.get(datastore_id)\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\nintake_settings = {\n'type': 'jdbc',\n'table': 'table_name',\n'schema': 'public', # optional, if supported by database\n'catalog': 'master', # optional, if supported by database\n'data_store_id': data_store.id,\n'credential_id': cred.credential_id,\n}\nBigQuery intake\nThis requires you to create a GCS Credential for your database:\n# get to make sure it exists\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\nintake_settings = {\n'type': 'bigquery',\n'dataset': 'dataset_name',\n'table': 'table_or_view_name',\n'bucket': 'bucket_in_gcs',\n'credential_id': cred.credential_id,\n}\nAI Catalog intake\nThis requires you to create a Dataset and identify the dataset_id of that to use as input.\n# get to make sure it exists\ndataset_id = '5a8ac9ab07a57a0001be501f'\ndataset = dr.Dataset.get(dataset_id)\nintake_settings={\n'type': 'dataset',\n'dataset': dataset\n}\nOr, in case you want another version_id than the latest, supply your own.\n# get to make sure it exists\ndataset_id = '5a8ac9ab07a57a0001be501f'\ndataset = dr.Dataset.get(dataset_id)\nintake_settings={\n'type': 'dataset',\n'dataset': dataset,\n'dataset_version_id': 'another_version_id'\n}\nDatasphere intake\nThis requires you to create a DataStore and\nCredential for your database:\n# get to make sure it exists\ndatastore_id = '5a8ac9ab07a57a0001be5011'\ndata_store = dr.DataStore.get(datastore_id)\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\nintake_settings = {\n'type': 'datasphere',\n'table': 'table_name',\n'schema': 'DATASPHERE_SPACE_NAME',\n'data_store_id': data_store.id,\n'credential_id': cred.credential_id,\n}\nSupported output types\nThese are the supported output types and descriptions of their configuration parameters:\nLocal file output\nFor local file output you have two options. You can either pass a path parameter and\nhave the client block and download the scored data concurrently. This is the fastest way\nto get predictions as it will upload, score and download concurrently:\noutput_settings={\n'type': 'localFile',\n'path': './predicted.csv',\n}\nAnother option is to leave out the parameter and subsequently call BatchPredictionJob.download\nat your own convenience. The BatchPredictionJob.score call will then return as soon as the upload is complete.\nIf the job is not finished scoring, the call to BatchPredictionJob.download will start\nstreaming the data that has been scored so far and block until more data is available.\nYou can poll for job completion using BatchPredictionJob.get_status or use\nBatchPredictionJob.wait_for_completion to wait.\nimport datarobot as dr\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\njob = dr.BatchPredictionJob.score(\ndeployment_id,\nintake_settings={\n'type': 'localFile',\n'file': './data_to_predict.csv',\n},\noutput_settings={\n'type': 'localFile',\n},\n)\njob.wait_for_completion()\nwith open('./predicted.csv', 'wb') as f:\njob.download(f)\nS3 CSV output\nThis requires you to pass an S3 URL to the CSV file where the scored data should be saved\nto in the url parameter:\noutput_settings={\n'type': 's3',\n'url': 's3://public-bucket/predicted.csv',\n}\nMost likely, the bucket is not publicly accessible for writes, but you can supply AWS\ncredentials using the three parameters:\naws_access_key_id\naws_secret_access_key\naws_session_token\nAnd save it to the Credential API. Here is an example:\n# get to make sure it exists\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\noutput_settings={\n'type': 's3',\n'url': 's3://private-bucket/predicted.csv',\n'credential_id': cred.credential_id,\n}\nJDBC output\nSame as for the input, this requires you to create a DataStore and\nCredential for your database, but for output_settings you also need to specify\nstatement_type, which should be one of datarobot.enums.AVAILABLE_STATEMENT_TYPES:\n# get to make sure it exists\ndatastore_id = '5a8ac9ab07a57a0001be5010'\ndata_store = dr.DataStore.get(datastore_id)\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\noutput_settings = {\n'type': 'jdbc',\n'table': 'table_name',\n'schema': 'public', # optional, if supported by database\n'catalog': 'master', # optional, if supported by database\n'statement_type': 'insert',\n'data_store_id': data_store.id,\n'credential_id': cred.credential_id,\n}\nBigQuery output\nSame as for the input, this requires you to create a GCS Credential\nto access BigQuery:\n# get to make sure it exists\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\noutput_settings = {\n'type': 'bigquery',\n'dataset': 'dataset_name',\n'table': 'table_name',\n'bucket': 'bucket_in_gcs',\n'credential_id': cred.credential_id,\n}\nDatasphere output\nSame as for the input, this requires you to create a DataStore and\nCredential for your database:\n# get to make sure it exists\ndatastore_id = '5a8ac9ab07a57a0001be5010'\ndata_store = dr.DataStore.get(datastore_id)\ncredential_id = '5a8ac9ab07a57a0001be501f'\ncred = dr.Credential.get(credential_id)\noutput_settings = {\n'type': 'datasphere',\n'table': 'table_name',\n'schema': 'DATASPHERE_SPACE_NAME',\n'data_store_id': data_store.id,\n'credential_id': cred.credential_id,\n}\nCopying a previously submitted job\nWe provide a small utility function for submitting a job using parameters from a job previously submitted:\nBatchPredictionJob.score_from_existing.\nThe first parameter is the job id of another job.\nimport datarobot as dr\npreviously_submitted_job_id = '5dc5b1015e6e762a6241f9aa'\ndr.BatchPredictionJob.score_from_existing(\npreviously_submitted_job_id,\n)\nScoring an in-memory Pandas DataFrame\nWhen working with DataFrames, we provide a method for scoring the data without first writing it to a\nCSV file and subsequently reading the data back from a CSV file.\nThis will also take care of joining the computed predictions into the existing DataFrame.\nUse the method BatchPredictionJob.score_pandas.\nThe first parameter is the deployment ID and then the DataFrame to score.\nimport datarobot as dr\nimport pandas as pd\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\ndf = pd.read_csv('testdata/titanic_predict.csv')\njob, df = dr.BatchPredictionJob.score_pandas(deployment_id, df)\nThe method returns a copy of the job status and the updated DataFrame with the predictions added.\nSo your DataFrame will now contain the following extra columns:\nSurvived_1_PREDICTION\nSurvived_0_PREDICTION\nSurvived_PREDICTION\nTHRESHOLD\nPOSITIVE_CLASS\nprediction_status\nprint(df)\nPassengerId  Pclass                                          Name  ... Survived_PREDICTION  THRESHOLD  POSITIVE_CLASS\n0            892       3                              Kelly, Mr. James  ...                   0        0.5               1\n1            893       3              Wilkes, Mrs. James (Ellen Needs)  ...                   1        0.5               1\n2            894       2                     Myles, Mr. Thomas Francis  ...                   0        0.5               1\n3            895       3                              Wirz, Mr. Albert  ...                   0        0.5               1\n4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  ...                   1        0.5               1\n..           ...     ...                                           ...  ...                 ...        ...             ...\n413         1305       3                            Spector, Mr. Woolf  ...                   0        0.5               1\n414         1306       1                  Oliva y Ocana, Dona. Fermina  ...                   0        0.5               1\n415         1307       3                  Saether, Mr. Simon Sivertsen  ...                   0        0.5               1\n416         1308       3                           Ware, Mr. Frederick  ...                   0        0.5               1\n417         1309       3                      Peter, Master. Michael J  ...                   1        0.5               1\n[418 rows x 16 columns]\nIf you don’t want all of them or if you’re not happy with the names of the added columns, they\ncan be modified using column remapping:\nimport datarobot as dr\nimport pandas as pd\ndeployment_id = '5dc5b1015e6e762a6241f9aa'\ndf = pd.read_csv('testdata/titanic_predict.csv')\njob, df = dr.BatchPredictionJob.score_pandas(\ndeployment_id,\ndf,\ncolumn_names_remapping={\n'Survived_1_PREDICTION': None,       # discard column\n'Survived_0_PREDICTION': None,       # discard column\n'Survived_PREDICTION': 'predicted',  # rename column\n'THRESHOLD': None,                   # discard column\n'POSITIVE_CLASS': None,              # discard column\n},\n)\nAny column mapped to None will be discarded. Any column mapped to a string will be renamed.\nAny column not mentioned will be kept in the output untouched.\nSo your DataFrame will now contain the following extra columns:\npredicted\nprediction_status\nRefer to the documentation for BatchPredictionJob.score\nfor the full range of available options.\nBatch Prediction Job Definitions\nTo submit a working Batch Prediction job, you must supply a variety of elements to the datarobot.models.BatchPredictionJob.score()\nrequest payload depending on what type of prediction is required. Additionally, you must consider the type of intake\nand output adapters used for a given job.\nEvery time a new Batch Prediction is created, the same amount of information must be stored somewhere outside of\nDataRobot and re-submitted every time.\nFor example, a request could look like:\nimport datarobot as dr\ndeployment_id = \"5dc5b1015e6e762a6241f9aa\"\njob = dr.BatchPredictionJob.score(\ndeployment_id,\nintake_settings={\n\"type\": \"s3\",\n\"url\": \"s3://bucket/container/file.csv\",\n\"credential_id\": \"5dc5b1015e6e762a6241f9bb\"\n},\noutput_settings={\n\"type\": \"s3\",\n\"url\": \"s3://bucket/container/output.csv\",\n\"credential_id\": \"5dc5b1015e6e762a6241f9bb\"\n},\n)\njob.wait_for_completion()\nwith open(\"./predicted.csv\", \"wb\") as f:\njob.download(f)\nJob Definitions\nIf your use case requires the same, or close to the same, type of prediction to be done multiple times, you can choose to\ncreate a Job Definition of the Batch Prediction job and store this inside DataRobot for future use.\nThe method for creating job definitions is identical to the existing datarobot.models.BatchPredictionJob.score() method,\nexcept for the addition of a enabled, name and schedule parameter: datarobot.models.BatchPredictionJobDefinition.create()\n>>> import datarobot as dr\n>>> job_spec = {\n...    \"num_concurrent\": 4,\n...    \"deployment_id\": \"5dc5b1015e6e762a6241f9aa\",\n...    \"intake_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\",\n...        \"credential_id\": \"5dc5b1015e6e762a6241f9bb\"\n...    },\n...    \"output_settings\": {\n...        \"url\": \"s3://foobar/123\",\n...        \"type\": \"s3\",\n...        \"format\": \"csv\",\n...        \"credential_id\": \"5dc5b1015e6e762a6241f9bb\"\n...    },\n...}\n>>> definition = BatchPredictionJobDefinition.create(\n...    enabled=False,\n...    batch_prediction_job=job_spec,\n...    name=\"some_definition_name\",\n...    schedule=None\n... )\n>>> definition\nBatchPredictionJobDefinition(foobar)\nNote\nThe name parameter must be unique across your organization. If you attempt to create multiple definitions\nwith the same name, the request will fail. If you wish to free up a name, you must first datarobot.models.BatchPredictionJobDefinition.delete()\nthe existing definition before creating this one. Alternatively you can just datarobot.models.BatchPredictionJobDefinition.update()\nthe existing definition with a new name.\nExecuting a job definition\nManual job execution\nTo submit a stored job definition for scoring, you can either do so on a scheduled basis, described\nbelow, or manually submit the definition ID using datarobot.models.BatchPredictionJobDefinition.run_once(),\nas such:\n>>> import datarobot as dr\n>>> definition = dr.BatchPredictionJobDefinition.get(\"5dc5b1015e6e762a6241f9aa\")\n>>> job = definition.run_once()\n>>> job.wait_for_completion()\nScheduled job execution\nA Scheduled Batch Prediction job works just like a regular Batch Prediction job, except DataRobot handles the execution\nof the job.\nIn order to schedule the execution of a Batch Prediction job, a definition must first be created, using\ndatarobot.models.BatchPredictionJobDefinition.create(), or updated, using\ndatarobot.models.BatchPredictionJobDefinition.update(), where enabled is set to True and a schedule\npayload is provided.\nAlternatively, you can use a short-hand version with datarobot.models.BatchPredictionJobDefinition.run_on_schedule()\nas such:\n>>> import datarobot as dr\n>>> schedule = {\n...    \"day_of_week\": [\n...        1\n...    ],\n...    \"month\": [\n...        \"*\"\n...    ],\n...    \"hour\": [\n...        16\n...    ],\n...    \"minute\": [\n...        0\n...    ],\n...    \"day_of_month\": [\n...        1\n...    ]\n...}\n>>> definition = dr.BatchPredictionJob.get(\"5dc5b1015e6e762a6241f9aa\")\n>>> job = definition.run_on_schedule(schedule)\nIf the created job was not enabled previously, this method will also enable it.\nThe Schedule payload\nThe schedule payload defines at what intervals the job should run, which can be combined in various ways to construct\ncomplex scheduling terms if needed. In all of the elements in the objects, you can supply either an asterisk [\"*\"]\ndenoting “every” time denomination or an array of integers (e.g. [1, 2, 3]) to define a specific interval.\nThe schedule payload elements\nKey\nPossible values\nExample\nDescription\nminute\n[\"*\"] or [0 ... 59]\n[15, 30, 45]\nThe job will run at these minute values for every hour of the day.\nhour\n[\"*\"] or [0 ... 23]\n[12,23]\nThe hour(s) of the day that the job will run.\nmonth\n[\"*\"] or [1 ... 12]\n[\"jan\"]\nStrings, either 3-letter abbreviations or the full name of the month, can be used interchangeably (e.g., “jan” or “october”).\nMonths that are not compatible with day_of_month are ignored, for example {\"day_of_month\": [31], \"month\":[\"feb\"]}.\nday_of_week\n[\"*\"] or [0 ... 6] where (Sunday=0)\n[\"sun\"]\nThe day(s) of the week that the job will run. Strings, either 3-letter abbreviations or the full name of the day, can be used interchangeably (e.g., “sunday”, “Sunday”, “sun”, or “Sun”, all map to [0]).\nNOTE: This field is additive with day_of_month, meaning the job will run both on the date specified by day_of_month and the day defined in this field.\nday_of_month\n[\"*\"] or [1 ... 31]\n[1, 25]\nThe date(s) of the month that the job will run. Allowed values are either [1 ... 31] or [\"*\"] for all days of the month.\nNOTE: This field is additive with day_of_week, meaning the job will run both on the date(s) defined in this field and the day specified\nby day_of_week (for example, dates 1st, 2nd, 3rd, plus every Tuesday). If day_of_month is set to [\"*\"] and day_of_week is defined,\nthe scheduler will trigger on every day of  the month that matches day_of_week (for example, Tuesday the 2nd, 9th, 16th, 23rd, 30th).\nInvalid dates such as February 31st are ignored.\nDisabling a scheduled job\nJob definitions are only be executed by the scheduler if enabled is set to True. If you have a job definition\nthat was previously running as a scheduled job, but should now be stopped, simply\ndatarobot.models.BatchPredictionJobDefinition.delete() to remove it completely, or datarobot.models.BatchPredictionJobDefinition.update()\nit with enabled=False if you want to keep the definition, but stop the scheduled job from executing at intervals.\nIf a job is currently running, this will finish execution regardless.\n>>> import datarobot as dr\n>>> definition = dr.BatchPredictionJobDefinition.get(\"5dc5b1015e6e762a6241f9aa\")\n>>> definition.delete()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/batch_predictions.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/batch_predictions.html",
        "content_length": 21426
      },
      "code_examples": [],
      "api_methods": [
        "dr.batchpredictionjob.score",
        "datarobot.enums.available_statement_types",
        "dr.credential.get",
        "dr.batchpredictionjob.score_gcp",
        "deployment.get",
        "dr.batchpredictionjob.score_to_file",
        "dr.batchpredictionjob.score_s3",
        "dr.dataset.get",
        "dr.batchpredictionjob.score_azure",
        "dr.batchpredictionjob.get",
        "dr.batchpredictionjob.score_pandas",
        "datarobot.models.batchpredictionjobdefinition",
        "dr.batchpredictionjob.score_from_existing",
        "dr.batchpredictionjobdefinition.get",
        "deployment.predict_batch",
        "datarobot.models.batchpredictionjob",
        "dr.deployment.get",
        "dr.datastore.get"
      ],
      "complexity_score": 0.7999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-39014876764731997",
      "title": "Use Cases",
      "content": "Use Cases\nThe Use Cases section provides details on how to utilize and manage DataRobot Use Cases in your Python code.",
      "content_type": "documentation",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/use_cases/index.html",
      "tags": [
        "api_reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/use_cases/index.html",
        "content_length": 119
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "general"
    },
    {
      "id": "readthedocs_6152160873708066784",
      "title": "Predictions",
      "content": "Predictions\nPredictions generation is an asynchronous process. This means that when starting\npredictions with Model.request_predictions() you will receive back a PredictJob for tracking\nthe process responsible for fulfilling your request.\nWith this object you can get info about the predictions generation process before it\nhas finished and be rerouted to the predictions themselves when the\nprocess is finished. For this you should use the PredictJob class.\nStarting predictions generation\nBefore actually requesting predictions, you should upload the dataset you wish to predict via\nProject.upload_dataset.  Previously uploaded datasets can be seen under Project.get_datasets.\nWhen uploading the dataset you can provide the path to a local file, a file object, raw file content,\na pandas.DataFrame object, or the url to a publicly available dataset.\nTo start predicting on new data using a finished model use Model.request_predictions().\nIt will create a new predictions generation process and return a PredictJob object tracking this process.\nWith it, you can monitor an existing PredictJob and retrieve generated predictions when the corresponding\nPredictJob is finished.\nimport datarobot as dr\nproject_id = '5506fcd38bd88f5953219da0'\nmodel_id = '5506fcd98bd88f1641a720a3'\nproject = dr.Project.get(project_id)\nmodel = dr.Model.get(\nproject=project_id,\nmodel_id=model_id,\n)\n# As of v3.0, in addition to passing a ``dataset_id``, you can pass in a ``dataset``, ``file``, ``file_path`` or\n# ``dataframe`` to `Model.request_predictions`.\npredict_job = model.request_predictions(file_path='./data_to_predict.csv')\n# Alternative version uploading the dataset from a local path and passing it by its id\ndataset_from_path = project.upload_dataset('./data_to_predict.csv')\npredict_job = model.request_predictions(dataset_id=dataset_from_path.id)\n# Alternative version: upload the dataset as a file object and pass it by using its dataset id\nwith open('./data_to_predict.csv') as data_to_predict:\ndataset_from_file = project.upload_dataset(data_to_predict)\npredict_job = model.request_predictions(dataset_id=dataset_from_file.id)  # OR predict_job = model.request_predictions(dataset_id=dataset_from_file.id)\nListing Predictions\nYou can use the Predictions.list() method to return a list of predictions generated on a project.\nimport datarobot as dr\npredictions = dr.Predictions.list('58591727100d2b57196701b3')\nprint(predictions)\n>>>[Predictions(prediction_id='5b6b163eca36c0108fc5d411',\nproject_id='5b61bd68ca36c04aed8aab7f',\nmodel_id='5b61bd7aca36c05744846630',\ndataset_id='5b6b1632ca36c03b5875e6a0'),\nPredictions(prediction_id='5b6b2315ca36c0108fc5d41b',\nproject_id='5b61bd68ca36c04aed8aab7f',\nmodel_id='5b61bd7aca36c0574484662e',\ndataset_id='5b6b1632ca36c03b5875e6a0'),\nPredictions(prediction_id='5b6b23b7ca36c0108fc5d422',\nproject_id='5b61bd68ca36c04aed8aab7f',\nmodel_id='5b61bd7aca36c0574484662e',\ndataset_id='55b6b1632ca36c03b5875e6a0')\n]\nYou can pass following parameters to filter the result:\nmodel_id – str, used to filter returned predictions by model_id.\ndataset_id – str, used to filter returned predictions by dataset_id.\nGet an existing PredictJob\nTo retrieve an existing PredictJob use the PredictJob.get method. This will give you\na PredictJob matching the latest status of the job if it has not completed.\nIf predictions have finished building, PredictJob.get will raise a PendingJobFinished\nexception.\nimport time\nimport datarobot as dr\npredict_job = dr.PredictJob.get(\nproject_id=project_id,\npredict_job_id=predict_job_id,\n)\npredict_job.status\n>>> 'queue'\n# wait for generation of predictions (in a very inefficient way)\ntime.sleep(10 * 60)\npredict_job = dr.PredictJob.get(\nproject_id=project_id,\npredict_job_id=predict_job_id,\n)\n>>> dr.errors.PendingJobFinished\n# now the predictions are finished\npredictions = dr.PredictJob.get_predictions(\nproject_id=project.id,\npredict_job_id=predict_job_id,\n)\nGet generated predictions\nAfter predictions are generated, you can use PredictJob.get_predictions\nto get newly generated predictions.\nIf predictions have not yet been finished, it will raise a JobNotFinished exception.\nimport datarobot as dr\npredictions = dr.PredictJob.get_predictions(\nproject_id=project.id,\npredict_job_id=predict_job_id,\n)\nWait for and Retrieve results\nIf you just want to get generated predictions from a PredictJob, you\ncan use the PredictJob.get_result_when_complete function.\nIt will poll the status of the predictions generation process until it has finished, and\nthen will return predictions.\ndataset = project.get_datasets()[0]\npredict_job = model.request_predictions(dataset.id)\npredictions = predict_job.get_result_when_complete()\nGet previously generated predictions\nIf you don’t have a Model.predict_job on hand, there are two more ways to retrieve predictions from the\nPredictions interface:\nGet all prediction rows as a pandas.DataFrame object:\nimport datarobot as dr\npreds = dr.Predictions.get(\"5b61bd68ca36c04aed8aab7f\", prediction_id=\"5b6b163eca36c0108fc5d411\")\ndf = preds.get_all_as_dataframe()\ndf_with_serializer = preds.get_all_as_dataframe(serializer='csv')\nDownload all prediction rows to a file as a CSV document:\nimport datarobot as dr\npreds = dr.Predictions.get(\"5b61bd68ca36c04aed8aab7f\", prediction_id=\"5b6b163eca36c0108fc5d411\")\npreds.download_to_csv('predictions.csv')\npreds.download_to_csv('predictions_with_serializer.csv', serializer='csv')\nTraining predictions\nThe training predictions interface allows computing and retrieving out-of-sample predictions for a model\nusing the original project dataset. The predictions can be computed for all the rows, or restricted to validation\nor holdout data. As the predictions generated will be out-of-sample, they can be expected to have different\nresults than if the project dataset were re-uploaded as a prediction dataset.\nQuick reference\nTraining predictions generation is an asynchronous process. This means that when starting\npredictions with datarobot.models.Model.request_training_predictions() you will receive back a\ndatarobot.models.TrainingPredictionsJob for tracking the process responsible for fulfilling your request.\nActual predictions may be obtained with the help of a\ndatarobot.models.training_predictions.TrainingPredictions object returned as the result of\nthe training predictions job.\nThere are three ways to retrieve them:\nIterate prediction rows one by one as named tuples:\nimport datarobot as dr\n# Calculate new training predictions on all dataset\ntraining_predictions_job = model.request_training_predictions(dr.enums.DATA_SUBSET.ALL)\ntraining_predictions = training_predictions_job.get_result_when_complete()\n# Fetch rows from API and print them\nfor prediction in training_predictions.iterate_rows(batch_size=250):\nprint(prediction.row_id, prediction.prediction)\nGet all prediction rows as a pandas.DataFrame object:\nimport datarobot from dr\n# Calculate new training predictions on holdout partition of dataset\ntraining_predictions_job = model.request_training_predictions(dr.enums.DATA_SUBSET.HOLDOUT)\ntraining_predictions = training_predictions_job.get_result_when_complete()\n# Fetch training predictions as data frame\ndataframe = training_predictions.get_all_as_dataframe()\nDownload all prediction rows to a file as a CSV document:\nimport datarobot from dr\n# Calculate new training predictions on all dataset\ntraining_predictions_job = model.request_training_predictions(dr.enums.DATA_SUBSET.ALL)\ntraining_predictions = training_predictions_job.get_result_when_complete()\n# Fetch training predictions and save them to file\ntraining_predictions.download_to_csv('my-training-predictions.csv')",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/predict_job.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/predict_job.html",
        "content_length": 7636
      },
      "code_examples": [],
      "api_methods": [
        "dr.predictjob.get",
        "dr.predictions.get",
        "dr.enums.data_subset",
        "model.request_predictions",
        "datarobot.models.model",
        "model.request_training_predictions",
        "dr.predictjob.get_predictions",
        "dr.predictions.list",
        "project.id",
        "datarobot.models.trainingpredictionsjob",
        "project.get",
        "project.upload_dataset",
        "datarobot.models.training_predictions",
        "model.get",
        "model.predict_job",
        "dr.model.get",
        "project.get_datasets",
        "dr.project.get",
        "dr.errors.pendingjobfinished"
      ],
      "complexity_score": 0.95,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-2241999082284701071",
      "title": "Use Cases",
      "content": "Use Cases\nUse Cases are folder-like containers in DataRobot Workbench that allow you to group all assets related to solving a specific business problem inside of a single, manageable entity. These assets include datasets, models, experiments, No-Code AI Apps, and notebooks. You can share entire Use Cases or the individual assets they contain.\nThe primary benefit of a Use Case is that it enables experiment-based, iterative workflows. By housing all key insights in a single location, data scientists have improved navigation of assets and a cleaner interface for experiment creation and model training, review, and evaluation.\nSpecifically, Use Cases allow you to:\nOrganize your work — group all related datasets, experiments, notebooks, etc. by the problem they solve.\nFind assets easily. Use Cases eliminate the need to search through hundreds of unrelated projects or scrape emails for hyperlinks to specific assets.\nShare collections of assets. You can share entire Use Cases, containing all the assets your team needs to participate.\nManage access. Add or remove members to a Use Case to control their access.\nMonitor changes. Receive notifications when a team member adds, removes, or modifies any asset in a Use Case.\nCurrently, Use Cases in the Python client support interactions with binary classification and regression projects, applications, and datasets. Development is ongoing, so see the release notes for a full list of supported capabilities.\nFor a more in-depth look at Use Cases and the DataRobot Workbench, refer to the Workbench documentation.\nAdd to a Use Case\nCurrently, only project, dataset, and application instances can be added to a Use Case via the Python client.\nThe process of adding a dataset is shown in the example below:\nimport datarobot as dr\ndr.Client(token=\"<token>\", endpoint=\"https://app.datarobot.com/api/v2\")\nrisk_use_case = dr.UseCase.create(\nname=\"Financial Risk Experimentation Environment\",\ndescription=\"For running experiments on modeling financial risks to our business.\",\n)\nnew_dataset = dr.Dataset.create_from_file(\nfile_path=\"/foo/bar/risk_data.csv\",\n)\nrisk_use_case.add(entity=new_dataset)\nrisk_use_case.list_datasets()\n>>> [Dataset(name='risk_data.csv', id='646e8bb507b108ce7b474b27')]\nYou can add an application to a Use Case in a similar way. The primary difference is that you cannot create applications with the Python client. Instead, retrieve an application using its ID or pull it from a retrieved list of applications and then add it to a Use Case:\nimport datarobot as dr\ndr.Client(token=\"<token>\", endpoint=\"https://app.datarobot.com/api/v2\")\nrisk_use_case = dr.UseCase.create(\nname=\"Financial Risk Experimentation Environment\",\ndescription=\"For running experiments on modeling financial risks to our business.\",\n)\nexisting_application = dr.Application.list()[0]\nrisk_use_case.add(entity=existing_application)\nrisk_use_case.list_applications()\n>>> [Application(name='Financial Risk Detection')]\nAlternatively, the UseCaseReferenceEntity returned from UseCase.add can be used to share an entity between Use Cases:\nimport datarobot as dr\ndr.Client(token=\"<token>\", endpoint=\"https://app.datarobot.com/api/v2\")\nrisk_use_case_1 = dr.UseCase.create(\nname=\"Financial Risk Experimentation Environment\",\ndescription=\"For running experiments on modeling financial risks to our business.\",\n)\nrisk_use_case_2 = dr.UseCase.create(\nname=\"Financial Risk Experimentation Environment 2\",\ndescription=\"For running experiments on modeling financial risks to our business.\",\n)\nnew_dataset = dr.Dataset.create_from_file(\nfile_path=\"/foo/bar/risk_data.csv\",\n)\ndataset_entity = risk_use_case_1.add(entity=new_dataset)\nrisk_use_case_2.add(entity=dataset_entity)\nrisk_use_case_2.list_datasets()\n>>> [Dataset(name='risk_data.csv', id='646e8bb507b108ce7b474b27')]\nTo add a project to a Use Case, it must meet the following conditions:\nIt must be binary classification or regression project\nThe associated dataset must be linked to the same Use Case\nModeling must be in progress (via UI, the analyze_and_model method, or any other methods that initiate modeling)\nimport datarobot as dr\ndr.Client(token=\"<token>\", endpoint=\"https://app.datarobot.com/api/v2\")\nrisk_use_case = dr.UseCase.create(\nname=\"Financial Risk Experimentation Environment\",\ndescription=\"For running experiments on modeling financial risks to our business.\",\n)\nnew_dataset = dr.Dataset.create_from_file(\nfile_path=\"/foo/bar/risk_data.csv\",\nuse_case=risk_use_case\n)\nrisk_use_case.add(entity=new_dataset)\nnew_project = dr.Project.create_from_dataset(\ndataset_id=new_dataset.dataset_id,\nproject_name=\"Risk Assessment v1\",\nuse_case=risk_use_case\n)\nnew_project.analyze_and_model(target=\"credit_risk\")\nrisk_use_case.add(entity=new_project)\nrisk_use_case.list_projects()\n>>> [Project(Risk Assessment v1)]\nrisk_use_case.list_datasets()\n>>> [Dataset(name='risk_data.csv', id='646e8bb507b108ce7b474b27')]\nConfiguration\nThere are three primary ways of adding new projects or datasets to Use Cases once they’ve been generated.\nThe easiest method is to directly pass a Use Case to one of the project or dataset creation methods. Passing the use case directly allows for you to finely control what is added to a Use Case in your code. For example, the following code example creates a new Use Case, then creates a new project that is automatically added to the Use Case.\nimport datarobot as dr\ndr.Client(token=\"<token>\", endpoint=\"https://app.datarobot.com/api/v2\")\nrisk_use_case = dr.UseCase.create(\nname=\"Financial Risk Experimentation Environment\",\ndescription=\"For running experiments on modeling financial risks to our business.\",\n)\nnew_project = dr.Project.create(\nsourcedata=\"/foo/bar/risk_data.csv\",\nproject_name=\"Risk Assessment v1\",\nuse_case=risk_use_case\n)\nrisk_use_case.list_projects()\n>>> [Project(Risk Assessment v1)]\nYou can also use a context manager to perform a series of actions that automatically result in projects or datasets being added to a Use Case without having to manually pass the Use Case yourself. This can be extremely useful if you have a series of calls you want to make that all should be added to a Use Case. For example:\nimport datarobot as dr\ndr.Client(token=\"<token>\", endpoint=\"https://app.datarobot.com/api/v2\")\nrisk_use_case = dr.UseCase.create(\nname=\"Financial Risk Experimentation Environment\",\ndescription=\"For running experiments on modeling financial risks to our business.\",\n)\nwith risk_use_case:\nnew_dataset = dr.Dataset.create_from_file(\nfile_path=\"/foo/bar/risk_data.csv\",\n)\nrisk_use_case.list_datasets()\n>>> [Dataset(name='risk_data.csv', id='646e8bb507b108ce7b474b27')]\nYou can also set a global Use Case to automatically add all project and dataset instances that are created by your code. This is useful if all of the work you are doing should be contained in a single Use Case, but risks accidentally adding projects and datasets that should not be included in your Use Case. Setting a global default Use Case requires knowing the ID of your Use Case ahead of time. For example:\nimport datarobot as dr\ndr.Client(token=\"<token>\", endpoint=\"https://app.datarobot.com/api/v2\", default_use_case=\"639ce542862e9b1b1bfa8f1b\")\nnew_dataset = dr.Dataset.create_from_file(file_path=\"/foo/bar/risk_data.csv\")\nrisk_use_case = dr.UseCase.get(id=\"639ce542862e9b1b1bfa8f1b\")\nrisk_use_case.list_datasets()\n>>> [Dataset(name='risk_data.csv', id='646e8bb507b108ce7b474b27')]\nSharing\nOverview\nInstances of datarobot.models.sharing.SharingRole can be created to define a new role grant (or revocation).\nThe UseCase.share() instance method takes a list of SharingRole as its only argument. Calling this method\nwill apply the list of SharingRoles to the given UseCase.\nUse Cases support SHARING_ROLE.OWNER, SHARING_ROLE.EDITOR, SHARING_ROLE.CONSUMER and SHARING_ROLE.NO_ROLE as possible role designations (see datarobot.enums.SHARING_ROLE).\nCurrently, the only supported SHARING_RECIPIENT_TYPE is USER.\nExamples\nSuppose you had a list of user IDs you wanted to share this Use Case with. You could use\na loop to generate a list of SharingRole objects for them, and bulk share this Use Case.\n>>> from datarobot.models.use_cases.use_case import UseCase\n>>> from datarobot.models.sharing import SharingRole\n>>> from datarobot.enums import SHARING_ROLE, SHARING_RECIPIENT_TYPE\n>>>\n>>> user_ids = [\"60912e09fd1f04e832a575c1\", \"639ce542862e9b1b1bfa8f1b\", \"63e185e7cd3a5f8e190c6393\"]\n>>> sharing_roles = []\n>>> for user_id in user_ids:\n...     new_sharing_role = SharingRole(\n...         role=SHARING_ROLE.CONSUMER,\n...         share_recipient_type=SHARING_RECIPIENT_TYPE.USER,\n...         id=user_id,\n...         can_share=True,\n...     )\n...     sharing_roles.append(new_sharing_role)\n>>> use_case = UseCase.get(use_case_id=\"5f33f1fd9071ae13568237b2\")\n>>> use_case.share(roles=sharing_roles)\nSimilarly, a SharingRole instance can be used to remove a user’s access if the role\nis set to SHARING_ROLE.NO_ROLE, like in this example:\n>>> from datarobot.models.use_cases.use_case import UseCase\n>>> from datarobot.models.sharing import SharingRole\n>>> from datarobot.enums import SHARING_ROLE, SHARING_RECIPIENT_TYPE\n>>>\n>>> user_to_remove = \"[email protected]\"\n... remove_sharing_role = SharingRole(\n...     role=SHARING_ROLE.NO_ROLE,\n...     share_recipient_type=SHARING_RECIPIENT_TYPE.USER,\n...     username=user_to_remove,\n...     can_share=False,\n... )\n>>> use_case = UseCase.get(use_case_id=\"5f33f1fd9071ae13568237b2\")\n>>> use_case.share(roles=[remove_sharing_role])\nLooking beyond a Use Case\nUse Cases are a powerful tool for organizing your work, and can help if you need to focus only on those resources\nrelevant to a specific business problem. However, occasionally you may want to look outside of a Use Case\nat other available DataRobot resources. The following code snippet demonstrates how to retrieve all Projects that\nyour user has access to:\nimport datarobot as dr\nfrom datarobot.client import client_configuration\nwith client_configuration(default_use_case=[]):\nall_projects = dr.Project.list()",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/use_cases/use_cases.html",
      "tags": [
        "api_reference",
        "example",
        "api-reference"
      ],
      "metadata": {
        "page_type": "api_reference",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/use_cases/use_cases.html",
        "content_length": 10033
      },
      "code_examples": [],
      "api_methods": [
        "project.create",
        "datarobot.models.use_cases",
        "dr.project.list",
        "project.list",
        "dr.project.create",
        "datarobot.enums.sharing_role",
        "dr.application.list",
        "project.analyze_and_model",
        "dr.usecase.get",
        "project.create_from_dataset",
        "dr.usecase.create",
        "datarobot.models.sharing",
        "dr.project.create_from_dataset",
        "dr.dataset.create_from_file"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_-8224471618655140224",
      "title": "Training predictions",
      "content": "Training predictions\nclass datarobot.models.training_predictions.TrainingPredictionsIterator\nLazily fetches training predictions from DataRobot API in chunks of specified size and then\niterates rows from responses as named tuples. Each row represents a training prediction\ncomputed for a dataset’s row. Each named tuple has the following structure:\nVariables:\nrow_id (int) – id of the record in original dataset for which training prediction is calculated\npartition_id (str or float) – The ID of the data partition that the row belongs to. “0.0” corresponds to the validation\npartition or backtest 1.\nprediction (float or str or list of str) – The model’s prediction for this data row.\nprediction_values (list of dict) – An array of dictionaries with a schema described as PredictionValue.\ntimestamp (str or None) – (New in version v2.11) an ISO string representing the time of the prediction\nin time series project; may be None for non-time series projects\nforecast_point (str or None) – (New in version v2.11) an ISO string representing the point in time\nused as a basis to generate the predictions in time series project;\nmay be None for non-time series projects\nforecast_distance (str or None) – (New in version v2.11) how many time steps are between the forecast point and the\ntimestamp in time series project; None for non-time series projects\nseries_id (str or None) – (New in version v2.11) the id of the series in a multiseries project;\nmay be NaN for single series projects; None for non-time series projects\nprediction_explanations (list of dict or None) – (New in version v2.21) The prediction explanations for each feature. The total elements in\nthe array are bounded by max_explanations and feature count. Only present if prediction\nexplanations were requested. Schema described as PredictionExplanations.\nshap_metadata (dict or None) – (New in version v2.21) The additional information necessary to understand SHAP based\nprediction explanations. Only present if explanation_algorithm equals\ndatarobot.enums.EXPLANATIONS_ALGORITHM.SHAP was added in compute request. Schema\ndescribed as ShapMetadata.\nNotes\nEach PredictionValue dict contains these keys:\nlabeldescribes what this model output corresponds to. For regression\nprojects, it is the name of the target feature. For classification and multiclass\nprojects, it is a label from the target feature.\nvaluethe output of the prediction. For regression projects, it is the\npredicted value of the target. For classification and multiclass projects, it is\nthe predicted probability that the row belongs to the class identified by the label.\nEach PredictionExplanations dictionary contains these keys:\nlabel (str)describes what output was driven by this prediction explanation. For regression\nprojects, it is the name of the target feature. For classification projects, it is the\nclass whose probability increasing would correspond to a positive strength of this\nprediction explanation.\nfeature (str)the name of the feature contributing to the prediction\nfeature_value (object)the value the feature took on for this row. The type corresponds to the feature\n(boolean, integer, number, string)\nstrength (float)algorithm-specific explanation value attributed to feature in this row\nShapMetadata dictionary contains these keys:\nshap_remaining_total (float)The total of SHAP values for features beyond the max_explanations. This can be\nidentically 0 in all rows, if max_explanations is greater than the number of features\nand thus all features are returned.\nshap_base_value (float)the model’s average prediction over the training data. SHAP values are deviations from\nthe base value.\nwarnings (dict or None)SHAP values calculation warnings (e.g. additivity check failures in XGBoost models).\nSchema described as ShapWarnings.\nShapWarnings dictionary contains these keys:\nmismatch_row_count (int)the count of rows for which additivity check failed\nmax_normalized_mismatch (float)the maximal relative normalized mismatch value\nExamples\nimport datarobot as dr\n# Fetch existing training predictions by their id\ntraining_predictions = dr.TrainingPredictions.get(project_id, prediction_id)\n# Iterate over predictions\nfor row in training_predictions.iterate_rows()\nprint(row.row_id, row.prediction)\nclass datarobot.models.training_predictions.TrainingPredictions\nRepresents training predictions metadata and provides access to prediction results.\nVariables:\nproject_id (str) – id of the project the model belongs to\nmodel_id (str) – id of the model\nprediction_id (str) – id of generated predictions\ndata_subset (datarobot.enums.DATA_SUBSET) – data set definition used to build predictions.\nChoices are:\ndatarobot.enums.DATA_SUBSET.ALLfor all data available. Not valid for models in datetime partitioned projects.\ndatarobot.enums.DATA_SUBSET.VALIDATION_AND_HOLDOUTfor all data except training set. Not valid for models in datetime partitioned projects.\ndatarobot.enums.DATA_SUBSET.HOLDOUTfor holdout data set only.\ndatarobot.enums.DATA_SUBSET.ALL_BACKTESTSfor downloading the predictions for all backtest validation folds.\nRequires the model to have successfully scored all backtests.\nDatetime partitioned projects only.\nexplanation_algorithm (datarobot.enums.EXPLANATIONS_ALGORITHM) – (New in version v2.21) Optional. If set to shap, the response will include prediction\nexplanations based on the SHAP explainer (SHapley Additive exPlanations). Defaults to null\n(no prediction explanations).\nmax_explanations (int) – (New in version v2.21) The number of top contributors that are included in prediction\nexplanations. Max 100. Defaults to null for datasets narrower than 100 columns, defaults to\n100 for datasets wider than 100 columns.\nshap_warnings (list) – (New in version v2.21) Will be present if explanation_algorithm was set to\ndatarobot.enums.EXPLANATIONS_ALGORITHM.SHAP and there were additivity failures during SHAP\nvalues calculation.\nNotes\nEach element in shap_warnings has the following schema:\npartition_name (str)the partition used for the prediction record.\nvalue (object)the warnings related to this partition.\nThe objects in value are:\nmismatch_row_count (int)the count of rows for which additivity check failed.\nmax_normalized_mismatch (float)the maximal relative normalized mismatch value.\nExamples\nCompute training predictions for a model on the whole dataset\nimport datarobot as dr\n# Request calculation of training predictions\ntraining_predictions_job = model.request_training_predictions(dr.enums.DATA_SUBSET.ALL)\ntraining_predictions = training_predictions_job.get_result_when_complete()\nprint('Training predictions {} are ready'.format(training_predictions.prediction_id))\n# Iterate over actual predictions\nfor row in training_predictions.iterate_rows():\nprint(row.row_id, row.partition_id, row.prediction)\nList all training predictions for a project\nimport datarobot as dr\n# Fetch all training predictions for a project\nall_training_predictions = dr.TrainingPredictions.list(project_id)\n# Inspect all calculated training predictions\nfor training_predictions in all_training_predictions:\nprint(\n'Prediction {} is made for data subset \"{}\"'.format(\ntraining_predictions.prediction_id,\ntraining_predictions.data_subset,\n)\n)\nRetrieve training predictions by id\nimport datarobot as dr\n# Getting training predictions by id\ntraining_predictions = dr.TrainingPredictions.get(project_id, prediction_id)\n# Iterate over actual predictions\nfor row in training_predictions.iterate_rows():\nprint(row.row_id, row.partition_id, row.prediction)\nclassmethod list(project_id)\nFetch all the computed training predictions for a project.\nParameters:\nproject_id (str) – id of the project\nReturn type:\nA list of TrainingPredictions objects\nclassmethod get(project_id, prediction_id)\nRetrieve training predictions on a specified data set.\nParameters:\nproject_id (str) – id of the project the model belongs to\nprediction_id (str) – id of the prediction set\nReturns:\nobject which is ready to operate with specified predictions\nReturn type:\nTrainingPredictions\niterate_rows(batch_size=None)\nRetrieve training prediction rows as an iterator.\nParameters:\nbatch_size (Optional[int]) – maximum number of training prediction rows to fetch per request\nReturns:\niterator – an iterator which yields named tuples representing training prediction rows\nReturn type:\nTrainingPredictionsIterator\nget_all_as_dataframe(class_prefix='class_', serializer='json')\nRetrieve all training prediction rows and return them as a pandas.DataFrame.\nReturned dataframe has the following structure:\nrow_id : row id from the original dataset\nprediction : the model’s prediction for this row\nclass_<label> : the probability that the target is this class (only appears for\nclassification and multiclass projects)\ntimestamp : the time of the prediction (only appears for out of time validation or\ntime series projects)\nforecast_point : the point in time used as a basis to generate the predictions\n(only appears for time series projects)\nforecast_distance : how many time steps are between timestamp and forecast_point\n(only appears for time series projects)\nseries_id : he id of the series in a multiseries project\nor None for a single series project\n(only appears for time series projects)\nParameters:\nclass_prefix (Optional[str]) – The prefix to append to labels in the final dataframe. Default is class_\n(e.g., apple -> class_apple)\nserializer (Optional[str]) – Serializer to use for the download. Options: json (default) or csv.\nReturns:\ndataframe\nReturn type:\npandas.DataFrame\ndownload_to_csv(filename, encoding='utf-8', serializer='json')\nSave training prediction rows into CSV file.\nParameters:\nfilename (str or file object) – path or file object to save training prediction rows\nencoding (Optional[str]) – A string representing the encoding to use in the output file, defaults to\n‘utf-8’\nserializer (Optional[str]) – Serializer to use for the download. Options: json (default) or csv.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/training_predictions.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/training_predictions.html",
        "content_length": 9916
      },
      "code_examples": [],
      "api_methods": [
        "model.request_training_predictions",
        "dr.trainingpredictions.get",
        "datarobot.enums.explanations_algorithm",
        "dr.enums.data_subset",
        "datarobot.enums.data_subset",
        "datarobot.models.training_predictions",
        "dr.trainingpredictions.list"
      ],
      "complexity_score": 0.75,
      "use_case_category": "modeling"
    },
    {
      "id": "readthedocs_3572279403310756384",
      "title": "Use cases",
      "content": "Use cases\nclass datarobot.UseCase\nRepresentation of a Use Case.\nVariables:\nid (str) – The ID of the Use Case.\nname (str) – The name of the Use Case.\ndescription (str) – The description of the Use Case. Nullable.\ncreated_at (str) – The timestamp generated at record creation.\ncreated (UseCaseUser) – The user who created the Use Case.\nupdated_at (str) – The timestamp generated when the record was last updated.\nupdated (UseCaseUser) – The most recent user to update the Use Case.\nmodels_count (int) – The number of models in a Use Case.\nprojects_count (int) – The number of projects in a Use Case.\ndatasets_count (int) – The number of datasets in a Use Case.\nnotebooks_count (int) – The number of notebooks in a Use Case.\napplications_count (int) – The number of applications in a Use Case.\nplaygrounds_count (int) – The number of playgrounds in a Use Case.\nvector_databases_count (int) – The number of vector databases in a Use Case.\nowners (List[UseCaseUser]) – The most recent user to update the Use Case.\nmembers (List[UseCaseUser]) – The most recent user to update the Use Case.\nExamples\nimport datarobot\nwith UseCase.get(\"2348ac\"):\nprint(f\"The current use case is {dr.Context.use_case}\")\nget_uri()\nReturns:\nurl – Permanent static hyperlink to this Use Case.\nReturn type:\nstr\nclassmethod get(use_case_id)\nGets information about a Use Case.\nParameters:\nuse_case_id (str) – The identifier of the Use Case you want to load.\nReturns:\nuse_case – The queried Use Case.\nReturn type:\nUseCase\nclassmethod list(search_params=None)\nReturns the Use Cases associated with this account.\nParameters:\nsearch_params (dict, optional.) – If not None, the returned projects are filtered by lookup.\nNotes\nCurrently, you can query use cases by:\noffset - The number of records to skip over. Default 0.\nlimit - The number of records to return in the range from 1 to 100. Default 100.\nsearch - Only return Use Cases with names that match the given string.\nproject_id - Only return Use Cases associated with the given project ID.\napplication_id - Only return Use Cases associated with the given app.\norderBy - The order to sort the Use Cases.\norderBy queries can use the following options:\nid or -id\nname or -name\ndescription or -description\nprojects_count or -projects_count\ndatasets_count or -datasets_count\nnotebooks_count or -notebooks_count\napplications_count or -applications_count\ncreated_at or -created_at\ncreated_by or -created_by\nupdated_at or -updated_at\nupdated_by or -updated_by\nReturns:\nuse_cases – Contains a list of Use Cases associated with this user\naccount.\nReturn type:\nlist of UseCase instances\nRaises:\nTypeError – Raised if search_params parameter is provided,\nbut is not of supported type.\nclassmethod create(name=None, description=None)\nCreate a new Use Case.\nParameters:\nname (str) – Optional. The name of the new Use Case.\ndescription (str) – The description of the new Use Case. Optional.\nReturns:\nuse_case – The created Use Case.\nReturn type:\nUseCase\nclassmethod delete(use_case_id)\nDelete a Use Case.\nParameters:\nuse_case_id (str) – The ID of the Use Case to be deleted.\nReturn type:\nNone\nupdate(name=None, description=None)\nUpdate a Use Case’s name or description.\nParameters:\nname (str) – The updated name of the Use Case.\ndescription (str) – The updated description of the Use Case.\nReturns:\nuse_case – The updated Use Case.\nReturn type:\nUseCase\nadd(entity=None, entity_type=None, entity_id=None)\nAdd an entity (project, dataset, etc.) to a Use Case. Can only accept either an entity or\nan entity type and entity ID, but not both.\nProjects and Applications can only be linked to a single Use Case. Datasets can be linked to multiple Use Cases.\nThere are some prerequisites for linking Projects to a Use Case which are explained in the\nuser guide.\nParameters:\nentity (Union[UseCaseReferenceEntity, Project, Dataset, Application]) – An existing entity to be linked to this Use Case.\nCannot be used if entity_type and entity_id are passed.\nentity_type (UseCaseEntityType) – The entity type of the entity to link to this Use Case. Cannot be used if entity is passed.\nentity_id (str) – The ID of the entity to link to this Use Case. Cannot be used if entity is passed.\nReturns:\nuse_case_reference_entity – The newly created reference link between this Use Case and the entity.\nReturn type:\nUseCaseReferenceEntity\nremove(entity=None, entity_type=None, entity_id=None)\nRemove an entity from a Use Case. Can only accept either an entity or\nan entity type and entity ID, but not both.\nParameters:\nentity (Union[UseCaseReferenceEntity, Project, Dataset, Application]) – An existing entity instance to be removed from a Use Case.\nCannot be used if entity_type and entity_id are passed.\nentity_type (UseCaseEntityType) – The entity type of the entity to link to this Use Case. Cannot be used if entity is passed.\nentity_id (str) – The ID of the entity to link to this Use Case.  Cannot be used if entity is passed.\nReturn type:\nNone\nshare(roles)\nShare this Use Case with or remove access from one or more user(s).\nParameters:\nroles (List[SharingRole]) – A list of SharingRole instances, each of which\nreferences a user and a role to be assigned.\nCurrently, the only supported roles for Use Cases are OWNER, EDITOR, and CONSUMER,\nand the only supported SHARING_RECIPIENT_TYPE is USER.\nTo remove access, set a user’s role to datarobot.enums.SHARING_ROLE.NO_ROLE.\nReturn type:\nNone\nExamples\nThe SharingRole class is needed in order to\nshare a Use Case with one or more users.\nFor example, suppose you had a list of user IDs you wanted to share this Use Case with. You could use\na loop to generate a list of SharingRole objects for them,\nand bulk share this Use Case.\n>>> from datarobot.models.use_cases.use_case import UseCase\n>>> from datarobot.models.sharing import SharingRole\n>>> from datarobot.enums import SHARING_ROLE, SHARING_RECIPIENT_TYPE\n>>>\n>>> user_ids = [\"60912e09fd1f04e832a575c1\", \"639ce542862e9b1b1bfa8f1b\", \"63e185e7cd3a5f8e190c6393\"]\n>>> sharing_roles = []\n>>> for user_id in user_ids:\n...     new_sharing_role = SharingRole(\n...         role=SHARING_ROLE.CONSUMER,\n...         share_recipient_type=SHARING_RECIPIENT_TYPE.USER,\n...         id=user_id,\n...     )\n...     sharing_roles.append(new_sharing_role)\n>>> use_case = UseCase.get(use_case_id=\"5f33f1fd9071ae13568237b2\")\n>>> use_case.share(roles=sharing_roles)\nSimilarly, a SharingRole instance can be used to\nremove a user’s access if the role is set to SHARING_ROLE.NO_ROLE, like in this example:\n>>> from datarobot.models.use_cases.use_case import UseCase\n>>> from datarobot.models.sharing import SharingRole\n>>> from datarobot.enums import SHARING_ROLE, SHARING_RECIPIENT_TYPE\n>>>\n>>> user_to_remove = \"[email protected]\"\n... remove_sharing_role = SharingRole(\n...     role=SHARING_ROLE.NO_ROLE,\n...     share_recipient_type=SHARING_RECIPIENT_TYPE.USER,\n...     username=user_to_remove,\n... )\n>>> use_case = UseCase.get(use_case_id=\"5f33f1fd9071ae13568237b2\")\n>>> use_case.share(roles=[remove_sharing_role])\nget_shared_roles(offset=None, limit=None, id=None)\nRetrieve access control information for this Use Case.\nParameters:\noffset (Optional[int]) – The number of records to skip over. Optional. Default is 0.\nlimit (Optional[int]) – The number of records to return. Optional. Default is 100.\nid (Optional[str]) – Return the access control information for a user with this user ID. Optional.\nReturn type:\nList[SharingRole]\nlist_projects()\nList all projects associated with this Use Case.\nReturns:\nprojects – All projects associated with this Use Case.\nReturn type:\nList[Project]\nlist_datasets()\nList all datasets associated with this Use Case.\nReturns:\ndatasets – All datasets associated with this Use Case.\nReturn type:\nList[Dataset]\nlist_applications()\nList all applications associated with this Use Case.\nReturns:\napplications – All applications associated with this Use Case.\nReturn type:\nList[Application]\nclassmethod from_data(data)\nInstantiate an object of this class using a dict.\nParameters:\ndata (dict) – Correctly snake_cased keys and their values.\nReturn type:\nTypeVar(T, bound= APIObject)\nclassmethod from_server_data(data, keep_attrs=None)\nInstantiate an object of this class using the data directly from the server,\nmeaning that the keys may have the wrong camel casing\nParameters:\ndata (dict) – The directly translated dict of JSON from the server. No casing fixes have\ntaken place\nkeep_attrs (iterable) – List, set or tuple of the dotted namespace notations for attributes to keep within the\nobject structure even if their values are None\nReturn type:\nTypeVar(T, bound= APIObject)\nopen_in_browser()\nOpens class’ relevant web browser location.\nIf default browser is not available the URL is logged.\nNote:\nIf text-mode browsers are used, the calling process will block\nuntil the user exits the browser.\nReturn type:\nNone\nclass datarobot.models.use_cases.use_case.UseCaseUser\nRepresentation of a Use Case user.\nVariables:\nid (str) – The id of the user.\nfull_name (str) – The full name of the user. Optional.\nemail (str) – The email address of the user. Optional.\nuserhash (str) – User’s gravatar hash. Optional.\nusername (str) – The username of the user. Optional.\nclass datarobot.models.use_cases.use_case.UseCaseReferenceEntity\nAn entity associated with a Use Case.\nVariables:\nentity_type (UseCaseEntityType) – The type of the entity.\nuse_case_id (str) – The Use Case this entity is associated with.\nid (str) – The ID of the entity.\ncreated_at (str) – The date and time this entity was linked with the Use Case.\nis_deleted (bool) – Whether or not the linked entity has been deleted.\ncreated (UseCaseUser) – The user who created the link between this entity and the Use Case.",
      "content_type": "code",
      "source_type": "readthedocs",
      "source_file": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/use-cases.html",
      "tags": [
        "example",
        "documentation",
        "api-reference"
      ],
      "metadata": {
        "page_type": "documentation",
        "url": "https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/use-cases.html",
        "content_length": 9685
      },
      "code_examples": [],
      "api_methods": [
        "datarobot.enums.sharing_role",
        "datarobot.models.sharing",
        "datarobot.models.use_cases",
        "dr.context.use_case"
      ],
      "complexity_score": 0.6000000000000001,
      "use_case_category": "modeling"
    },
    {
      "id": "github_python_-2372061722699295470",
      "title": "dataprep_functions.py",
      "content": "__author__ = 'Callum Finlayson'\n\nimport requests,json,re\nfrom collections import OrderedDict\n\n# (1) Get Library Name and schema from LibraryID\ndef get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_schema_dict\n\n# (1a) Get Library Name and schema from LibraryID (and Version)\ndef get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    library_name = \"\"\n    library_schema_dict = {}\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources.get('name')\n        library_schema_dict = jdata_datasources.get('schema')\n    return library_name,library_schema_dict\n\n\n# (2) Get all of the datasources from Paxata that are tagged with \"tag\"\ndef get_tagged_library_items(auth_token,paxata_url,tag):\n    tagged_datasets = []\n    get_tags_request = (paxata_url + \"/rest/library/tags\")\n    get_tags_response = requests.get(get_tags_request, auth=auth_token, verify=False)\n    if (get_tags_response.ok):\n        AllTagsDatasetsJson = json.loads(get_tags_response.content)\n        i=0\n        number_of_datasets = len(AllTagsDatasetsJson)\n        while i < number_of_datasets:\n            if (AllTagsDatasetsJson[i].get('name') == tag):\n                tagged_datasets.append(AllTagsDatasetsJson[i].get('dataFileId'), AllTagsDatasetsJson[i].get('name'))\n            i += 1\n    else:\n        print(\"bad request> \" + get_tags_response.status_code)\n    return tagged_datasets\n\n# (3) POST Library data from Paxata and load it into a JSON structure\ndef get_paxata_library_data(auth_token,paxata_url,library_dataset_id):\n    post_request = (paxata_url + \"/rest/datasource/exports/local/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData\n\n# (4) Get the Name of a Library from it's DatasetID\ndef get_name_of_datasource(auth_token_source,paxata_url_source,libraryId):\n    url_request = (paxata_url_source + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataDataSources = json.loads(my_response.content)\n        libraryName = jDataDataSources[0].get('name')\n    return libraryName\n\n# (5) Get all the Projects that have been described with a specific \"description_tag\"\ndef get_all_project_information(auth_token_source, paxata_url_source, description_tag):\n    Package_Tagged_Projects = []\n    package_counter = 0\n    max_num_of_projects = 0\n    ProjectNames = []\n    url_request = (paxata_url_source + \"/rest/projects\")\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataProjectIds = json.loads(my_response.content)\n        for item in jDataProjectIds:\n            if description_tag == jDataProjectIds[package_counter].get('description'):\n                ProjectNames.append(jDataProjectIds[package_counter].get('name'))\n                Package_Tagged_Projects.append(jDataProjectIds[package_counter].get('projectId'))\n                max_num_of_projects += 1\n            package_counter += 1\n    return Package_Tagged_Projects\n\n# (6) Post a file to the Paxata library (Paxata will guess how to parse it), return the new libraryId\ndef post_file_to_paxata_library(auth_token_target,paxata_url_target, new_file_name):\n    new_libraryId = \"\"\n    if new_file_name is None:\n        print(\"File doesn't exist??\")\n    else:\n        ds = str(new_file_name)\n        print(\"Uploading \\\"\" + str(new_file_name) + \"\\\" to Library\")\n        sourcetype = {'source': 'local'}\n        files = {'data': open(ds, 'rb')}\n        dataset_upload_response = \"\"\n        try:\n            dataset_upload_response = requests.post(paxata_url_target + \"/rest/datasource/imports/local\", data=sourcetype,\n                                                  files=files, auth=auth_token_target)\n        except:\n            print(\"Connection error. Please validate the URL provided: \" + paxata_url_target.url)\n        if not (dataset_upload_response.ok):\n            print(\"Couldn't upload the library data. Status Code = \" + str(dataset_upload_response.status_code))\n        else:\n            jDataDataSources = json.loads(dataset_upload_response.content)\n            new_libraryId = jDataDataSources.get('dataFileId')\n    return new_libraryId\n\n# (7) Check if a Project Name exists and return it's ID\ndef check_if_a_project_exists(auth_token,paxata_url,project_name):\n    projectId = \"\"\n    url_request = (paxata_url + \"/rest/projects?name=\" + project_name)\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        jdata_new_project_response = json.loads(my_response.content)\n        if (not jdata_new_project_response):\n            projectId = 0\n        else:\n            projectId = jdata_new_project_response[0]['projectId']\n    else:\n        my_response.raise_for_status()\n    return projectId\n\n# (8) Delete a project based on ID (TEST THIS), not sure if i can access the content directly\ndef delete_a_project_if_it_exists(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/projects/\" + str(projectId))\n    my_response = requests.delete(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", my_response.content.get('name'), \"\\\" deleted.\")\n    else:\n        my_response.raise_for_status()\n\n# (9) Run a Project and publish the answerset to the library\ndef run_a_project(auth_token,paxata_url,projectId):\n    post_request = (paxata_url + \"/rest/project/publish?projectId=\" + projectId + \"&all=true\")\n    postResponse = requests.post(post_request, auth=auth_token, verify=False)\n    if (postResponse.ok):\n        print(\"Project Run - \", projectId)\n    else:\n        print(\"Something went wrong with POST call \", str(postResponse))\n    # I need to investigate the below, sometimes postResponse.content is a dict, sometimes a list, hence the two below trys\n    try:\n        AnswersetId = json.loads(postResponse.content)[0].get('dataFileId')\n    except(AttributeError):\n        AnswersetId = json.loads(postResponse.content).get('dataFileId', 0)\n    return AnswersetId\n\n# (10) Get the script of a projectId\ndef get_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        json_of_empty_project = json.loads(my_response.content)\n    else:\n        json_of_empty_project = 0\n        my_response.raise_for_status()\n    #the below return has an index of 0 to only return the latest version\n    return json_of_empty_project[0]\n\n# (11) Replace values in a json file (useful for updating Paxata project scripts)\ndef replace_json_values(json_file,oldvalue,newvalue):\n    fileinput = json.dumps(json_file)\n    newfileoutput = []\n    newfile = \"\"\n    newfile = re.sub(oldvalue, newvalue, fileinput.rstrip())\n    newfileoutput = json.loads(newfile)\n    return newfileoutput\n\n# (12) Create a new (empty) Paxata project. Will return the projectId\ndef create_a_new_project(auth_token_target,paxata_url_target,Project_Name):\n    projectId = \"\"\n    url_request = (paxata_url_target + \"/rest/projects?name=\" + Project_Name)\n    my_response = requests.post(url_request,auth=auth_token_target , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", Project_Name ,\"\\\" created.\")\n        jdata_new_project_response = json.loads(my_response.content)\n        projectId = jdata_new_project_response['projectId']\n    else:\n        if my_response.status_code == 409:\n            print(\"Project Already Exists\")\n        else:\n            my_response.raise_for_status()\n    return projectId\n\n# (13) Update an existing Project with a new script file (this is not recommended)\ndef update_project_with_new_script(auth_token,paxata_url,final_updated_json_script,projectId,working_path):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(final_updated_json_script)}\n    my_response = requests.put(url_request, data=s, auth=auth_token, verify=False)\n    if (not my_response.ok):\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        with open(working_path + '/invalid_script_dump.json', 'w') as f:\n            json.dump(final_updated_json_script, f)\n        my_response.raise_for_status()\n\n# (14) Get an existing Project's script file\ndef get_new_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId + \"&version=\" + \"0\")\n    myResponse = requests.get(url_request, auth=auth_token, verify=False)\n    if (myResponse.ok):\n        # Loads (Load String) takes a Json file and converts into python data structure (dict or list, depending on JSON)\n        json_of_empty_project = json.loads(myResponse.content)\n    else:\n        json_of_empty_project = 0\n        myResponse.raise_for_status()\n    return(json_of_empty_project)\n\n# (15) Delete Library Data from LibraryID\ndef delete_library_item(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.delete(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        library_name = jdata_datasources.get('name')\n    return library_name\n\n# (16) Export/POST a libraryItem(Answerset) to a target\ndef post_paxata_library_data(auth_token,paxata_url,library_dataset_id,pax_datasourceId,pax_connectorId):\n    post_request = (paxata_url + \"/rest/datasource/exports/\" + pax_datasourceId +\"/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData\n\n# (17) Get DatasourceId and ConnectorId from Name of the Datasource\ndef get_datasource_id_from_name(auth_token,paxata_url,datasource_name):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == datasource_name:\n                pax_datasourceId = jdata_datasources[0].get('dataSourceId')\n                pax_connectorId = jdata_datasources[0].get('connectorId')\n            row_count +=1\n    return pax_datasourceId,pax_connectorId\n\n# (18) Get the UserID From the REST API Token\ndef get_user_from_token(auth_token, paxata_url, resttoken):\n    url_request = (paxata_url + \"/rest/users?authToken=true\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('authToken') == resttoken:\n                pax_name = jdata_datasources[row_count].get('name')\n                pax_userId = jdata_datasources[row_count].get('userId')\n            row_count += 1\n    return pax_userId, pax_name\n\n# (19) Check that an Export/POST to an external source has been completed (version 2.22)\ndef get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token)\n    if (get_response.ok):\n        jdata_datasources = json.loads(get_response.content)\n        print(\"Succesfully have the exportId status\")\n        exportIdStatus = jdata_datasources[0].get('exportId')\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return exportIdStatus\n\n# (20) Check that an Export/POST to an external source has been completed (all versions)\ndef get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token, verify=False)\n    if (get_response.ok):\n        # In version 2.22 postResponse.content is a dict, prior to that it is a list which i need to manually iterate through, hence the two below trys\n        print('.')\n        jdata_datasources = json.loads(get_response.content)\n        try:\n            exportIdState = jdata_datasources.get('state')\n            exporttimeStarted = jdata_datasources.get('timeStarted')\n            exporttimeFinished = jdata_datasources.get('timeFinished')\n        except(AttributeError):\n            row_count = 0\n            for row in jdata_datasources:\n                if jdata_datasources[row_count].get('exportId') == pax_exportId:\n                    exportIdState = jdata_datasources[row_count].get('state')\n                    exporttimeStarted = jdata_datasources[row_count].get('timeStarted')\n                    exporttimeFinished = jdata_datasources[row_count].get('timeFinished')\n                row_count += 1\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return(exportIdState,exporttimeStarted,exporttimeFinished)\n\n# (21) Get the datasetId of a Library from it's Name\ndef get_id_of_datasource(auth_token,paxata_url,dataset_name):\n    url_request = (paxata_url + \"/library/data/\")\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    dataFileId = 0\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == dataset_name:\n                dataFileId = jdata_datasources[row_count].get('dataFileId')\n            row_count +=1\n    return dataFileId\n\n\n# (22) Get all of the datasources (ordered_ for a tenant)\ndef get_datasource_configs(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    myResponse = requests.get(url_request, auth=authorization_token, verify=True)\n    if (myResponse.ok):\n        json_of_datasource_configs = json.loads(myResponse.content)\n    else:\n        json_of_datasource_configs = 0\n        myResponse.raise_for_status()\n    dict_of_datasources = {}\n    for item in json_of_datasource_configs:\n#        dict_of_datasources[item.get('connectorId')] = item.get('name')\n        dict_of_datasources[item.get('name')] = item.get('dataSourceId')\n\n    dict_of_datasources['0'] = ' - No Connector - Data already exists in Paxata - '\n    #returning a sorted dictionary\n    return(OrderedDict(sorted(dict_of_datasources.items(), key=lambda kv:(kv[0].lower(),kv[1]))))\n\n# (23) Get All columns names for a library item\ndef get_library_item_metadata(authorization_token,paxata_url,dataFileID):\n    url_request = (paxata_url + \"/rest/library/data/\"+ dataFileID)\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns\n\n# (24) Get ALl users on a tenant\ndef get_users_on_tenant(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/users\")\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns\n\n# (25) Extract key values out of a json file\ndef extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n\n# (26) Get the Library items of a Project\ndef get_libraryIds_of_lenses_exported_by_project(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/project/publish?projectId=\"+ projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items\n\n\n# (27) Get the information of where a =Library item was exported (ie which external system)\ndef get_info_of_exported_library_items(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/exports\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    json_of_library_items = []\n    if(my_response.ok):\n        library_export_info = json.loads(my_response.content)\n        for item in library_export_info:\n            if item.get('dataFileId') == libraryId:\n                i+=1\n                print(item.get('destination'))\n                print(\"Data Export - \" ,str(i) + \" for libraryId(\" ,libraryId + \")\")\n                print(\"Filename = \" , item.get('destination').get('name'))\n                print(\"Path = \" , item.get('destination').get('itemPath'))\n                print(\"ConnectorID = \" , item.get('destination').get('connectorId') + \"\\n\")\n                json_of_library_items.append(str(item.get('destination')))\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items\n\n\n# (28) Update an existing Project's script file\ndef update_project_with_new_script(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result\n\n# (29) Update an existing Project's Datasource file\ndef update_project_with_new_dataset(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=datasets&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result\n\n# (30) get_name_latest_version_and_schema_of_datasource\ndef get_name_latest_version_and_schema_of_datasource(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request, auth=auth_token, verify=True)\n    library_name = \"\"\n    library_schema_dict = \"\"\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_version = jdata_datasources[0].get('version')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_version,library_schema_dict\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "DataPrep/dataprep_functions.py",
      "tags": [],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "DataPrep/dataprep_functions.py",
        "size": 21427,
        "code_chunks": 32
      },
      "code_examples": [
        "def get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_schema_dict",
        "def get_name_and_schema_of_datasource(auth_token,paxata_url,libraryId,library_version):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId)+\"/\"+str(library_version))\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    library_name = \"\"\n    library_schema_dict = {}\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources.get('name')\n        library_schema_dict = jdata_datasources.get('schema')\n    return library_name,library_schema_dict",
        "def get_tagged_library_items(auth_token,paxata_url,tag):\n    tagged_datasets = []\n    get_tags_request = (paxata_url + \"/rest/library/tags\")\n    get_tags_response = requests.get(get_tags_request, auth=auth_token, verify=False)\n    if (get_tags_response.ok):\n        AllTagsDatasetsJson = json.loads(get_tags_response.content)\n        i=0\n        number_of_datasets = len(AllTagsDatasetsJson)\n        while i < number_of_datasets:\n            if (AllTagsDatasetsJson[i].get('name') == tag):\n                tagged_datasets.append(AllTagsDatasetsJson[i].get('dataFileId'), AllTagsDatasetsJson[i].get('name'))\n            i += 1\n    else:\n        print(\"bad request> \" + get_tags_response.status_code)\n    return tagged_datasets",
        "def get_paxata_library_data(auth_token,paxata_url,library_dataset_id):\n    post_request = (paxata_url + \"/rest/datasource/exports/local/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData",
        "def get_name_of_datasource(auth_token_source,paxata_url_source,libraryId):\n    url_request = (paxata_url_source + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataDataSources = json.loads(my_response.content)\n        libraryName = jDataDataSources[0].get('name')\n    return libraryName",
        "def get_all_project_information(auth_token_source, paxata_url_source, description_tag):\n    Package_Tagged_Projects = []\n    package_counter = 0\n    max_num_of_projects = 0\n    ProjectNames = []\n    url_request = (paxata_url_source + \"/rest/projects\")\n    my_response = requests.get(url_request,auth=auth_token_source , verify=False)\n    if(my_response.ok):\n        jDataProjectIds = json.loads(my_response.content)\n        for item in jDataProjectIds:\n            if description_tag == jDataProjectIds[package_counter].get('description'):\n                ProjectNames.append(jDataProjectIds[package_counter].get('name'))\n                Package_Tagged_Projects.append(jDataProjectIds[package_counter].get('projectId'))\n                max_num_of_projects += 1\n            package_counter += 1\n    return Package_Tagged_Projects",
        "def post_file_to_paxata_library(auth_token_target,paxata_url_target, new_file_name):\n    new_libraryId = \"\"\n    if new_file_name is None:\n        print(\"File doesn't exist??\")\n    else:\n        ds = str(new_file_name)\n        print(\"Uploading \\\"\" + str(new_file_name) + \"\\\" to Library\")\n        sourcetype = {'source': 'local'}\n        files = {'data': open(ds, 'rb')}\n        dataset_upload_response = \"\"\n        try:\n            dataset_upload_response = requests.post(paxata_url_target + \"/rest/datasource/imports/local\", data=sourcetype,\n                                                  files=files, auth=auth_token_target)\n        except:\n            print(\"Connection error. Please validate the URL provided: \" + paxata_url_target.url)\n        if not (dataset_upload_response.ok):\n            print(\"Couldn't upload the library data. Status Code = \" + str(dataset_upload_response.status_code))\n        else:\n            jDataDataSources = json.loads(dataset_upload_response.content)\n            new_libraryId = jDataDataSources.get('dataFileId')\n    return new_libraryId",
        "def check_if_a_project_exists(auth_token,paxata_url,project_name):\n    projectId = \"\"\n    url_request = (paxata_url + \"/rest/projects?name=\" + project_name)\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        jdata_new_project_response = json.loads(my_response.content)\n        if (not jdata_new_project_response):\n            projectId = 0\n        else:\n            projectId = jdata_new_project_response[0]['projectId']\n    else:\n        my_response.raise_for_status()\n    return projectId",
        "def delete_a_project_if_it_exists(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/projects/\" + str(projectId))\n    my_response = requests.delete(url_request,auth=auth_token , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", my_response.content.get('name'), \"\\\" deleted.\")\n    else:\n        my_response.raise_for_status()",
        "def run_a_project(auth_token,paxata_url,projectId):\n    post_request = (paxata_url + \"/rest/project/publish?projectId=\" + projectId + \"&all=true\")\n    postResponse = requests.post(post_request, auth=auth_token, verify=False)\n    if (postResponse.ok):\n        print(\"Project Run - \", projectId)\n    else:\n        print(\"Something went wrong with POST call \", str(postResponse))\n    # I need to investigate the below, sometimes postResponse.content is a dict, sometimes a list, hence the two below trys\n    try:\n        AnswersetId = json.loads(postResponse.content)[0].get('dataFileId')\n    except(AttributeError):\n        AnswersetId = json.loads(postResponse.content).get('dataFileId', 0)\n    return AnswersetId",
        "def get_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        json_of_empty_project = json.loads(my_response.content)\n    else:\n        json_of_empty_project = 0\n        my_response.raise_for_status()\n    #the below return has an index of 0 to only return the latest version\n    return json_of_empty_project[0]",
        "def replace_json_values(json_file,oldvalue,newvalue):\n    fileinput = json.dumps(json_file)\n    newfileoutput = []\n    newfile = \"\"\n    newfile = re.sub(oldvalue, newvalue, fileinput.rstrip())\n    newfileoutput = json.loads(newfile)\n    return newfileoutput",
        "def create_a_new_project(auth_token_target,paxata_url_target,Project_Name):\n    projectId = \"\"\n    url_request = (paxata_url_target + \"/rest/projects?name=\" + Project_Name)\n    my_response = requests.post(url_request,auth=auth_token_target , verify=False)\n    if(my_response.ok):\n        print(\"Project \\\"\", Project_Name ,\"\\\" created.\")\n        jdata_new_project_response = json.loads(my_response.content)\n        projectId = jdata_new_project_response['projectId']\n    else:\n        if my_response.status_code == 409:\n            print(\"Project Already Exists\")\n        else:\n            my_response.raise_for_status()\n    return projectId",
        "def update_project_with_new_script(auth_token,paxata_url,final_updated_json_script,projectId,working_path):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(final_updated_json_script)}\n    my_response = requests.put(url_request, data=s, auth=auth_token, verify=False)\n    if (not my_response.ok):\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        with open(working_path + '/invalid_script_dump.json', 'w') as f:\n            json.dump(final_updated_json_script, f)\n        my_response.raise_for_status()",
        "def get_new_project_script(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/rest/scripts?projectId=\" + projectId + \"&version=\" + \"0\")\n    myResponse = requests.get(url_request, auth=auth_token, verify=False)\n    if (myResponse.ok):\n        # Loads (Load String) takes a Json file and converts into python data structure (dict or list, depending on JSON)\n        json_of_empty_project = json.loads(myResponse.content)\n    else:\n        json_of_empty_project = 0\n        myResponse.raise_for_status()\n    return(json_of_empty_project)",
        "def delete_library_item(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/rest/library/data/\"+str(libraryId))\n    my_response = requests.delete(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        library_name = jdata_datasources.get('name')\n    return library_name",
        "def post_paxata_library_data(auth_token,paxata_url,library_dataset_id,pax_datasourceId,pax_connectorId):\n    post_request = (paxata_url + \"/rest/datasource/exports/\" + pax_datasourceId +\"/\" + library_dataset_id + \"?format=json\")\n    post_response = requests.post(post_request,auth=auth_token)\n    if (post_response.ok):\n        JsonData = json.loads(post_response.content, object_pairs_hook=OrderedDict)\n    return JsonData",
        "def get_datasource_id_from_name(auth_token,paxata_url,datasource_name):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == datasource_name:\n                pax_datasourceId = jdata_datasources[0].get('dataSourceId')\n                pax_connectorId = jdata_datasources[0].get('connectorId')\n            row_count +=1\n    return pax_datasourceId,pax_connectorId",
        "def get_user_from_token(auth_token, paxata_url, resttoken):\n    url_request = (paxata_url + \"/rest/users?authToken=true\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if (my_response.ok):\n        jdata_datasources = json.loads(my_response.content, object_pairs_hook=OrderedDict)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('authToken') == resttoken:\n                pax_name = jdata_datasources[row_count].get('name')\n                pax_userId = jdata_datasources[row_count].get('userId')\n            row_count += 1\n    return pax_userId, pax_name",
        "def get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token)\n    if (get_response.ok):\n        jdata_datasources = json.loads(get_response.content)\n        print(\"Succesfully have the exportId status\")\n        exportIdStatus = jdata_datasources[0].get('exportId')\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return exportIdStatus",
        "def get_paxata_export_status(auth_token,pax_url,pax_exportId):\n    get_request = (pax_url + \"/rest/library/exports?exportId=\" + pax_exportId)\n    get_response = requests.get(get_request,auth=auth_token, verify=False)\n    if (get_response.ok):\n        # In version 2.22 postResponse.content is a dict, prior to that it is a list which i need to manually iterate through, hence the two below trys\n        print('.')\n        jdata_datasources = json.loads(get_response.content)\n        try:\n            exportIdState = jdata_datasources.get('state')\n            exporttimeStarted = jdata_datasources.get('timeStarted')\n            exporttimeFinished = jdata_datasources.get('timeFinished')\n        except(AttributeError):\n            row_count = 0\n            for row in jdata_datasources:\n                if jdata_datasources[row_count].get('exportId') == pax_exportId:\n                    exportIdState = jdata_datasources[row_count].get('state')\n                    exporttimeStarted = jdata_datasources[row_count].get('timeStarted')\n                    exporttimeFinished = jdata_datasources[row_count].get('timeFinished')\n                row_count += 1\n    else:\n        print(\"Unsucessfully tried to get the exportId Status of exportId - \", pax_exportId)\n    return(exportIdState,exporttimeStarted,exporttimeFinished)",
        "def get_id_of_datasource(auth_token,paxata_url,dataset_name):\n    url_request = (paxata_url + \"/library/data/\")\n    my_response = requests.get(url_request,auth=auth_token , verify=False)\n    dataFileId = 0\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        row_count = 0\n        for row in jdata_datasources:\n            if jdata_datasources[row_count].get('name') == dataset_name:\n                dataFileId = jdata_datasources[row_count].get('dataFileId')\n            row_count +=1\n    return dataFileId",
        "def get_datasource_configs(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/datasource/configs\")\n    myResponse = requests.get(url_request, auth=authorization_token, verify=True)\n    if (myResponse.ok):\n        json_of_datasource_configs = json.loads(myResponse.content)\n    else:\n        json_of_datasource_configs = 0\n        myResponse.raise_for_status()\n    dict_of_datasources = {}\n    for item in json_of_datasource_configs:\n#        dict_of_datasources[item.get('connectorId')] = item.get('name')\n        dict_of_datasources[item.get('name')] = item.get('dataSourceId')\n\n    dict_of_datasources['0'] = ' - No Connector - Data already exists in Paxata - '\n    #returning a sorted dictionary\n    return(OrderedDict(sorted(dict_of_datasources.items(), key=lambda kv:(kv[0].lower(),kv[1]))))",
        "def get_library_item_metadata(authorization_token,paxata_url,dataFileID):\n    url_request = (paxata_url + \"/rest/library/data/\"+ dataFileID)\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns",
        "def get_users_on_tenant(authorization_token,paxata_url):\n    url_request = (paxata_url + \"/rest/users\")\n    my_response = requests.get(url_request, auth=authorization_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    list_of_library_columns = []\n    if json_of_library_items[0]['schema']:\n        for item in json_of_library_items[0]['schema']:\n            if item.get('type') == \"String\":\n                list_of_library_columns.append((str(item.get('name'))))\n    return list_of_library_columns",
        "def extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results",
        "def get_libraryIds_of_lenses_exported_by_project(auth_token,paxata_url,projectId):\n    url_request = (paxata_url + \"/project/publish?projectId=\"+ projectId)\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    if(my_response.ok):\n        json_of_library_items = json.loads(my_response.content)\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items",
        "def get_info_of_exported_library_items(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/exports\")\n    my_response = requests.get(url_request, auth=auth_token, verify=False)\n    json_of_library_items = []\n    if(my_response.ok):\n        library_export_info = json.loads(my_response.content)\n        for item in library_export_info:\n            if item.get('dataFileId') == libraryId:\n                i+=1\n                print(item.get('destination'))\n                print(\"Data Export - \" ,str(i) + \" for libraryId(\" ,libraryId + \")\")\n                print(\"Filename = \" , item.get('destination').get('name'))\n                print(\"Path = \" , item.get('destination').get('itemPath'))\n                print(\"ConnectorID = \" , item.get('destination').get('connectorId') + \"\\n\")\n                json_of_library_items.append(str(item.get('destination')))\n    else:\n        json_of_library_items = 0\n        my_response.raise_for_status()\n    return json_of_library_items",
        "def update_project_with_new_script(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=script&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result",
        "def update_project_with_new_dataset(auth_token,paxata_url,updated_json_script,projectId):\n    url_request = (paxata_url + \"/rest/scripts?update=datasets&force=true&projectId=\" + str(projectId))\n    s = {'script': json.dumps(updated_json_script)}\n    myResponse = requests.put(url_request, data=s, auth=auth_token)\n    result = False\n    print(myResponse)\n    if (myResponse.ok):\n        # json_of_existing_project = json.loads(myResponse.content)\n        result = True\n    else:\n        #if there is a problem in updating the project, it would indicate a problem with the script, so lets output it\n        print(myResponse.content)\n        result = False\n    return result",
        "def get_name_latest_version_and_schema_of_datasource(auth_token,paxata_url,libraryId):\n    url_request = (paxata_url + \"/library/data/\"+str(libraryId))\n    my_response = requests.get(url_request, auth=auth_token, verify=True)\n    library_name = \"\"\n    library_schema_dict = \"\"\n    if(my_response.ok):\n        jdata_datasources = json.loads(my_response.content)\n        library_name = jdata_datasources[0].get('name')\n        library_version = jdata_datasources[0].get('version')\n        library_schema_dict = jdata_datasources[0].get('schema')\n    return library_name,library_version,library_schema_dict",
        "def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr"
      ],
      "api_methods": [],
      "complexity_score": 0.55,
      "use_case_category": "data_prep"
    },
    {
      "id": "github_python_-5484562761313981719",
      "title": "run_specifc_blueprints.py",
      "content": "#Author: Thodoris Petropoulos\n\ndef run_specific_blueprints(project_object, search_term, featurelist_id = None):\n    \"\"\"Runs all of the blueprints that match the search term use provides\n        Input:\n        - project_object <DataRobot Project> (Your DataRobot project)\n        - search_term <string> (What to search for in the name of the Blueprint. e.g: \"Gradient\") \n        - featurelist_id <DataRobot Featurelist id> (Optional parameter to specify featurelist to use)\n    \"\"\"\n\n    blueprints = project_object.get_blueprints()\n    models_to_run = [blueprint for blueprint in blueprints if blueprint.model_type == search_term]\n    for model in models_to_run:\n        project_object.train(model, sample_pct = 80, featurelist_id=featurelist_id)\n\n    while len(project_object.get_all_jobs()) > 0:\n    time.sleep(1)\n    pass\n        \n    ",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Modeling/Python/run_specifc_blueprints.py",
      "tags": [
        "blueprint",
        "modeling"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Modeling/Python/run_specifc_blueprints.py",
        "size": 837,
        "code_chunks": 1
      },
      "code_examples": [
        "#Author: Thodoris Petropoulos\n\ndef run_specific_blueprints(project_object, search_term, featurelist_id = None):\n    \"\"\"Runs all of the blueprints that match the search term use provides\n        Input:\n        - project_object <DataRobot Project> (Your DataRobot project)\n        - search_term <string> (What to search for in the name of the Blueprint. e.g: \"Gradient\") \n        - featurelist_id <DataRobot Featurelist id> (Optional parameter to specify featurelist to use)\n    \"\"\"\n\n    blueprints = project_object.get_blueprints()\n    models_to_run = [blueprint for blueprint in blueprints if blueprint.model_type == search_term]\n    for model in models_to_run:\n        project_object.train(model, sample_pct = 80, featurelist_id=featurelist_id)\n\n    while len(project_object.get_all_jobs()) > 0:\n    time.sleep(1)\n    pass\n        \n    "
      ],
      "api_methods": [],
      "complexity_score": 0.55,
      "use_case_category": "modeling"
    },
    {
      "id": "github_python_1361293882597415569",
      "title": "ts_clone_project.py",
      "content": "#Author: Katy Chow Haynie\n\n#Make sure you are connected to DataRobot Client.\n\n\n#This function will help you create a copy of a TS project using the same exact settings. Manipulate as you see fit.\n\nimport datarobot as dr\n\ndef clone_ts_project(pid):\n    \"\"\"This function will copy a DataRobot TS project with the same settings\n        Input:\n         - pid <str> the id of the project you want to copy\n         \n        Manipulate this function as you see fit in case you dont want a complete 1:1 copy.\n    \"\"\"\n    p = dr.Project.get(pid)\n    c_p = p.clone_project('Clone of {}'.format(p.project_name))\n    c_pid = c_p.id\n    \n    #Get datetimePartitioning data\n    #This will include calendar, backtesting and known-in-advance features.\n    dtp = dr.DatetimePartitioning.get(pid)\n    dtp_spec_for_clone = dtp.to_specification()\n    \n    #Fix the datetime_partition_column which will have an ' (actual)' string appended to it.\n    dtp_spec_for_clone.datetime_partition_column = dtp_spec_for_clone.datetime_partition_column.replace(' (actual)','')\n    \n    ##Place changes below##\n    #Manipulate dtp_spec_for_clone as you see fit (you can directly change its attribites)\n    \n    ##\n    \n    c_p.set_target(target = 'Sales',\n                       partitioning_method = dtp_spec_for_clone,\n                       mode = 'auto',\n                       worker_count = -1\n                  )\n    \n##Usage##\n#clone_ts_project('YOUR_PROJECT_ID')",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_clone_project.py",
      "tags": [
        "datarobot-api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_clone_project.py",
        "size": 1438,
        "code_chunks": 1
      },
      "code_examples": [
        "def clone_ts_project(pid):\n    \"\"\"This function will copy a DataRobot TS project with the same settings\n        Input:\n         - pid <str> the id of the project you want to copy\n         \n        Manipulate this function as you see fit in case you dont want a complete 1:1 copy.\n    \"\"\"\n    p = dr.Project.get(pid)\n    c_p = p.clone_project('Clone of {}'.format(p.project_name))\n    c_pid = c_p.id\n    \n    #Get datetimePartitioning data\n    #This will include calendar, backtesting and known-in-advance features.\n    dtp = dr.DatetimePartitioning.get(pid)\n    dtp_spec_for_clone = dtp.to_specification()\n    \n    #Fix the datetime_partition_column which will have an ' (actual)' string appended to it.\n    dtp_spec_for_clone.datetime_partition_column = dtp_spec_for_clone.datetime_partition_column.replace(' (actual)','')\n    \n    ##Place changes below##\n    #Manipulate dtp_spec_for_clone as you see fit (you can directly change its attribites)\n    \n    ##\n    \n    c_p.set_target(target = 'Sales',\n                       partitioning_method = dtp_spec_for_clone,\n                       mode = 'auto',\n                       worker_count = -1\n                  )"
      ],
      "api_methods": [
        "dr.project.get",
        "project.get",
        "dr.datetimepartitioning.get"
      ],
      "complexity_score": 0.8,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_7547452195412838180",
      "title": "ts_clustering.py",
      "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport datetime as dt\nimport operator\n\nimport datarobot as dr\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy.spatial.distance import cdist, pdist, squareform\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom statsmodels.tsa.stattools import pacf\n\nfrom ts_metrics import *\nfrom ts_modeling import create_dr_project\nfrom ts_projects import get_preds_and_actuals\n\n\n####################\n# Series Clustering\n####################\n\n\ndef _split_series(df, series_id, target, by='quantiles', cuts=5, split_col='Cluster'):\n    \"\"\"\n    Split series into clusters by rank or quantile  of average target value\n\n    by: str\n        Rank or quantiles\n    cuts: int\n        Number of clusters\n    split_col: str\n        Name of new column\n\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    group = df.groupby([series_id]).mean()\n\n    if by == 'quantiles':\n        group[split_col] = pd.qcut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    elif by == 'rank':\n        group[split_col] = pd.cut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    else:\n        raise ValueError(f'{by} is not a supported value. Must be set to either quantiles or rank')\n\n    df = df.merge(\n        group[split_col], how='left', left_on=series_id, right_index=True, validate='many_to_one'\n    )\n\n    df[split_col] = df[split_col].astype('str')\n    n_clusters = len(df[split_col].unique())\n    mapper_clusters = {k: v for (k, v) in zip(df[split_col].unique(), range(1, n_clusters + 1))}\n    df[split_col] = df[split_col].map(mapper_clusters)\n\n    return df.reset_index(drop=True)\n\n\ndef _get_pacf_coefs(df, col, nlags, alpha, scale, scale_method):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    df: pandas df\n    col: str\n        Series name\n    nlags: int\n        Number of AR coefficients to include in pacf\n    alpha: float\n        Cutoff value for p-values to determine statistical significance\n    scale: boolean\n        Whether to standardize input data\n    scale_method: str\n        Choose from 'min_max' or 'normalize'\n\n    Returns:\n    --------\n    List of AR(n) coefficients\n\n    \"\"\"\n    if scale:\n        if scale_method == 'min_max':\n            df = df.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)), axis=0)\n        elif scale_method == 'normalize':\n            df = df.apply(lambda x: (x - np.mean(x)) / np.std(x), axis=0)\n        else:\n            raise ValueError(\n                f'{scale_method} is not a supported value. scale_method must be set to either min_max or normalize'\n            )\n\n    # if df[col].dropna().shape[0] == 0:\n    #     print(col, df[col].dropna())\n    # print('Running PAC...')\n    clf = pacf(df[col].dropna(), method='ols', nlags=nlags, alpha=alpha)\n    if alpha:\n        coefs = clf[0][1:]\n        zero_in_interval = [not i[0] < 0 < i[1] for i in clf[1][1:]]\n        adj_coefs = [c if z else 0.0 for c, z in zip(coefs, zero_in_interval)]\n        return adj_coefs\n    else:\n        coefs = clf[1:]\n        return coefs\n\ndef _get_optimal_n_clusters(df, n_series, max_clusters, plot=True):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    Get the number of clusters that results in the max silhouette score\n\n    Returns:\n    --------\n    int\n\n    \"\"\"\n    clusters = list(np.arange(min(max_clusters, n_series)) + 2)[:-1]\n    print(f'Testing {clusters[0]} to {clusters[-1]} clusters')\n    scores = {}\n    d = []\n    for c in clusters:\n        kmean = KMeans(n_clusters=c).fit(df)\n        d.append(sum(np.min(cdist(df, kmean.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n        preds = kmean.predict(df)\n        score = silhouette_score(df, preds, metric='euclidean')\n        scores[c] = score\n        print(f'For n_clusters = {c}, silhouette score is {score}')\n\n    n_clusters = max(scores.items(), key=operator.itemgetter(1))[0]\n    best_score = scores[n_clusters]\n    print(f'optimal n_clusters = {n_clusters}, max silhouette score is {best_score}')\n\n    if max_clusters > 2:\n        if plot:\n            fig = px.line(x=clusters, y=d)\n            fig.update_layout(height=500, width=750, title_text='Kmeans Optimal Number of Clusters')\n            fig.update_xaxes(title='Number of Clusters', range=[clusters[0], clusters[-1]])\n            fig.update_yaxes(title='Distortion')\n            fig.show()\n\n    return n_clusters\n\n\ndef add_cluster_labels(\n    df,\n    ts_settings,\n    method,\n    nlags=None,\n    scale=True,\n    scale_method='min_max',\n    alpha=0.05,\n    split_method=None,\n    n_clusters=None,\n    max_clusters=None,\n    plot=True,\n):\n    \"\"\"\n    Calculates series clusters and appends a column of cluster labels to the input df. This will only work on regularly spaced time series datasets.\n\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    method: type of clustering technique: must choose from either pacf, correlation, or target\n    nlags: int (Optional)\n        Number of AR(n) lags. Only applies to PACF method\n    scale: boolean (Optional)\n        Only applies to PACF method\n    scale_method: str (Optiona)\n        Choose between normalize (subtract the mean and divide by the std) or min_max (subtract the min and divide by the range)\n    split_method: str (Optional)\n        Choose between rank and quanitles. Only applies to target method\n    n_clusters: int\n        Number of clusters to create. If None, defaults to maximum silhouette score\n    max_clusters: int\n        Maximum number of clusters to create. If None, default to the number of series - 1\n\n    Returns:\n    --------\n    Updated pandas df with a new column 'Cluster' of clusters labels\n            -silhouette score per cluster:\n            (The best value is 1 and the worst value is -1. Values near 0 indicate overlapping\n            clusters. Negative values generally indicate that a sample has been assigned to the\n            wrong cluster.)\n            -plot of distortion per cluster\n    \"\"\"\n    target = ts_settings['target']\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    series = df[series_id].unique()\n    n_series = len(series)\n\n    if max_clusters is None:\n        max_clusters = n_series - 1\n\n    assert (\n        1 < max_clusters < n_series\n    ), 'max_clusters must be greater than 1 and less than or equal to the number of unique series -1'\n\n    if n_clusters:\n        assert (\n            1 < n_clusters <= max_clusters\n        ), f'n_clusters must be greater than 1 and less than {max_clusters}'\n\n    c = df.pivot(index=date_col, columns=series_id, values=target)\n\n    if method == 'pacf':\n        d = pd.DataFrame(\n            [_get_pacf_coefs(c, x, nlags, alpha, scale, scale_method) for x in c.columns]\n        )  # ignore missing values\n        d.index = c.columns\n        distances = pdist(d, 'minkowski', p=2)  # 1 for manhattan distance and 2 for euclidean\n        dist_matrix = squareform(distances)\n        dist_df = pd.DataFrame(dist_matrix)\n        dist_df.columns = series\n        dist_df.index = dist_df.columns\n\n    elif method == 'correlation':\n        dist_df = c.corr(method='pearson')\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=1)\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    elif method == 'target':\n        if split_method is not None:\n            if n_clusters:\n                cuts = n_clusters\n            else:\n                cuts = max_clusters\n\n            new_df = _split_series(df, series_id, target, by=split_method, cuts=cuts)\n            return new_df  # exit function\n        else:\n            dist_df = df.groupby(series_id).agg({target: 'mean'})\n\n    else:\n        raise ValueError(\n            f'{method} is not a supported value. Must be set to either pacf, correlation, or target'\n        )\n\n    # Find optimal number of clulsters is n_clusters is not specified\n    if n_clusters is None:\n        n_clusters = _get_optimal_n_clusters(\n            df=dist_df, n_series=n_series, max_clusters=max_clusters, plot=plot\n        )\n\n    kmeans = KMeans(n_clusters).fit(dist_df)\n    labels = kmeans.predict(dist_df)\n\n    df_clusters = (\n        pd.concat([pd.Series(series), pd.Series(labels)], axis=1)\n        .sort_values(by=1)\n        .reset_index(drop=True)\n    )\n    df_clusters.columns = [series_id, 'Cluster']\n\n    df_w_cluster_labels = df.merge(df_clusters, how='left', on=series_id)\n\n    return df_w_cluster_labels.reset_index(drop=True)\n\n\ndef plot_clusters(df, ts_settings, split_col='Cluster', max_sample_size=50000):\n    \"\"\"\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    col: cluster_id columns\n\n    Returns:\n    --------\n    Plotly bar plot\n\n    \"\"\"\n    assert split_col in df.columns, f'{split_col} must be a column in the df'\n\n    date_col = ts_settings['date_col']\n    target = ts_settings['target']\n    series_id = ts_settings['series_id']\n\n    n_clusters = len(df[split_col].unique())\n\n    if df.shape[0] > max_sample_size:  # limit the data points displayed in the charts to reduce lag\n        df = df.sample(n=max_sample_size).reset_index(drop=True)\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[split_col, date_col], inplace=True)\n    df_agg = df.groupby([split_col, date_col]).agg({target: 'mean'}).reset_index()\n    groups = df_agg.groupby([split_col])\n    fig = make_subplots(rows=n_clusters, cols=1)\n\n    a = 1\n    for name, group in groups:\n        n_series = len(df.loc[df[split_col] == name, series_id].unique())\n        fig.append_trace(\n            go.Line(\n                x=group[date_col], y=group[target], name=f'{split_col}={name} - {n_series} Series'\n            ),\n            row=a,\n            col=1,\n        )\n        a += 1\n\n    fig.update_layout(height=1000, width=1000, title_text=\"Cluster Subplots\")\n    fig.show()\n\n\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_clustering.py",
      "tags": [
        "pandas",
        "datarobot-api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_clustering.py",
        "size": 10020,
        "code_chunks": 5
      },
      "code_examples": [
        "def _split_series(df, series_id, target, by='quantiles', cuts=5, split_col='Cluster'):\n    \"\"\"\n    Split series into clusters by rank or quantile  of average target value\n\n    by: str\n        Rank or quantiles\n    cuts: int\n        Number of clusters\n    split_col: str\n        Name of new column\n\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    group = df.groupby([series_id]).mean()\n\n    if by == 'quantiles':\n        group[split_col] = pd.qcut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    elif by == 'rank':\n        group[split_col] = pd.cut(group[target], cuts, labels=np.arange(1, cuts + 1))\n    else:\n        raise ValueError(f'{by} is not a supported value. Must be set to either quantiles or rank')\n\n    df = df.merge(\n        group[split_col], how='left', left_on=series_id, right_index=True, validate='many_to_one'\n    )\n\n    df[split_col] = df[split_col].astype('str')\n    n_clusters = len(df[split_col].unique())\n    mapper_clusters = {k: v for (k, v) in zip(df[split_col].unique(), range(1, n_clusters + 1))}\n    df[split_col] = df[split_col].map(mapper_clusters)\n\n    return df.reset_index(drop=True)",
        "def _get_pacf_coefs(df, col, nlags, alpha, scale, scale_method):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    df: pandas df\n    col: str\n        Series name\n    nlags: int\n        Number of AR coefficients to include in pacf\n    alpha: float\n        Cutoff value for p-values to determine statistical significance\n    scale: boolean\n        Whether to standardize input data\n    scale_method: str\n        Choose from 'min_max' or 'normalize'\n\n    Returns:\n    --------\n    List of AR(n) coefficients\n\n    \"\"\"\n    if scale:\n        if scale_method == 'min_max':\n            df = df.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)), axis=0)\n        elif scale_method == 'normalize':\n            df = df.apply(lambda x: (x - np.mean(x)) / np.std(x), axis=0)\n        else:\n            raise ValueError(\n                f'{scale_method} is not a supported value. scale_method must be set to either min_max or normalize'\n            )\n\n    # if df[col].dropna().shape[0] == 0:\n    #     print(col, df[col].dropna())\n    # print('Running PAC...')\n    clf = pacf(df[col].dropna(), method='ols', nlags=nlags, alpha=alpha)\n    if alpha:\n        coefs = clf[0][1:]\n        zero_in_interval = [not i[0] < 0 < i[1] for i in clf[1][1:]]\n        adj_coefs = [c if z else 0.0 for c, z in zip(coefs, zero_in_interval)]\n        return adj_coefs\n    else:\n        coefs = clf[1:]\n        return coefs",
        "def _get_optimal_n_clusters(df, n_series, max_clusters, plot=True):\n    \"\"\"\n    Helper function for add_cluster_labels()\n\n    Get the number of clusters that results in the max silhouette score\n\n    Returns:\n    --------\n    int\n\n    \"\"\"\n    clusters = list(np.arange(min(max_clusters, n_series)) + 2)[:-1]\n    print(f'Testing {clusters[0]} to {clusters[-1]} clusters')\n    scores = {}\n    d = []\n    for c in clusters:\n        kmean = KMeans(n_clusters=c).fit(df)\n        d.append(sum(np.min(cdist(df, kmean.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n        preds = kmean.predict(df)\n        score = silhouette_score(df, preds, metric='euclidean')\n        scores[c] = score\n        print(f'For n_clusters = {c}, silhouette score is {score}')\n\n    n_clusters = max(scores.items(), key=operator.itemgetter(1))[0]\n    best_score = scores[n_clusters]\n    print(f'optimal n_clusters = {n_clusters}, max silhouette score is {best_score}')\n\n    if max_clusters > 2:\n        if plot:\n            fig = px.line(x=clusters, y=d)\n            fig.update_layout(height=500, width=750, title_text='Kmeans Optimal Number of Clusters')\n            fig.update_xaxes(title='Number of Clusters', range=[clusters[0], clusters[-1]])\n            fig.update_yaxes(title='Distortion')\n            fig.show()\n\n    return n_clusters",
        "def add_cluster_labels(\n    df,\n    ts_settings,\n    method,\n    nlags=None,\n    scale=True,\n    scale_method='min_max',\n    alpha=0.05,\n    split_method=None,\n    n_clusters=None,\n    max_clusters=None,\n    plot=True,\n):\n    \"\"\"\n    Calculates series clusters and appends a column of cluster labels to the input df. This will only work on regularly spaced time series datasets.\n\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    method: type of clustering technique: must choose from either pacf, correlation, or target\n    nlags: int (Optional)\n        Number of AR(n) lags. Only applies to PACF method\n    scale: boolean (Optional)\n        Only applies to PACF method\n    scale_method: str (Optiona)\n        Choose between normalize (subtract the mean and divide by the std) or min_max (subtract the min and divide by the range)\n    split_method: str (Optional)\n        Choose between rank and quanitles. Only applies to target method\n    n_clusters: int\n        Number of clusters to create. If None, defaults to maximum silhouette score\n    max_clusters: int\n        Maximum number of clusters to create. If None, default to the number of series - 1\n\n    Returns:\n    --------\n    Updated pandas df with a new column 'Cluster' of clusters labels\n            -silhouette score per cluster:\n            (The best value is 1 and the worst value is -1. Values near 0 indicate overlapping\n            clusters. Negative values generally indicate that a sample has been assigned to the\n            wrong cluster.)\n            -plot of distortion per cluster\n    \"\"\"\n    target = ts_settings['target']\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    series = df[series_id].unique()\n    n_series = len(series)\n\n    if max_clusters is None:\n        max_clusters = n_series - 1\n\n    assert (\n        1 < max_clusters < n_series\n    ), 'max_clusters must be greater than 1 and less than or equal to the number of unique series -1'\n\n    if n_clusters:\n        assert (\n            1 < n_clusters <= max_clusters\n        ), f'n_clusters must be greater than 1 and less than {max_clusters}'\n\n    c = df.pivot(index=date_col, columns=series_id, values=target)\n\n    if method == 'pacf':\n        d = pd.DataFrame(\n            [_get_pacf_coefs(c, x, nlags, alpha, scale, scale_method) for x in c.columns]\n        )  # ignore missing values\n        d.index = c.columns\n        distances = pdist(d, 'minkowski', p=2)  # 1 for manhattan distance and 2 for euclidean\n        dist_matrix = squareform(distances)\n        dist_df = pd.DataFrame(dist_matrix)\n        dist_df.columns = series\n        dist_df.index = dist_df.columns\n\n    elif method == 'correlation':\n        dist_df = c.corr(method='pearson')\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=1)\n        dist_df = dist_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n\n    elif method == 'target':\n        if split_method is not None:\n            if n_clusters:\n                cuts = n_clusters\n            else:\n                cuts = max_clusters\n\n            new_df = _split_series(df, series_id, target, by=split_method, cuts=cuts)\n            return new_df  # exit function\n        else:\n            dist_df = df.groupby(series_id).agg({target: 'mean'})\n\n    else:\n        raise ValueError(\n            f'{method} is not a supported value. Must be set to either pacf, correlation, or target'\n        )\n\n    # Find optimal number of clulsters is n_clusters is not specified\n    if n_clusters is None:\n        n_clusters = _get_optimal_n_clusters(\n            df=dist_df, n_series=n_series, max_clusters=max_clusters, plot=plot\n        )\n\n    kmeans = KMeans(n_clusters).fit(dist_df)\n    labels = kmeans.predict(dist_df)\n\n    df_clusters = (\n        pd.concat([pd.Series(series), pd.Series(labels)], axis=1)\n        .sort_values(by=1)\n        .reset_index(drop=True)\n    )\n    df_clusters.columns = [series_id, 'Cluster']\n\n    df_w_cluster_labels = df.merge(df_clusters, how='left', on=series_id)\n\n    return df_w_cluster_labels.reset_index(drop=True)",
        "def plot_clusters(df, ts_settings, split_col='Cluster', max_sample_size=50000):\n    \"\"\"\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    col: cluster_id columns\n\n    Returns:\n    --------\n    Plotly bar plot\n\n    \"\"\"\n    assert split_col in df.columns, f'{split_col} must be a column in the df'\n\n    date_col = ts_settings['date_col']\n    target = ts_settings['target']\n    series_id = ts_settings['series_id']\n\n    n_clusters = len(df[split_col].unique())\n\n    if df.shape[0] > max_sample_size:  # limit the data points displayed in the charts to reduce lag\n        df = df.sample(n=max_sample_size).reset_index(drop=True)\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[split_col, date_col], inplace=True)\n    df_agg = df.groupby([split_col, date_col]).agg({target: 'mean'}).reset_index()\n    groups = df_agg.groupby([split_col])\n    fig = make_subplots(rows=n_clusters, cols=1)\n\n    a = 1\n    for name, group in groups:\n        n_series = len(df.loc[df[split_col] == name, series_id].unique())\n        fig.append_trace(\n            go.Line(\n                x=group[date_col], y=group[target], name=f'{split_col}={name} - {n_series} Series'\n            ),\n            row=a,\n            col=1,\n        )\n        a += 1\n\n    fig.update_layout(height=1000, width=1000, title_text=\"Cluster Subplots\")\n    fig.show()"
      ],
      "api_methods": [],
      "complexity_score": 0.7,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_-8008418534146930898",
      "title": "ts_create_project_advanced.py",
      "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport datetime as dt\nimport time\n\nimport datarobot as dr\nimport numpy as np\nimport pandas as pd\n\nfrom ts_data_quality import get_timestep #ts_data_quality is a helper function that exists within this Repo\n\n\n###################\n# Project Creation\n###################\n\n\ndef create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project\n\n\ndef create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects\n\n\ndef wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_create_project_advanced.py",
      "tags": [
        "numpy",
        "blueprint",
        "autopilot",
        "pandas",
        "time-series",
        "datarobot-api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_create_project_advanced.py",
        "size": 10104,
        "code_chunks": 3
      },
      "code_examples": [
        "def create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project",
        "def create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects",
        "def wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')"
      ],
      "api_methods": [
        "project.set_target",
        "dr.partitioning_methods.construct_duration_string",
        "project.create",
        "dr.autopilot_mode.full_auto",
        "dr.project.create"
      ],
      "complexity_score": 1.0,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_7309538084824883239",
      "title": "ts_data_quality_check.py",
      "content": "#Authors:  Mark Philip, Justin Swansburg\n\nimport datetime as dt\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport statsmodels.api as sm\n\n\n###################################\n# Time Series Data Quality Checks\n###################################\n\n\nclass DataQualityCheck:\n    \"\"\"\n    A class used to capture summary stats and data quality checks prior to uploading time series data to DataRobot\n    Attributes:\n    -----------\n    df : DataFrame\n        time series data, including a date column and target variable at a minimum\n    settings : dict\n        definitions of date_col, target_col, series_id and time series parameters\n    stats : dict\n        summary statistics generated from `calc_summary_stats`\n    duplicate_dates : int\n        duplicate dates in the time series date_col\n    series_timesteps : series\n        steps between time units for each series_id\n    series_max_gap : series\n        maximum time gap per series\n    series_lenth : series\n        length of each series_id\n    series_pct : series\n        percent of series with complete time steps\n    irregular : boolean\n        True if df contains irregular time series data\n    series_negative_target_pct : float\n        Percent of target values that are negative\n    Methods:\n    --------\n    calc_summary_stats(settings, df)\n        generates a dictionary of summary statistics\n    calc_time_steps(settings, df)\n        calculate time steps per series_id\n    hierarchical_check(settings, df)\n        check if time series data passes heirarchical check\n    zero_inflated_check(settings, df)\n        check if target value contains zeros\n    negative_values_check(settings, df)\n        check if target value contains negative values\n    time_steps_gap_check(settings, df)\n        check if any series has missing time steps\n    irregular_check(settings, df)\n        check is time series data irregular\n    \"\"\"\n\n    def __init__(self, df, ts_settings):\n        self.df = df\n        self.settings = ts_settings\n        self.stats = None\n        self.duplicate_dates = None\n        self.series_time_steps = None\n        self.series_length = None\n        self.series_pct = None\n        self.irregular = None\n        self.series_negative_target_pct = None\n        self.project_time_unit = None\n        self.project_time_step = None\n        self.calc_summary_stats()\n        self.calc_time_steps()\n        self.run_all_checks()\n\n    def calc_summary_stats(self):\n        \"\"\"\n        Analyze time series data to perform checks and gather summary statistics prior to modeling.\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df[date_col] = pd.to_datetime(df[date_col])\n        df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n        # Create dictionary of helpful statistics\n        stats = dict()\n\n        stats['rows'] = df.shape[0]\n        stats['columns'] = df.shape[1]\n        stats['min_' + str(target)] = df[target].min()\n        stats['max_' + str(target)] = df[target].max()\n        stats['series'] = len(df[series_id].unique())\n        stats['start_date'] = df[date_col].min()\n        stats['end_date'] = df[date_col].max()\n        stats['timespan'] = stats['end_date'] - stats['start_date']\n        stats['median_timestep'] = df.groupby([series_id])[date_col].diff().median()\n        stats['min_timestep'] = df.groupby([series_id])[date_col].diff().min()\n        stats['max_timestep'] = df.groupby([series_id])[date_col].diff().max()\n\n        # create data for histogram of series lengths\n        stats['series_length'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.max() - x.min())\n            / stats['median_timestep']\n        )\n\n        # calculate max gap per series\n        stats['series_max_gap'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max())\n            / stats['median_timestep']\n        )\n\n        self.stats = stats\n\n    def calc_percent_missing(self, missing_value=np.nan):\n        \"\"\"\n        Calculate percentage of rows where target is np.nan\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if np.isnan(missing_value):\n            percent_missing = sum(np.isnan(df[target])) / len(df)\n        else:\n            percent_missing = sum(df[target] == missing_value) / len(df)\n\n        self.stats['percent_missing'] = percent_missing\n        print('{:0.2f}% of the rows are missing a target value'.format(percent_missing * 100))\n\n    def get_zero_inflated_series(self, cutoff=0.99):\n        \"\"\"\n        Identify series where the target is 0.0 in more than x% of the rows\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        assert 0 < cutoff <= 1.0, 'cutoff must be between 0 and 1'\n\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n        series = df[df >= cutoff].index.values\n\n        pct = len(series) / self.stats['series']\n\n        print(\n            '{:0.2f}% series have zeros in more than {:0.2f}% or more of the rows'.format(\n                pct * 100, cutoff * 100\n            )\n        )\n\n    def calc_time_steps(self):\n        \"\"\"\n        Calculate timesteps per series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # create data for histogram of timestep\n        series_timesteps = df.groupby([series_id])[date_col].diff() / self.stats['median_timestep']\n        self.series_time_steps = series_timesteps\n\n    def hierarchical_check(self):\n        \"\"\"\n        Calculate percentage of series that appear on each timestep\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # Test if series passes the hierarchical check\n        series_pct = df.groupby([date_col])[series_id].apply(\n            lambda x: x.count() / self.stats['series']\n        )\n        if np.where(series_pct > 0.95, 1, 0).mean() > 0.95:\n            self.stats['passes_hierarchical_check'] = True\n            print(\n                'Data passes hierarchical check! DataRobot hierarchical blueprints will run if you enable cross series features.'\n            )\n        else:\n            print('Data fails hierarchical check! No hierarchical blueprints will run.')\n            self.stats['passes_hierarchical_check'] = False\n\n        self.series_pct = series_pct\n\n    def zero_inflated_check(self):\n        \"\"\"\n        Check if minimum target value is 0.0\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if min(df[target]) == 0:\n            self.stats['passes_zero_inflated_check'] = False\n            print('The minimum target value is zero. Zero-Inflated blueprints will run.')\n        else:\n            self.stats['passes_zero_inflated_check'] = True\n            print('Minimum target value is <> 0. Zero-inflated blueprints will not run.')\n\n    def negative_values_check(self):\n        \"\"\"\n        Check if any series contain negative values. If yes, identify and call out which series by id.\n        \"\"\"\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df['target_sign'] = np.sign(df[target])\n\n        try:\n            # Get percent of series that have at least one negative value\n            any_series_negative = (\n                df.groupby([series_id])['target_sign'].value_counts().unstack()[-1]\n            )\n            series_negative_target_pct = np.sign(any_series_negative).sum() / len(\n                df[series_id].unique()\n            )\n            df.drop('target_sign', axis=1, inplace=True)\n            self.stats['passes_negative_values_check'] = False\n\n            print(\n                '{0:.2f}% of series have at least one negative {1} value.'.format(\n                    (round(series_negative_target_pct * 100), 2), target\n                )\n            )\n\n            # Identify which series have negative values\n            # print('{} contain negative values. Consider creating a seperate project for these series.'.format(any_series_negative[any_series_negative == 1].index.values))\n        except:\n            series_negative_target_pct = 0\n            self.stats['passes_negative_values_check'] = True\n            print('No negative values are contained in {}.'.format(target))\n\n        self.series_negative_target_pct = series_negative_target_pct\n\n    def new_series_check(self):\n        \"\"\"\n        Check if any series start after the the minimum datetime\n        \"\"\"\n        min_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].min()\n        new_series = min_dates > self.stats['start_date'] + dt.timedelta(days=30)\n\n        if new_series.sum() == 0:\n            self.stats['series_introduced_over_time'] = False\n            print('No new series were introduced after the start of the training data')\n        else:\n            self.stats['series_introduced_over_time'] = True\n            print(\n                'Warning: You may encounter new series at prediction time. \\n {0:.2f}% of the series appeared after the start of the training data'.format(\n                    round(new_series.mean() * 100, 0)\n                )\n            )\n\n    def old_series_check(self):\n        \"\"\"\n        Check if any series end before the maximum datetime\n        \"\"\"\n        max_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].max()\n        old_series = max_dates < self.stats['end_date'] - dt.timedelta(days=30)\n\n        if old_series.sum() == 0:\n            self.stats['series_removed_over_time'] = False\n            print('No series were removed before the end of the training data')\n        else:\n            self.stats['series_removed_over_time'] = True\n            print(\n                'Warning: You may encounter fewer series at prediction time. \\n {0:.2f}% of the series were removed before the end of the training data'.format(\n                    round(old_series.mean() * 100, 0)\n                )\n            )\n\n    def leading_or_trailing_zeros_check(self, threshold=5, drop=True):\n        \"\"\"\n        Check for contain consecutive zeros at the beginning or end of each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        new_df = remove_leading_and_trailing_zeros(\n            df,\n            series_id,\n            date_col,\n            target,\n            leading_threshold=threshold,\n            trailing_threshold=threshold,\n            drop=drop,\n        )\n\n        if new_df.shape[0] < df.shape[0]:\n            print(f'Warning: Leading and trailing zeros detected within series')\n        else:\n            print(f'No leading or trailing zeros detected within series')\n\n    def duplicate_dates_check(self):\n        \"\"\"\n        Check for duplicate datetimes within each series\n        \"\"\"\n\n        duplicate_dates = self.df.groupby([self.settings['series_id'], self.settings['date_col']])[\n            self.settings['date_col']\n        ].count()\n        duplicate_dates = duplicate_dates[duplicate_dates > 1]\n        if len(duplicate_dates) == 0:\n            print(f'No duplicate timestamps detected within any series')\n            self.stats['passes_duplicate_timestamp_check'] = True\n        else:\n            print('Warning: Data contains duplicate timestamps within series!')\n            self.stats['passes_duplicate_timestamp_check'] = False\n\n    def time_steps_gap_check(self):\n        \"\"\"\n        Check for missing timesteps within each series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n        gap_size = self.stats['median_timestep']\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # check is series has any missing time steps\n        self.stats['pct_series_w_gaps'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max()) > gap_size\n        ).mean()\n\n        print(\n            '{0:.2f}% of series have at least one missing time step.'.format(\n                round(self.stats['pct_series_w_gaps'] * 100), 2\n            )\n        )\n\n    def _get_spacing(self, df, project_time_unit):\n        \"\"\"\n        Helper function for self.irregular_check()\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        project_time_unit = self.project_time_unit\n        ts_settings = self.settings\n        date_col = ts_settings['date_col']\n        series_id = ts_settings['series_id']\n\n        df['indicator'] = 1\n        df = fill_missing_dates(df=df, ts_settings=ts_settings)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        sums = df.groupby([series_id, project_time_unit])['indicator'].sum()\n        counts = df.groupby([series_id, project_time_unit])['indicator'].agg(\n            lambda x: x.fillna(0).count()\n        )\n\n        pcts = sums / counts\n\n        irregular = pcts.reset_index(drop=True) < 0.8\n        irregular = irregular[irregular]\n\n        return irregular\n\n    def irregular_check(self, plot=False):\n        \"\"\"\n        Check for irregular spacing within each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        df = self.df.copy()\n\n        # first cast date column to a pandas datetime type\n        df[date_col] = pd.to_datetime(df[date_col])\n\n        project_time_unit, project_time_step = get_timestep(self.df, self.settings)\n\n        self.project_time_unit = project_time_unit\n        self.project_time_step = project_time_step\n\n        print('Project Timestep: ', project_time_step, ' ', project_time_unit)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        # Plot histogram of timesteps\n        time_unit_counts = df[project_time_unit].value_counts()\n\n        if plot:\n            time_unit_percent = time_unit_counts / sum(time_unit_counts.values)\n\n            fig = px.bar(\n                time_unit_percent,\n                x=time_unit_percent.index,\n                y=time_unit_percent.values,\n                title=f'Percentage of records per {project_time_unit}',\n            )\n            fig.update_xaxes(title=project_time_unit)\n            fig.update_yaxes(title='Percentage')\n            fig.show()\n\n        # Detect uncommon time steps\n        # If time bin has less than 30% of most common bin then it is an uncommon time bin\n        uncommon_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) < 0.3].index\n        )\n        common_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) >= 0.3].index\n        )\n\n        if len(uncommon_time_bins) > 0:\n            print(f'Uncommon {project_time_unit}s:', uncommon_time_bins)\n        else:\n            print('There are no uncommon time steps')\n\n        # Detect irregular series\n        df = df.loc[df[project_time_unit].isin(common_time_bins), :]\n        irregular_series = self._get_spacing(df, project_time_unit)\n\n        if len(irregular_series) > 0:\n            print(\n                'Series are irregularly spaced. Projects will only be able to run in row-based mode!'\n            )\n            self.stats['passes_irregular_check'] = False\n        else:\n            self.stats['passes_irregular_check'] = True\n            print(\n                'Timesteps are regularly spaced. You will be able to run projects in either time-based or row-based mode'\n            )\n\n    def detect_periodicity(self, alpha=0.05):\n        \"\"\"\n        Calculate project-level periodicity\n        \"\"\"\n\n        timestep = self.project_time_unit\n        df = self.df\n        target = self.settings['target']\n        date_col = self.settings['date_col']\n        metric = self.settings['metric']\n\n        metrics = {\n            'LogLoss': sm.families.Binomial(),\n            'RMSE': sm.families.Gaussian(),\n            'Poisson Deviance': sm.families.Poisson(),\n            'Gamma Deviance': sm.families.Gamma(),\n        }\n\n        periodicity = {\n            'moh': 'hourly',\n            'hod': 'daily',\n            'dow': 'weekly',\n            'dom': 'monthly',\n            'month': 'yearly',\n        }\n\n        try:\n            loss = metrics[metric]\n        except KeyError:\n            loss = metrics['RMSE']\n\n        # Instantiate a glm with the default link function.\n        df[date_col] = pd.to_datetime(df[date_col])\n        df = df.loc[np.isfinite(df[target]), :].copy()\n\n        df['moh'] = df[date_col].dt.minute\n        df['hod'] = df[date_col].dt.hour\n        df['dow'] = df[date_col].dt.dayofweek\n        df['dom'] = df[date_col].dt.day\n        df['month'] = df[date_col].dt.month\n\n        if timestep == 'minute':\n            inputs = ['moh', 'hod', 'dow', 'dom', 'month']\n        elif timestep == 'hour':\n            inputs = ['hod', 'dow', 'dom', 'month']\n        elif timestep == 'day':\n            inputs = ['dow', 'dom', 'month']\n        elif timestep == 'week':\n            inputs = ['month']\n        else:\n            raise ValueError('timestep has to be either minute, hour, day, week, or month')\n\n        output = []\n        for i in inputs:\n            x = pd.DataFrame(df[i])\n            y = df[target]\n\n            x = pd.get_dummies(x.astype('str'), drop_first=True)\n            x['const'] = 1\n\n            clf = sm.GLM(endog=y, exog=x, family=loss)\n            model = clf.fit()\n\n            if any(model.pvalues[:-1] <= alpha):\n                output.append(periodicity[i])\n                # print(f'Detected periodicity: {periodicity[i]}')\n                # return periodicity[i]\n\n        if len(output) > 0:\n            print(f'Detected periodicity: {output}')\n        else:\n            print('No periodicity detected')\n\n    def run_all_checks(self):\n        \"\"\"\n        Runner function to run all data checks in one call\n        \"\"\"\n        print('Running all data quality checks...\\n')\n\n        series = self.stats['series']\n        start_date = self.stats['start_date']\n        end_date = self.stats['end_date']\n        rows = self.stats['rows']\n        cols = self.stats['columns']\n\n        print(f'There are {rows} rows and {cols} columns')\n        print(f'There are {series} series')\n        print(f'The data spans from  {start_date} to {end_date}')\n\n        self.hierarchical_check()\n        self.zero_inflated_check()\n        self.new_series_check()\n        self.old_series_check()\n        self.duplicate_dates_check()\n        self.leading_or_trailing_zeros_check()\n        self.time_steps_gap_check()\n        self.calc_percent_missing()\n        self.get_zero_inflated_series()\n        self.irregular_check()\n        self.detect_periodicity()\n\n\ndef get_timestep(df, ts_settings):\n    \"\"\"\n    Calculate the project-level timestep\n    Returns:\n    --------\n    project_time_unit: minute, hour, day, week, or month\n    project_time_step: int\n    Examples:\n    --------\n    '1 days'\n    '4 days'\n    '1 week'\n    '2 months'\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    # Cast date column to a pandas datetime type and sort df\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n    # Calculate median timestep\n    deltas = df.groupby([series_id])[date_col].diff().reset_index(drop=True)\n    median_timestep = deltas.apply(lambda x: x.total_seconds()).median()\n\n    # Logic to detect project time step and time unit\n    if (60 <= median_timestep < 3600) & (median_timestep % 60 == 0):\n        project_time_unit = 'minute'\n        project_time_step = int(median_timestep / 60)\n        df['minute'] = df[date_col].dt.minute\n    elif (3600 <= median_timestep < 86400) & (median_timestep % 3600 == 0):\n        project_time_unit = 'hour'\n        project_time_step = int(median_timestep / 3600)\n        df['hour'] = df[date_col].dt.hour\n    elif (86400 <= median_timestep < 604800) & (median_timestep % 86400 == 0):\n        project_time_unit = 'day'\n        project_time_step = int(median_timestep / 86400)\n        df['day'] = df[date_col].dt.strftime('%A')\n    elif (604800 <= median_timestep < 2.628e6) & (median_timestep % 604800 == 0):\n        project_time_unit = 'week'\n        project_time_step = int(median_timestep / 604800)\n        df['week'] = df[date_col].dt.week\n    elif (median_timestep >= 2.628e6) & (median_timestep % 2.628e6 == 0):\n        project_time_unit = 'month'\n        project_time_step = int(median_timestep / 2.628e6)\n        df['month'] = df[date_col].dt.month\n    else:\n        raise ValueError(f'{median_timestep} seconds is not a supported timestep')\n\n    # print('Project Timestep: 1', project_time_unit)\n\n    return project_time_unit, project_time_step\n\n\ndef _reindex_dates(group, freq):\n    \"\"\"\n    Helper function for fill_missing_dates()\n    \"\"\"\n    date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    group = group.reindex(date_range)\n    return group\n\n\ndef fill_missing_dates(df, ts_settings, freq=None):\n    \"\"\"\n    Insert rows with np.nan targets for series with missing timesteps between the series start and end dates\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    freq: project time unit and timestep\n    Returns:\n    --------\n    pandas df with inserted rows\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    if freq is None:\n        mapper = {'minute': 'min', 'hour': 'H', 'day': 'D', 'week': 'W', 'month': 'M'}\n        project_time_unit, project_time_step = get_timestep(df, ts_settings)\n        freq = str(project_time_step) + mapper[project_time_unit]\n\n    df = (\n        df.set_index(date_col)\n        .groupby(series_id)\n        .apply(_reindex_dates, freq)\n        .rename_axis((series_id, date_col))\n        .drop(series_id, axis=1)\n        .reset_index()\n    )\n\n    return df.reset_index(drop=True)\n\n\ndef _remove_leading_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    min_date = df_non_zero[date_col].min()\n    df_begin = df[df[date_col] < min_date]\n    if df_begin[target].dropna().shape[0] >= threshold or pd.isnull(min_date):\n        if drop:\n            if pd.isnull(min_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] >= min_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(min_date) or row[date_col] < min_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df\n\n\ndef _remove_trailing_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    max_date = df_non_zero[date_col].max()\n    df_end = df[df[date_col] > max_date]\n    if df_end[target].dropna().shape[0] >= threshold or pd.isnull(max_date):\n        if drop:\n            if pd.isnull(max_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] <= max_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(max_date) or row[date_col] > max_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df\n\n\ndef remove_leading_and_trailing_zeros(\n    df, series_id, date_col, target, leading_threshold=5, trailing_threshold=5, drop=False\n):\n    \"\"\"\n    Remove excess zeros at the beginning or end of series\n    df: pandas df\n    leading_threshold: minimum number of consecutive zeros at the beginning of a series before rows are dropped\n    trailing_threshold: minimum number of consecutive zeros at the end of series before rows are dropped\n    drop: specifies whether to drop the zeros or set them to np.nan\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_leading_zeros, date_col, target, leading_threshold, drop)\n        .reset_index(drop=True)\n    )\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_trailing_zeros, date_col, target, trailing_threshold, drop)\n        .reset_index(drop=True)\n    )\n\n    return df.reset_index(drop=True)\n\n\n#####################\n# Data Visualization\n#####################\n\n\ndef _cut_series_by_rank(df, ts_settings, n=1, top=True):\n    df_agg = df.groupby(ts_settings['series_id']).mean()\n    selected_series_names = (\n        df_agg.sort_values(by=ts_settings['target'], ascending=top).tail(n).index.values\n    )\n\n    return selected_series_names\n\n\ndef _cut_series_by_quantile(df, ts_settings, quantile=0.95, top=True):\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df_agg = df.groupby(series_id).mean()\n\n    if top:\n        selected_series_names = df_agg[\n            df_agg[target] >= df_agg[target].quantile(quantile)\n        ].index.values\n    else:\n        selected_series_names = df_agg[\n            df_agg[target] <= df_agg[target].quantile(quantile)\n        ].index.values\n\n    return selected_series_names\n\n\ndef plot_series_average(df, settings):\n    date_col = settings['date_col']\n    target = settings['target']\n\n    # Average of all series over time\n    df_agg = df.groupby(date_col).mean()\n    df_agg['Date'] = pd.to_datetime(df_agg.index.values)\n\n    fig = px.line(df_agg, x='Date', y=target)\n    fig.update_layout(title_text='Average of all Series')\n    fig.show()\n\n\ndef plot_individual_series(df, ts_settings, n=None, top=True):\n    \"\"\"\n    Plot individual series on the same chart\n    n: (int) number of series to plot\n    top: (boolean) whether to select the top n largest or smallest series ranked by average target value\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    if n is None:\n        n = len(df[series_id].unique())\n\n    series = _cut_series_by_rank(df, ts_settings, n=n, top=top)\n    df_subset = df[df[series_id].isin(series)]\n\n    fig = px.line(df_subset, x=date_col, y=target, color=df_subset[series_id])\n    fig.update_layout(title_text='Top Series By Target Over Time')\n    fig.show()\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_data_quality_check.py",
      "tags": [
        "pandas",
        "numpy",
        "blueprint"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_data_quality_check.py",
        "size": 27776,
        "code_chunks": 28
      },
      "code_examples": [
        "class DataQualityCheck:\n    \"\"\"\n    A class used to capture summary stats and data quality checks prior to uploading time series data to DataRobot\n    Attributes:\n    -----------\n    df : DataFrame\n        time series data, including a date column and target variable at a minimum\n    settings : dict\n        definitions of date_col, target_col, series_id and time series parameters\n    stats : dict\n        summary statistics generated from `calc_summary_stats`\n    duplicate_dates : int\n        duplicate dates in the time series date_col\n    series_timesteps : series\n        steps between time units for each series_id\n    series_max_gap : series\n        maximum time gap per series\n    series_lenth : series\n        length of each series_id\n    series_pct : series\n        percent of series with complete time steps\n    irregular : boolean\n        True if df contains irregular time series data\n    series_negative_target_pct : float\n        Percent of target values that are negative\n    Methods:\n    --------\n    calc_summary_stats(settings, df)\n        generates a dictionary of summary statistics\n    calc_time_steps(settings, df)\n        calculate time steps per series_id\n    hierarchical_check(settings, df)\n        check if time series data passes heirarchical check\n    zero_inflated_check(settings, df)\n        check if target value contains zeros\n    negative_values_check(settings, df)\n        check if target value contains negative values\n    time_steps_gap_check(settings, df)\n        check if any series has missing time steps\n    irregular_check(settings, df)\n        check is time series data irregular\n    \"\"\"\n\n    def __init__(self, df, ts_settings):\n        self.df = df\n        self.settings = ts_settings\n        self.stats = None\n        self.duplicate_dates = None\n        self.series_time_steps = None\n        self.series_length = None\n        self.series_pct = None\n        self.irregular = None\n        self.series_negative_target_pct = None\n        self.project_time_unit = None\n        self.project_time_step = None\n        self.calc_summary_stats()\n        self.calc_time_steps()\n        self.run_all_checks()\n\n    def calc_summary_stats(self):\n        \"\"\"\n        Analyze time series data to perform checks and gather summary statistics prior to modeling.\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df[date_col] = pd.to_datetime(df[date_col])\n        df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n        # Create dictionary of helpful statistics\n        stats = dict()\n\n        stats['rows'] = df.shape[0]\n        stats['columns'] = df.shape[1]\n        stats['min_' + str(target)] = df[target].min()\n        stats['max_' + str(target)] = df[target].max()\n        stats['series'] = len(df[series_id].unique())\n        stats['start_date'] = df[date_col].min()\n        stats['end_date'] = df[date_col].max()\n        stats['timespan'] = stats['end_date'] - stats['start_date']\n        stats['median_timestep'] = df.groupby([series_id])[date_col].diff().median()\n        stats['min_timestep'] = df.groupby([series_id])[date_col].diff().min()\n        stats['max_timestep'] = df.groupby([series_id])[date_col].diff().max()\n\n        # create data for histogram of series lengths\n        stats['series_length'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.max() - x.min())\n            / stats['median_timestep']\n        )\n\n        # calculate max gap per series\n        stats['series_max_gap'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max())\n            / stats['median_timestep']\n        )\n\n        self.stats = stats\n\n    def calc_percent_missing(self, missing_value=np.nan):\n        \"\"\"\n        Calculate percentage of rows where target is np.nan\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if np.isnan(missing_value):\n            percent_missing = sum(np.isnan(df[target])) / len(df)\n        else:\n            percent_missing = sum(df[target] == missing_value) / len(df)\n\n        self.stats['percent_missing'] = percent_missing\n        print('{:0.2f}% of the rows are missing a target value'.format(percent_missing * 100))\n\n    def get_zero_inflated_series(self, cutoff=0.99):\n        \"\"\"\n        Identify series where the target is 0.0 in more than x% of the rows\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        assert 0 < cutoff <= 1.0, 'cutoff must be between 0 and 1'\n\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n        series = df[df >= cutoff].index.values\n\n        pct = len(series) / self.stats['series']\n\n        print(\n            '{:0.2f}% series have zeros in more than {:0.2f}% or more of the rows'.format(\n                pct * 100, cutoff * 100\n            )\n        )\n\n    def calc_time_steps(self):\n        \"\"\"\n        Calculate timesteps per series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # create data for histogram of timestep\n        series_timesteps = df.groupby([series_id])[date_col].diff() / self.stats['median_timestep']\n        self.series_time_steps = series_timesteps\n\n    def hierarchical_check(self):\n        \"\"\"\n        Calculate percentage of series that appear on each timestep\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # Test if series passes the hierarchical check\n        series_pct = df.groupby([date_col])[series_id].apply(\n            lambda x: x.count() / self.stats['series']\n        )\n        if np.where(series_pct > 0.95, 1, 0).mean() > 0.95:\n            self.stats['passes_hierarchical_check'] = True\n            print(\n                'Data passes hierarchical check! DataRobot hierarchical blueprints will run if you enable cross series features.'\n            )\n        else:\n            print('Data fails hierarchical check! No hierarchical blueprints will run.')\n            self.stats['passes_hierarchical_check'] = False\n\n        self.series_pct = series_pct\n\n    def zero_inflated_check(self):\n        \"\"\"\n        Check if minimum target value is 0.0\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if min(df[target]) == 0:\n            self.stats['passes_zero_inflated_check'] = False\n            print('The minimum target value is zero. Zero-Inflated blueprints will run.')\n        else:\n            self.stats['passes_zero_inflated_check'] = True\n            print('Minimum target value is <> 0. Zero-inflated blueprints will not run.')\n\n    def negative_values_check(self):\n        \"\"\"\n        Check if any series contain negative values. If yes, identify and call out which series by id.\n        \"\"\"\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df['target_sign'] = np.sign(df[target])\n\n        try:\n            # Get percent of series that have at least one negative value\n            any_series_negative = (\n                df.groupby([series_id])['target_sign'].value_counts().unstack()[-1]\n            )\n            series_negative_target_pct = np.sign(any_series_negative).sum() / len(\n                df[series_id].unique()\n            )\n            df.drop('target_sign', axis=1, inplace=True)\n            self.stats['passes_negative_values_check'] = False\n\n            print(\n                '{0:.2f}% of series have at least one negative {1} value.'.format(\n                    (round(series_negative_target_pct * 100), 2), target\n                )\n            )\n\n            # Identify which series have negative values\n            # print('{} contain negative values. Consider creating a seperate project for these series.'.format(any_series_negative[any_series_negative == 1].index.values))\n        except:\n            series_negative_target_pct = 0\n            self.stats['passes_negative_values_check'] = True\n            print('No negative values are contained in {}.'.format(target))\n\n        self.series_negative_target_pct = series_negative_target_pct\n\n    def new_series_check(self):\n        \"\"\"\n        Check if any series start after the the minimum datetime\n        \"\"\"\n        min_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].min()\n        new_series = min_dates > self.stats['start_date'] + dt.timedelta(days=30)\n\n        if new_series.sum() == 0:\n            self.stats['series_introduced_over_time'] = False\n            print('No new series were introduced after the start of the training data')\n        else:\n            self.stats['series_introduced_over_time'] = True\n            print(\n                'Warning: You may encounter new series at prediction time. \\n {0:.2f}% of the series appeared after the start of the training data'.format(\n                    round(new_series.mean() * 100, 0)\n                )\n            )\n\n    def old_series_check(self):\n        \"\"\"\n        Check if any series end before the maximum datetime\n        \"\"\"\n        max_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].max()\n        old_series = max_dates < self.stats['end_date'] - dt.timedelta(days=30)\n\n        if old_series.sum() == 0:\n            self.stats['series_removed_over_time'] = False\n            print('No series were removed before the end of the training data')\n        else:\n            self.stats['series_removed_over_time'] = True\n            print(\n                'Warning: You may encounter fewer series at prediction time. \\n {0:.2f}% of the series were removed before the end of the training data'.format(\n                    round(old_series.mean() * 100, 0)\n                )\n            )\n\n    def leading_or_trailing_zeros_check(self, threshold=5, drop=True):\n        \"\"\"\n        Check for contain consecutive zeros at the beginning or end of each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        new_df = remove_leading_and_trailing_zeros(\n            df,\n            series_id,\n            date_col,\n            target,\n            leading_threshold=threshold,\n            trailing_threshold=threshold,\n            drop=drop,\n        )\n\n        if new_df.shape[0] < df.shape[0]:\n            print(f'Warning: Leading and trailing zeros detected within series')\n        else:\n            print(f'No leading or trailing zeros detected within series')\n\n    def duplicate_dates_check(self):\n        \"\"\"\n        Check for duplicate datetimes within each series\n        \"\"\"\n\n        duplicate_dates = self.df.groupby([self.settings['series_id'], self.settings['date_col']])[\n            self.settings['date_col']\n        ].count()\n        duplicate_dates = duplicate_dates[duplicate_dates > 1]\n        if len(duplicate_dates) == 0:\n            print(f'No duplicate timestamps detected within any series')\n            self.stats['passes_duplicate_timestamp_check'] = True\n        else:\n            print('Warning: Data contains duplicate timestamps within series!')\n            self.stats['passes_duplicate_timestamp_check'] = False\n\n    def time_steps_gap_check(self):\n        \"\"\"\n        Check for missing timesteps within each series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n        gap_size = self.stats['median_timestep']\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # check is series has any missing time steps\n        self.stats['pct_series_w_gaps'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max()) > gap_size\n        ).mean()\n\n        print(\n            '{0:.2f}% of series have at least one missing time step.'.format(\n                round(self.stats['pct_series_w_gaps'] * 100), 2\n            )\n        )\n\n    def _get_spacing(self, df, project_time_unit):\n        \"\"\"\n        Helper function for self.irregular_check()\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        project_time_unit = self.project_time_unit\n        ts_settings = self.settings\n        date_col = ts_settings['date_col']\n        series_id = ts_settings['series_id']\n\n        df['indicator'] = 1\n        df = fill_missing_dates(df=df, ts_settings=ts_settings)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        sums = df.groupby([series_id, project_time_unit])['indicator'].sum()\n        counts = df.groupby([series_id, project_time_unit])['indicator'].agg(\n            lambda x: x.fillna(0).count()\n        )\n\n        pcts = sums / counts\n\n        irregular = pcts.reset_index(drop=True) < 0.8\n        irregular = irregular[irregular]\n\n        return irregular\n\n    def irregular_check(self, plot=False):\n        \"\"\"\n        Check for irregular spacing within each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        df = self.df.copy()\n\n        # first cast date column to a pandas datetime type\n        df[date_col] = pd.to_datetime(df[date_col])\n\n        project_time_unit, project_time_step = get_timestep(self.df, self.settings)\n\n        self.project_time_unit = project_time_unit\n        self.project_time_step = project_time_step\n\n        print('Project Timestep: ', project_time_step, ' ', project_time_unit)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        # Plot histogram of timesteps\n        time_unit_counts = df[project_time_unit].value_counts()\n\n        if plot:\n            time_unit_percent = time_unit_counts / sum(time_unit_counts.values)\n\n            fig = px.bar(\n                time_unit_percent,\n                x=time_unit_percent.index,\n                y=time_unit_percent.values,\n                title=f'Percentage of records per {project_time_unit}',\n            )\n            fig.update_xaxes(title=project_time_unit)\n            fig.update_yaxes(title='Percentage')\n            fig.show()\n\n        # Detect uncommon time steps\n        # If time bin has less than 30% of most common bin then it is an uncommon time bin\n        uncommon_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) < 0.3].index\n        )\n        common_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) >= 0.3].index\n        )\n\n        if len(uncommon_time_bins) > 0:\n            print(f'Uncommon {project_time_unit}s:', uncommon_time_bins)\n        else:\n            print('There are no uncommon time steps')\n\n        # Detect irregular series\n        df = df.loc[df[project_time_unit].isin(common_time_bins), :]\n        irregular_series = self._get_spacing(df, project_time_unit)\n\n        if len(irregular_series) > 0:\n            print(\n                'Series are irregularly spaced. Projects will only be able to run in row-based mode!'\n            )\n            self.stats['passes_irregular_check'] = False\n        else:\n            self.stats['passes_irregular_check'] = True\n            print(\n                'Timesteps are regularly spaced. You will be able to run projects in either time-based or row-based mode'\n            )\n\n    def detect_periodicity(self, alpha=0.05):\n        \"\"\"\n        Calculate project-level periodicity\n        \"\"\"\n\n        timestep = self.project_time_unit\n        df = self.df\n        target = self.settings['target']\n        date_col = self.settings['date_col']\n        metric = self.settings['metric']\n\n        metrics = {\n            'LogLoss': sm.families.Binomial(),\n            'RMSE': sm.families.Gaussian(),\n            'Poisson Deviance': sm.families.Poisson(),\n            'Gamma Deviance': sm.families.Gamma(),\n        }\n\n        periodicity = {\n            'moh': 'hourly',\n            'hod': 'daily',\n            'dow': 'weekly',\n            'dom': 'monthly',\n            'month': 'yearly',\n        }\n\n        try:\n            loss = metrics[metric]\n        except KeyError:\n            loss = metrics['RMSE']\n\n        # Instantiate a glm with the default link function.\n        df[date_col] = pd.to_datetime(df[date_col])\n        df = df.loc[np.isfinite(df[target]), :].copy()\n\n        df['moh'] = df[date_col].dt.minute\n        df['hod'] = df[date_col].dt.hour\n        df['dow'] = df[date_col].dt.dayofweek\n        df['dom'] = df[date_col].dt.day\n        df['month'] = df[date_col].dt.month\n\n        if timestep == 'minute':\n            inputs = ['moh', 'hod', 'dow', 'dom', 'month']\n        elif timestep == 'hour':\n            inputs = ['hod', 'dow', 'dom', 'month']\n        elif timestep == 'day':\n            inputs = ['dow', 'dom', 'month']\n        elif timestep == 'week':\n            inputs = ['month']\n        else:\n            raise ValueError('timestep has to be either minute, hour, day, week, or month')\n\n        output = []\n        for i in inputs:\n            x = pd.DataFrame(df[i])\n            y = df[target]\n\n            x = pd.get_dummies(x.astype('str'), drop_first=True)\n            x['const'] = 1\n\n            clf = sm.GLM(endog=y, exog=x, family=loss)\n            model = clf.fit()\n\n            if any(model.pvalues[:-1] <= alpha):\n                output.append(periodicity[i])\n                # print(f'Detected periodicity: {periodicity[i]}')\n                # return periodicity[i]\n\n        if len(output) > 0:\n            print(f'Detected periodicity: {output}')\n        else:\n            print('No periodicity detected')\n\n    def run_all_checks(self):\n        \"\"\"\n        Runner function to run all data checks in one call\n        \"\"\"\n        print('Running all data quality checks...\\n')\n\n        series = self.stats['series']\n        start_date = self.stats['start_date']\n        end_date = self.stats['end_date']\n        rows = self.stats['rows']\n        cols = self.stats['columns']\n\n        print(f'There are {rows} rows and {cols} columns')\n        print(f'There are {series} series')\n        print(f'The data spans from  {start_date} to {end_date}')\n\n        self.hierarchical_check()\n        self.zero_inflated_check()\n        self.new_series_check()\n        self.old_series_check()\n        self.duplicate_dates_check()\n        self.leading_or_trailing_zeros_check()\n        self.time_steps_gap_check()\n        self.calc_percent_missing()\n        self.get_zero_inflated_series()\n        self.irregular_check()\n        self.detect_periodicity()",
        "def get_timestep(df, ts_settings):\n    \"\"\"\n    Calculate the project-level timestep\n    Returns:\n    --------\n    project_time_unit: minute, hour, day, week, or month\n    project_time_step: int\n    Examples:\n    --------\n    '1 days'\n    '4 days'\n    '1 week'\n    '2 months'\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    # Cast date column to a pandas datetime type and sort df\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n    # Calculate median timestep\n    deltas = df.groupby([series_id])[date_col].diff().reset_index(drop=True)\n    median_timestep = deltas.apply(lambda x: x.total_seconds()).median()\n\n    # Logic to detect project time step and time unit\n    if (60 <= median_timestep < 3600) & (median_timestep % 60 == 0):\n        project_time_unit = 'minute'\n        project_time_step = int(median_timestep / 60)\n        df['minute'] = df[date_col].dt.minute\n    elif (3600 <= median_timestep < 86400) & (median_timestep % 3600 == 0):\n        project_time_unit = 'hour'\n        project_time_step = int(median_timestep / 3600)\n        df['hour'] = df[date_col].dt.hour\n    elif (86400 <= median_timestep < 604800) & (median_timestep % 86400 == 0):\n        project_time_unit = 'day'\n        project_time_step = int(median_timestep / 86400)\n        df['day'] = df[date_col].dt.strftime('%A')\n    elif (604800 <= median_timestep < 2.628e6) & (median_timestep % 604800 == 0):\n        project_time_unit = 'week'\n        project_time_step = int(median_timestep / 604800)\n        df['week'] = df[date_col].dt.week\n    elif (median_timestep >= 2.628e6) & (median_timestep % 2.628e6 == 0):\n        project_time_unit = 'month'\n        project_time_step = int(median_timestep / 2.628e6)\n        df['month'] = df[date_col].dt.month\n    else:\n        raise ValueError(f'{median_timestep} seconds is not a supported timestep')\n\n    # print('Project Timestep: 1', project_time_unit)\n\n    return project_time_unit, project_time_step",
        "def _reindex_dates(group, freq):\n    \"\"\"\n    Helper function for fill_missing_dates()\n    \"\"\"\n    date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    group = group.reindex(date_range)\n    return group",
        "def fill_missing_dates(df, ts_settings, freq=None):\n    \"\"\"\n    Insert rows with np.nan targets for series with missing timesteps between the series start and end dates\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    freq: project time unit and timestep\n    Returns:\n    --------\n    pandas df with inserted rows\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    df = df.copy()\n\n    df[date_col] = pd.to_datetime(df[date_col])\n    df.sort_values(by=[series_id, date_col], ascending=True, inplace=True)\n\n    if freq is None:\n        mapper = {'minute': 'min', 'hour': 'H', 'day': 'D', 'week': 'W', 'month': 'M'}\n        project_time_unit, project_time_step = get_timestep(df, ts_settings)\n        freq = str(project_time_step) + mapper[project_time_unit]\n\n    df = (\n        df.set_index(date_col)\n        .groupby(series_id)\n        .apply(_reindex_dates, freq)\n        .rename_axis((series_id, date_col))\n        .drop(series_id, axis=1)\n        .reset_index()\n    )\n\n    return df.reset_index(drop=True)",
        "def _remove_leading_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    min_date = df_non_zero[date_col].min()\n    df_begin = df[df[date_col] < min_date]\n    if df_begin[target].dropna().shape[0] >= threshold or pd.isnull(min_date):\n        if drop:\n            if pd.isnull(min_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] >= min_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(min_date) or row[date_col] < min_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df",
        "def _remove_trailing_zeros(df, date_col, target, threshold=5, drop=False):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df_non_zero = df[(df[target] != 0) & (~pd.isnull(df[target]))]\n    max_date = df_non_zero[date_col].max()\n    df_end = df[df[date_col] > max_date]\n    if df_end[target].dropna().shape[0] >= threshold or pd.isnull(max_date):\n        if drop:\n            if pd.isnull(max_date):\n                return pd.DataFrame(columns=df.columns, dtype=float)\n            return df[df[date_col] <= max_date]\n        else:\n            df[target] = df.apply(\n                lambda row: np.nan\n                if pd.isnull(max_date) or row[date_col] > max_date\n                else row[target],\n                axis=1,\n            )\n            return df\n    else:\n        return df",
        "def remove_leading_and_trailing_zeros(\n    df, series_id, date_col, target, leading_threshold=5, trailing_threshold=5, drop=False\n):\n    \"\"\"\n    Remove excess zeros at the beginning or end of series\n    df: pandas df\n    leading_threshold: minimum number of consecutive zeros at the beginning of a series before rows are dropped\n    trailing_threshold: minimum number of consecutive zeros at the end of series before rows are dropped\n    drop: specifies whether to drop the zeros or set them to np.nan\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_leading_zeros, date_col, target, leading_threshold, drop)\n        .reset_index(drop=True)\n    )\n    df = (\n        df.groupby(series_id)\n        .apply(_remove_trailing_zeros, date_col, target, trailing_threshold, drop)\n        .reset_index(drop=True)\n    )\n\n    return df.reset_index(drop=True)",
        "def _cut_series_by_rank(df, ts_settings, n=1, top=True):\n    df_agg = df.groupby(ts_settings['series_id']).mean()\n    selected_series_names = (\n        df_agg.sort_values(by=ts_settings['target'], ascending=top).tail(n).index.values\n    )\n\n    return selected_series_names",
        "def _cut_series_by_quantile(df, ts_settings, quantile=0.95, top=True):\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df_agg = df.groupby(series_id).mean()\n\n    if top:\n        selected_series_names = df_agg[\n            df_agg[target] >= df_agg[target].quantile(quantile)\n        ].index.values\n    else:\n        selected_series_names = df_agg[\n            df_agg[target] <= df_agg[target].quantile(quantile)\n        ].index.values\n\n    return selected_series_names",
        "def plot_series_average(df, settings):\n    date_col = settings['date_col']\n    target = settings['target']\n\n    # Average of all series over time\n    df_agg = df.groupby(date_col).mean()\n    df_agg['Date'] = pd.to_datetime(df_agg.index.values)\n\n    fig = px.line(df_agg, x='Date', y=target)\n    fig.update_layout(title_text='Average of all Series')\n    fig.show()",
        "def plot_individual_series(df, ts_settings, n=None, top=True):\n    \"\"\"\n    Plot individual series on the same chart\n    n: (int) number of series to plot\n    top: (boolean) whether to select the top n largest or smallest series ranked by average target value\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    if n is None:\n        n = len(df[series_id].unique())\n\n    series = _cut_series_by_rank(df, ts_settings, n=n, top=top)\n    df_subset = df[df[series_id].isin(series)]\n\n    fig = px.line(df_subset, x=date_col, y=target, color=df_subset[series_id])\n    fig.update_layout(title_text='Top Series By Target Over Time')\n    fig.show()",
        "def __init__(self, df, ts_settings):\n        self.df = df\n        self.settings = ts_settings\n        self.stats = None\n        self.duplicate_dates = None\n        self.series_time_steps = None\n        self.series_length = None\n        self.series_pct = None\n        self.irregular = None\n        self.series_negative_target_pct = None\n        self.project_time_unit = None\n        self.project_time_step = None\n        self.calc_summary_stats()\n        self.calc_time_steps()\n        self.run_all_checks()",
        "def calc_summary_stats(self):\n        \"\"\"\n        Analyze time series data to perform checks and gather summary statistics prior to modeling.\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df[date_col] = pd.to_datetime(df[date_col])\n        df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n\n        # Create dictionary of helpful statistics\n        stats = dict()\n\n        stats['rows'] = df.shape[0]\n        stats['columns'] = df.shape[1]\n        stats['min_' + str(target)] = df[target].min()\n        stats['max_' + str(target)] = df[target].max()\n        stats['series'] = len(df[series_id].unique())\n        stats['start_date'] = df[date_col].min()\n        stats['end_date'] = df[date_col].max()\n        stats['timespan'] = stats['end_date'] - stats['start_date']\n        stats['median_timestep'] = df.groupby([series_id])[date_col].diff().median()\n        stats['min_timestep'] = df.groupby([series_id])[date_col].diff().min()\n        stats['max_timestep'] = df.groupby([series_id])[date_col].diff().max()\n\n        # create data for histogram of series lengths\n        stats['series_length'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.max() - x.min())\n            / stats['median_timestep']\n        )\n\n        # calculate max gap per series\n        stats['series_max_gap'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max())\n            / stats['median_timestep']\n        )\n\n        self.stats = stats",
        "def calc_percent_missing(self, missing_value=np.nan):\n        \"\"\"\n        Calculate percentage of rows where target is np.nan\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if np.isnan(missing_value):\n            percent_missing = sum(np.isnan(df[target])) / len(df)\n        else:\n            percent_missing = sum(df[target] == missing_value) / len(df)\n\n        self.stats['percent_missing'] = percent_missing\n        print('{:0.2f}% of the rows are missing a target value'.format(percent_missing * 100))",
        "def get_zero_inflated_series(self, cutoff=0.99):\n        \"\"\"\n        Identify series where the target is 0.0 in more than x% of the rows\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        assert 0 < cutoff <= 1.0, 'cutoff must be between 0 and 1'\n\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n        series = df[df >= cutoff].index.values\n\n        pct = len(series) / self.stats['series']\n\n        print(\n            '{:0.2f}% series have zeros in more than {:0.2f}% or more of the rows'.format(\n                pct * 100, cutoff * 100\n            )\n        )",
        "def calc_time_steps(self):\n        \"\"\"\n        Calculate timesteps per series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # create data for histogram of timestep\n        series_timesteps = df.groupby([series_id])[date_col].diff() / self.stats['median_timestep']\n        self.series_time_steps = series_timesteps",
        "def hierarchical_check(self):\n        \"\"\"\n        Calculate percentage of series that appear on each timestep\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # Test if series passes the hierarchical check\n        series_pct = df.groupby([date_col])[series_id].apply(\n            lambda x: x.count() / self.stats['series']\n        )\n        if np.where(series_pct > 0.95, 1, 0).mean() > 0.95:\n            self.stats['passes_hierarchical_check'] = True\n            print(\n                'Data passes hierarchical check! DataRobot hierarchical blueprints will run if you enable cross series features.'\n            )\n        else:\n            print('Data fails hierarchical check! No hierarchical blueprints will run.')\n            self.stats['passes_hierarchical_check'] = False\n\n        self.series_pct = series_pct",
        "def zero_inflated_check(self):\n        \"\"\"\n        Check if minimum target value is 0.0\n        \"\"\"\n        target = self.settings['target']\n        df = self.df\n\n        if min(df[target]) == 0:\n            self.stats['passes_zero_inflated_check'] = False\n            print('The minimum target value is zero. Zero-Inflated blueprints will run.')\n        else:\n            self.stats['passes_zero_inflated_check'] = True\n            print('Minimum target value is <> 0. Zero-inflated blueprints will not run.')",
        "def negative_values_check(self):\n        \"\"\"\n        Check if any series contain negative values. If yes, identify and call out which series by id.\n        \"\"\"\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        df['target_sign'] = np.sign(df[target])\n\n        try:\n            # Get percent of series that have at least one negative value\n            any_series_negative = (\n                df.groupby([series_id])['target_sign'].value_counts().unstack()[-1]\n            )\n            series_negative_target_pct = np.sign(any_series_negative).sum() / len(\n                df[series_id].unique()\n            )\n            df.drop('target_sign', axis=1, inplace=True)\n            self.stats['passes_negative_values_check'] = False\n\n            print(\n                '{0:.2f}% of series have at least one negative {1} value.'.format(\n                    (round(series_negative_target_pct * 100), 2), target\n                )\n            )\n\n            # Identify which series have negative values\n            # print('{} contain negative values. Consider creating a seperate project for these series.'.format(any_series_negative[any_series_negative == 1].index.values))\n        except:\n            series_negative_target_pct = 0\n            self.stats['passes_negative_values_check'] = True\n            print('No negative values are contained in {}.'.format(target))\n\n        self.series_negative_target_pct = series_negative_target_pct",
        "def new_series_check(self):\n        \"\"\"\n        Check if any series start after the the minimum datetime\n        \"\"\"\n        min_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].min()\n        new_series = min_dates > self.stats['start_date'] + dt.timedelta(days=30)\n\n        if new_series.sum() == 0:\n            self.stats['series_introduced_over_time'] = False\n            print('No new series were introduced after the start of the training data')\n        else:\n            self.stats['series_introduced_over_time'] = True\n            print(\n                'Warning: You may encounter new series at prediction time. \\n {0:.2f}% of the series appeared after the start of the training data'.format(\n                    round(new_series.mean() * 100, 0)\n                )\n            )",
        "def old_series_check(self):\n        \"\"\"\n        Check if any series end before the maximum datetime\n        \"\"\"\n        max_dates = self.df.groupby(self.settings['series_id'])[self.settings['date_col']].max()\n        old_series = max_dates < self.stats['end_date'] - dt.timedelta(days=30)\n\n        if old_series.sum() == 0:\n            self.stats['series_removed_over_time'] = False\n            print('No series were removed before the end of the training data')\n        else:\n            self.stats['series_removed_over_time'] = True\n            print(\n                'Warning: You may encounter fewer series at prediction time. \\n {0:.2f}% of the series were removed before the end of the training data'.format(\n                    round(old_series.mean() * 100, 0)\n                )\n            )",
        "def leading_or_trailing_zeros_check(self, threshold=5, drop=True):\n        \"\"\"\n        Check for contain consecutive zeros at the beginning or end of each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        target = self.settings['target']\n        df = self.df\n\n        new_df = remove_leading_and_trailing_zeros(\n            df,\n            series_id,\n            date_col,\n            target,\n            leading_threshold=threshold,\n            trailing_threshold=threshold,\n            drop=drop,\n        )\n\n        if new_df.shape[0] < df.shape[0]:\n            print(f'Warning: Leading and trailing zeros detected within series')\n        else:\n            print(f'No leading or trailing zeros detected within series')",
        "def duplicate_dates_check(self):\n        \"\"\"\n        Check for duplicate datetimes within each series\n        \"\"\"\n\n        duplicate_dates = self.df.groupby([self.settings['series_id'], self.settings['date_col']])[\n            self.settings['date_col']\n        ].count()\n        duplicate_dates = duplicate_dates[duplicate_dates > 1]\n        if len(duplicate_dates) == 0:\n            print(f'No duplicate timestamps detected within any series')\n            self.stats['passes_duplicate_timestamp_check'] = True\n        else:\n            print('Warning: Data contains duplicate timestamps within series!')\n            self.stats['passes_duplicate_timestamp_check'] = False",
        "def time_steps_gap_check(self):\n        \"\"\"\n        Check for missing timesteps within each series\n        \"\"\"\n        date_col = self.settings['date_col']\n        series_id = self.settings['series_id']\n        df = self.df\n        gap_size = self.stats['median_timestep']\n\n        if self.stats is None:\n            print('calc_summary_stats must be run first!')\n\n        # check is series has any missing time steps\n        self.stats['pct_series_w_gaps'] = (\n            df.groupby([series_id])[date_col].apply(lambda x: x.diff().max()) > gap_size\n        ).mean()\n\n        print(\n            '{0:.2f}% of series have at least one missing time step.'.format(\n                round(self.stats['pct_series_w_gaps'] * 100), 2\n            )\n        )",
        "def _get_spacing(self, df, project_time_unit):\n        \"\"\"\n        Helper function for self.irregular_check()\n        Returns:\n        --------\n        List of series\n        \"\"\"\n        project_time_unit = self.project_time_unit\n        ts_settings = self.settings\n        date_col = ts_settings['date_col']\n        series_id = ts_settings['series_id']\n\n        df['indicator'] = 1\n        df = fill_missing_dates(df=df, ts_settings=ts_settings)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        sums = df.groupby([series_id, project_time_unit])['indicator'].sum()\n        counts = df.groupby([series_id, project_time_unit])['indicator'].agg(\n            lambda x: x.fillna(0).count()\n        )\n\n        pcts = sums / counts\n\n        irregular = pcts.reset_index(drop=True) < 0.8\n        irregular = irregular[irregular]\n\n        return irregular",
        "def irregular_check(self, plot=False):\n        \"\"\"\n        Check for irregular spacing within each series\n        \"\"\"\n\n        date_col = self.settings['date_col']\n        df = self.df.copy()\n\n        # first cast date column to a pandas datetime type\n        df[date_col] = pd.to_datetime(df[date_col])\n\n        project_time_unit, project_time_step = get_timestep(self.df, self.settings)\n\n        self.project_time_unit = project_time_unit\n        self.project_time_step = project_time_step\n\n        print('Project Timestep: ', project_time_step, ' ', project_time_unit)\n\n        if project_time_unit == 'minute':\n            df['minute'] = df[date_col].dt.minute\n        elif project_time_unit == 'hour':\n            df['hour'] = df[date_col].dt.hour\n        elif project_time_unit == 'day':\n            df['day'] = df[date_col].dt.dayofweek\n        elif project_time_unit == 'week':\n            df['week'] = df[date_col].dt.week\n        elif project_time_unit == 'month':\n            df['month'] = df[date_col].dt.month\n\n        # Plot histogram of timesteps\n        time_unit_counts = df[project_time_unit].value_counts()\n\n        if plot:\n            time_unit_percent = time_unit_counts / sum(time_unit_counts.values)\n\n            fig = px.bar(\n                time_unit_percent,\n                x=time_unit_percent.index,\n                y=time_unit_percent.values,\n                title=f'Percentage of records per {project_time_unit}',\n            )\n            fig.update_xaxes(title=project_time_unit)\n            fig.update_yaxes(title='Percentage')\n            fig.show()\n\n        # Detect uncommon time steps\n        # If time bin has less than 30% of most common bin then it is an uncommon time bin\n        uncommon_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) < 0.3].index\n        )\n        common_time_bins = list(\n            time_unit_counts[(time_unit_counts / time_unit_counts.max()) >= 0.3].index\n        )\n\n        if len(uncommon_time_bins) > 0:\n            print(f'Uncommon {project_time_unit}s:', uncommon_time_bins)\n        else:\n            print('There are no uncommon time steps')\n\n        # Detect irregular series\n        df = df.loc[df[project_time_unit].isin(common_time_bins), :]\n        irregular_series = self._get_spacing(df, project_time_unit)\n\n        if len(irregular_series) > 0:\n            print(\n                'Series are irregularly spaced. Projects will only be able to run in row-based mode!'\n            )\n            self.stats['passes_irregular_check'] = False\n        else:\n            self.stats['passes_irregular_check'] = True\n            print(\n                'Timesteps are regularly spaced. You will be able to run projects in either time-based or row-based mode'\n            )",
        "def detect_periodicity(self, alpha=0.05):\n        \"\"\"\n        Calculate project-level periodicity\n        \"\"\"\n\n        timestep = self.project_time_unit\n        df = self.df\n        target = self.settings['target']\n        date_col = self.settings['date_col']\n        metric = self.settings['metric']\n\n        metrics = {\n            'LogLoss': sm.families.Binomial(),\n            'RMSE': sm.families.Gaussian(),\n            'Poisson Deviance': sm.families.Poisson(),\n            'Gamma Deviance': sm.families.Gamma(),\n        }\n\n        periodicity = {\n            'moh': 'hourly',\n            'hod': 'daily',\n            'dow': 'weekly',\n            'dom': 'monthly',\n            'month': 'yearly',\n        }\n\n        try:\n            loss = metrics[metric]\n        except KeyError:\n            loss = metrics['RMSE']\n\n        # Instantiate a glm with the default link function.\n        df[date_col] = pd.to_datetime(df[date_col])\n        df = df.loc[np.isfinite(df[target]), :].copy()\n\n        df['moh'] = df[date_col].dt.minute\n        df['hod'] = df[date_col].dt.hour\n        df['dow'] = df[date_col].dt.dayofweek\n        df['dom'] = df[date_col].dt.day\n        df['month'] = df[date_col].dt.month\n\n        if timestep == 'minute':\n            inputs = ['moh', 'hod', 'dow', 'dom', 'month']\n        elif timestep == 'hour':\n            inputs = ['hod', 'dow', 'dom', 'month']\n        elif timestep == 'day':\n            inputs = ['dow', 'dom', 'month']\n        elif timestep == 'week':\n            inputs = ['month']\n        else:\n            raise ValueError('timestep has to be either minute, hour, day, week, or month')\n\n        output = []\n        for i in inputs:\n            x = pd.DataFrame(df[i])\n            y = df[target]\n\n            x = pd.get_dummies(x.astype('str'), drop_first=True)\n            x['const'] = 1\n\n            clf = sm.GLM(endog=y, exog=x, family=loss)\n            model = clf.fit()\n\n            if any(model.pvalues[:-1] <= alpha):\n                output.append(periodicity[i])\n                # print(f'Detected periodicity: {periodicity[i]}')\n                # return periodicity[i]\n\n        if len(output) > 0:\n            print(f'Detected periodicity: {output}')\n        else:\n            print('No periodicity detected')",
        "def run_all_checks(self):\n        \"\"\"\n        Runner function to run all data checks in one call\n        \"\"\"\n        print('Running all data quality checks...\\n')\n\n        series = self.stats['series']\n        start_date = self.stats['start_date']\n        end_date = self.stats['end_date']\n        rows = self.stats['rows']\n        cols = self.stats['columns']\n\n        print(f'There are {rows} rows and {cols} columns')\n        print(f'There are {series} series')\n        print(f'The data spans from  {start_date} to {end_date}')\n\n        self.hierarchical_check()\n        self.zero_inflated_check()\n        self.new_series_check()\n        self.old_series_check()\n        self.duplicate_dates_check()\n        self.leading_or_trailing_zeros_check()\n        self.time_steps_gap_check()\n        self.calc_percent_missing()\n        self.get_zero_inflated_series()\n        self.irregular_check()\n        self.detect_periodicity()"
      ],
      "api_methods": [
        "model.pvalues"
      ],
      "complexity_score": 0.95,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_-6347104308148615191",
      "title": "ts_fill_dates_per_series.py",
      "content": "#Author: Thodoris Petropoulos\n\nimport pandas as pd\n\ndef fill_missing_dates(group, date_col, freq = 'D', per_group_imputation = False):\n    \"\"\"This function can be used together with apply to fill in missing dates per group will fill missing dates per time series group. values per time series group follow the full script to see how you\n    Input:\n        - group <grouped pandas DataFrame> (No need to specify anything. It will work with .apply method)\n        - date_col <string> (Column that represents time)\n        - freq <string> Frequency of data imputation (\"D\" for daily, \"W\" for weekly, \"M\" for monthly)\n        - per_group_imputation <BOOLEAN> (If True, then the min and max value of dates will be specified by the individual series.\n                                          If False, then the min and max value of dates will be specified by the whole dataset).\n    \"\"\"\n    if per_group_imputation == True:\n        date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    else: \n        date_range = pd.date_range(df[date_col].min(), df[date_col].max(), freq=freq)\n    group = group.reindex(date_range)\n    return group\n\n##########\n#Usage\n##########s\n\n#To use the function above: \ndataframe = dataframe.set_index(date_col).groupby('series_id').apply(fill_missing_dates).rename_axis(('series_id',date_col)).drop('series_id', 1).reset_index()\n\n#Impute missing values of target feature with 0\ndataframe['target'].fillna(0,inplace=True)\n\n#Forward fill on the categorical features if needed (depending on dataset)\ndm_imputed.update(dm_imputed.groupby('series_id')[categorical_features].ffill())\n\n#Backward fill on the categorical features if needed (depending on dataset)\ndm_imputed.update(dm_imputed.groupby('series_id')[categorical_features].bfill())\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_fill_dates_per_series.py",
      "tags": [
        "pandas"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_fill_dates_per_series.py",
        "size": 1783,
        "code_chunks": 1
      },
      "code_examples": [
        "def fill_missing_dates(group, date_col, freq = 'D', per_group_imputation = False):\n    \"\"\"This function can be used together with apply to fill in missing dates per group will fill missing dates per time series group. values per time series group follow the full script to see how you\n    Input:\n        - group <grouped pandas DataFrame> (No need to specify anything. It will work with .apply method)\n        - date_col <string> (Column that represents time)\n        - freq <string> Frequency of data imputation (\"D\" for daily, \"W\" for weekly, \"M\" for monthly)\n        - per_group_imputation <BOOLEAN> (If True, then the min and max value of dates will be specified by the individual series.\n                                          If False, then the min and max value of dates will be specified by the whole dataset).\n    \"\"\"\n    if per_group_imputation == True:\n        date_range = pd.date_range(group.index.min(), group.index.max(), freq=freq)\n    else: \n        date_range = pd.date_range(df[date_col].min(), df[date_col].max(), freq=freq)\n    group = group.reindex(date_range)\n    return group"
      ],
      "api_methods": [],
      "complexity_score": 0.35,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_3641769898629491453",
      "title": "ts_ion_cannon.py",
      "content": "#Author: Lukas Innig\n\n#Make sure you are connected to DataRobot and have a completed TS project.\n\nimport datarobot as dr\nfrom datarobot.errors import ClientError\n\nclass TimeSeriesIonCannon(dr.Project):\n    \"\"\" This class takes as input a DataRobot Object and initiates a brute force search to increase accuracy.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        all_models = self.get_models()\n        self.supported_metrics = all_models[0].metrics.keys()\n        self.training_duration = [m for m in all_models if m.training_duration][0].training_duration\n    sort_order = {'MASE': False,\n     'FVE Poisson': True,\n     \"Theil's U\": False,\n     'RMSE': False,\n     'FVE Gamma': True,\n     'R Squared': True,\n     'Gamma Deviance': False,\n     'FVE Tweedie': True,\n     'MAE': False,\n     'SMAPE': True,\n     'MAPE': True,\n     'Gini Norm': True,\n     'Tweedie Deviance': False,\n     'Poisson Deviance': False,\n     'RMSLE': False}\n    \n    @classmethod\n    def aim(cls, *args, **kwargs):\n        return super().get(*args, **kwargs)\n    \n    def get_models_sorted(self, partition='validation', metric='RMSE', model_type_filter = ['']):\n        if partition not in ['backtesting', 'holdout', 'validation']:\n            raise ValueError(f\"Partition {partition} not in ['backtesting', 'holdout', 'validation']\")\n        if partition == 'holdout' and not self.holdout_unlocked:\n            print(\"Holdout not unlocked!\")\n            return []\n        if metric not in self.supported_metrics:\n            raise ValueError(f'Metric {metric} not supported')\n        reverse = self.sort_order.get(metric)\n        return sorted([m for m in self.get_datetime_models() \n                       if metric in m.metrics \n                       and m.metrics[metric][partition] \n                       and any([f in m.model_type for f in model_type_filter])], \n                      key=lambda m: m.metrics[metric][partition], reverse=reverse)\n    \n    def calculate_backtests(self, models):\n        def score_backtests(m):\n            try: \n                return m.score_backtests()\n            except ClientError as e:\n                return None\n        jobs = [score_backtests(m) for m in models]\n        [job.wait_for_completion() for job in jobs if job]\n    \n    def identify_best_featurelist(self):\n        best_models = self.get_models_sorted('backtesting')\n        if not best_models:\n            print('calculate some backtests')\n        featurelists = [m.featurelist_id for m in best_models[:20] if 'Blender' not in m.model_type]\n        reduced_fl = [fl for fl in featurelists if 'Reduced' in fl.name]\n        other_fl = [fl for fl in featurelists if 'Reduced' not in fl.name]\n        return reduced_fl + other_fl[:1]\n    \n    def run_all_blueprints(self, featurelist, training_duration=None, \n                           model_type_filter=['Mean', 'Eureqa', 'Keras', 'VARMAX']):\n        if not training_duration:\n            training_duration = self.training_duration\n        def train_blueprint(bp, fl):\n            try:\n                return self.train_datetime(bp.id, fl.id, training_duration=training_duration)\n            except ClientError as e:\n                print(e)\n                return None\n        bps = [bp for bp in self.get_blueprints() if all([f not in bp.model_type for f in model_type_filter])]\n        jobs = [train_blueprint(bp, featurelist) for bp in bps]\n        [job.wait_for_completion() for job in jobs if job]\n    def run_blenders(self):\n        def blend(model_ids, blender_method):\n            try:\n                return self.blend(model_ids, blender_method)\n            except ClientError as e:\n                print(e)\n                return None\n        best_models = self.get_models_sorted('backtesting')\n        best_models = [m for m in best_models if 'Blender' not in m.model_type]\n        jobs = []\n        for n in [3, 5, 7]:\n            for blender_method in [dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_AVG, \n                                   dr.enums.BLENDER_METHOD.AVERAGE,\n                                   dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_ENET]:\n                jobs.append(blend([m.id for m in best_models[:n]], blender_method=blender_method))\n        blender_models = [j.get_result_when_complete() for j in jobs if j]\n        blender_models = [dr.DatetimeModel.get(self.id, bm.id) for bm in blender_models]\n        return blender_models\n    def shoot(self):\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        fls = self.identify_best_featurelist()\n        for fl in fls:\n            self.run_all_blueprints(fl)\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        self.run_blenders()\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n\n\n##USAGE##\n#cannon = TimeSeriesIonCannon.aim('YOUR_PROJECT_ID')\n#cannon.shoot()",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_ion_cannon.py",
      "tags": [
        "blueprint",
        "datarobot-api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_ion_cannon.py",
        "size": 4904,
        "code_chunks": 12
      },
      "code_examples": [
        "class TimeSeriesIonCannon(dr.Project):\n    \"\"\" This class takes as input a DataRobot Object and initiates a brute force search to increase accuracy.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        all_models = self.get_models()\n        self.supported_metrics = all_models[0].metrics.keys()\n        self.training_duration = [m for m in all_models if m.training_duration][0].training_duration\n    sort_order = {'MASE': False,\n     'FVE Poisson': True,\n     \"Theil's U\": False,\n     'RMSE': False,\n     'FVE Gamma': True,\n     'R Squared': True,\n     'Gamma Deviance': False,\n     'FVE Tweedie': True,\n     'MAE': False,\n     'SMAPE': True,\n     'MAPE': True,\n     'Gini Norm': True,\n     'Tweedie Deviance': False,\n     'Poisson Deviance': False,\n     'RMSLE': False}\n    \n    @classmethod\n    def aim(cls, *args, **kwargs):\n        return super().get(*args, **kwargs)\n    \n    def get_models_sorted(self, partition='validation', metric='RMSE', model_type_filter = ['']):\n        if partition not in ['backtesting', 'holdout', 'validation']:\n            raise ValueError(f\"Partition {partition} not in ['backtesting', 'holdout', 'validation']\")\n        if partition == 'holdout' and not self.holdout_unlocked:\n            print(\"Holdout not unlocked!\")\n            return []\n        if metric not in self.supported_metrics:\n            raise ValueError(f'Metric {metric} not supported')\n        reverse = self.sort_order.get(metric)\n        return sorted([m for m in self.get_datetime_models() \n                       if metric in m.metrics \n                       and m.metrics[metric][partition] \n                       and any([f in m.model_type for f in model_type_filter])], \n                      key=lambda m: m.metrics[metric][partition], reverse=reverse)\n    \n    def calculate_backtests(self, models):\n        def score_backtests(m):\n            try: \n                return m.score_backtests()\n            except ClientError as e:\n                return None\n        jobs = [score_backtests(m) for m in models]\n        [job.wait_for_completion() for job in jobs if job]\n    \n    def identify_best_featurelist(self):\n        best_models = self.get_models_sorted('backtesting')\n        if not best_models:\n            print('calculate some backtests')\n        featurelists = [m.featurelist_id for m in best_models[:20] if 'Blender' not in m.model_type]\n        reduced_fl = [fl for fl in featurelists if 'Reduced' in fl.name]\n        other_fl = [fl for fl in featurelists if 'Reduced' not in fl.name]\n        return reduced_fl + other_fl[:1]\n    \n    def run_all_blueprints(self, featurelist, training_duration=None, \n                           model_type_filter=['Mean', 'Eureqa', 'Keras', 'VARMAX']):\n        if not training_duration:\n            training_duration = self.training_duration\n        def train_blueprint(bp, fl):\n            try:\n                return self.train_datetime(bp.id, fl.id, training_duration=training_duration)\n            except ClientError as e:\n                print(e)\n                return None\n        bps = [bp for bp in self.get_blueprints() if all([f not in bp.model_type for f in model_type_filter])]\n        jobs = [train_blueprint(bp, featurelist) for bp in bps]\n        [job.wait_for_completion() for job in jobs if job]\n    def run_blenders(self):\n        def blend(model_ids, blender_method):\n            try:\n                return self.blend(model_ids, blender_method)\n            except ClientError as e:\n                print(e)\n                return None\n        best_models = self.get_models_sorted('backtesting')\n        best_models = [m for m in best_models if 'Blender' not in m.model_type]\n        jobs = []\n        for n in [3, 5, 7]:\n            for blender_method in [dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_AVG, \n                                   dr.enums.BLENDER_METHOD.AVERAGE,\n                                   dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_ENET]:\n                jobs.append(blend([m.id for m in best_models[:n]], blender_method=blender_method))\n        blender_models = [j.get_result_when_complete() for j in jobs if j]\n        blender_models = [dr.DatetimeModel.get(self.id, bm.id) for bm in blender_models]\n        return blender_models\n    def shoot(self):\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        fls = self.identify_best_featurelist()\n        for fl in fls:\n            self.run_all_blueprints(fl)\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        self.run_blenders()\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])",
        "def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        all_models = self.get_models()\n        self.supported_metrics = all_models[0].metrics.keys()\n        self.training_duration = [m for m in all_models if m.training_duration][0].training_duration",
        "def aim(cls, *args, **kwargs):\n        return super().get(*args, **kwargs)",
        "def get_models_sorted(self, partition='validation', metric='RMSE', model_type_filter = ['']):\n        if partition not in ['backtesting', 'holdout', 'validation']:\n            raise ValueError(f\"Partition {partition} not in ['backtesting', 'holdout', 'validation']\")\n        if partition == 'holdout' and not self.holdout_unlocked:\n            print(\"Holdout not unlocked!\")\n            return []\n        if metric not in self.supported_metrics:\n            raise ValueError(f'Metric {metric} not supported')\n        reverse = self.sort_order.get(metric)\n        return sorted([m for m in self.get_datetime_models() \n                       if metric in m.metrics \n                       and m.metrics[metric][partition] \n                       and any([f in m.model_type for f in model_type_filter])], \n                      key=lambda m: m.metrics[metric][partition], reverse=reverse)",
        "def calculate_backtests(self, models):\n        def score_backtests(m):\n            try: \n                return m.score_backtests()\n            except ClientError as e:\n                return None\n        jobs = [score_backtests(m) for m in models]\n        [job.wait_for_completion() for job in jobs if job]",
        "def identify_best_featurelist(self):\n        best_models = self.get_models_sorted('backtesting')\n        if not best_models:\n            print('calculate some backtests')\n        featurelists = [m.featurelist_id for m in best_models[:20] if 'Blender' not in m.model_type]\n        reduced_fl = [fl for fl in featurelists if 'Reduced' in fl.name]\n        other_fl = [fl for fl in featurelists if 'Reduced' not in fl.name]\n        return reduced_fl + other_fl[:1]",
        "def run_all_blueprints(self, featurelist, training_duration=None, \n                           model_type_filter=['Mean', 'Eureqa', 'Keras', 'VARMAX']):\n        if not training_duration:\n            training_duration = self.training_duration\n        def train_blueprint(bp, fl):\n            try:\n                return self.train_datetime(bp.id, fl.id, training_duration=training_duration)\n            except ClientError as e:\n                print(e)\n                return None\n        bps = [bp for bp in self.get_blueprints() if all([f not in bp.model_type for f in model_type_filter])]\n        jobs = [train_blueprint(bp, featurelist) for bp in bps]\n        [job.wait_for_completion() for job in jobs if job]",
        "def run_blenders(self):\n        def blend(model_ids, blender_method):\n            try:\n                return self.blend(model_ids, blender_method)\n            except ClientError as e:\n                print(e)\n                return None\n        best_models = self.get_models_sorted('backtesting')\n        best_models = [m for m in best_models if 'Blender' not in m.model_type]\n        jobs = []\n        for n in [3, 5, 7]:\n            for blender_method in [dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_AVG, \n                                   dr.enums.BLENDER_METHOD.AVERAGE,\n                                   dr.enums.BLENDER_METHOD.FORECAST_DISTANCE_ENET]:\n                jobs.append(blend([m.id for m in best_models[:n]], blender_method=blender_method))\n        blender_models = [j.get_result_when_complete() for j in jobs if j]\n        blender_models = [dr.DatetimeModel.get(self.id, bm.id) for bm in blender_models]\n        return blender_models",
        "def shoot(self):\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        fls = self.identify_best_featurelist()\n        for fl in fls:\n            self.run_all_blueprints(fl)\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])\n        self.run_blenders()\n        self.calculate_backtests(self.get_models_sorted('validation')[:20])",
        "def score_backtests(m):\n            try: \n                return m.score_backtests()\n            except ClientError as e:\n                return None",
        "def train_blueprint(bp, fl):\n            try:\n                return self.train_datetime(bp.id, fl.id, training_duration=training_duration)\n            except ClientError as e:\n                print(e)\n                return None",
        "def blend(model_ids, blender_method):\n            try:\n                return self.blend(model_ids, blender_method)\n            except ClientError as e:\n                print(e)\n                return None"
      ],
      "api_methods": [
        "model.get",
        "dr.datetimemodel.get",
        "dr.enums.blender_method"
      ],
      "complexity_score": 0.8500000000000001,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_328893563998959657",
      "title": "ts_metrics.py",
      "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n\n#These functions can be used to calculate common evaluation metrics\n\nimport numpy as np\n\n\n#####################\n# Evaluation Metrics\n#####################\n\ndef mae(act, pred, weight=None):\n    \"\"\"\n    MAE = Mean Absolute Error = mean( abs(act - pred) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    ad = np.abs(d)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        ad = ad * weight / weight.mean()\n    mae = ad.mean()\n\n    if np.isnan(mae):\n        return np.finfo(np.float64).max\n    else:\n        return mae\n\n\ndef mape(act, pred, nan='ignore'):\n\n    # ignore NAN (drop rows), do nothing, replace Nan with 0\n    if nan not in ['ignore', 'set_to_zero', 'error']:\n        raise ValueError(f'{nan} must be either ignore, set_to_zero, or error')\n\n    act, pred = np.array(act), np.array(pred)\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(act - pred)\n    d = act\n    ape = n / d\n\n    if nan == 'set_to_zero':\n        ape[~np.isfinite(ape)] = 0\n    elif nan == 'ignore':\n        ape = ape[np.isfinite(ape)]\n\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape\n\n\ndef smape(act, pred):\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(pred - act)\n    d = (np.abs(pred) + np.abs(act)) / 2\n    ape = n / d\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape\n\n\ndef rmse(act, pred, weight=None):\n    \"\"\"\n    RMSE = Root Mean Squared Error = sqrt( mean( (act - pred)**2 ) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    sd = np.power(d, 2)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        sd = sd * weight / weight.mean()\n    mse = sd.mean()\n    rmse = np.sqrt(mse)\n\n    if np.isnan(rmse):\n        return np.finfo(np.float64).max\n    else:\n        return rmse\n\n\ndef gamma_loss(act, pred, weight=None):\n    \"\"\"Gamma deviance\"\"\"\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, eps)  # ensure actuals are strictly positive\n    d = 2 * (-np.log(act / pred) + (act - pred) / pred)\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)\n\n\ndef tweedie_loss(act, pred, weight=None, p=1.5):\n    \"\"\"tweedie deviance for p = 1.5 only\"\"\"\n\n    if p <= 1 or p >= 2:\n        raise ValueError('p equal to %s is not supported' % p)\n\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are not negative\n    d = (\n        (act ** (2.0 - p)) / ((1 - p) * (2 - p))\n        - (act * (pred ** (1 - p))) / (1 - p)\n        + (pred ** (2 - p)) / (2 - p)\n    )\n    d = 2 * d\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)\n\n\ndef poisson_loss(act, pred, weight=None):\n    \"\"\"\n        Poisson Deviance = 2*(act*log(act/pred)-(act-pred))\n        ONLY WORKS FOR POSITIVE RESPONSES\n    \"\"\"\n    if len(pred.shape) > 1:\n        pred = pred.ravel()\n    pred = np.maximum(pred, 1e-8)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are non-negative\n    d = np.zeros(len(act))\n    d[act == 0] = pred[act == 0]\n    cond = act > 0\n    d[cond] = act[cond] * np.log(act[cond] / pred[cond]) - (act[cond] - pred[cond])\n    d = d * 2\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        d = d * weight / weight.mean()\n    return d.mean()",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_metrics.py",
      "tags": [
        "numpy"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_metrics.py",
        "size": 3935,
        "code_chunks": 7
      },
      "code_examples": [
        "def mae(act, pred, weight=None):\n    \"\"\"\n    MAE = Mean Absolute Error = mean( abs(act - pred) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    ad = np.abs(d)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        ad = ad * weight / weight.mean()\n    mae = ad.mean()\n\n    if np.isnan(mae):\n        return np.finfo(np.float64).max\n    else:\n        return mae",
        "def mape(act, pred, nan='ignore'):\n\n    # ignore NAN (drop rows), do nothing, replace Nan with 0\n    if nan not in ['ignore', 'set_to_zero', 'error']:\n        raise ValueError(f'{nan} must be either ignore, set_to_zero, or error')\n\n    act, pred = np.array(act), np.array(pred)\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(act - pred)\n    d = act\n    ape = n / d\n\n    if nan == 'set_to_zero':\n        ape[~np.isfinite(ape)] = 0\n    elif nan == 'ignore':\n        ape = ape[np.isfinite(ape)]\n\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape",
        "def smape(act, pred):\n    pred = pred.astype(np.float64, copy=False)\n    n = np.abs(pred - act)\n    d = (np.abs(pred) + np.abs(act)) / 2\n    ape = n / d\n    smape = np.mean(ape)\n\n    if np.isnan(smape):\n        return np.finfo(np.float64).max\n\n    return smape",
        "def rmse(act, pred, weight=None):\n    \"\"\"\n    RMSE = Root Mean Squared Error = sqrt( mean( (act - pred)**2 ) )\n    \"\"\"\n    if len(pred.shape) > 1:\n        if pred.shape[1] == 2:\n            pred = pred[:, 1]\n        else:\n            pred = pred.ravel()\n\n    pred = pred.astype(np.float64, copy=False)\n    d = act - pred\n    sd = np.power(d, 2)\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        sd = sd * weight / weight.mean()\n    mse = sd.mean()\n    rmse = np.sqrt(mse)\n\n    if np.isnan(rmse):\n        return np.finfo(np.float64).max\n    else:\n        return rmse",
        "def gamma_loss(act, pred, weight=None):\n    \"\"\"Gamma deviance\"\"\"\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, eps)  # ensure actuals are strictly positive\n    d = 2 * (-np.log(act / pred) + (act - pred) / pred)\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)",
        "def tweedie_loss(act, pred, weight=None, p=1.5):\n    \"\"\"tweedie deviance for p = 1.5 only\"\"\"\n\n    if p <= 1 or p >= 2:\n        raise ValueError('p equal to %s is not supported' % p)\n\n    eps = 0.001\n    pred = np.maximum(pred, eps)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are not negative\n    d = (\n        (act ** (2.0 - p)) / ((1 - p) * (2 - p))\n        - (act * (pred ** (1 - p))) / (1 - p)\n        + (pred ** (2 - p)) / (2 - p)\n    )\n    d = 2 * d\n    if weight is not None:\n        d = d * weight / np.mean(weight)\n    return np.mean(d)",
        "def poisson_loss(act, pred, weight=None):\n    \"\"\"\n        Poisson Deviance = 2*(act*log(act/pred)-(act-pred))\n        ONLY WORKS FOR POSITIVE RESPONSES\n    \"\"\"\n    if len(pred.shape) > 1:\n        pred = pred.ravel()\n    pred = np.maximum(pred, 1e-8)  # ensure predictions are strictly positive\n    act = np.maximum(act, 0)  # ensure actuals are non-negative\n    d = np.zeros(len(act))\n    d[act == 0] = pred[act == 0]\n    cond = act > 0\n    d[cond] = act[cond] * np.log(act[cond] / pred[cond]) - (act[cond] - pred[cond])\n    d = d * 2\n    if weight is not None:\n        if weight.sum() == 0:\n            return 0\n        d = d * weight / weight.mean()\n    return d.mean()"
      ],
      "api_methods": [],
      "complexity_score": 0.44999999999999996,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_-5057494361039239991",
      "title": "ts_modeling.py",
      "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n\n#This function will help you create DataRobot Time series projects.\n\nimport datetime as dt\nimport time\n\nimport datarobot as dr\nimport numpy as np\nimport pandas as pd\n\nfrom ts_data_quality_check import get_timestep\n\n\n###################\n# Project Creation\n###################\n\n\ndef create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project\n\n\ndef create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects\n\n\ndef wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_modeling.py",
      "tags": [
        "numpy",
        "blueprint",
        "autopilot",
        "modeling",
        "pandas",
        "time-series",
        "datarobot-api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_modeling.py",
        "size": 10163,
        "code_chunks": 3
      },
      "code_examples": [
        "def create_dr_project(df, project_name, ts_settings, **advanced_options):\n    \"\"\"\n    Kickoff single DataRobot project\n    df: pandas df\n    project_name: name of project\n    ts_settings: dictionary of parameters for time series project\n    Returns:\n    --------\n    DataRobot project object\n    \"\"\"\n\n    print(f'Building Next Project \\n...\\n')\n\n    #######################\n    # Get Advanced Options\n    #######################\n    opts = {\n        'weights': None,\n        'response_cap': None,\n        'blueprint_threshold': None,\n        'seed': None,\n        'smart_downsampled': False,\n        'majority_downsampling_rate': None,\n        'offset': None,\n        'exposure': None,\n        'accuracy_optimized_mb': None,\n        'scaleout_modeling_mode': None,\n        'events_count': None,\n        'monotonic_increasing_featurelist_id': None,\n        'monotonic_decreasing_featurelist_id': None,\n        'only_include_monotonic_blueprints': None,\n    }\n\n    for opt in advanced_options.items():\n        opts[opt[0]] = opt[1]\n\n    opts = dr.AdvancedOptions(\n        weights=opts['weights'],\n        seed=opts['seed'],\n        monotonic_increasing_featurelist_id=opts['monotonic_increasing_featurelist_id'],\n        monotonic_decreasing_featurelist_id=opts['monotonic_decreasing_featurelist_id'],\n        only_include_monotonic_blueprints=opts['only_include_monotonic_blueprints'],\n        accuracy_optimized_mb=opts['accuracy_optimized_mb'],\n        smart_downsampled=opts['smart_downsampled'],\n    )\n\n    ############################\n    # Get Datetime Specification\n    ############################\n    settings = {\n        'max_date': None,\n        'known_in_advance': None,\n        'num_backtests': None,\n        'validation_duration': None,\n        'holdout_duration': None,\n        'holdout_start_date': None,\n        'disable_holdout': False,\n        'number_of_backtests': None,\n        'backtests': None,\n        'use_cross_series_features': None,\n        'aggregation_type': None,\n        'cross_series_group_by_columns': None,\n        'calendar_id': None,\n        'use_time_series': False,\n        'series_id': None,\n        'metric': None,\n        'target': None,\n        'mode': dr.AUTOPILOT_MODE.FULL_AUTO,  # MANUAL #QUICK\n        'date_col': None,\n        'fd_start': None,\n        'fd_end': None,\n        'fdw_start': None,\n        'fdw_end': None,\n    }\n\n    for s in ts_settings.items():\n        settings[s[0]] = s[1]\n\n    df[settings['date_col']] = pd.to_datetime(df[settings['date_col']])\n\n    if settings['max_date'] is None:\n        settings['max_date'] = df[settings['date_col']].max()\n    else:\n        settings['max_date'] = pd.to_datetime(settings['max_date'])\n\n    if ts_settings['known_in_advance']:\n        settings['known_in_advance'] = [\n            dr.FeatureSettings(feat_name, known_in_advance=True)\n            for feat_name in settings['known_in_advance']\n        ]\n\n    # Update validation and holdout duration, start, and end date\n    project_time_unit, project_time_step = get_timestep(df, settings)\n\n    validation_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n    holdout_durations = {'minute': 0, 'hour': 0, 'day': 0, 'month': 0}\n\n    if project_time_unit == 'minute':\n        validation_durations['minute'] = settings['validation_duration']\n        holdout_durations['minute'] = settings['holdout_duration']\n\n    elif project_time_unit == 'hour':\n        validation_durations['hour'] = settings['validation_duration']\n        holdout_durations['hour'] = settings['holdout_duration']\n\n    elif project_time_unit == 'day':\n        validation_durations['day'] = settings['validation_duration']\n        holdout_durations['day'] = settings['holdout_duration']\n\n    elif project_time_unit == 'week':\n        validation_durations['day'] = settings['validation_duration'] * 7\n        holdout_durations['day'] = settings['holdout_duration'] * 7\n\n    elif project_time_unit == 'month':\n        validation_durations['day'] = settings['validation_duration'] * 31\n        holdout_durations['day'] = settings['holdout_duration'] * 31\n\n    else:\n        raise ValueError(f'{project_time_unit} is not a supported timestep')\n\n    if settings['disable_holdout']:\n        settings['holdout_duration'] = None\n        settings['holdout_start_date'] = None\n    else:\n        settings['holdout_start_date'] = settings['max_date'] - dt.timedelta(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n        settings['holdout_duration'] = dr.partitioning_methods.construct_duration_string(\n            minutes=holdout_durations['minute'],\n            hours=holdout_durations['hour'],\n            days=holdout_durations['day'],\n        )\n\n    ###############################\n    # Create Datetime Specification\n    ###############################\n    time_partition = dr.DatetimePartitioningSpecification(\n        feature_settings=settings['known_in_advance'],\n        # gap_duration = dr.partitioning_methods.construct_duration_string(years=0, months=0, days=0),\n        validation_duration=dr.partitioning_methods.construct_duration_string(\n            minutes=validation_durations['minute'],\n            hours=validation_durations['hour'],\n            days=validation_durations['day'],\n        ),\n        datetime_partition_column=settings['date_col'],\n        use_time_series=settings['use_time_series'],\n        disable_holdout=settings['disable_holdout'],  # set this if disable_holdout is set to False\n        holdout_start_date=settings['holdout_start_date'],\n        holdout_duration=settings[\n            'holdout_duration'\n        ],  # set this if disable_holdout is set to False\n        multiseries_id_columns=[settings['series_id']],\n        forecast_window_start=int(settings['fd_start']),\n        forecast_window_end=int(settings['fd_end']),\n        feature_derivation_window_start=int(settings['fdw_start']),\n        feature_derivation_window_end=int(settings['fdw_end']),\n        number_of_backtests=settings['num_backtests'],\n        calendar_id=settings['calendar_id'],\n        use_cross_series_features=settings['use_cross_series_features'],\n        aggregation_type=settings['aggregation_type'],\n        cross_series_group_by_columns=settings['cross_series_group_by_columns'],\n    )\n\n    ################\n    # Create Project\n    ################\n    project = dr.Project.create(\n        project_name=project_name, sourcedata=df, max_wait=14400, read_timeout=14400\n    )\n\n    print(f'Project {project_name} Created...')\n\n    #################\n    # Start Autopilot\n    #################\n    project.set_target(\n        target=settings['target'],\n        metric=settings['metric'],\n        mode=settings['mode'],\n        advanced_options=opts,\n        worker_count=-1,\n        partitioning_method=time_partition,\n        max_wait=14400,\n    )\n\n    return project",
        "def create_dr_projects(\n    df, ts_settings, prefix='TS', split_col=None, fdws=None, fds=None, **advanced_options\n):\n    \"\"\"\n    Kickoff multiple DataRobot projects\n    df: pandas df\n    ts_settings: dictionary of parameters for time series project\n    prefix: str to concatenate to start of project name\n    split_col: column in df that identifies cluster labels\n    fdws: list of tuples containing feature derivation window start and end values\n    fds: list of tuples containing forecast distance start and end values\n    Returns:\n    --------\n    List of projects\n    Example:\n    --------\n    split_col = 'Cluster'\n    fdws=[(-14,0),(-28,0),(-62,0)]\n    fds = [(1,7),(8,14)]\n    \"\"\"\n\n    if fdws is None:\n        fdws = [(ts_settings['fdw_start'], ts_settings['fdw_end'])]\n\n    if fds is None:\n        fds = [(ts_settings['fd_start'], ts_settings['fd_end'])]\n\n    clusters = range(1) if split_col is None else df[split_col].unique()\n\n    assert isinstance(fdws, list), 'fdws must be a list object'\n    assert isinstance(fds, list), 'fds must be a list object'\n    if split_col:\n        assert len(df[split_col].unique()) > 1, 'There must be at least 2 clusters'\n\n    n_projects = len(clusters) * len(fdws) * len(fds)\n    print(f'Kicking off {n_projects} projects\\n')\n\n    projects = []\n    for c in clusters:\n        for fdw in fdws:\n            for fd in fds:\n                ts_settings['fd_start'], ts_settings['fd_end'] = fd[0], fd[1]\n                ts_settings['fdw_start'], ts_settings['fdw_end'] = fdw[0], fdw[1]\n                cluster_suffix = 'all_series' if split_col is None else 'Cluster-' + c.astype('str')\n\n                # Name project\n                project_name = '{prefix}_FD:{start}-{end}_FDW:{fdw}_{cluster}'.format(\n                    prefix=prefix,\n                    fdw=ts_settings['fdw_start'],\n                    start=ts_settings['fd_start'],\n                    end=ts_settings['fd_end'],\n                    cluster=cluster_suffix,\n                )\n\n                if split_col is not None:\n                    data = df.loc[df[split_col] == c, :].copy()\n                    data.drop(columns=split_col, axis=1, inplace=True)\n                else:\n                    data = df.copy()\n\n                # Create project\n                project = create_dr_project(\n                    data, project_name, ts_settings, advanced_options=advanced_options\n                )\n                projects.append(project)\n\n    return projects",
        "def wait_for_jobs_to_process(projects):\n    \"\"\"\n    Check if any DataRobot jobs are still processing\n    \"\"\"\n    all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n    while all_jobs > 0:\n        print(f'There are {all_jobs} jobs still processing')\n        time.sleep(60)\n        all_jobs = np.sum([len(p.get_all_jobs()) for p in projects])\n\n    print('All jobs have finished processing...')"
      ],
      "api_methods": [
        "project.set_target",
        "dr.partitioning_methods.construct_duration_string",
        "project.create",
        "dr.autopilot_mode.full_auto",
        "dr.project.create"
      ],
      "complexity_score": 1.0,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_-6793623690832519749",
      "title": "ts_preprocessing.py",
      "content": "#Authors: Justin Swansburg, Mark Philip\n\nimport numpy as np\n\n\n#####################\n# Preprocessing Funcs\n#####################\n\n\ndef dataset_reduce_memory(df):\n    \"\"\"\n    Recast numerics to lower precision\n    \"\"\"\n    for c in df.select_dtypes(include=['float64']).columns:\n        df[c] = df[c].astype(np.float32)\n    for c in df.select_dtypes(include=['int64']).columns:\n        df[c] = df[c].astype(np.int32)\n    return df\n\n\ndef create_series_id(df, cols_to_concat, convert=True):\n    \"\"\"\n    Concatenate columns\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    df = df[cols_to_concat].copy()\n    non_strings = [c for c in df[cols_to_concat] if df[c].dtype != 'object']\n\n    if len(non_strings) > 0:\n        if convert:\n            df[non_strings] = df[non_strings].applymap(str)\n        else:\n            raise TypeError(\"columns must all be type str\")\n\n    df['series_id'] = df[cols_to_concat].apply(lambda x: '_'.join(x), axis=1)\n    return df['series_id']\n\n\ndef _create_cross_series_feature(df, group, col, func):\n    col_name = col + '_' + func\n    df.loc[:, col_name] = df.groupby(group)[col].transform(func)\n    return df\n\n\ndef create_cross_series_features(df, group, cols, funcs):\n    \"\"\"\n    Create custom aggregations across groups\n    Returns:\n    --------\n    pandas df with new cross series features\n    Example:\n    --------\n    df_agg = create_cross_series_features(df,\n                                          group=[date_col,'Cluster'],\n                                          cols=[target,'feat_1'],\n                                          funcs=['mean','std'])\n    \"\"\"\n    for c in cols:\n        for f in funcs:\n            df = _create_cross_series_feature(df, group, c, f)\n    return df.reset_index(drop=True)\n\n\ndef get_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    \"\"\"\n    Identify series where the target is 0.0 in more than x% of the rows\n    Returns:\n    --------\n    List of series\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n    series = df[df >= cutoff].index.values\n\n    return series\n\n\ndef drop_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    series_id = ts_settings['series_id']\n\n    series_to_drop = get_zero_inflated_series(df, ts_settings, cutoff=cutoff)\n\n    if len(series_to_drop) > 0:\n        print('Dropping ', len(series_to_drop), ' zero-inflated series')\n        df = df.loc[~df[series_id].isin(series_to_drop), :].reset_index(drop=True)\n        print('Remaining series: ', len(df[series_id].unique()))\n    else:\n        print('There are no zero-inflated series to drop')\n\n    return df\n\n\ndef sample_series(df, series_id, date_col, target, x=1, method='random', **kwargs):\n    \"\"\"\n    Sample series\n    x: percent of series to sample\n    random: sample x% of the series at random\n    target: sample the largest x% of series\n    timespan: sample the top x% of series with the longest histories\n    \"\"\"\n    if (x > 1) | (x < 0):\n        raise ValueError('x must be between 0 and 1')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series = round(x * len(df[series_id].unique()))\n\n    if method == 'random':\n        series_to_keep = np.random.choice(df[series_id].values, size=series)\n\n    elif method == 'target':\n        series_to_keep = (\n            df.groupby([series_id])[target]\n            .mean()\n            .sort_values(ascending=False)\n            .reset_index()\n            .loc[0:series, series_id]\n        )\n\n    elif method == 'timespan':\n        max_timespan = df[date_col].max() - df[date_col].min()\n        series_timespans = (\n            df.groupby([series_id])[date_col]\n            .apply(lambda x: x.max() - x.min())\n            .sort_values(ascending=False)\n            .reset_index()\n        )\n        series_to_keep = series_timespans.loc[0:series, series_id]\n        if kwargs.get('full_timespan'):\n            series_to_keep = series_timespans.loc[series_timespans == max_timespan, series_id]\n\n    else:\n        raise ValueError('Method not supported. Must be either random, target, or timespan')\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n\n    return sampled_df.reset_index(drop=True)\n\n\ndef drop_series_w_gaps(df, series_id, date_col, target, max_gap=1, output_dropped_series=False):\n    \"\"\"\n    Sample series\n    max_gap: number of timesteps\n    \"\"\"\n    if not isinstance(max_gap, int):\n        raise TypeError('max gap must be an int')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series_max_gap = df.groupby([series_id]).apply(lambda x: x[date_col].diff().max())\n    median_timestep = df.groupby([series_id])[date_col].diff().median()\n    series_to_keep = series_max_gap[(series_max_gap / median_timestep) <= max_gap].index.values\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n    dropped_df = df.loc[~df[series_id].isin(series_to_keep), :]\n\n    if output_dropped_series:\n        return sampled_df, dropped_df\n    else:\n        return sampled_df",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_preprocessing.py",
      "tags": [
        "pandas",
        "numpy"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_preprocessing.py",
        "size": 5119,
        "code_chunks": 8
      },
      "code_examples": [
        "def dataset_reduce_memory(df):\n    \"\"\"\n    Recast numerics to lower precision\n    \"\"\"\n    for c in df.select_dtypes(include=['float64']).columns:\n        df[c] = df[c].astype(np.float32)\n    for c in df.select_dtypes(include=['int64']).columns:\n        df[c] = df[c].astype(np.int32)\n    return df",
        "def create_series_id(df, cols_to_concat, convert=True):\n    \"\"\"\n    Concatenate columns\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    df = df[cols_to_concat].copy()\n    non_strings = [c for c in df[cols_to_concat] if df[c].dtype != 'object']\n\n    if len(non_strings) > 0:\n        if convert:\n            df[non_strings] = df[non_strings].applymap(str)\n        else:\n            raise TypeError(\"columns must all be type str\")\n\n    df['series_id'] = df[cols_to_concat].apply(lambda x: '_'.join(x), axis=1)\n    return df['series_id']",
        "def _create_cross_series_feature(df, group, col, func):\n    col_name = col + '_' + func\n    df.loc[:, col_name] = df.groupby(group)[col].transform(func)\n    return df",
        "def create_cross_series_features(df, group, cols, funcs):\n    \"\"\"\n    Create custom aggregations across groups\n    Returns:\n    --------\n    pandas df with new cross series features\n    Example:\n    --------\n    df_agg = create_cross_series_features(df,\n                                          group=[date_col,'Cluster'],\n                                          cols=[target,'feat_1'],\n                                          funcs=['mean','std'])\n    \"\"\"\n    for c in cols:\n        for f in funcs:\n            df = _create_cross_series_feature(df, group, c, f)\n    return df.reset_index(drop=True)",
        "def get_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    \"\"\"\n    Identify series where the target is 0.0 in more than x% of the rows\n    Returns:\n    --------\n    List of series\n    \"\"\"\n    date_col = ts_settings['date_col']\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    df = df.groupby([series_id])[target].apply(lambda x: (x.dropna() == 0).mean())\n    series = df[df >= cutoff].index.values\n\n    return series",
        "def drop_zero_inflated_series(df, ts_settings, cutoff=0.99):\n    series_id = ts_settings['series_id']\n\n    series_to_drop = get_zero_inflated_series(df, ts_settings, cutoff=cutoff)\n\n    if len(series_to_drop) > 0:\n        print('Dropping ', len(series_to_drop), ' zero-inflated series')\n        df = df.loc[~df[series_id].isin(series_to_drop), :].reset_index(drop=True)\n        print('Remaining series: ', len(df[series_id].unique()))\n    else:\n        print('There are no zero-inflated series to drop')\n\n    return df",
        "def sample_series(df, series_id, date_col, target, x=1, method='random', **kwargs):\n    \"\"\"\n    Sample series\n    x: percent of series to sample\n    random: sample x% of the series at random\n    target: sample the largest x% of series\n    timespan: sample the top x% of series with the longest histories\n    \"\"\"\n    if (x > 1) | (x < 0):\n        raise ValueError('x must be between 0 and 1')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series = round(x * len(df[series_id].unique()))\n\n    if method == 'random':\n        series_to_keep = np.random.choice(df[series_id].values, size=series)\n\n    elif method == 'target':\n        series_to_keep = (\n            df.groupby([series_id])[target]\n            .mean()\n            .sort_values(ascending=False)\n            .reset_index()\n            .loc[0:series, series_id]\n        )\n\n    elif method == 'timespan':\n        max_timespan = df[date_col].max() - df[date_col].min()\n        series_timespans = (\n            df.groupby([series_id])[date_col]\n            .apply(lambda x: x.max() - x.min())\n            .sort_values(ascending=False)\n            .reset_index()\n        )\n        series_to_keep = series_timespans.loc[0:series, series_id]\n        if kwargs.get('full_timespan'):\n            series_to_keep = series_timespans.loc[series_timespans == max_timespan, series_id]\n\n    else:\n        raise ValueError('Method not supported. Must be either random, target, or timespan')\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n\n    return sampled_df.reset_index(drop=True)",
        "def drop_series_w_gaps(df, series_id, date_col, target, max_gap=1, output_dropped_series=False):\n    \"\"\"\n    Sample series\n    max_gap: number of timesteps\n    \"\"\"\n    if not isinstance(max_gap, int):\n        raise TypeError('max gap must be an int')\n\n    df.sort_values(by=[date_col, series_id], ascending=True, inplace=True)\n    series_max_gap = df.groupby([series_id]).apply(lambda x: x[date_col].diff().max())\n    median_timestep = df.groupby([series_id])[date_col].diff().median()\n    series_to_keep = series_max_gap[(series_max_gap / median_timestep) <= max_gap].index.values\n\n    sampled_df = df.loc[df[series_id].isin(series_to_keep), :]\n    dropped_df = df.loc[~df[series_id].isin(series_to_keep), :]\n\n    if output_dropped_series:\n        return sampled_df, dropped_df\n    else:\n        return sampled_df"
      ],
      "api_methods": [],
      "complexity_score": 0.8,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_2306162657264323069",
      "title": "ts_projects.py",
      "content": "#Author: Justin Swansburg, Mark Philip\n\n#Make sure you are connected to DataRobot Client.\n\n#The functions below will help you evaluate a DataRobot TS project.\n\nimport datarobot as dr\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom ts_metrics import *\n\n\n######################\n# Project Evaluation\n######################\n\n\ndef get_top_models_from_project(\n    project, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    project: project object\n        DataRobot project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from a DataRobot project\n    \"\"\"\n    assert data_subset in [\n        'backtest_1',\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either backtest_1, allBacktests, or holdout'\n    if n_models is not None:\n        assert isinstance(n_models, int), 'n_models must be an int'\n    if n_models is not None:\n        assert n_models >= 1, 'n_models must be greater than or equal to 1'\n    assert isinstance(include_blenders, bool), 'include_blenders must be a boolean'\n\n    mapper = {\n        'backtest_1': 'backtestingScores',\n        'allBacktests': 'backtesting',\n        'holdout': 'holdout',\n    }\n\n    if metric is None:\n        metric = project.metric\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    models = [\n        m\n        for m in project.get_datetime_models()\n        if m.backtests[0]['status'] != 'BACKTEST_BOUNDARIES_EXCEEDED'\n    ]  # if m.holdout_status != 'HOLDOUT_BOUNDARIES_EXCEEDED']\n\n    if data_subset == 'backtest_1':\n        # models = sorted(models, key=lambda m: np.mean([i for i in m.metrics[metric][mapper[data_subset]][0] if i]), reverse=False)\n        models = sorted(\n            models, key=lambda m: m.metrics[metric][mapper[data_subset]][0], reverse=False\n        )\n    elif data_subset == 'allBacktests':\n        models = sorted(\n            models,\n            key=lambda m: m.metrics[metric][mapper[data_subset]]\n            if m.metrics[metric][mapper[data_subset]] is not None\n            else np.nan,\n            reverse=False,\n        )\n    else:\n        models = sorted(models, key=lambda m: m.metrics[metric][mapper[data_subset]], reverse=False)\n\n    if not include_blenders:\n        models = [m for m in models if m.model_category != 'blend']\n\n    if n_models is None:\n        n_models = len(models)\n\n    models = models[0:n_models]\n\n    assert len(models) > 0, 'You have not run any models for this project'\n\n    return models\n\n\ndef get_top_models_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Pull top models from leaderboard across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from DataRobot project(s)\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    models_all = []\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n        models_all.extend(models)\n    return models_all\n\n\ndef compute_backtests(\n    projects, n_models=5, data_subset='backtest_1', include_blenders=True, metric=None\n):\n    \"\"\"\n    Compute all backtests for top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        for m in models:\n            try:\n                m.score_backtests()  # request backtests for top models\n                print(f'Computing backtests for model {m.id} in Project {p.project_name}')\n            except dr.errors.ClientError:\n                pass\n        print(\n            f'All available backtests have been submitted for scoring for project {p.project_name}'\n        )\n\n\ndef get_or_request_backtest_scores(\n    projects, n_models=5, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get or request backtest and holdout scores from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    scores = pd.DataFrame()\n    for p in projects:\n\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        if metric is None:\n            metric = p.metric\n\n        backtest_scores = pd.DataFrame(\n            [\n                {\n                    'Project_Name': p.project_name,\n                    'Project_ID': p.id,\n                    'Model_ID': m.id,\n                    'Model_Type': m.model_type,\n                    'Featurelist': m.featurelist_name,\n                    f'Backtest_1_{metric}': m.metrics[metric]['backtestingScores'][0],\n                    'Backtest_1_MASE': m.metrics['MASE']['backtestingScores'][0],\n                    'Backtest_1_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'][0],\n                    'Backtest_1_SMAPE': m.metrics['SMAPE']['backtestingScores'][0],\n                    'Backtest_1_R_Squared': m.metrics['R Squared']['backtestingScores'][0],\n                    f'All_Backtests_{metric}': m.metrics[metric]['backtestingScores'],\n                    'All_Backtests_MASE': m.metrics['MASE']['backtestingScores'],\n                    'All_Backtests_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'],\n                    'All_Backtests_SMAPE': m.metrics['SMAPE']['backtestingScores'],\n                    'All_Backtests_R_Squared': m.metrics['R Squared']['backtestingScores'],\n                    f'Holdout_{metric}': m.metrics[metric]['holdout'],\n                    'Holdout_MASE': m.metrics['MASE']['holdout'],\n                    'Holdout_Theils_U': m.metrics[\"Theil's U\"]['holdout'],\n                    'Holdout_SMAPE': m.metrics['SMAPE']['holdout'],\n                    'Holdout_R_Squared': m.metrics['R Squared']['holdout'],\n                }\n                for m in models\n            ]\n        ).sort_values(by=[f'Backtest_1_{metric}'])\n\n        scores = scores.append(backtest_scores).reset_index(\n            drop=True\n        )  # append top model from each project\n\n    print(f'Scores for all {len(projects)} projects have been computed')\n\n    return scores\n\n\ndef get_or_request_training_predictions_from_model(model, data_subset='allBacktests'):\n    project = dr.Project.get(model.project_id)\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    try:\n        predict_job = model.request_training_predictions(data_subset)\n        training_predictions = predict_job.get_result_when_complete(max_wait=10000)\n\n    except dr.errors.ClientError:\n        prediction_id = [\n            p.prediction_id\n            for p in dr.TrainingPredictions.list(project.id)\n            if p.model_id == model.id and p.data_subset == data_subset\n        ][0]\n        training_predictions = dr.TrainingPredictions.get(project.id, prediction_id)\n\n    return training_predictions.get_all_as_dataframe(serializer='csv')\n\n\ndef get_or_request_training_predictions_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get row-level backtest or holdout predictions from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = pd.DataFrame()\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            tmp = get_or_request_training_predictions_from_model(m, data_subset)\n            tmp['Project_Name'] = p.project_name\n            tmp['Project_ID'] = p.id\n            tmp['Model_ID'] = m.id\n            tmp['Model_Type'] = m.model_type\n        preds = preds.append(tmp).reset_index(drop=True)\n\n    return preds\n\n\ndef get_preds_and_actuals(\n    df,\n    projects,\n    ts_settings,\n    n_models=1,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Get row-level predictions and merge onto actuals\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = get_or_request_training_predictions_from_projects(\n        projects,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    preds['timestamp'] = pd.to_datetime(preds['timestamp'].apply(lambda x: x[:-8]))\n    df = df.merge(\n        preds,\n        how='left',\n        left_on=[ts_settings['date_col'], ts_settings['series_id']],\n        right_on=['timestamp', 'series_id'],\n        validate='one_to_many',\n    )\n    df = df.loc[~np.isnan(df['prediction']), :].reset_index(drop=True)\n    return df\n\n\ndef get_cluster_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get cluster-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    print('Getting cluster accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby(['Cluster'])\n        .apply(lambda x: acc_calc(x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n    groups.columns = ['Cluster', f'Cluster_{acc_calc.__name__.upper()}']\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(\n        act=df[ts_settings['target']], pred=df['prediction']\n    )\n\n    return groups\n\n\ndef plot_cluster_acc(cluster_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse):\n    \"\"\"\n    Plots cluster-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_cluster_acc()\n    ts_settings: dict\n        Pparameters for time series project\n    data_subset: str\n        Choose either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    cluster_acc['Label'] = '=' + cluster_acc['Cluster']\n\n    fig = px.bar(cluster_acc, x='Label', y=f'Cluster_{acc_calc.__name__.upper()}').for_each_trace(\n        lambda t: t.update(name=t.name.replace('=', ''))\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=cluster_acc['Label'],\n            y=cluster_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Cluster Accuracy - {data_subset}')\n    fig.show()\n\n\ndef get_series_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get series-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    print('Getting series accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby([series_id]).apply(lambda x: acc_calc(x[target], x['prediction'])).reset_index()\n    )\n    groups.columns = [series_id, f'Series_{acc_calc.__name__.upper()}']\n    right = df[[series_id, 'Cluster']].drop_duplicates().reset_index(drop=True)\n    groups = groups.merge(right, how='left', on=series_id)\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(act=df[target], pred=df['prediction'])\n\n    return groups\n\n\ndef plot_series_acc(series_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse, n=50):\n    \"\"\"\n    Plots series-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_series_acc()\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Choose from either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    n_series = len(series_acc[ts_settings['series_id']].unique())\n    n = min(n_series, n)\n\n    series_acc.sort_values(by=f'Series_{acc_calc.__name__.upper()}', ascending=False, inplace=True)\n\n    series_acc = series_acc[0:n]\n\n    fig = px.bar(\n        series_acc,\n        x=ts_settings['series_id'],\n        y=f'Series_{acc_calc.__name__.upper()}',\n        color='Cluster',\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.add_trace(\n        go.Scatter(\n            x=series_acc[ts_settings['series_id']],\n            y=series_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Series Accuracy - {data_subset}')\n    fig.show()\n\n\ndef get_project_info(df):\n    \"\"\"\n    Parse project name to get FD, FDW, and Cluster information\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = df.copy()\n    try:\n        df['Cluster'] = df['Project_Name'].apply(lambda x: x.split('_Cluster-')[1])\n    except:\n        df['Cluster'] = 'all_series'\n\n    df['FD'] = df['Project_Name'].apply(lambda x: x.split('_FD:')[1].split('_FDW:')[0])\n    df['FDW'] = df['Project_Name'].apply(lambda x: x.split('_FDW:')[1].split('_Cluster-')[0])\n\n    return df\n\n\ndef filter_best_fdw_scores(scores, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset df to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = get_project_info(scores)\n    df['_tmp'] = df[col_error].apply(lambda x: np.nanmean(np.array(x, dtype=np.float32)))\n    idx = df.groupby(['Cluster', 'FD']).apply(lambda x: x['_tmp'].idxmin()).values\n    return scores.iloc[idx, :]\n\n\ndef filter_best_fdw_projects(scores, projects, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset list to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    projects: list\n        DataRobot projects object(s)\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    list\n    \"\"\"\n    df = filter_best_fdw_scores(scores, col_error)\n    return [p for p in projects if p.project_name in df['Project_Name'].unique()]\n\n\ndef plot_fd_accuracy(df, projects, ts_settings, data_subset='allBacktests', metric='RMSE'):\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    mapper = {\n        'MAE': mae,\n        'RMSE': rmse,\n        'Gamma Deviance': gamma_loss,\n        'Tweedie Deviance': tweedie_loss,\n        'Poisson Deviance': poisson_loss,\n    }\n\n    df = get_preds_and_actuals(\n        df, projects, ts_settings, n_models=1, data_subset=data_subset, metric=metric\n    )\n    df = (\n        df.groupby(['Project_Name', 'forecast_distance'])\n        .apply(lambda x: mapper[metric](x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n\n    df.columns = ['Project_Name', 'forecast_distance', mapper[metric].__name__.upper()]\n    fig = px.line(\n        df, x='forecast_distance', y=mapper[metric].__name__.upper(), color='Project_Name'\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.update_layout(title_text='Forecasting Accuracy per Forecast Distance')\n    fig.update_yaxes(title=mapper[metric].__name__.upper())\n    fig.update_xaxes(title='Forecast Distance')\n    fig.show()\n\n\ndef plot_fd_accuracy_by_cluster(\n    df, scores, projects, ts_settings, data_subset='holdout', metric='RMSE', split_col='Cluster'\n):\n    scores = get_project_info(scores)\n\n    for c in scores[split_col].unique():\n        project_names = list(\n            scores.loc[scores[split_col] == c, 'Project_Name'].reset_index(drop=True)\n        )\n        projects_by_cluster = [p for p in projects if p.project_name in project_names]\n        plot_fd_accuracy(df, projects_by_cluster, ts_settings, data_subset, metric)\n\n\n###########################\n# Performance Improvements\n###########################\n\n\ndef get_reduced_features_featurelist(project, model, threshold=0.99):\n    \"\"\"\n    Helper function for train_reduced_features_models()\n    project: DataRobot project object\n    model: DataRobot model object\n    threshold: np.float\n    Returns:\n    --------\n    DataRobot featurelist\n    \"\"\"\n    print(\n        f'Collecting Feature Impact for M{model.model_number} in project {project.project_name}...'\n    )\n\n    impact = pd.DataFrame.from_records(model.get_or_request_feature_impact())\n    impact['impactUnnormalized'] = np.where(\n        impact['impactUnnormalized'] < 0, 0, impact['impactUnnormalized']\n    )\n    impact['cumulative_impact'] = (\n        impact['impactUnnormalized'].cumsum() / impact['impactUnnormalized'].sum()\n    )\n\n    to_keep = np.where(impact['cumulative_impact'] <= threshold)[0]\n    if len(to_keep) < 1:\n        print('Applying this threshold would result in a featurelist with no features')\n        return None\n\n    idx = np.max(to_keep)\n\n    selected_features = impact.loc[0:idx, 'featureName'].to_list()\n    feature_list = project.create_modeling_featurelist(\n        f'Top {len(selected_features)} features M{model.model_number}', selected_features\n    )\n\n    return feature_list\n\n\ndef train_reduced_features_models(\n    projects,\n    n_models=1,\n    threshold=0.99,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Retrain top models with reduced feature featurelists\n    projects: list\n        DataRobot project objects(s)\n    n_models: int\n        Number of models to retrain with reduced feature featurelists\n    threshold: np.float\n        Controls the number of features to keep in the reduced feature list. Percentage of cumulative feature impact\n    data_subset: str\n        Choose from either holdout or allBacktests\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            try:\n                feature_list = get_reduced_features_featurelist(p, m, threshold)\n                if feature_list is None:\n                    continue\n                try:\n                    m.retrain(featurelist_id=feature_list.id)\n                    print(f'Training {m.model_type} on Featurelist {feature_list.name}')\n                except dr.errors.ClientError as e:\n                    print(e)\n            except dr.errors.ClientError as e:\n                print(e)",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "Helper Functions/Time Series/Python/ts_projects.py",
      "tags": [
        "pandas",
        "feature-engineering",
        "datarobot-api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Helper Functions/Time Series/Python/ts_projects.py",
        "size": 25426,
        "code_chunks": 18
      },
      "code_examples": [
        "def get_top_models_from_project(\n    project, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    project: project object\n        DataRobot project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from a DataRobot project\n    \"\"\"\n    assert data_subset in [\n        'backtest_1',\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either backtest_1, allBacktests, or holdout'\n    if n_models is not None:\n        assert isinstance(n_models, int), 'n_models must be an int'\n    if n_models is not None:\n        assert n_models >= 1, 'n_models must be greater than or equal to 1'\n    assert isinstance(include_blenders, bool), 'include_blenders must be a boolean'\n\n    mapper = {\n        'backtest_1': 'backtestingScores',\n        'allBacktests': 'backtesting',\n        'holdout': 'holdout',\n    }\n\n    if metric is None:\n        metric = project.metric\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    models = [\n        m\n        for m in project.get_datetime_models()\n        if m.backtests[0]['status'] != 'BACKTEST_BOUNDARIES_EXCEEDED'\n    ]  # if m.holdout_status != 'HOLDOUT_BOUNDARIES_EXCEEDED']\n\n    if data_subset == 'backtest_1':\n        # models = sorted(models, key=lambda m: np.mean([i for i in m.metrics[metric][mapper[data_subset]][0] if i]), reverse=False)\n        models = sorted(\n            models, key=lambda m: m.metrics[metric][mapper[data_subset]][0], reverse=False\n        )\n    elif data_subset == 'allBacktests':\n        models = sorted(\n            models,\n            key=lambda m: m.metrics[metric][mapper[data_subset]]\n            if m.metrics[metric][mapper[data_subset]] is not None\n            else np.nan,\n            reverse=False,\n        )\n    else:\n        models = sorted(models, key=lambda m: m.metrics[metric][mapper[data_subset]], reverse=False)\n\n    if not include_blenders:\n        models = [m for m in models if m.model_category != 'blend']\n\n    if n_models is None:\n        n_models = len(models)\n\n    models = models[0:n_models]\n\n    assert len(models) > 0, 'You have not run any models for this project'\n\n    return models",
        "def get_top_models_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Pull top models from leaderboard across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    List of model objects from DataRobot project(s)\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    models_all = []\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n        models_all.extend(models)\n    return models_all",
        "def compute_backtests(\n    projects, n_models=5, data_subset='backtest_1', include_blenders=True, metric=None\n):\n    \"\"\"\n    Compute all backtests for top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        for m in models:\n            try:\n                m.score_backtests()  # request backtests for top models\n                print(f'Computing backtests for model {m.id} in Project {p.project_name}')\n            except dr.errors.ClientError:\n                pass\n        print(\n            f'All available backtests have been submitted for scoring for project {p.project_name}'\n        )",
        "def get_or_request_backtest_scores(\n    projects, n_models=5, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get or request backtest and holdout scores from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    scores = pd.DataFrame()\n    for p in projects:\n\n        models = get_top_models_from_project(\n            p,\n            n_models=n_models,\n            data_subset=data_subset,\n            include_blenders=include_blenders,\n            metric=metric,\n        )\n\n        if metric is None:\n            metric = p.metric\n\n        backtest_scores = pd.DataFrame(\n            [\n                {\n                    'Project_Name': p.project_name,\n                    'Project_ID': p.id,\n                    'Model_ID': m.id,\n                    'Model_Type': m.model_type,\n                    'Featurelist': m.featurelist_name,\n                    f'Backtest_1_{metric}': m.metrics[metric]['backtestingScores'][0],\n                    'Backtest_1_MASE': m.metrics['MASE']['backtestingScores'][0],\n                    'Backtest_1_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'][0],\n                    'Backtest_1_SMAPE': m.metrics['SMAPE']['backtestingScores'][0],\n                    'Backtest_1_R_Squared': m.metrics['R Squared']['backtestingScores'][0],\n                    f'All_Backtests_{metric}': m.metrics[metric]['backtestingScores'],\n                    'All_Backtests_MASE': m.metrics['MASE']['backtestingScores'],\n                    'All_Backtests_Theils_U': m.metrics[\"Theil's U\"]['backtestingScores'],\n                    'All_Backtests_SMAPE': m.metrics['SMAPE']['backtestingScores'],\n                    'All_Backtests_R_Squared': m.metrics['R Squared']['backtestingScores'],\n                    f'Holdout_{metric}': m.metrics[metric]['holdout'],\n                    'Holdout_MASE': m.metrics['MASE']['holdout'],\n                    'Holdout_Theils_U': m.metrics[\"Theil's U\"]['holdout'],\n                    'Holdout_SMAPE': m.metrics['SMAPE']['holdout'],\n                    'Holdout_R_Squared': m.metrics['R Squared']['holdout'],\n                }\n                for m in models\n            ]\n        ).sort_values(by=[f'Backtest_1_{metric}'])\n\n        scores = scores.append(backtest_scores).reset_index(\n            drop=True\n        )  # append top model from each project\n\n    print(f'Scores for all {len(projects)} projects have been computed')\n\n    return scores",
        "def get_or_request_training_predictions_from_model(model, data_subset='allBacktests'):\n    project = dr.Project.get(model.project_id)\n\n    if data_subset == 'holdout':\n        project.unlock_holdout()\n\n    try:\n        predict_job = model.request_training_predictions(data_subset)\n        training_predictions = predict_job.get_result_when_complete(max_wait=10000)\n\n    except dr.errors.ClientError:\n        prediction_id = [\n            p.prediction_id\n            for p in dr.TrainingPredictions.list(project.id)\n            if p.model_id == model.id and p.data_subset == data_subset\n        ][0]\n        training_predictions = dr.TrainingPredictions.get(project.id, prediction_id)\n\n    return training_predictions.get_all_as_dataframe(serializer='csv')",
        "def get_or_request_training_predictions_from_projects(\n    projects, n_models=1, data_subset='allBacktests', include_blenders=True, metric=None\n):\n    \"\"\"\n    Get row-level backtest or holdout predictions from top models across multiple DataRobot projects\n    projects: list\n        DataRobot project object(s)\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas Series\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = pd.DataFrame()\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            tmp = get_or_request_training_predictions_from_model(m, data_subset)\n            tmp['Project_Name'] = p.project_name\n            tmp['Project_ID'] = p.id\n            tmp['Model_ID'] = m.id\n            tmp['Model_Type'] = m.model_type\n        preds = preds.append(tmp).reset_index(drop=True)\n\n    return preds",
        "def get_preds_and_actuals(\n    df,\n    projects,\n    ts_settings,\n    n_models=1,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Get row-level predictions and merge onto actuals\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    n_models: int\n        Number of top models to return\n    data_subset: str (optional)\n        Can be set to either allBacktests or holdout\n    include_blenders: boolean (optional)\n        Controls whether to include ensemble models\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    preds = get_or_request_training_predictions_from_projects(\n        projects,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    preds['timestamp'] = pd.to_datetime(preds['timestamp'].apply(lambda x: x[:-8]))\n    df = df.merge(\n        preds,\n        how='left',\n        left_on=[ts_settings['date_col'], ts_settings['series_id']],\n        right_on=['timestamp', 'series_id'],\n        validate='one_to_many',\n    )\n    df = df.loc[~np.isnan(df['prediction']), :].reset_index(drop=True)\n    return df",
        "def get_cluster_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get cluster-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    print('Getting cluster accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby(['Cluster'])\n        .apply(lambda x: acc_calc(x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n    groups.columns = ['Cluster', f'Cluster_{acc_calc.__name__.upper()}']\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(\n        act=df[ts_settings['target']], pred=df['prediction']\n    )\n\n    return groups",
        "def plot_cluster_acc(cluster_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse):\n    \"\"\"\n    Plots cluster-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_cluster_acc()\n    ts_settings: dict\n        Pparameters for time series project\n    data_subset: str\n        Choose either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    cluster_acc['Label'] = '=' + cluster_acc['Cluster']\n\n    fig = px.bar(cluster_acc, x='Label', y=f'Cluster_{acc_calc.__name__.upper()}').for_each_trace(\n        lambda t: t.update(name=t.name.replace('=', ''))\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=cluster_acc['Label'],\n            y=cluster_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Cluster Accuracy - {data_subset}')\n    fig.show()",
        "def get_series_acc(\n    df,\n    projects,\n    ts_settings,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n    acc_calc=rmse,\n):\n    \"\"\"\n    Get series-level and overall accuracy across multiple DataRobot projects\n    df: pandas df\n    projects: list\n        DataRobot project object(s)\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Valid values are either holdout or allBacktests\n    include_backtests: boolean (optional)\n        Controls whether blender models are considered\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    acc_calc: function\n        Function to calculate row-level prediction accuracy. Choose from mae, rmse, mape, smape, gamma, poission, and tweedie\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    series_id = ts_settings['series_id']\n    target = ts_settings['target']\n\n    print('Getting series accuracy...')\n\n    df = get_preds_and_actuals(\n        df,\n        projects,\n        ts_settings,\n        n_models=1,\n        data_subset=data_subset,\n        include_blenders=include_blenders,\n        metric=metric,\n    )\n    df = get_project_info(df)\n\n    groups = (\n        df.groupby([series_id]).apply(lambda x: acc_calc(x[target], x['prediction'])).reset_index()\n    )\n    groups.columns = [series_id, f'Series_{acc_calc.__name__.upper()}']\n    right = df[[series_id, 'Cluster']].drop_duplicates().reset_index(drop=True)\n    groups = groups.merge(right, how='left', on=series_id)\n    groups[f'Total_{acc_calc.__name__.upper()}'] = acc_calc(act=df[target], pred=df['prediction'])\n\n    return groups",
        "def plot_series_acc(series_acc, ts_settings, data_subset='allBacktests', acc_calc=rmse, n=50):\n    \"\"\"\n    Plots series-level and overall accuracy across multiple DataRobot projects\n    cluster_acc: pandas df\n        Output from get_series_acc()\n    ts_settings: dict\n        Parameters for time series project\n    data_subset: str\n        Choose from either holdout or allBacktests\n    metric: str (optional)\n        Project metric used to sort the DataRobot leaderboard\n        Choose from list of 'MASE', 'RMSE', 'MAPE', 'SMAPE', 'MAE', 'R Squared', 'Gamma Deviance',\n                            'SMAPE', 'Tweedie Deviance', 'Poisson Deviance', or 'RMSLE'\n    Returns:\n    --------\n    Plotly barplot\n    \"\"\"\n    n_series = len(series_acc[ts_settings['series_id']].unique())\n    n = min(n_series, n)\n\n    series_acc.sort_values(by=f'Series_{acc_calc.__name__.upper()}', ascending=False, inplace=True)\n\n    series_acc = series_acc[0:n]\n\n    fig = px.bar(\n        series_acc,\n        x=ts_settings['series_id'],\n        y=f'Series_{acc_calc.__name__.upper()}',\n        color='Cluster',\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.add_trace(\n        go.Scatter(\n            x=series_acc[ts_settings['series_id']],\n            y=series_acc[f'Total_{acc_calc.__name__.upper()}'],\n            mode='lines',\n            marker=dict(color='black'),\n            name=f'Overall {acc_calc.__name__.upper()}',\n        )\n    )\n\n    fig.update_yaxes(title=acc_calc.__name__.upper())\n    fig.update_xaxes(tickangle=45)\n    fig.update_layout(title_text=f'Series Accuracy - {data_subset}')\n    fig.show()",
        "def get_project_info(df):\n    \"\"\"\n    Parse project name to get FD, FDW, and Cluster information\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = df.copy()\n    try:\n        df['Cluster'] = df['Project_Name'].apply(lambda x: x.split('_Cluster-')[1])\n    except:\n        df['Cluster'] = 'all_series'\n\n    df['FD'] = df['Project_Name'].apply(lambda x: x.split('_FD:')[1].split('_FDW:')[0])\n    df['FDW'] = df['Project_Name'].apply(lambda x: x.split('_FDW:')[1].split('_Cluster-')[0])\n\n    return df",
        "def filter_best_fdw_scores(scores, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset df to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    pandas df\n    \"\"\"\n    df = get_project_info(scores)\n    df['_tmp'] = df[col_error].apply(lambda x: np.nanmean(np.array(x, dtype=np.float32)))\n    idx = df.groupby(['Cluster', 'FD']).apply(lambda x: x['_tmp'].idxmin()).values\n    return scores.iloc[idx, :]",
        "def filter_best_fdw_projects(scores, projects, col_error='All_Backtests_RMSE'):\n    \"\"\"\n    Subset list to projects with the best error metric for each FD and Cluster pair\n    scores: pandas df\n        Output from get_or_request_backtest_scores()\n    projects: list\n        DataRobot projects object(s)\n    col_error: str\n        Column name from scores df\n    Returns:\n    --------\n    list\n    \"\"\"\n    df = filter_best_fdw_scores(scores, col_error)\n    return [p for p in projects if p.project_name in df['Project_Name'].unique()]",
        "def plot_fd_accuracy(df, projects, ts_settings, data_subset='allBacktests', metric='RMSE'):\n    assert isinstance(projects, list), 'Projects must be a list object'\n    assert data_subset in [\n        'allBacktests',\n        'holdout',\n    ], 'data_subset must be either allBacktests or holdout'\n\n    mapper = {\n        'MAE': mae,\n        'RMSE': rmse,\n        'Gamma Deviance': gamma_loss,\n        'Tweedie Deviance': tweedie_loss,\n        'Poisson Deviance': poisson_loss,\n    }\n\n    df = get_preds_and_actuals(\n        df, projects, ts_settings, n_models=1, data_subset=data_subset, metric=metric\n    )\n    df = (\n        df.groupby(['Project_Name', 'forecast_distance'])\n        .apply(lambda x: mapper[metric](x[ts_settings['target']], x['prediction']))\n        .reset_index()\n    )\n\n    df.columns = ['Project_Name', 'forecast_distance', mapper[metric].__name__.upper()]\n    fig = px.line(\n        df, x='forecast_distance', y=mapper[metric].__name__.upper(), color='Project_Name'\n    ).for_each_trace(lambda t: t.update(name=t.name.replace('Project_Name=', '')))\n\n    fig.update_layout(title_text='Forecasting Accuracy per Forecast Distance')\n    fig.update_yaxes(title=mapper[metric].__name__.upper())\n    fig.update_xaxes(title='Forecast Distance')\n    fig.show()",
        "def plot_fd_accuracy_by_cluster(\n    df, scores, projects, ts_settings, data_subset='holdout', metric='RMSE', split_col='Cluster'\n):\n    scores = get_project_info(scores)\n\n    for c in scores[split_col].unique():\n        project_names = list(\n            scores.loc[scores[split_col] == c, 'Project_Name'].reset_index(drop=True)\n        )\n        projects_by_cluster = [p for p in projects if p.project_name in project_names]\n        plot_fd_accuracy(df, projects_by_cluster, ts_settings, data_subset, metric)",
        "def get_reduced_features_featurelist(project, model, threshold=0.99):\n    \"\"\"\n    Helper function for train_reduced_features_models()\n    project: DataRobot project object\n    model: DataRobot model object\n    threshold: np.float\n    Returns:\n    --------\n    DataRobot featurelist\n    \"\"\"\n    print(\n        f'Collecting Feature Impact for M{model.model_number} in project {project.project_name}...'\n    )\n\n    impact = pd.DataFrame.from_records(model.get_or_request_feature_impact())\n    impact['impactUnnormalized'] = np.where(\n        impact['impactUnnormalized'] < 0, 0, impact['impactUnnormalized']\n    )\n    impact['cumulative_impact'] = (\n        impact['impactUnnormalized'].cumsum() / impact['impactUnnormalized'].sum()\n    )\n\n    to_keep = np.where(impact['cumulative_impact'] <= threshold)[0]\n    if len(to_keep) < 1:\n        print('Applying this threshold would result in a featurelist with no features')\n        return None\n\n    idx = np.max(to_keep)\n\n    selected_features = impact.loc[0:idx, 'featureName'].to_list()\n    feature_list = project.create_modeling_featurelist(\n        f'Top {len(selected_features)} features M{model.model_number}', selected_features\n    )\n\n    return feature_list",
        "def train_reduced_features_models(\n    projects,\n    n_models=1,\n    threshold=0.99,\n    data_subset='allBacktests',\n    include_blenders=True,\n    metric=None,\n):\n    \"\"\"\n    Retrain top models with reduced feature featurelists\n    projects: list\n        DataRobot project objects(s)\n    n_models: int\n        Number of models to retrain with reduced feature featurelists\n    threshold: np.float\n        Controls the number of features to keep in the reduced feature list. Percentage of cumulative feature impact\n    data_subset: str\n        Choose from either holdout or allBacktests\n    \"\"\"\n    assert isinstance(projects, list), 'Projects must be a list object'\n\n    for p in projects:\n        models = get_top_models_from_project(p, n_models, data_subset, include_blenders, metric)\n\n        for m in models:\n            try:\n                feature_list = get_reduced_features_featurelist(p, m, threshold)\n                if feature_list is None:\n                    continue\n                try:\n                    m.retrain(featurelist_id=feature_list.id)\n                    print(f'Training {m.model_type} on Featurelist {feature_list.name}')\n                except dr.errors.ClientError as e:\n                    print(e)\n            except dr.errors.ClientError as e:\n                print(e)"
      ],
      "api_methods": [
        "model.request_training_predictions",
        "dr.trainingpredictions.get",
        "model.id",
        "project.project_name",
        "model.get_or_request_feature_impact",
        "project.get_datetime_models",
        "project.create_modeling_featurelist",
        "project.id",
        "model.project_id",
        "dr.project.get",
        "project.get",
        "project.metric",
        "dr.errors.clienterror",
        "project.unlock_holdout",
        "model.model_number",
        "dr.trainingpredictions.list"
      ],
      "complexity_score": 1.0,
      "use_case_category": "time_series"
    },
    {
      "id": "github_python_-4399322562483198489",
      "title": "quickstart.py",
      "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# type: ignore\nimport argparse\nimport json\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nif sys.version_info[0] < 3 or (sys.version_info[0] >= 3 and sys.version_info[1] < 9):\n    print(\"Must be using Python version 3.9 or higher\")\n    exit(1)\n\nwork_dir = Path(os.path.dirname(__file__))\ndot_env_file = Path(work_dir / \".env\")\nvenv_dir = work_dir / \".venv\"\n\n\ndef is_datarobot_codespace():\n    return os.getenv(\"DATAROBOT_NOTEBOOK_IMAGE\") is not None\n\n\ndef check_pulumi_installed():\n    try:\n        subprocess.check_call(\n            [\"pulumi\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Is pulumi installed? If not, please go to `https://www.pulumi.com/docs/iac/download-install/`\"\n        )\n        exit(1)\n\n\ndef check_pulumi_login():\n    try:\n        subprocess.check_call(\n            [\"pulumi\", \"whoami\", \"-j\", \"--non-interactive\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT,\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Please login to pulumi and rerun. Use `pulumi login --local` to log in locally or `pulumi login` to login to pulumi cloud with an API token\"\n        )\n        exit(1)\n\n\ndef check_dotenv_exists():\n    if not dot_env_file.exists():\n        print(\n            \"Could not find `.env`. Please rename the file `.env.template` and fill in your details\"\n        )\n        exit(1)\n\n\ndef is_windows():\n    return os.name == \"nt\"\n\n\ndef get_activate_command():\n    if is_datarobot_codespace():\n        return []\n    if is_conda_environment():\n        if is_windows():\n            activate_cmd = [\"conda\", \"activate\", f\"{venv_dir}\", \"&&\"]\n        else:\n            # see https://github.com/conda/conda/issues/7980\n            activate_cmd = [\n                \"eval\",\n                '\"$(conda shell.bash hook)\"',\n                \"&&\",\n                \"conda\",\n                \"activate\",\n                f\"{str(venv_dir)}\",\n                \"&&\",\n            ]\n\n    else:\n        # Regular venv activation\n        if is_windows():\n            activate_script = str(venv_dir / \"Scripts\" / \"activate.bat\")\n            activate_cmd = [\"call\", f\"{activate_script}\", \"&&\"]\n        else:\n            activate_script = str(venv_dir / \"bin\" / \"activate\")\n            activate_cmd = [\n                \"source\",\n                f\"{activate_script}\",\n                \"&&\",\n            ]\n    return activate_cmd\n\n\ndef is_conda_environment():\n    return os.environ.get(\"CONDA_DEFAULT_ENV\") is not None\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Infrastructure management script\")\n    parser.add_argument(\"stack_name\", help=\"Stack name to use\")\n    parser.add_argument(\n        \"--action\",\n        choices=[\"up\", \"destroy\"],\n        default=\"up\",\n        required=False,\n        help=\"Action to perform (up or destroy)\",\n    )\n    return parser.parse_args()\n\n\ndef get_python_executable():\n    if is_conda_environment():\n        return shutil.which(\"python\")\n    return sys.executable\n\n\ndef run_subprocess_in_venv(command: list[str]):\n    if is_windows():\n        full_cmd = get_activate_command() + command\n        # shell = True, otherwise CMD complains it can't find the file\n        result = subprocess.run(\n            \" \".join(full_cmd),\n            check=True,\n            cwd=work_dir,\n            shell=True,\n            capture_output=True,\n            text=True,\n        )\n    else:\n        full_cmd = [\"bash\", \"-c\", \" \".join(get_activate_command() + command)]\n        print(full_cmd)\n        result = subprocess.run(\n            full_cmd,\n            check=True,\n            cwd=work_dir,\n            capture_output=True,\n            text=True,\n        )\n\n    if result.returncode != 0:\n        print(f\"Error running command: {result.stderr}\")\n        raise subprocess.CalledProcessError(\n            result.returncode, result.args, result.stdout, result.stderr\n        )\n\n\ndef create_virtual_environment() -> None:\n    if not venv_dir.exists():\n        if is_conda_environment():\n            print(\"Creating conda environment...\")\n            subprocess.run(\n                [\n                    \"conda\",\n                    \"create\",\n                    \"--prefix\",\n                    str(venv_dir),\n                    \"python=3.11\",\n                    \"pip\",\n                    \"-y\",\n                ],\n                check=True,\n                cwd=work_dir,\n            )\n        else:\n            # Regular venv creation\n            python_executable = get_python_executable()\n            subprocess.run(\n                [python_executable, \"-m\", \"venv\", \".venv\"],\n                check=True,\n                cwd=work_dir,\n            )\n\n\ndef setup_virtual_environment() -> None:\n    \"\"\"Create and configure a virtual environment in a cross-platform manner.\"\"\"\n    print(\"Preparing virtual environment...\")\n\n    try:\n        # Handle activation and package installation\n\n        try:\n            run_subprocess_in_venv([\"pip\", \"install\", \"-U\", \"uv\"])\n            if is_datarobot_codespace():\n                os.system(\"uv pip install $(pip freeze | grep ipykernel)\")\n            # Install requirements using uv\n            run_subprocess_in_venv([\"uv\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n        except Exception as e:\n            print(f\"UV installation/usage failed: {e}\")\n            print(\"Falling back to pip\")\n\n            run_subprocess_in_venv([\"pip\", \"install\", \"-r\", \"requirements.txt\"])\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during virtual environment setup: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected error during virtual environment setup: {e}\")\n        raise\n\n\ndef load_dotenv():\n    with open(\".env\") as f:\n        content = f.read()\n\n    pattern = r\"\"\"\n        (?:^|\\n)         # Must start at beginning of string or after newline\n        (?![\\s#])        # Negative lookahead: next char cannot be whitespace or #\n        ([A-Za-z_]\\w*)   # Key: start with letter/underscore, then word chars\n        =\\s*             # Equals with optional whitespace after\n        (?:\n            '(.*?)'      # Single quoted (group 2)\n            |\"(.*?)\"     # Double quoted (group 3)\n            |([^\\n]+)    # Unquoted: everything until newline (group 4)\n        )\n    \"\"\"\n\n    env_vars = {}\n    for match in re.finditer(pattern, content, re.MULTILINE | re.DOTALL | re.VERBOSE):\n        key = match.group(1).strip()\n\n        if match.group(2) is not None:  # Single quoted\n            value = match.group(2).strip()\n        elif match.group(3) is not None:  # Double quoted\n            value = match.group(3).strip()\n        else:  # Unquoted\n            value = match.group(4)\n            if \" #\" in value:  # Only strip comments after space\n                value = value.split(\" #\", 1)[0]\n            value = value.strip()\n\n        env_vars[key] = value\n\n    os.environ.update(env_vars)\n    return env_vars\n\n\ndef run_pulumi_command(command: list, work_dir: Path, env_vars: dict):\n    \"\"\"Run a Pulumi command using shell activation with PTY support.\"\"\"\n    cmd_str = \" \".join(command)\n    try:\n        if is_windows():\n            os.system(f\"{' '.join(get_activate_command())}{cmd_str}\")\n        else:\n            os.system(f\"bash -c '{' '.join(get_activate_command())}{cmd_str}'\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        sys.exit(1)\n\n\ndef setup_pulumi_config(work_dir: Path, stack_name: str, env_vars: dict):\n    stack_select = [\"pulumi\", \"stack\", \"select\", stack_name, \"--create\"]\n\n    # Run commands\n    run_pulumi_command(stack_select, work_dir, env_vars)\n\n\ndef print_app_url():\n    try:\n        pulumi_output = subprocess.check_output([\"pulumi\", \"stack\", \"output\", \"-j\"])\n        pulumi_output_dict = json.loads(pulumi_output)\n        application_id = pulumi_output_dict[\"DATAROBOT_APPLICATION_ID\"]\n        datarobot_endpoint = os.environ[\"DATAROBOT_ENDPOINT\"]\n        url = f\"{datarobot_endpoint.rstrip('/').replace('api/v2', '')}custom_applications/{application_id}/\"\n        print(\"\\n\\n\")\n        print(\"=\" * 80)\n        print(f\"\\n    Your app is ready! Application URL:\\n\\n    {url}\\n\")\n        print(\"=\" * 80)\n    except Exception as e:\n        print(e)\n\n\ndef main():\n    args = parse_args()\n    if args.stack_name == \"YOUR_PROJECT_NAME\":\n        print(\"Please use a different project name\")\n        sys.exit(0)\n    check_dotenv_exists()\n    # Load environment variables\n    env_vars = load_dotenv()\n\n    check_pulumi_installed()\n    check_pulumi_login()\n\n    # Skip venv setup in Codespaces or if explicitly requested\n    if not is_datarobot_codespace():\n        create_virtual_environment()\n    setup_virtual_environment()\n    # Setup Pulumi configuration\n    setup_pulumi_config(work_dir, args.stack_name, env_vars)\n\n    # Refresh the stack\n    print(\"\\nRefreshing stack...\")\n    run_pulumi_command(\n        [\"pulumi\", \"refresh\", \"--yes\", \"--skip-pending-creates\"], work_dir, env_vars\n    )\n\n    if args.action == \"destroy\":\n        print(\"\\nDestroying stack...\")\n        run_pulumi_command([\"pulumi\", \"destroy\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack destroy complete\")\n    else:\n        print(\"\\nCreate/update stack...\")\n        run_pulumi_command([\"pulumi\", \"up\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack update complete\")\n\n        print_app_url()\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "quickstart.py",
      "tags": [
        "datarobot"
      ],
      "metadata": {
        "repo_name": "talk-to-my-data-agent",
        "file_path": "quickstart.py",
        "size": 10067,
        "code_chunks": 17
      },
      "code_examples": [
        "def is_datarobot_codespace():\n    return os.getenv(\"DATAROBOT_NOTEBOOK_IMAGE\") is not None",
        "def check_pulumi_installed():\n    try:\n        subprocess.check_call(\n            [\"pulumi\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Is pulumi installed? If not, please go to `https://www.pulumi.com/docs/iac/download-install/`\"\n        )\n        exit(1)",
        "def check_pulumi_login():\n    try:\n        subprocess.check_call(\n            [\"pulumi\", \"whoami\", \"-j\", \"--non-interactive\"],\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT,\n        )\n    except subprocess.CalledProcessError:\n        print(\n            \"Please login to pulumi and rerun. Use `pulumi login --local` to log in locally or `pulumi login` to login to pulumi cloud with an API token\"\n        )\n        exit(1)",
        "def check_dotenv_exists():\n    if not dot_env_file.exists():\n        print(\n            \"Could not find `.env`. Please rename the file `.env.template` and fill in your details\"\n        )\n        exit(1)",
        "def is_windows():\n    return os.name == \"nt\"",
        "def get_activate_command():\n    if is_datarobot_codespace():\n        return []\n    if is_conda_environment():\n        if is_windows():\n            activate_cmd = [\"conda\", \"activate\", f\"{venv_dir}\", \"&&\"]\n        else:\n            # see https://github.com/conda/conda/issues/7980\n            activate_cmd = [\n                \"eval\",\n                '\"$(conda shell.bash hook)\"',\n                \"&&\",\n                \"conda\",\n                \"activate\",\n                f\"{str(venv_dir)}\",\n                \"&&\",\n            ]\n\n    else:\n        # Regular venv activation\n        if is_windows():\n            activate_script = str(venv_dir / \"Scripts\" / \"activate.bat\")\n            activate_cmd = [\"call\", f\"{activate_script}\", \"&&\"]\n        else:\n            activate_script = str(venv_dir / \"bin\" / \"activate\")\n            activate_cmd = [\n                \"source\",\n                f\"{activate_script}\",\n                \"&&\",\n            ]\n    return activate_cmd",
        "def is_conda_environment():\n    return os.environ.get(\"CONDA_DEFAULT_ENV\") is not None",
        "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Infrastructure management script\")\n    parser.add_argument(\"stack_name\", help=\"Stack name to use\")\n    parser.add_argument(\n        \"--action\",\n        choices=[\"up\", \"destroy\"],\n        default=\"up\",\n        required=False,\n        help=\"Action to perform (up or destroy)\",\n    )\n    return parser.parse_args()",
        "def get_python_executable():\n    if is_conda_environment():\n        return shutil.which(\"python\")\n    return sys.executable",
        "def run_subprocess_in_venv(command: list[str]):\n    if is_windows():\n        full_cmd = get_activate_command() + command\n        # shell = True, otherwise CMD complains it can't find the file\n        result = subprocess.run(\n            \" \".join(full_cmd),\n            check=True,\n            cwd=work_dir,\n            shell=True,\n            capture_output=True,\n            text=True,\n        )\n    else:\n        full_cmd = [\"bash\", \"-c\", \" \".join(get_activate_command() + command)]\n        print(full_cmd)\n        result = subprocess.run(\n            full_cmd,\n            check=True,\n            cwd=work_dir,\n            capture_output=True,\n            text=True,\n        )\n\n    if result.returncode != 0:\n        print(f\"Error running command: {result.stderr}\")\n        raise subprocess.CalledProcessError(\n            result.returncode, result.args, result.stdout, result.stderr\n        )",
        "def create_virtual_environment() -> None:\n    if not venv_dir.exists():\n        if is_conda_environment():\n            print(\"Creating conda environment...\")\n            subprocess.run(\n                [\n                    \"conda\",\n                    \"create\",\n                    \"--prefix\",\n                    str(venv_dir),\n                    \"python=3.11\",\n                    \"pip\",\n                    \"-y\",\n                ],\n                check=True,\n                cwd=work_dir,\n            )\n        else:\n            # Regular venv creation\n            python_executable = get_python_executable()\n            subprocess.run(\n                [python_executable, \"-m\", \"venv\", \".venv\"],\n                check=True,\n                cwd=work_dir,\n            )",
        "def setup_virtual_environment() -> None:\n    \"\"\"Create and configure a virtual environment in a cross-platform manner.\"\"\"\n    print(\"Preparing virtual environment...\")\n\n    try:\n        # Handle activation and package installation\n\n        try:\n            run_subprocess_in_venv([\"pip\", \"install\", \"-U\", \"uv\"])\n            if is_datarobot_codespace():\n                os.system(\"uv pip install $(pip freeze | grep ipykernel)\")\n            # Install requirements using uv\n            run_subprocess_in_venv([\"uv\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n        except Exception as e:\n            print(f\"UV installation/usage failed: {e}\")\n            print(\"Falling back to pip\")\n\n            run_subprocess_in_venv([\"pip\", \"install\", \"-r\", \"requirements.txt\"])\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during virtual environment setup: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected error during virtual environment setup: {e}\")\n        raise",
        "def load_dotenv():\n    with open(\".env\") as f:\n        content = f.read()\n\n    pattern = r\"\"\"\n        (?:^|\\n)         # Must start at beginning of string or after newline\n        (?![\\s#])        # Negative lookahead: next char cannot be whitespace or #\n        ([A-Za-z_]\\w*)   # Key: start with letter/underscore, then word chars\n        =\\s*             # Equals with optional whitespace after\n        (?:\n            '(.*?)'      # Single quoted (group 2)\n            |\"(.*?)\"     # Double quoted (group 3)\n            |([^\\n]+)    # Unquoted: everything until newline (group 4)\n        )\n    \"\"\"\n\n    env_vars = {}\n    for match in re.finditer(pattern, content, re.MULTILINE | re.DOTALL | re.VERBOSE):\n        key = match.group(1).strip()\n\n        if match.group(2) is not None:  # Single quoted\n            value = match.group(2).strip()\n        elif match.group(3) is not None:  # Double quoted\n            value = match.group(3).strip()\n        else:  # Unquoted\n            value = match.group(4)\n            if \" #\" in value:  # Only strip comments after space\n                value = value.split(\" #\", 1)[0]\n            value = value.strip()\n\n        env_vars[key] = value\n\n    os.environ.update(env_vars)\n    return env_vars",
        "def run_pulumi_command(command: list, work_dir: Path, env_vars: dict):\n    \"\"\"Run a Pulumi command using shell activation with PTY support.\"\"\"\n    cmd_str = \" \".join(command)\n    try:\n        if is_windows():\n            os.system(f\"{' '.join(get_activate_command())}{cmd_str}\")\n        else:\n            os.system(f\"bash -c '{' '.join(get_activate_command())}{cmd_str}'\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        sys.exit(1)",
        "def setup_pulumi_config(work_dir: Path, stack_name: str, env_vars: dict):\n    stack_select = [\"pulumi\", \"stack\", \"select\", stack_name, \"--create\"]\n\n    # Run commands\n    run_pulumi_command(stack_select, work_dir, env_vars)",
        "def print_app_url():\n    try:\n        pulumi_output = subprocess.check_output([\"pulumi\", \"stack\", \"output\", \"-j\"])\n        pulumi_output_dict = json.loads(pulumi_output)\n        application_id = pulumi_output_dict[\"DATAROBOT_APPLICATION_ID\"]\n        datarobot_endpoint = os.environ[\"DATAROBOT_ENDPOINT\"]\n        url = f\"{datarobot_endpoint.rstrip('/').replace('api/v2', '')}custom_applications/{application_id}/\"\n        print(\"\\n\\n\")\n        print(\"=\" * 80)\n        print(f\"\\n    Your app is ready! Application URL:\\n\\n    {url}\\n\")\n        print(\"=\" * 80)\n    except Exception as e:\n        print(e)",
        "def main():\n    args = parse_args()\n    if args.stack_name == \"YOUR_PROJECT_NAME\":\n        print(\"Please use a different project name\")\n        sys.exit(0)\n    check_dotenv_exists()\n    # Load environment variables\n    env_vars = load_dotenv()\n\n    check_pulumi_installed()\n    check_pulumi_login()\n\n    # Skip venv setup in Codespaces or if explicitly requested\n    if not is_datarobot_codespace():\n        create_virtual_environment()\n    setup_virtual_environment()\n    # Setup Pulumi configuration\n    setup_pulumi_config(work_dir, args.stack_name, env_vars)\n\n    # Refresh the stack\n    print(\"\\nRefreshing stack...\")\n    run_pulumi_command(\n        [\"pulumi\", \"refresh\", \"--yes\", \"--skip-pending-creates\"], work_dir, env_vars\n    )\n\n    if args.action == \"destroy\":\n        print(\"\\nDestroying stack...\")\n        run_pulumi_command([\"pulumi\", \"destroy\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack destroy complete\")\n    else:\n        print(\"\\nCreate/update stack...\")\n        run_pulumi_command([\"pulumi\", \"up\", \"--yes\"], work_dir, env_vars)\n        print(\"Stack update complete\")\n\n        print_app_url()"
      ],
      "api_methods": [],
      "complexity_score": 0.8,
      "use_case_category": "general"
    },
    {
      "id": "github_python_-2624411937587284335",
      "title": "api.py",
      "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import annotations\n\nimport ast\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport tempfile\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom types import ModuleType, TracebackType\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Type,\n    TypeVar,\n    cast,\n)\n\nimport datarobot as dr\nimport instructor\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport polars as pl\nimport psutil\nimport scipy\nimport sklearn\nimport statsmodels as sm\nfrom datarobot.client import RESTClientObject\nfrom joblib import Memory\nfrom openai import AsyncOpenAI\nfrom openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\nfrom openai.types.chat.chat_completion_system_message_param import (\n    ChatCompletionSystemMessageParam,\n)\nfrom openai.types.chat.chat_completion_user_message_param import (\n    ChatCompletionUserMessageParam,\n)\nfrom plotly.subplots import make_subplots\nfrom pydantic import ValidationError\n\nsys.path.append(os.path.dirname(os.path.realpath(__file__)))\nfrom utils import prompts, tools\nfrom utils.analyst_db import AnalystDB, DataSourceType\nfrom utils.code_execution import (\n    InvalidGeneratedCode,\n    MaxReflectionAttempts,\n    execute_python,\n    reflect_code_generation_errors,\n)\nfrom utils.data_cleansing_helpers import (\n    add_summary_statistics,\n    process_column,\n)\nfrom utils.database_helpers import get_external_database\nfrom utils.logging_helper import get_logger, log_api_call\nfrom utils.resources import LLMDeployment\nfrom utils.schema import (\n    AnalysisError,\n    AnalystChatMessage,\n    AnalystDataset,\n    BusinessAnalysisGeneration,\n    ChartGenerationExecutionResult,\n    ChatRequest,\n    CleansedDataset,\n    CodeGeneration,\n    Component,\n    DatabaseAnalysisCodeGeneration,\n    DataDictionary,\n    DataDictionaryColumn,\n    DataRegistryDataset,\n    DictionaryGeneration,\n    DownloadedRegistryDataset,\n    EnhancedQuestionGeneration,\n    GetBusinessAnalysisMetadata,\n    GetBusinessAnalysisRequest,\n    GetBusinessAnalysisResult,\n    QuestionListGeneration,\n    RunAnalysisRequest,\n    RunAnalysisResult,\n    RunAnalysisResultMetadata,\n    RunChartsRequest,\n    RunChartsResult,\n    RunDatabaseAnalysisRequest,\n    RunDatabaseAnalysisResult,\n    RunDatabaseAnalysisResultMetadata,\n    Tool,\n    ValidatedQuestion,\n)\n\nlogger = get_logger()\nlogging.getLogger(\"openai\").setLevel(logging.WARNING)\nlogging.getLogger(\"openai.http_client\").setLevel(logging.WARNING)\n\nVALUE_ERROR_MESSAGE = \"Input data cannot be empty (no dataset provided)\"\nDEFAULT_LLM_GATEWAY_MODEL = \"azure/gpt-4o\"\nDEFAULT_LLM_GATEWAY_MODEL_SMALL = \"azure/gpt-4o-mini\"\n\n\ndef log_memory() -> None:\n    process = psutil.Process()\n    memory = process.memory_info().rss / 1024 / 1024  # MB\n    logger.info(f\"Memory usage: {memory:.2f} MB\")\n\n\n@functools.cache\ndef initialize_deployment() -> tuple[RESTClientObject, str]:\n    \"\"\"Initialize either LLM Gateway or DataRobot-hosted LLM deployment based on environment settings and credential priority.\"\"\"\n    try:\n        dr_client = dr.Client()\n        chat_agent_deployment_id = LLMDeployment().id\n        if chat_agent_deployment_id is None:\n            raise ValueError(\n                \"LLM Deployment ID is required but not found. Please check your infrastructure setup.\"\n            )\n        deployment_chat_base_url = (\n            f\"{dr_client.endpoint.rstrip('/')}/deployments/{chat_agent_deployment_id}/\"\n        )\n        logger.info(\n            f\"Using the DataRobot-hosted LLM deployment (configured at infrastructure time) at: {deployment_chat_base_url}\"\n        )\n        return dr_client, deployment_chat_base_url\n\n    except ValidationError as e:\n        raise ValueError(\n            \"Unable to load Deployment ID.\"\n            \"If running locally, verify you have selected the correct \"\n            \"stack and that it is active using `pulumi stack output`. \"\n            \"If running in DataRobot, verify your runtime parameters have been set correctly.\"\n        ) from e\n\n\nclass AsyncLLMClient:\n    async def __aenter__(self) -> instructor.AsyncInstructor:\n        dr_client, deployment_base_url = initialize_deployment()\n        self.openai_client = AsyncOpenAI(\n            api_key=dr_client.token,\n            base_url=deployment_base_url,\n            timeout=90,\n            max_retries=2,\n        )\n        self.client = instructor.from_openai(\n            self.openai_client, mode=instructor.Mode.MD_JSON\n        )\n        return self.client\n\n    async def __aexit__(\n        self,\n        exc_type: Type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.openai_client.close()  # Properly close the client\n\n\nALTERNATIVE_LLM_BIG = \"datarobot-deployed-llm\"\nALTERNATIVE_LLM_SMALL = \"datarobot-deployed-llm\"\nDICTIONARY_BATCH_SIZE = 10\nMAX_REGISTRY_DATASET_SIZE = 400e6  # aligns to 400MB set in streamlit config.toml\nDISK_CACHE_LIMIT_BYTES = 512e6\nDICTIONARY_PARALLEL_BATCH_SIZE = 2\nDICTIONARY_TIMEOUT = 45.0\n\n_memory = Memory(tempfile.gettempdir(), verbose=0)\n_memory.clear(warn=False)  # clear cache on startup\n\nT = TypeVar(\"T\")\n\n\ndef cache(f: T) -> T:\n    \"\"\"Cache function and coroutine results to disk using joblib.\"\"\"\n    cached_f = _memory.cache(f)\n\n    if asyncio.iscoroutinefunction(f):\n\n        async def awrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = await cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"\n                )\n            return result\n\n        return cast(T, awrapper)\n    else:\n\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"  # type: ignore[attr-defined]\n                )\n            return result\n\n        return cast(T, wrapper)\n\n\n# This can be large as we are not storing the actual datasets in memory, just metadata\ndef list_registry_datasets(limit: int = 100) -> list[DataRegistryDataset]:\n    \"\"\"\n    Fetch datasets from Data Registry with specified limit\n\n    Args:\n        limit: int\n        Datasets to retrieve. Max value: 100\n    \"\"\"\n\n    url = f\"datasets?limit={limit}\"\n\n    # Get all datasets and manually limit the results\n    datasets = dr.client.get_client().get(url).json()[\"data\"]\n\n    return [\n        DataRegistryDataset(\n            id=ds[\"datasetId\"],\n            name=ds[\"name\"],\n            created=(\n                ds[\"creationDate\"][:10] if \"creationDate\" in ds else \"N/A\"  # %Y-%m-%d\n            ),\n            size=(\n                f\"{ds['datasetSize'] / (1024 * 1024):.1f} MB\"\n                if \"datasetSize\" in ds\n                else \"N/A\"\n            ),\n        )\n        for ds in datasets\n    ]\n\n\nasync def download_registry_datasets(\n    dataset_ids: list[str], analyst_db: AnalystDB\n) -> list[DownloadedRegistryDataset]:\n    \"\"\"Load selected datasets as pandas DataFrames\n\n    Args:\n        *args: list of dataset IDs to download\n\n    Returns:\n        list[AnalystDataset]: Dictionary of dataset names and data\n    \"\"\"\n    downloaded_datasets = []\n    datasets = [dr.Dataset.get(id_) for id_ in dataset_ids]\n    if (\n        sum([ds.size for ds in datasets if ds.size is not None])\n        > MAX_REGISTRY_DATASET_SIZE\n    ):\n        raise ValueError(\n            f\"The requested Data Registry datasets must total <= {int(MAX_REGISTRY_DATASET_SIZE)} bytes\"\n        )\n\n    result_datasets: list[AnalystDataset] = []\n    for dataset in datasets:\n        try:\n            df = dataset.get_as_dataframe()\n            result_datasets.append(AnalystDataset(name=dataset.name, data=df))\n            logger.info(f\"Successfully downloaded {dataset.name}\")\n        except Exception as e:\n            logger.error(f\"Failed to read dataset {dataset.name}: {str(e)}\")\n            downloaded_datasets.append(\n                DownloadedRegistryDataset(name=dataset.name, error=str(e))\n            )\n            continue\n    for result_dataset in result_datasets:\n        await analyst_db.register_dataset(\n            result_dataset, DataSourceType.REGISTRY, dataset.size or 0\n        )\n        downloaded_datasets.append(DownloadedRegistryDataset(name=result_dataset.name))\n    return downloaded_datasets\n\n\nasync def _get_dictionary_batch(\n    columns: list[str], df: pl.DataFrame, batch_size: int = 5\n) -> list[DataDictionaryColumn]:\n    \"\"\"Process a batch of columns to get their descriptions\"\"\"\n\n    # Get sample data and stats for just these columns\n    # Convert timestamps to ISO format strings for JSON serialization\n    try:\n        logger.debug(f\"Processing batch of {len(columns)} columns\")\n        sample_data = {}\n        logger.debug(\"Converting datetime columns to ISO format\")\n        num_samples = 10\n        for col in columns:\n            if df[col].dtype.is_temporal():\n                # Convert timestamps to ISO format strings\n                sample_data[col] = (\n                    df.select(\n                        pl.col(col)\n                        .cast(pl.Datetime)\n                        .map_elements(\n                            lambda x: x.isoformat() if x is not None else None\n                        )\n                    )\n                    .head(num_samples)\n                    .to_dict()\n                )\n            else:\n                # For non-datetime columns, just take the samples as is\n                sample_data[col] = df.select(pl.col(col)).head(num_samples).to_dict()\n\n        # Handle numeric summary\n        numeric_summary = {}\n        logger.debug(\"Calculating numeric summaries\")\n        for col in columns:\n            if df[col].dtype.is_numeric():\n                desc = df[col].describe()\n                numeric_summary[col] = desc.to_dict()\n\n        # Get categories for non-numeric columns\n        categories = []\n        logger.debug(\"Getting categories for non-numeric columns\")\n        for column in columns:\n            if not df[column].dtype.is_numeric():\n                try:\n                    value_counts = (\n                        df[column].sample(n=1000, seed=42).value_counts().head(10)\n                    )\n                    # Convert any timestamp values to strings\n                    if df[column].dtype.is_temporal():\n                        value_counts[column] = value_counts[column].cast(pl.String)\n                    categories.append({column: value_counts[column].to_list()})\n                except Exception:\n                    continue\n\n        # Create messages for OpenAI\n        messages: list[ChatCompletionMessageParam] = [\n            ChatCompletionSystemMessageParam(\n                role=\"system\", content=prompts.SYSTEM_PROMPT_GET_DICTIONARY\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Data:\\n{sample_data}\\n\"\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Statistical Summary:\\n{numeric_summary}\\n\"\n            ),\n        ]\n\n        if categories:\n            messages.append(\n                ChatCompletionUserMessageParam(\n                    role=\"user\", content=f\"Categorical Values:\\n{categories}\\n\"\n                )\n            )\n        logger.debug(\n            f\"total_characters: {len(''.join([str(msg) for msg in messages]))}\"\n        )\n        # Get descriptions from OpenAI\n        async with AsyncLLMClient() as client:\n            completion: DictionaryGeneration = await client.chat.completions.create(\n                response_model=DictionaryGeneration,\n                model=ALTERNATIVE_LLM_SMALL,\n                messages=messages,\n            )\n\n        # Convert to dictionary format\n        descriptions = completion.to_dict()\n\n        # Only return descriptions for requested columns\n        return [\n            DataDictionaryColumn(\n                column=col,\n                description=descriptions.get(col, \"No description available\"),\n                data_type=str(df[col].dtype),\n            )\n            for col in columns\n        ]\n\n    except ValueError as e:\n        logger.error(f\"Invalid dictionary response: {str(e)}\")\n        return [\n            DataDictionaryColumn(\n                column=col,\n                description=\"No valid description available\",\n                data_type=str(df[col].dtype),\n            )\n            for col in columns\n        ]\n\n\n@log_api_call\nasync def get_dictionary(dataset: AnalystDataset) -> DataDictionary:\n    \"\"\"Process a single dataset with parallel column batch processing\"\"\"\n\n    try:\n        logger.info(f\"Processing dataset {dataset.name} init\")\n        # Convert JSON to DataFrame\n        df_full = dataset.to_df()\n        df = df_full.sample(n=min(10000, len(df_full)), seed=42)\n\n        # Add debug logging\n        logger.info(f\"Processing dataset {dataset.name} with shape {df.shape}\")\n\n        # Handle empty dataset\n        if df.is_empty():\n            logger.warning(f\"Dataset {dataset.name} is empty\")\n            return DataDictionary(\n                name=dataset.name,\n                column_descriptions=[],\n            )\n\n        # Split columns into batches\n        column_batches = [\n            list(df.columns[i : i + DICTIONARY_BATCH_SIZE])\n            for i in range(0, len(df.columns), DICTIONARY_BATCH_SIZE)\n        ]\n        logger.info(\n            f\"Created {len(column_batches)} batches for {len(df.columns)} columns\"\n        )\n\n        # Create a semaphore to limit concurrent tasks to 2\n        sem = asyncio.Semaphore(DICTIONARY_PARALLEL_BATCH_SIZE)\n\n        async def throttled_get_dictionary_batch(\n            batch: list[str],\n        ) -> list[DataDictionaryColumn]:\n            try:\n                async with sem:\n                    return await asyncio.wait_for(\n                        _get_dictionary_batch(batch, df, DICTIONARY_BATCH_SIZE),\n                        timeout=DICTIONARY_TIMEOUT,\n                    )\n            except asyncio.TimeoutError:\n                logger.warning(f\"Timeout processing batch: {batch}\")\n                return [\n                    DataDictionaryColumn(\n                        column=col,\n                        description=\"No Description Available\",\n                        data_type=str(df[col].dtype),\n                    )\n                    for col in batch\n                ]\n            except Exception as e:\n                logger.error(f\"Error processing batch {batch}: {str(e)}\")\n                return [\n                    DataDictionaryColumn(\n                        column=col,\n                        description=\"No Description Available\",\n                        data_type=str(df[col].dtype),\n                    )\n                    for col in batch\n                ]\n\n        tasks = [throttled_get_dictionary_batch(batch) for batch in column_batches]\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        # Filter out any exceptions and flatten results\n        dictionary: list[DataDictionaryColumn] = []\n        for result in results:\n            if isinstance(result, BaseException):\n                logger.error(f\"Task failed with error: {str(result)}\")\n                continue\n            dictionary.extend(result)\n\n        logger.info(\n            f\"Created dictionary with {len(dictionary)} entries for dataset {dataset.name}\"\n        )\n\n        return DataDictionary(\n            name=dataset.name,\n            column_descriptions=dictionary,\n        )\n\n    except Exception:\n        return DataDictionary(\n            name=dataset.name,\n            column_descriptions=[\n                DataDictionaryColumn(\n                    column=c,\n                    data_type=str(dataset.to_df()[c].dtype),\n                    description=\"No Description Available\",\n                )\n                for c in dataset.columns\n            ],\n        )\n\n\ndef _validate_question_feasibility(\n    question: str, available_columns: list[str]\n) -> ValidatedQuestion | None:\n    \"\"\"Validate if a question can be answered with available data\n\n    Checks if common data elements mentioned in the question exist in columns\n    \"\"\"\n    # Convert question and columns to lowercase for matching\n    question_lower = question.lower()\n    columns_lower = [col.lower() for col in available_columns]\n\n    # Extract potential column references from question\n    words = set(re.findall(r\"\\b\\w+\\b\", question_lower))\n\n    # Find matches and missing terms\n    found_columns = [col for col in columns_lower if any(word in col for word in words)]\n\n    is_valid = len(found_columns) > 0\n    if is_valid:\n        return ValidatedQuestion(\n            question=question,\n        )\n    return None\n\n\n@log_api_call\nasync def suggest_questions(\n    datasets: list[AnalystDataset], max_columns: int = 40\n) -> list[ValidatedQuestion]:\n    \"\"\"Generate and validate suggested analysis questions\n\n    Args:\n        dictionary: DataFrame containing data dictionary\n        max_columns: Maximum number of columns to include in prompt\n\n    Returns:\n        Dict containing:\n            - questions: list of validated question objects\n            - metadata: Dictionary of processing information\n    \"\"\"\n    # Validate input\n    dictionary = sum(\n        [\n            DataDictionary.from_analyst_df(\n                ds.to_df(),\n                column_descriptions=f\"Column from dataset {ds.name}\",\n            ).column_descriptions\n            for ds in datasets\n        ],\n        [],\n    )\n\n    if len(dictionary) < 1:\n        raise ValueError(\"Dictionary DataFrame cannot be empty\")\n\n    # Limit columns for OpenAI prompt\n    total_columns = len(dictionary)\n    if total_columns > max_columns:\n        # Take first and last 20 columns\n        half_max = max_columns // 2\n        first_half = dictionary[:half_max]\n        last_half = dictionary[-half_max:]\n\n        # Remove any duplicates\n        dictionary = first_half + last_half\n\n        # deduplicate\n        dictionary = list({item.column: item for item in dictionary}.values())\n\n    # Convert dictionary to format expected by OpenAI\n    dict_data = {\n        \"columns\": [d.column for d in dictionary],\n        \"descriptions\": [d.description for d in dictionary],\n        \"data_types\": [d.data_type for d in dictionary],\n    }\n\n    # Create OpenAI messages\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\", content=prompts.SYSTEM_PROMPT_SUGGEST_A_QUESTION\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Dictionary:\\n{json.dumps(dict_data)}\"\n        ),\n    ]\n    async with AsyncLLMClient() as client:\n        completion: QuestionListGeneration = await client.chat.completions.create(\n            response_model=QuestionListGeneration,\n            model=ALTERNATIVE_LLM_SMALL,\n            messages=messages,\n        )\n\n    available_columns = dict_data[\"columns\"]\n    validated_questions: list[ValidatedQuestion] = []\n\n    for question in completion.questions:\n        validated_question = _validate_question_feasibility(question, available_columns)\n        if validated_question is not None:\n            validated_questions.append(validated_question)\n\n    return validated_questions\n\n\ndef find_imports(module: ModuleType) -> list[str]:\n    \"\"\"\n    Get top-level third-party imports from a Python module.\n\n    Args:\n        module: Python module object to analyze\n\n    Returns:\n        list of third-party package names\n\n    Example:\n        >>> import my_module\n        >>> imports = find_third_party_imports(my_module)\n        >>> print(imports)  # ['pandas', 'numpy', 'requests']\n    \"\"\"\n    try:\n        # Get the source code of the module\n        source = inspect.getsource(module)\n        tree = ast.parse(source)\n\n        stdlib_modules = set(sys.stdlib_module_names)\n        third_party = set()\n\n        # Only look at top-level imports\n        for node in tree.body:\n            if isinstance(node, ast.Import):\n                for name in node.names:\n                    module_name = name.name.split(\".\")[0]\n                    if module_name not in stdlib_modules:\n                        third_party.add(module_name)\n\n            elif isinstance(node, ast.ImportFrom):\n                if node.module is None:\n                    continue\n                module_name = node.module.split(\".\")[0]\n                if module_name not in stdlib_modules:\n                    third_party.add(module_name)\n\n        return sorted(third_party)\n    except Exception:\n        return []\n\n\ndef get_tools() -> list[Tool]:\n    try:\n        # find all functions defined in the tools module\n        tool_functions = [func for func in dir(tools) if callable(getattr(tools, func))]\n\n        # find the function signatures and doc strings\n        tools_list = []\n        for func_name in tool_functions:\n            func = getattr(tools, func_name)\n            signature = inspect.signature(func)\n            docstring = inspect.getdoc(func)\n            tools_list.append(\n                Tool(\n                    name=func_name,\n                    signature=str(signature),\n                    docstring=docstring,\n                    function=func,\n                )\n            )\n        return tools_list\n    except Exception:\n        return []\n\n\nasync def _generate_run_charts_python_code(\n    request: RunChartsRequest,\n    validation_error: InvalidGeneratedCode | None = None,\n) -> str:\n    df = request.dataset.to_df().to_pandas()\n    question = request.question\n    dataframe_metadata = {\n        \"shape\": {\"rows\": int(df.shape[0]), \"columns\": int(df.shape[1])},\n        \"statistics\": df.describe(include=\"all\").to_dict(),\n        \"dtypes\": df.dtypes.astype(str).to_dict(),\n    }\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\",\n            content=prompts.SYSTEM_PROMPT_PLOTLY_CHART,\n        ),\n        ChatCompletionUserMessageParam(role=\"user\", content=f\"Question: {question}\"),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Metadata:\\n{dataframe_metadata}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data top 25 rows:\\n{df.head(25).to_string()}\"\n        ),\n    ]\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n\n    # Get response based on model mode\n    async with AsyncLLMClient() as client:\n        response: CodeGeneration = await client.chat.completions.create(\n            response_model=CodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0,\n            messages=messages,\n        )\n    return response.code\n\n\nasync def _generate_run_analysis_python_code(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n    validation_error: InvalidGeneratedCode | None = None,\n    attempt: int = 0,\n) -> str:\n    \"\"\"\n    Generate Python analysis code based on JSON data and question.\n\n    Parameters:\n    - request: RunAnalysisRequest containing data and question\n    - validation_errors: Past validation errors to include in prompt\n\n    Returns:\n    - Generated code\n    \"\"\"\n    # Convert dictionary data structure to list of columns for all datasets\n    logger.info(\"Starting code gen\")\n\n    all_columns = []\n    all_descriptions = []\n    all_data_types = []\n\n    dictionaries = [\n        await analyst_db.get_data_dictionary(name) for name in request.dataset_names\n    ]\n    for dictionary in dictionaries:\n        if dictionary is None:\n            continue\n        for entry in dictionary.column_descriptions:\n            all_columns.append(f\"{dictionary.name}.{entry.column}\")\n            all_descriptions.append(entry.description)\n            all_data_types.append(entry.data_type)\n\n    # Create dictionary format for prompt\n    dictionary_data = {\n        \"columns\": all_columns,\n        \"descriptions\": all_descriptions,\n        \"data_types\": all_data_types,\n    }\n\n    # Get sample data and shape info for all datasets\n    all_samples = []\n    all_shapes = []\n\n    logger.debug(f\"datasets: {request.dataset_names}\")\n    for dataset_name in request.dataset_names:\n        try:\n            dataset = (await analyst_db.get_cleansed_dataset(dataset_name)).to_df()\n        except Exception:\n            dataset = (await analyst_db.get_dataset(dataset_name)).to_df()\n        all_shapes.append(\n            f\"{dataset_name}: {dataset.shape[0]} rows x {dataset.shape[1]} columns\"\n        )\n        # Limit sample to 10 rows\n        sample_df = dataset.head(10)\n        all_samples.append(f\"{dataset_name}:\\n{sample_df}\")\n\n    shape_info = \"\\n\".join(all_shapes)\n    sample_data = \"\\n\\n\".join(all_samples)\n    logger.debug(\"Assembling messages\")\n    # Create messages for OpenAI\n    messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            role=\"system\", content=prompts.SYSTEM_PROMPT_PYTHON_ANALYST\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Business Question: {request.question}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Data Shapes:\\n{shape_info}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=f\"Sample Data:\\n{sample_data}\"\n        ),\n        ChatCompletionUserMessageParam(\n            role=\"user\",\n            content=f\"Data Dictionary:\\n{json.dumps(dictionary_data)}\",\n        ),\n    ]\n\n    tools_list = get_tools()\n    if len(tools_list) > 0:\n        messages.append(\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=\"If it helps the analysis, you can optionally use following functions:\\n\"\n                + \"\\n\".join([str(t) for t in tools_list]),\n            )\n        )\n\n    logger.debug(f\"total_characters: {len(''.join([str(msg) for msg in messages]))}\")\n    # Add error context if available\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n        if attempt > 2:\n            messages.append(\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Convert the dataframe to pandas!\",\n                )\n            )\n    logger.info(\"Running Code Gen\")\n    logger.debug(messages)\n    async with AsyncLLMClient() as client:\n        completion: CodeGeneration = await client.chat.completions.create(\n            response_model=CodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0.1,\n            messages=messages,\n            max_retries=10,\n        )\n    logger.info(\"Code Gen complete\")\n    return completion.code\n\n\nasync def cleanse_dataframe(dataset: AnalystDataset) -> CleansedDataset:\n    \"\"\"Clean and standardize multiple pandas DataFrames in parallel.\n\n    Args:\n        datasets: List of AnalystDataset objects to clean\n    Returns:\n        List of CleansedDataset objects containing cleaned data and reports\n    Raises:\n        ValueError: If a dataset is empty\n    \"\"\"\n\n    if dataset.to_df().is_empty():\n        raise ValueError(f\"Dataset {dataset.name} is empty\")\n\n    df = dataset.to_df()\n    sample_df = df.sample(min(100, len(df)))\n\n    results = []\n    for col in df.columns:\n        results.append(process_column(df, col, sample_df))\n\n    # Create new DataFrame from processed columns\n    new_columns = {}\n    reports = []\n\n    for new_name, series, report in results:\n        new_columns[new_name] = series\n        reports.append(report)\n\n    cleaned_df = pl.DataFrame(new_columns)\n    add_summary_statistics(cleaned_df, reports)\n\n    return CleansedDataset(\n        dataset=AnalystDataset(\n            name=dataset.name,\n            data=cleaned_df,\n        ),\n        cleaning_report=reports,\n    )\n\n\n@log_api_call\nasync def rephrase_message(messages: ChatRequest) -> str:\n    \"\"\"Process chat messages history and return a new question\n\n    Args:\n        messages: list of message dictionaries with 'role' and 'content' fields\n\n    Returns:\n        Dict[str, str]: Dictionary containing response content\n    \"\"\"\n    # Convert messages to string format for prompt\n    messages_str = \"\\n\".join(\n        [f\"{msg['role']}: {msg['content']}\" for msg in messages.messages]\n    )\n\n    prompt_messages: list[ChatCompletionMessageParam] = [\n        ChatCompletionSystemMessageParam(\n            content=prompts.SYSTEM_PROMPT_REPHRASE_MESSAGE,\n            role=\"system\",\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Message History:\\n{messages_str}\",\n            role=\"user\",\n        ),\n    ]\n    async with AsyncLLMClient() as client:\n        completion: EnhancedQuestionGeneration = await client.chat.completions.create(\n            response_model=EnhancedQuestionGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            messages=prompt_messages,\n        )\n\n    return completion.enhanced_user_message\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_charts(\n    request: RunChartsRequest,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunChartsResult:\n    \"\"\"Generate and validate chart code with retry logic\"\"\"\n    # Create messages for OpenAI\n    start_time = datetime.now()\n\n    if not request.dataset:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    df = request.dataset.to_df().to_pandas()\n    if exception_history is None:\n        exception_history = []\n\n    code = await _generate_run_charts_python_code(\n        request, next(iter(exception_history[::-1]), None)\n    )\n    try:\n        result = execute_python(\n            modules={\n                \"pd\": pd,\n                \"np\": np,\n                \"go\": go,\n                \"pl\": pl,\n                \"scipy\": scipy,\n            },\n            functions={\n                \"make_subplots\": make_subplots,\n            },\n            expected_function=\"create_charts\",\n            code=code,\n            input_data=df,\n            output_type=ChartGenerationExecutionResult,\n            allowed_modules={\n                \"pandas\",\n                \"numpy\",\n                \"plotly\",\n                \"scipy\",\n                \"datetime\",\n                \"polars\",\n            },\n        )\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=code, exception=e)\n\n    duration = datetime.now() - start_time\n\n    return RunChartsResult(\n        status=\"success\",\n        code=code,\n        fig1_json=result.fig1.to_json(),\n        fig2_json=result.fig2.to_json(),\n        metadata=RunAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history) + 1,\n        ),\n    )\n\n\n@log_api_call\nasync def run_charts(request: RunChartsRequest) -> RunChartsResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    try:\n        chart_result = await _run_charts(request)\n        return chart_result\n    except ValidationError:\n        return RunChartsResult(\n            status=\"error\", metadata=RunAnalysisResultMetadata(duration=0, attempts=1)\n        )\n    except MaxReflectionAttempts as e:\n        return RunChartsResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n\n\n@log_api_call\nasync def get_business_analysis(\n    request: GetBusinessAnalysisRequest,\n) -> GetBusinessAnalysisResult:\n    \"\"\"\n    Generate business analysis based on data and question.\n\n    Parameters:\n    - request: BusinessAnalysisRequest containing data and question\n\n    Returns:\n    - Dictionary containing analysis components\n    \"\"\"\n    try:\n        # Convert JSON data to DataFrame for analysis\n        start = datetime.now()\n\n        df = request.dataset.to_df().to_pandas()\n\n        # Get first 1000 rows as CSV with quoted values for context\n        df_csv = df.head(750).to_csv(index=False, quoting=1)\n\n        # Create messages for OpenAI\n        messages: list[ChatCompletionMessageParam] = [\n            ChatCompletionSystemMessageParam(\n                role=\"system\", content=prompts.SYSTEM_PROMPT_BUSINESS_ANALYSIS\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=f\"Business Question: {request.question}\",\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\", content=f\"Analyzed Data:\\n{df_csv}\"\n            ),\n            ChatCompletionUserMessageParam(\n                role=\"user\",\n                content=f\"Data Dictionary:\\n{request.dictionary.model_dump_json()}\",\n            ),\n        ]\n        async with AsyncLLMClient() as client:\n            completion: BusinessAnalysisGeneration = (\n                await client.chat.completions.create(\n                    response_model=BusinessAnalysisGeneration,\n                    model=ALTERNATIVE_LLM_BIG,\n                    temperature=0.1,\n                    messages=messages,\n                )\n            )\n        duration = (datetime.now() - start).total_seconds()\n        # Ensure all response fields are present\n        metadata = GetBusinessAnalysisMetadata(\n            duration=duration,\n            question=request.question,\n            rows_analyzed=len(df),\n            columns_analyzed=len(df.columns),\n        )\n        return GetBusinessAnalysisResult(\n            status=\"success\",\n            **completion.model_dump(),\n            metadata=metadata,\n        )\n\n    except Exception as e:\n        msg = type(e).__name__ + f\": {str(e)}\"\n        logger.error(f\"Error in get_business_analysis: {msg}\")\n        return GetBusinessAnalysisResult(\n            status=\"error\",\n            metadata=GetBusinessAnalysisMetadata(exception_str=msg),\n            additional_insights=\"\",\n            follow_up_questions=[],\n            bottom_line=\"\",\n        )\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_analysis(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunAnalysisResult:\n    start_time = datetime.now()\n\n    if not request.dataset_names:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    if exception_history is None:\n        exception_history = []\n    logger.info(f\"Running analysis (attempt {len(exception_history)})\")\n    code = await _generate_run_analysis_python_code(\n        request,\n        analyst_db,\n        next(iter(exception_history[::-1]), None),\n        attempt=len(exception_history),\n    )\n    logger.info(\"Code generated, preparing execution\")\n    dataframes: dict[str, pl.DataFrame] = {}\n\n    for dataset_name in request.dataset_names:\n        try:\n            dataset = (\n                await analyst_db.get_cleansed_dataset(dataset_name, max_rows=None)\n            ).to_df()\n        except Exception:\n            dataset = (\n                await analyst_db.get_dataset(dataset_name, max_rows=None)\n            ).to_df()\n        dataframes[dataset_name] = dataset\n    functions = {}\n    tool_functions = get_tools()\n    for tool in tool_functions:\n        functions[tool.name] = tool.function\n    try:\n        logger.info(\"Executing\")\n        result = execute_python(\n            modules={\n                \"pd\": pd,\n                \"np\": np,\n                \"sm\": sm,\n                \"pl\": pl,\n                \"scipy\": scipy,\n                \"sklearn\": sklearn,\n            },\n            functions=functions,\n            expected_function=\"analyze_data\",\n            code=code,\n            input_data=dataframes,\n            output_type=AnalystDataset,\n            allowed_modules={\n                \"pandas\",\n                \"numpy\",\n                \"scipy\",\n                \"sklearn\",\n                \"statsmodels\",\n                \"datetime\",\n                \"polars\",\n                *find_imports(tools),\n            },\n        )\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=code, exception=e)\n    logger.info(\"Execution done\")\n    duration = datetime.now() - start_time\n    return RunAnalysisResult(\n        status=\"success\",\n        code=code,\n        dataset=result,\n        metadata=RunAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history) + 1,\n            datasets_analyzed=len(dataframes),\n            total_rows_analyzed=sum(\n                len(df) for df in dataframes.values() if not df.is_empty()\n            ),\n            total_columns_analyzed=sum(\n                len(df.columns) for df in dataframes.values() if not df.is_empty()\n            ),\n        ),\n    )\n\n\n@log_api_call\nasync def run_analysis(\n    request: RunAnalysisRequest,\n    analyst_db: AnalystDB,\n) -> RunAnalysisResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    logger.debug(\"Entering run_analysis\")\n    log_memory()\n    try:\n        return await _run_analysis(request, analyst_db=analyst_db)\n    except MaxReflectionAttempts as e:\n        return RunAnalysisResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n    except ValueError as e:\n        return RunAnalysisResult(\n            status=\"error\",\n            metadata=RunAnalysisResultMetadata(\n                duration=0,\n                attempts=1,\n                exception=AnalysisError.from_value_error(e),\n            ),\n        )\n\n\nasync def _generate_database_analysis_code(\n    request: RunDatabaseAnalysisRequest,\n    analyst_db: AnalystDB,\n    validation_error: InvalidGeneratedCode | None = None,\n) -> str:\n    \"\"\"\n    Generate Snowflake SQL analysis code based on data samples and question.\n\n    Parameters:\n    - request: DatabaseAnalysisRequest containing data samples and question\n\n    Returns:\n    - Dictionary containing generated code and description\n    \"\"\"\n\n    # Convert dictionary data structure to list of columns for all tables\n    dictionaries = [\n        await analyst_db.get_data_dictionary(name) for name in request.dataset_names\n    ]\n    all_tables_info = [d.model_dump(mode=\"json\") for d in dictionaries if d is not None]\n\n    # Get sample data for all tables\n    all_samples = []\n\n    for table in request.dataset_names:\n        df = (await analyst_db.get_dataset(table)).to_df().to_pandas()\n\n        sample_str = f\"Table: {table}\\n{df.head(10).to_string()}\"\n        all_samples.append(sample_str)\n\n    # Create messages for OpenAI\n    messages: list[ChatCompletionMessageParam] = [\n        get_external_database().get_system_prompt(),\n        ChatCompletionUserMessageParam(\n            content=f\"Business Question: {request.question}\",\n            role=\"user\",\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Sample Data:\\n{chr(10).join(all_samples)}\", role=\"user\"\n        ),\n        ChatCompletionUserMessageParam(\n            content=f\"Data Dictionary:\\n{json.dumps(all_tables_info)}\", role=\"user\"\n        ),\n    ]\n    if validation_error:\n        msg = type(validation_error).__name__ + f\": {str(validation_error)}\"\n        messages.extend(\n            [\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Previous attempt failed with error: {msg}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=f\"Failed code: {validation_error.code}\",\n                ),\n                ChatCompletionUserMessageParam(\n                    role=\"user\",\n                    content=\"Please generate new code that avoids this error.\",\n                ),\n            ]\n        )\n\n    # Get response from OpenAI\n    async with AsyncLLMClient() as client:\n        completion = await client.chat.completions.create(\n            response_model=DatabaseAnalysisCodeGeneration,\n            model=ALTERNATIVE_LLM_BIG,\n            temperature=0.1,\n            messages=messages,\n        )\n\n    return completion.code\n\n\n@reflect_code_generation_errors(max_attempts=7)\nasync def _run_database_analysis(\n    request: RunDatabaseAnalysisRequest,\n    analyst_db: AnalystDB,\n    exception_history: list[InvalidGeneratedCode] | None = None,\n) -> RunDatabaseAnalysisResult:\n    start_time = datetime.now()\n    if not request.dataset_names:\n        raise ValueError(VALUE_ERROR_MESSAGE)\n\n    if exception_history is None:\n        exception_history = []\n\n    sql_code = await _generate_database_analysis_code(\n        request, analyst_db, next(iter(exception_history[::-1]), None)\n    )\n    try:\n        results = get_external_database().execute_query(query=sql_code)\n        results = cast(list[dict[str, Any]], results)\n        duration = datetime.now() - start_time\n\n    except InvalidGeneratedCode:\n        raise\n    except Exception as e:\n        raise InvalidGeneratedCode(code=sql_code, exception=e)\n    return RunDatabaseAnalysisResult(\n        status=\"success\",\n        code=sql_code,\n        dataset=AnalystDataset(\n            data=results,\n        ),\n        metadata=RunDatabaseAnalysisResultMetadata(\n            duration=duration.total_seconds(),\n            attempts=len(exception_history),\n            datasets_analyzed=len(request.dataset_names),\n            # total_columns_analyzed=sum(len(ds.columns) for ds in request.datasets),\n        ),\n    )\n\n\n@log_api_call\nasync def run_database_analysis(\n    request: RunDatabaseAnalysisRequest, analyst_db: AnalystDB\n) -> RunDatabaseAnalysisResult:\n    \"\"\"Execute analysis workflow on datasets.\"\"\"\n    try:\n        return await _run_database_analysis(request, analyst_db)\n    except MaxReflectionAttempts as e:\n        return RunDatabaseAnalysisResult(\n            status=\"error\",\n            metadata=RunDatabaseAnalysisResultMetadata(\n                duration=e.duration,\n                attempts=len(e.exception_history) if e.exception_history else 0,\n                exception=AnalysisError.from_max_reflection_exception(e),\n            ),\n        )\n    except ValueError as e:\n        return RunDatabaseAnalysisResult(\n            status=\"error\",\n            metadata=RunDatabaseAnalysisResultMetadata(\n                duration=0,\n                attempts=1,\n                exception=AnalysisError.from_value_error(e),\n            ),\n        )\n\n\n# Type definitions\n@dataclass\nclass AnalysisGenerationError:\n    message: str\n    original_error: BaseException | None = None\n\n\nasync def execute_business_analysis_and_charts(\n    analysis_result: RunAnalysisResult | RunDatabaseAnalysisResult,\n    enhanced_message: str,\n    enable_chart_generation: bool = True,\n    enable_business_insights: bool = True,\n) -> tuple[\n    RunChartsResult | BaseException | None,\n    GetBusinessAnalysisResult | BaseException | None,\n]:\n    analysis_result.dataset = cast(AnalystDataset, analysis_result.dataset)\n    # Prepare both requests\n    chart_request = RunChartsRequest(\n        dataset=analysis_result.dataset,\n        question=enhanced_message,\n    )\n\n    business_request = GetBusinessAnalysisRequest(\n        dataset=analysis_result.dataset,\n        dictionary=DataDictionary.from_analyst_df(analysis_result.dataset.to_df()),\n        question=enhanced_message,\n    )\n\n    if enable_chart_generation and enable_business_insights:\n        # Run both analyses concurrently\n        result = await asyncio.gather(\n            run_charts(chart_request),\n            get_business_analysis(business_request),\n            return_exceptions=True,\n        )\n\n        return (result[0], result[1])\n    elif enable_chart_generation:\n        charts_result = await run_charts(chart_request)\n        return charts_result, None\n    else:\n        business_result = await get_business_analysis(business_request)\n        return None, business_result\n\n\nasync def run_complete_analysis(\n    chat_request: ChatRequest,\n    data_source: DataSourceType,\n    datasets_names: list[str],\n    analyst_db: AnalystDB,\n    chat_id: str,\n    message_id: str,\n    enable_chart_generation: bool = True,\n    enable_business_insights: bool = True,\n) -> AsyncGenerator[Component | AnalysisGenerationError, None]:\n    user_message = await analyst_db.get_chat_message(message_id=message_id)\n    if user_message is None or user_message.role != \"user\":\n        yield AnalysisGenerationError(\"Message not found\")\n\n        return\n    # Get enhanced message\n    try:\n        logger.info(\"Getting rephrased question...\")\n        enhanced_message = await rephrase_message(chat_request)\n        logger.info(\"Getting rephrased question done\")\n\n        yield enhanced_message\n\n    except ValidationError:\n        user_message.error = \"LLM Error, please retry\"\n        user_message.in_progress = False\n        await analyst_db.update_chat_message(\n            message_id=message_id,\n            message=user_message,\n        )\n        yield AnalysisGenerationError(user_message.error)\n\n        return\n\n    assistant_message = AnalystChatMessage(\n        role=\"assistant\",\n        content=enhanced_message,\n        components=[EnhancedQuestionGeneration(enhanced_user_message=enhanced_message)],\n    )\n\n    user_message.in_progress = False\n    await analyst_db.update_chat_message(\n        message_id=message_id,\n        message=user_message,\n    )\n    await analyst_db.add_chat_message(chat_id=chat_id, message=assistant_message)\n    # Run main analysis\n    logger.info(\"Start main analysis\")\n    try:\n        is_database = data_source == DataSourceType.DATABASE\n        logger.info(\"Getting analysis result...\")\n        log_memory()\n\n        if is_database:\n            analysis_result: (\n                RunAnalysisResult | RunDatabaseAnalysisResult\n            ) = await run_database_analysis(\n                RunDatabaseAnalysisRequest(\n                    dataset_names=datasets_names,\n                    question=enhanced_message,\n                ),\n                analyst_db,\n            )\n        else:\n            analysis_result = await run_analysis(\n                RunAnalysisRequest(\n                    dataset_names=datasets_names,\n                    question=enhanced_message,\n                ),\n                analyst_db,\n            )\n\n        log_memory()\n        logger.info(\"Getting analysis result done\")\n\n        if isinstance(analysis_result, BaseException):\n            error_message = f\"Error running initial analysis. Try rephrasing: {str(analysis_result)}\"\n            assistant_message.in_progress = False\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n            return\n\n        yield analysis_result\n\n        assistant_message.components.append(analysis_result)\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n    except Exception as e:\n        error_message = f\"Error running initial analysis. Try rephrasing: {str(e)}\"\n        assistant_message.in_progress = False\n        assistant_message.error = error_message\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n        yield AnalysisGenerationError(error_message)\n\n        return\n\n    # Only proceed with additional analysis if we have valid initial results\n    if not (\n        analysis_result\n        and analysis_result.dataset\n        and (enable_chart_generation or enable_business_insights)\n    ):\n        assistant_message.in_progress = False\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n        return\n\n    # Run concurrent analyses\n    try:\n        charts_result, business_result = await execute_business_analysis_and_charts(\n            analysis_result,\n            enhanced_message,\n            enable_business_insights=enable_business_insights,\n            enable_chart_generation=enable_chart_generation,\n        )\n\n        # Handle chart results\n        if isinstance(charts_result, BaseException):\n            error_message = \"Error generating charts\"\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n        elif charts_result is not None:\n            assistant_message.components.append(charts_result)\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield charts_result\n\n        # Handle business analysis results\n        if isinstance(business_result, BaseException):\n            error_message = \"Error generating business insights\"\n            assistant_message.error = error_message\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield AnalysisGenerationError(error_message)\n\n        elif business_result is not None:\n            assistant_message.components.append(business_result)\n            assistant_message.in_progress = False\n\n            await analyst_db.update_chat_message(\n                message_id=assistant_message.id, message=assistant_message\n            )\n\n            yield business_result\n\n    except Exception as e:\n        error_message = f\"Error setting up additional analysis: {str(e)}\"\n        assistant_message.in_progress = False\n        assistant_message.error = error_message\n        await analyst_db.update_chat_message(\n            message_id=assistant_message.id, message=assistant_message\n        )\n\n        yield AnalysisGenerationError(error_message)\n\n\nasync def process_data_and_update_state(\n    new_dataset_names: list[str],\n    analyst_db: AnalystDB,\n    data_source: str | DataSourceType,\n) -> AsyncGenerator[str, None]:\n    \"\"\"Process datasets and yield progress updates asynchronously.\"\"\"\n    # Start processing and yield initial message\n    logger.info(\"Starting data processing\")\n    log_memory()\n    yield \"Starting data processing\"\n\n    # Handle data cleansing based on the source\n    # Convert string data_source to DataSourceType if needed\n    data_source_type = (\n        data_source\n        if isinstance(data_source, DataSourceType)\n        else DataSourceType(data_source)\n    )\n    if data_source_type != DataSourceType.DATABASE:\n        try:\n            logger.info(\"Cleansing datasets\")\n            yield \"Cleansing datasets\"\n            for analysis_dataset_name in new_dataset_names:\n                analysis_dataset = await analyst_db.get_dataset(\n                    analysis_dataset_name, max_rows=None\n                )\n                cleansed_dataset = await cleanse_dataframe(analysis_dataset)\n                await analyst_db.register_dataset(\n                    cleansed_dataset, data_source=DataSourceType.GENERATED\n                )\n                yield f\"Cleansed dataset: {analysis_dataset_name}\"\n                del cleansed_dataset\n                del analysis_dataset\n                log_memory()\n\n            logger.info(\"Cleansing datasets complete\")\n            yield \"Cleansing datasets complete\"\n            log_memory()\n        except Exception:\n            logger.error(\"Data processing failed\", exc_info=True)\n            yield \"Data processing failed\"\n            raise\n    else:\n        pass\n\n    # Generate data dictionaries\n    logger.info(\"Data processing successful, generating dictionaries\")\n    yield \"Data processing successful, generating dictionaries\"\n    log_memory()\n    try:\n        for analysis_dataset_name in new_dataset_names:\n            try:\n                existing_dictionary = await analyst_db.get_data_dictionary(\n                    analysis_dataset_name\n                )\n                logger.info(\n                    f\"Found existing dictionary for dataset: {analysis_dataset_name}\"\n                )\n                if existing_dictionary is not None:\n                    continue\n\n            except Exception:\n                pass\n            logger.info(f\"Creating dictionary for dataset: {analysis_dataset_name}\")\n            analysis_dataset = await analyst_db.get_dataset(analysis_dataset_name)\n            new_dictionary = await get_dictionary(analysis_dataset)\n            logger.info(new_dictionary.to_application_df())\n            del analysis_dataset\n            await analyst_db.register_data_dictionary(new_dictionary)\n            logger.info(f\"Registered dictionary for dataset: {analysis_dataset_name}\")\n            yield f\"Registered data dictionary: {analysis_dataset_name}\"\n            log_memory()\n            continue\n    except Exception:\n        logger.error(\"Failed to generate data dictionaries\", exc_info=True)\n        yield \"Failed to generate data dictionaries\"\n        raise\n    log_memory()\n    # Final completion message\n    yield \"Processing complete\"\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "utils/api.py",
      "tags": [
        "numpy",
        "deployment",
        "llm",
        "pandas",
        "datarobot",
        "openai",
        "streamlit"
      ],
      "metadata": {
        "repo_name": "talk-to-my-data-agent",
        "file_path": "utils/api.py",
        "size": 55548,
        "code_chunks": 10
      },
      "code_examples": [
        "def log_memory() -> None:\n    process = psutil.Process()\n    memory = process.memory_info().rss / 1024 / 1024  # MB\n    logger.info(f\"Memory usage: {memory:.2f} MB\")",
        "def initialize_deployment() -> tuple[RESTClientObject, str]:\n    \"\"\"Initialize either LLM Gateway or DataRobot-hosted LLM deployment based on environment settings and credential priority.\"\"\"\n    try:\n        dr_client = dr.Client()\n        chat_agent_deployment_id = LLMDeployment().id\n        if chat_agent_deployment_id is None:\n            raise ValueError(\n                \"LLM Deployment ID is required but not found. Please check your infrastructure setup.\"\n            )\n        deployment_chat_base_url = (\n            f\"{dr_client.endpoint.rstrip('/')}/deployments/{chat_agent_deployment_id}/\"\n        )\n        logger.info(\n            f\"Using the DataRobot-hosted LLM deployment (configured at infrastructure time) at: {deployment_chat_base_url}\"\n        )\n        return dr_client, deployment_chat_base_url\n\n    except ValidationError as e:\n        raise ValueError(\n            \"Unable to load Deployment ID.\"\n            \"If running locally, verify you have selected the correct \"\n            \"stack and that it is active using `pulumi stack output`. \"\n            \"If running in DataRobot, verify your runtime parameters have been set correctly.\"\n        ) from e",
        "class AsyncLLMClient:\n    async def __aenter__(self) -> instructor.AsyncInstructor:\n        dr_client, deployment_base_url = initialize_deployment()\n        self.openai_client = AsyncOpenAI(\n            api_key=dr_client.token,\n            base_url=deployment_base_url,\n            timeout=90,\n            max_retries=2,\n        )\n        self.client = instructor.from_openai(\n            self.openai_client, mode=instructor.Mode.MD_JSON\n        )\n        return self.client\n\n    async def __aexit__(\n        self,\n        exc_type: Type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        await self.openai_client.close()",
        "def cache(f: T) -> T:\n    \"\"\"Cache function and coroutine results to disk using joblib.\"\"\"\n    cached_f = _memory.cache(f)\n\n    if asyncio.iscoroutinefunction(f):\n\n        async def awrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = await cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"\n                )\n            return result\n\n        return cast(T, awrapper)\n    else:\n\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"  # type: ignore[attr-defined]\n                )\n            return result\n\n        return cast(T, wrapper)",
        "def list_registry_datasets(limit: int = 100) -> list[DataRegistryDataset]:\n    \"\"\"\n    Fetch datasets from Data Registry with specified limit\n\n    Args:\n        limit: int\n        Datasets to retrieve. Max value: 100\n    \"\"\"\n\n    url = f\"datasets?limit={limit}\"\n\n    # Get all datasets and manually limit the results\n    datasets = dr.client.get_client().get(url).json()[\"data\"]\n\n    return [\n        DataRegistryDataset(\n            id=ds[\"datasetId\"],\n            name=ds[\"name\"],\n            created=(\n                ds[\"creationDate\"][:10] if \"creationDate\" in ds else \"N/A\"  # %Y-%m-%d\n            ),\n            size=(\n                f\"{ds['datasetSize'] / (1024 * 1024):.1f} MB\"\n                if \"datasetSize\" in ds\n                else \"N/A\"\n            ),\n        )\n        for ds in datasets\n    ]",
        "def _validate_question_feasibility(\n    question: str, available_columns: list[str]\n) -> ValidatedQuestion | None:\n    \"\"\"Validate if a question can be answered with available data\n\n    Checks if common data elements mentioned in the question exist in columns\n    \"\"\"\n    # Convert question and columns to lowercase for matching\n    question_lower = question.lower()\n    columns_lower = [col.lower() for col in available_columns]\n\n    # Extract potential column references from question\n    words = set(re.findall(r\"\\b\\w+\\b\", question_lower))\n\n    # Find matches and missing terms\n    found_columns = [col for col in columns_lower if any(word in col for word in words)]\n\n    is_valid = len(found_columns) > 0\n    if is_valid:\n        return ValidatedQuestion(\n            question=question,\n        )\n    return None",
        "def find_imports(module: ModuleType) -> list[str]:\n    \"\"\"\n    Get top-level third-party imports from a Python module.\n\n    Args:\n        module: Python module object to analyze\n\n    Returns:\n        list of third-party package names\n\n    Example:\n        >>> import my_module\n        >>> imports = find_third_party_imports(my_module)\n        >>> print(imports)  # ['pandas', 'numpy', 'requests']\n    \"\"\"\n    try:\n        # Get the source code of the module\n        source = inspect.getsource(module)\n        tree = ast.parse(source)\n\n        stdlib_modules = set(sys.stdlib_module_names)\n        third_party = set()\n\n        # Only look at top-level imports\n        for node in tree.body:\n            if isinstance(node, ast.Import):\n                for name in node.names:\n                    module_name = name.name.split(\".\")[0]\n                    if module_name not in stdlib_modules:\n                        third_party.add(module_name)\n\n            elif isinstance(node, ast.ImportFrom):\n                if node.module is None:\n                    continue\n                module_name = node.module.split(\".\")[0]\n                if module_name not in stdlib_modules:\n                    third_party.add(module_name)\n\n        return sorted(third_party)\n    except Exception:\n        return []",
        "def get_tools() -> list[Tool]:\n    try:\n        # find all functions defined in the tools module\n        tool_functions = [func for func in dir(tools) if callable(getattr(tools, func))]\n\n        # find the function signatures and doc strings\n        tools_list = []\n        for func_name in tool_functions:\n            func = getattr(tools, func_name)\n            signature = inspect.signature(func)\n            docstring = inspect.getdoc(func)\n            tools_list.append(\n                Tool(\n                    name=func_name,\n                    signature=str(signature),\n                    docstring=docstring,\n                    function=func,\n                )\n            )\n        return tools_list\n    except Exception:\n        return []",
        "class AnalysisGenerationError:\n    message: str\n    original_error: BaseException | None = None",
        "def wrapper(*args: Any, **kwargs: Any) -> Any:\n            in_cache = cached_f.check_call_in_cache(*args, **kwargs)\n            result = cached_f(*args, **kwargs)\n            if not in_cache:\n                _memory.reduce_size(DISK_CACHE_LIMIT_BYTES)\n            else:\n                logger.info(\n                    f\"Using previously cached result for function `{f.__name__}`\"  # type: ignore[attr-defined]\n                )\n            return result"
      ],
      "api_methods": [
        "dr.dataset.get",
        "dr.client.get_client"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_python_1055321351941153478",
      "title": "tools.py",
      "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Add additional tools that can be used by the analysis code execution. Remember to include the necessary imports and provide a docstring and signature for each function.\n# signature and docstring will be provided to the LLM in the prompt.\n# Uncomment the examples below to get started.\n\n\n# import datarobot as dr\n# import pandas as pd\n# from datarobot_predict.deployment import predict\n\n# def calculate_summary(df: pd.DataFrame) -> pd.DataFrame:\n#     \"\"\"\n#     Calculate summary statistics for a DataFrame.\n\n#     Args:\n#         df (pd.DataFrame): Input DataFrame.\n\n#     Returns:\n#         pd.DataFrame: Summary statistics including count, mean, std, min, max, and percentiles.\n#     \"\"\"\n#     description = df.describe(percentiles=[0.2, 0.4, 0.6, 0.8])\n#     return description\n\n\n# def filter_data(df: pd.DataFrame, column: str, value: float) -> pd.DataFrame:\n#     \"\"\"\n#     Filter DataFrame based on a condition.\n#     Args:\n#         df (pd.DataFrame): Input DataFrame.\n#         column (str): Column to apply the filter on.\n#         value (float): Value to compare against in the filter.\n#     Returns:\n#         pd.DataFrame: Filtered DataFrame where the specified column's values are greater than the given value.\n#     \"\"\"\n#     filtered_df = df[df[column] > value]\n#     return filtered_df\n\n\n# def call_datarobot_deployment(df: pd.DataFrame, deployment_id: str) -> pd.DataFrame:\n#     \"\"\"\n#     Call a DataRobot deployment to get predictions.\n\n#     Args:\n#         df (pd.DataFrame): Input DataFrame with features for prediction.\n#         deployment_id (str): ID of the DataRobot deployment to use for predictions.\n\n#     Returns:\n#         pd.DataFrame: DataFrame containing the predictions from DataRobot. The prediction column is named 'predictions'.\n#     \"\"\"\n#     deployment = dr.Deployment.get(deployment_id)  # type: ignore[attr-defined]\n#     prediction_response: pd.DataFrame = predict(\n#         deployment=deployment, data_frame=df\n#     ).dataframe\n\n#     prediction_response.columns = [\n#         c.replace(\"_PREDICTION\", \"\")\n#         for c in prediction_response.columns  # type: ignore[assignment]\n#     ]\n\n#     if deployment.model is not None:\n#         target_column = deployment.model.get(\"target_name\")\n#         if target_column:\n#             prediction_response[\"predictions\"] = prediction_response[target_column]\n\n#     return prediction_response\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "utils/tools.py",
      "tags": [
        "pandas",
        "deployment",
        "datarobot",
        "llm"
      ],
      "metadata": {
        "repo_name": "talk-to-my-data-agent",
        "file_path": "utils/tools.py",
        "size": 2968,
        "code_chunks": 0
      },
      "code_examples": [],
      "api_methods": [
        "model.get",
        "deployment.model",
        "dr.deployment.get",
        "deployment.get"
      ],
      "complexity_score": 0.95,
      "use_case_category": "general"
    },
    {
      "id": "github_python_3151187766035014248",
      "title": "settings_generative.py",
      "content": "# Copyright 2024 DataRobot, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import annotations\n\nimport datarobot as dr\nimport pulumi\nimport pulumi_datarobot as datarobot\nfrom datarobot_pulumi_utils.pulumi.stack import PROJECT_NAME\nfrom datarobot_pulumi_utils.schema.custom_models import (\n    CustomModelArgs,\n    DeploymentArgs,\n    RegisteredModelArgs,\n)\nfrom datarobot_pulumi_utils.schema.exec_envs import RuntimeEnvironments\nfrom datarobot_pulumi_utils.schema.llms import (\n    LLMBlueprintArgs,\n    LLMs,\n    LLMSettings,\n    PlaygroundArgs,\n)\n\nfrom utils.schema import LLMDeploymentSettings\n\nLLM = LLMs.AZURE_OPENAI_GPT_4_O\n\ncustom_model_args = CustomModelArgs(\n    resource_name=f\"Generative Analyst Custom Model [{PROJECT_NAME}]\",\n    name=\"Generative Analyst Assistant\",  # built-in QA app uses this as the AI's name\n    target_name=LLMDeploymentSettings().target_feature_name,\n    target_type=dr.enums.TARGET_TYPE.TEXT_GENERATION,\n    replicas=2,\n    base_environment_id=RuntimeEnvironments.PYTHON_312_MODERATIONS.value.id,\n    opts=pulumi.ResourceOptions(delete_before_replace=True),\n)\n\nregistered_model_args = RegisteredModelArgs(\n    resource_name=f\"Generative Analyst Registered Model [{PROJECT_NAME}]\",\n)\n\n\ndeployment_args = DeploymentArgs(\n    resource_name=f\"Generative Analyst Deployment [{PROJECT_NAME}]\",\n    label=f\"Generative Analyst Deployment [{PROJECT_NAME}]\",\n    association_id_settings=datarobot.DeploymentAssociationIdSettingsArgs(\n        column_names=[\"association_id\"],\n        auto_generate_id=False,\n        required_in_prediction_requests=True,\n    ),\n    predictions_data_collection_settings=datarobot.DeploymentPredictionsDataCollectionSettingsArgs(\n        enabled=True,\n    ),\n    predictions_settings=(\n        datarobot.DeploymentPredictionsSettingsArgs(min_computes=0, max_computes=2)\n    ),\n)\n\nplayground_args = PlaygroundArgs(\n    resource_name=f\"Generative Analyst Playground [{PROJECT_NAME}]\",\n)\n\nllm_blueprint_args = LLMBlueprintArgs(\n    resource_name=f\"Generative Analyst LLM Blueprint [{PROJECT_NAME}]\",\n    llm_id=LLM.name,\n    llm_settings=LLMSettings(\n        max_completion_length=2048,\n        temperature=0.1,\n    ),\n)\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "infra/settings_generative.py",
      "tags": [
        "blueprint",
        "deployment",
        "llm",
        "datarobot",
        "openai"
      ],
      "metadata": {
        "repo_name": "talk-to-my-data-agent",
        "file_path": "infra/settings_generative.py",
        "size": 2692,
        "code_chunks": 0
      },
      "code_examples": [],
      "api_methods": [
        "dr.enums.target_type"
      ],
      "complexity_score": 0.9999999999999999,
      "use_case_category": "general"
    },
    {
      "id": "github_python_1129786245720876129",
      "title": "__init__.py",
      "content": "",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "infra/__init__.py",
      "tags": [],
      "metadata": {
        "repo_name": "talk-to-my-data-agent",
        "file_path": "infra/__init__.py",
        "size": 0,
        "code_chunks": 0
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "general"
    },
    {
      "id": "github_python_3157805405366172222",
      "title": "__init__.py",
      "content": "",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "utils/__init__.py",
      "tags": [],
      "metadata": {
        "repo_name": "talk-to-my-data-agent",
        "file_path": "utils/__init__.py",
        "size": 0,
        "code_chunks": 0
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "general"
    },
    {
      "id": "github_python_-4017511268363821972",
      "title": "utils.py",
      "content": "#!/usr/bin/env python3\n# -*- mode: python; python-indent-offset: 4 -*-\n\nfrom IPython.display import display\nimport pandas as pd\nimport snowflake\nfrom snowflake.connector.pandas_tools import write_pandas\nimport yaml\n\n# we can skip the import snowfalke step entirely https://stephenallwright.com/python-connector-write-pandas-snowflake/\n# this causes issues if they install snowflake connector and \"snowflake\" - https://stackoverflow.com/questions/74223900/snowflake-connector-python-package-not-recognized\n\n\ndef prepare_demo_tables_in_db(\n    db_user=None,  # username to access snowflake database\n    db_password=None,  # password\n    account=None,  # Snowflake Account Identifier can be found in the db_url\n    db=None,  # Database to Write_To\n    warehouse=None,  # Warehouse\n    schema=None,  # schema\n):\n    \"\"\"description: method to prepare demo table in snowflake database\n    reads from datasets.yaml\n\n    by: gongoraj, demidov91 and jpgomes\n        date: 12/22/2022\n    \"\"\"\n\n    with snowflake.connector.connect(\n        user=db_user,\n        password=db_password,\n        account=account,\n        warehouse=warehouse,\n        database=db,\n        schema=schema,\n    ) as con:\n        with open(\"datasets.yaml\") as f:\n            config = yaml.safe_load(f)\n            for key, value in config[\"datasets\"].items():\n                print(\"*\" * 30)\n                print(\"table:\", value[\"table_name\"])\n                try:\n                    df = pd.read_csv(value[\"url\"], encoding=\"utf8\")\n                except:\n                    df = pd.read_csv(value[\"url\"], encoding=\"cp850\")\n                display(df.head())\n                print(\"info for \", value[\"table_name\"])\n                print(df.info())\n                print(\n                    \"writing\", value[\"table_name\"], \"to snowflake from: \", value[\"url\"]\n                )\n                write_pandas(\n                    con, df, value[\"table_name\"], auto_create_table=True, overwrite=True\n                )\n                con.commit()\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "ecosystem_integration_templates/Snowflake_template/utils.py",
      "tags": [
        "integration",
        "ecosystem",
        "pandas",
        "ai-accelerators",
        "templates"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/Snowflake_template/utils.py",
        "size": 2009,
        "code_chunks": 1
      },
      "code_examples": [
        "def prepare_demo_tables_in_db(\n    db_user=None,  # username to access snowflake database\n    db_password=None,  # password\n    account=None,  # Snowflake Account Identifier can be found in the db_url\n    db=None,  # Database to Write_To\n    warehouse=None,  # Warehouse\n    schema=None,  # schema\n):\n    \"\"\"description: method to prepare demo table in snowflake database\n    reads from datasets.yaml\n\n    by: gongoraj, demidov91 and jpgomes\n        date: 12/22/2022\n    \"\"\"\n\n    with snowflake.connector.connect(\n        user=db_user,\n        password=db_password,\n        account=account,\n        warehouse=warehouse,\n        database=db,\n        schema=schema,\n    ) as con:\n        with open(\"datasets.yaml\") as f:\n            config = yaml.safe_load(f)\n            for key, value in config[\"datasets\"].items():\n                print(\"*\" * 30)\n                print(\"table:\", value[\"table_name\"])\n                try:\n                    df = pd.read_csv(value[\"url\"], encoding=\"utf8\")\n                except:\n                    df = pd.read_csv(value[\"url\"], encoding=\"cp850\")\n                display(df.head())\n                print(\"info for \", value[\"table_name\"])\n                print(df.info())\n                print(\n                    \"writing\", value[\"table_name\"], \"to snowflake from: \", value[\"url\"]\n                )\n                write_pandas(\n                    con, df, value[\"table_name\"], auto_create_table=True, overwrite=True\n                )\n                con.commit()"
      ],
      "api_methods": [],
      "complexity_score": 0.44999999999999996,
      "use_case_category": "general"
    },
    {
      "id": "github_python_3673691968750738178",
      "title": "app.py",
      "content": "# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License.\r\n\r\nfrom datetime import datetime\r\nimport sys\r\nimport traceback\r\n\r\nfrom aiohttp import web\r\nfrom aiohttp.web import json_response, Request, Response\r\nfrom bot import MyBot\r\nfrom botbuilder.core import (\r\n    BotFrameworkAdapter,\r\n    BotFrameworkAdapterSettings,\r\n    TurnContext,\r\n)\r\nfrom botbuilder.core.integration import aiohttp_error_middleware\r\nfrom botbuilder.schema import Activity, ActivityTypes\r\nfrom config import DefaultConfig\r\n\r\n# Configure HTTPS here\r\n# import ssl\r\n# ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\r\n# ssl_context.load_cert_chain('./selfsigned.crt', './private.key')\r\n\r\nCONFIG = DefaultConfig()\r\n\r\n# Create adapter.\r\n# See https://aka.ms/about-bot-adapter to learn more about how bots work.\r\nSETTINGS = BotFrameworkAdapterSettings(CONFIG.APP_ID, CONFIG.APP_PASSWORD)\r\nADAPTER = BotFrameworkAdapter(SETTINGS)\r\nprint(CONFIG.APP_ID, CONFIG.APP_PASSWORD)\r\n\r\n\r\n# Catch-all for errors.\r\nasync def on_error(context: TurnContext, error: Exception):\r\n    # This check writes out errors to console log .vs. app insights.\r\n    # NOTE: In production environment, you should consider logging this to Azure\r\n    #       application insights.\r\n    print(f\"\\n [on_turn_error] unhandled error: {error}\", file=sys.stderr)\r\n    traceback.print_exc()\r\n\r\n    # Send a message to the user\r\n    await context.send_activity(\"The bot encountered an error or bug.\")\r\n    await context.send_activity(\r\n        \"To continue to run this bot, please fix the bot source code.\"\r\n    )\r\n    # Send a trace activity if we're talking to the Bot Framework Emulator\r\n    if context.activity.channel_id == \"emulator\":\r\n        # Create a trace activity that contains the error object\r\n        trace_activity = Activity(\r\n            label=\"TurnError\",\r\n            name=\"on_turn_error Trace\",\r\n            timestamp=datetime.utcnow(),\r\n            type=ActivityTypes.trace,\r\n            value=f\"{error}\",\r\n            value_type=\"https://www.botframework.com/schemas/error\",\r\n        )\r\n        # Send a trace activity, which will be displayed in Bot Framework Emulator\r\n        await context.send_activity(trace_activity)\r\n\r\n\r\nADAPTER.on_turn_error = on_error\r\n\r\n# Create the Bot\r\nBOT = MyBot(CONFIG.APP_ID, CONFIG.APP_PASSWORD)\r\n\r\n\r\ndef home(req: Request) -> Response:\r\n    return json_response(data=\"success\", status=Response(status=201).status)\r\n\r\n\r\n# Listen for incoming requests on /api/messages\r\nasync def messages(req: Request) -> Response:\r\n    # Main bot message handler.\r\n    if \"application/json\" in req.headers[\"Content-Type\"]:\r\n        body = await req.json()\r\n    else:\r\n        return Response(status=415)\r\n    activity = Activity().deserialize(body)\r\n    auth_header = req.headers[\"Authorization\"] if \"Authorization\" in req.headers else \"\"\r\n    response = await ADAPTER.process_activity(activity, auth_header, BOT.on_turn)\r\n    if response:\r\n        return json_response(data=response.body, status=response.status)\r\n    return Response(status=201)\r\n\r\n\r\nAPP = web.Application(middlewares=[aiohttp_error_middleware])\r\nAPP.router.add_get(\"/\", home)\r\nAPP.router.add_post(\"/api/messages\", messages)\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Bot backend is Active!\")\r\n    web.run_app(\r\n        APP,\r\n        host=CONFIG.SERVER,\r\n        port=CONFIG.PORT,\r\n        # ssl_context=ssl_context Set SSL\r\n    )\r\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "ecosystem_integration_templates/teams_datarobot/app.py",
      "tags": [
        "templates",
        "integration",
        "ai-accelerators",
        "ecosystem"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/teams_datarobot/app.py",
        "size": 3422,
        "code_chunks": 1
      },
      "code_examples": [
        "def home(req: Request) -> Response:\r\n    return json_response(data=\"success\", status=Response(status=201).status)"
      ],
      "api_methods": [],
      "complexity_score": 0.8,
      "use_case_category": "general"
    },
    {
      "id": "github_python_3707081185272891676",
      "title": "bot.py",
      "content": "# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License.\r\n\r\nfrom datetime import datetime\r\nimport json\r\n\r\nfrom botbuilder.core import (\r\n    ActivityHandler,\r\n    CardFactory,\r\n    ConversationState,\r\n    MemoryStorage,\r\n    MessageFactory,\r\n    TurnContext,\r\n)\r\nfrom botbuilder.core.teams import TeamsActivityHandler\r\nfrom botbuilder.schema import Activity, ActivityTypes, Attachment, ChannelAccount\r\nfrom config import DefaultConfig\r\nimport datarobotx as drx\r\nfrom utilities import generate_links\r\n\r\nadaptive_card_json = json.load(open(\"./card_template.json\"))\r\n\r\nCONFIG = DefaultConfig()\r\n# DataRobot Connect\r\ndrx.Context(token=CONFIG.DATAROBOT_TOKEN, endpoint=CONFIG.DATAROBOT_ENDPOINT)\r\ndeployment_id = CONFIG.DATAROBOT_DEPLOYMENT\r\ndeployment = drx.Deployment(deployment_id)\r\n\r\n# store chat history\r\nmemstore = MemoryStorage()\r\nconvstate = ConversationState(memstore)\r\n\r\n\r\nclass ConversationData:\r\n    def __init__(self, chat_history=[]):\r\n        self.chat_history = chat_history\r\n\r\n\r\nclass MyBot(TeamsActivityHandler):\r\n    # See https://aka.ms/about-bot-activity-message to learn more about the message and other activity types.\r\n    def __init__(self, app_id: str, app_password: str):\r\n        self._app_id = app_id\r\n        self._app_password = app_password\r\n        self.convstate_accessor = convstate.create_property(\"convdata\")\r\n\r\n    async def on_message_activity(self, turn_context: TurnContext):\r\n        time_start = datetime.now()\r\n        convdata = await self.convstate_accessor.get(turn_context, ConversationData)\r\n        chat_history = convdata.chat_history\r\n        TurnContext.remove_recipient_mention(turn_context.activity)\r\n        answer = deployment.predict_unstructured(\r\n            {\"question\": turn_context.activity.text, \"chat_history\": chat_history}\r\n        )\r\n        print(\"deployment response\")\r\n        print(answer)\r\n        answer_formatted = answer[\"answer\"]\r\n        references = generate_links(answer[\"references\"])\r\n        print(json.dumps(answer))\r\n        chat_history.append((turn_context.activity.text, answer_formatted))\r\n        time_end = datetime.now()\r\n        link_block = []\r\n        if len(references) > 0:\r\n            link_block.append(\r\n                {\"type\": \"TextBlock\", \"size\": \"medium\", \"text\": \"References:\"}\r\n            )\r\n            fs = {\"type\": \"FactSet\", \"facts\": []}\r\n            for link in references:\r\n                # [Adaptive cards!](https://adaptivecards.io)\r\n                fs[\"facts\"].append({\"value\": \"[\" + link + \"](\" + link + \")\"})\r\n            link_block.append(fs)\r\n        adaptive_card_json[\"body\"] = link_block\r\n        message = Activity(\r\n            text=answer_formatted,\r\n            type=ActivityTypes.message,\r\n            attachments=[CardFactory.adaptive_card(adaptive_card_json)],\r\n        )\r\n        await turn_context.send_activity(message)\r\n\r\n    async def on_members_added_activity(\r\n        self, members_added: ChannelAccount, turn_context: TurnContext\r\n    ):\r\n        for member_added in members_added:\r\n            if member_added.id != turn_context.activity.recipient.id:\r\n                await turn_context.send_activity(\r\n                    \"Hello!. I am DataRobot's Generative AI Bot, how can I help you?\"\r\n                )\r\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "ecosystem_integration_templates/teams_datarobot/bot.py",
      "tags": [
        "integration",
        "deployment",
        "ecosystem",
        "datarobot",
        "ai-accelerators",
        "templates"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/teams_datarobot/bot.py",
        "size": 3284,
        "code_chunks": 4
      },
      "code_examples": [
        "class ConversationData:\r\n    def __init__(self, chat_history=[]):\r\n        self.chat_history = chat_history",
        "class MyBot(TeamsActivityHandler):\r\n    # See https://aka.ms/about-bot-activity-message to learn more about the message and other activity types.\r\n    def __init__(self, app_id: str, app_password: str):\r\n        self._app_id = app_id\r\n        self._app_password = app_password\r\n        self.convstate_accessor = convstate.create_property(\"convdata\")\r\n\r\n    async def on_message_activity(self, turn_context: TurnContext):\r\n        time_start = datetime.now()\r\n        convdata = await self.convstate_accessor.get(turn_context, ConversationData)\r\n        chat_history = convdata.chat_history\r\n        TurnContext.remove_recipient_mention(turn_context.activity)\r\n        answer = deployment.predict_unstructured(\r\n            {\"question\": turn_context.activity.text, \"chat_history\": chat_history}\r\n        )\r\n        print(\"deployment response\")\r\n        print(answer)\r\n        answer_formatted = answer[\"answer\"]\r\n        references = generate_links(answer[\"references\"])\r\n        print(json.dumps(answer))\r\n        chat_history.append((turn_context.activity.text, answer_formatted))\r\n        time_end = datetime.now()\r\n        link_block = []\r\n        if len(references) > 0:\r\n            link_block.append(\r\n                {\"type\": \"TextBlock\", \"size\": \"medium\", \"text\": \"References:\"}\r\n            )\r\n            fs = {\"type\": \"FactSet\", \"facts\": []}\r\n            for link in references:\r\n                # [Adaptive cards!](https://adaptivecards.io)\r\n                fs[\"facts\"].append({\"value\": \"[\" + link + \"](\" + link + \")\"})\r\n            link_block.append(fs)\r\n        adaptive_card_json[\"body\"] = link_block\r\n        message = Activity(\r\n            text=answer_formatted,\r\n            type=ActivityTypes.message,\r\n            attachments=[CardFactory.adaptive_card(adaptive_card_json)],\r\n        )\r\n        await turn_context.send_activity(message)\r\n\r\n    async def on_members_added_activity(\r\n        self, members_added: ChannelAccount, turn_context: TurnContext\r\n    ):\r\n        for member_added in members_added:\r\n            if member_added.id != turn_context.activity.recipient.id:\r\n                await turn_context.send_activity(\r\n                    \"Hello!. I am DataRobot's Generative AI Bot, how can I help you?\"\r\n                )",
        "def __init__(self, chat_history=[]):\r\n        self.chat_history = chat_history",
        "def __init__(self, app_id: str, app_password: str):\r\n        self._app_id = app_id\r\n        self._app_password = app_password\r\n        self.convstate_accessor = convstate.create_property(\"convdata\")"
      ],
      "api_methods": [
        "deployment.predict_unstructured"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "general"
    },
    {
      "id": "github_python_4586538826597028379",
      "title": "config.py",
      "content": "#!/usr/bin/env python3\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License.\r\n\r\nimport os\r\n\r\n\r\nclass DefaultConfig:\r\n    \"\"\"Bot Configuration\"\"\"\r\n\r\n    PORT = os.environ.get(\"PORT\", 443)  # Port to run the bot service on\r\n    SERVER = os.environ.get(\"SERVER\", \"0.0.0.0\")  # Server IP. Accepts localhost\r\n    APP_ID = os.environ.get(\r\n        \"MicrosoftAppId\", \"\"\r\n    )  # Bot Identifier from https://dev.teams.microsoft.com/bots/<<IDENTIFIER>>/configure\r\n    APP_PASSWORD = os.environ.get(\r\n        \"MicrosoftAppPassword\", \"\"\r\n    )  # Client Secret from Bot Configure page.\r\n    DATAROBOT_TOKEN = os.environ.get(\r\n        \"apiToken\", \"\"\r\n    )  # DataRobot API Token for authorization\r\n    DATAROBOT_ENDPOINT = os.environ.get(\r\n        \"DATAROBOT_ENDPOINT\", \"https://app.datarobot.com/api/v2\"\r\n    )  # DataRobot Endpoint\r\n    DATAROBOT_DEPLOYMENT = os.environ.get(\r\n        \"deploymentId\", \"65cd8cb6481e3be33dcd194c\"\r\n    )  # DataRobot LLM Deployment Identifier\r\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "ecosystem_integration_templates/teams_datarobot/config.py",
      "tags": [
        "integration",
        "deployment",
        "ecosystem",
        "datarobot",
        "ai-accelerators",
        "templates"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/teams_datarobot/config.py",
        "size": 1009,
        "code_chunks": 1
      },
      "code_examples": [
        "class DefaultConfig:\r\n    \"\"\"Bot Configuration\"\"\"\r\n\r\n    PORT = os.environ.get(\"PORT\", 443)  # Port to run the bot service on\r\n    SERVER = os.environ.get(\"SERVER\", \"0.0.0.0\")  # Server IP. Accepts localhost\r\n    APP_ID = os.environ.get(\r\n        \"MicrosoftAppId\", \"\"\r\n    )  # Bot Identifier from https://dev.teams.microsoft.com/bots/<<IDENTIFIER>>/configure\r\n    APP_PASSWORD = os.environ.get(\r\n        \"MicrosoftAppPassword\", \"\"\r\n    )  # Client Secret from Bot Configure page.\r\n    DATAROBOT_TOKEN = os.environ.get(\r\n        \"apiToken\", \"\"\r\n    )  # DataRobot API Token for authorization\r\n    DATAROBOT_ENDPOINT = os.environ.get(\r\n        \"DATAROBOT_ENDPOINT\", \"https://app.datarobot.com/api/v2\"\r\n    )  # DataRobot Endpoint\r\n    DATAROBOT_DEPLOYMENT = os.environ.get(\r\n        \"deploymentId\", \"65cd8cb6481e3be33dcd194c\"\r\n    )"
      ],
      "api_methods": [],
      "complexity_score": 0.5,
      "use_case_category": "general"
    },
    {
      "id": "github_python_-6181530093734661067",
      "title": "utilities.py",
      "content": "import re\n\n\ndef generate_links(locs, max_links=None):\n    dr_docs_dir = \"storage/vectordb_training/datarobot_docs/en/\"\n    dr_security_dir = \"storage/vectordb_training/datarobot_docs/\"\n    rfp_docs_dir = \"storage/vectordb_training/reprocessed_data/\"\n    links = []\n    for link in locs:\n        if link.startswith(dr_docs_dir):\n            link = link[len(dr_docs_dir) :]\n            link = \"https://docs.datarobot.com/en/docs/\" + link\n            link = link[:-2] + \"html\"  # Remove \"md\", add html\n        elif link.startswith(dr_security_dir):\n            link = \"https://www.datarobot.com/trustcenter/\"\n        elif link.startswith(rfp_docs_dir):\n            try:\n                file = open(link, \"r\")\n                file_contents = file.read()\n                link = re.search(r\"https://\\S+\", file_contents)\n                file.close()\n                link = link.group()\n            except AttributeError:\n                print(\"Attribute error\")\n                link = None\n                pass\n            except FileNotFoundError:\n                link = None\n                pass\n        links.append(link)\n    links = list(set([link for link in links if link != None]))\n    return links\n",
      "content_type": "code",
      "source_type": "github_python",
      "source_file": "ecosystem_integration_templates/teams_datarobot/utilities.py",
      "tags": [
        "integration",
        "ecosystem",
        "datarobot",
        "ai-accelerators",
        "templates"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/teams_datarobot/utilities.py",
        "size": 1199,
        "code_chunks": 1
      },
      "code_examples": [
        "def generate_links(locs, max_links=None):\n    dr_docs_dir = \"storage/vectordb_training/datarobot_docs/en/\"\n    dr_security_dir = \"storage/vectordb_training/datarobot_docs/\"\n    rfp_docs_dir = \"storage/vectordb_training/reprocessed_data/\"\n    links = []\n    for link in locs:\n        if link.startswith(dr_docs_dir):\n            link = link[len(dr_docs_dir) :]\n            link = \"https://docs.datarobot.com/en/docs/\" + link\n            link = link[:-2] + \"html\"  # Remove \"md\", add html\n        elif link.startswith(dr_security_dir):\n            link = \"https://www.datarobot.com/trustcenter/\"\n        elif link.startswith(rfp_docs_dir):\n            try:\n                file = open(link, \"r\")\n                file_contents = file.read()\n                link = re.search(r\"https://\\S+\", file_contents)\n                file.close()\n                link = link.group()\n            except AttributeError:\n                print(\"Attribute error\")\n                link = None\n                pass\n            except FileNotFoundError:\n                link = None\n                pass\n        links.append(link)\n    links = list(set([link for link in links if link != None]))\n    return links"
      ],
      "api_methods": [],
      "complexity_score": 0.5,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-2595276658478959517",
      "title": "Starting a Binary Classification Project",
      "content": "## Starting a Binary Classification Project\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Modeling Options\n### Scope\nThe scope of this notebook is to provide instructions on how to initiate a DataRobot project for a Binary Classification target using the Python API.\n\n### Background\nBinary classification is the task of classifying the elements of a given set into two groups.\n\nExamples:\n\n- A customer is a churner or not.\n- A loan is going to default or not.\n- A patient has a disease or not.\n\nMost commonly, the target column will have values:\n\n- 0/1\n- Yes/No\n- True/False\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport numpy as np\n```\n\n#### Import Dataset\nWe will be loading the breast cancer dataset. A very simple binary classification dataset that is available through sk-learn.\n\n```python\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()\n```\n\n#### Connect to DataRobot\nConnect to DataRobot using your credentials and your endpoint. Change input below accordingly.\n\n```python\ndr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')\n```\n\n#### Initiate Project\nI will be initiating a project calling the method <code>dr.Project.start</code>:\n* project_name: Name of project\n* source_data: Data source (Path to file or pandas dataframe)\n* target: String with target variable name\n* worker_count: Amount of workers to use\n* metric: Optimisation metric to use\n\n```python\nproject = dr.Project.start(project_name='MyBinaryClassificationProject',\n                        sourcedata= df,\n                        target='target')\n\nproject.wait_for_autopilot() #Wait for autopilot to complete\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Initiating Projects/Python/Starting a Binary Classification Project.ipynb",
      "tags": [
        "jupyter-notebook",
        "classification",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Binary Classification Project.ipynb",
        "size": 12194,
        "cell_count": 9,
        "code_cell_count": 4
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport numpy as np",
        "from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()",
        "dr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')",
        "project = dr.Project.start(project_name='MyBinaryClassificationProject',\n                        sourcedata= df,\n                        target='target')\n\nproject.wait_for_autopilot() #Wait for autopilot to complete"
      ],
      "api_methods": [
        "dr.project.start",
        "project.start",
        "project.wait_for_autopilot"
      ],
      "complexity_score": 0.95,
      "use_case_category": "modeling"
    },
    {
      "id": "github_notebook_-3743194081857964946",
      "title": "Starting a Multiclass Classification Project",
      "content": "## Starting a Multiclass Classification Project\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Modeling Options\n### Scope\nThe scope of this notebook is to provide instructions on how to initiate a DataRobot project for a Multiclass Classification target using the Python API.\n\n### Background\nMulticlass classification is the task of classifying the elements of a given set into more than two groups.\n\nExamples:\n\n- A customer would be more interested in one of A,B,C,D... products.\n- A patient has one of A,B,C,D... diseases.\n- A customer would have a higher propensity to respond to one of A,B,C,D... campaigns.\n\nMost commonly, the target column will have values:\n\n- AAA/BBB/CCC/...(example text)\n- 0/1/2/3/4/...\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport numpy as np\n```\n\n#### Import Dataset\nWe will be loading the iris dataset. A very simple Multiclass classification dataset that is available through sk-learn.\n\n```python\nfrom sklearn.datasets import load_iris\ndata = load_iris()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()\n```\n\n#### Connect to DataRobot\nConnect to DataRobot using your credentials and your endpoint. Change input below accordingly.\n\n```python\ndr.Client(token='YOUR_API_KEY}', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')\n```\n\n#### Initiate Project\nI will be initiating a project calling the method <code>dr.Project.start</code>:\n* project_name: Name of project\n* source_data: Data source (Path to file or pandas dataframe)\n* target: String with target variable name\n* worker_count: Amount of workers to use\n* metric: Optimization metric to use\n\nIf your target is categorical and has a cardinality of up to 10, we will automatically select a Multiclass target_type and that argument is not needed when calling Project.start. However, if the target is numerical and you would like to force it to be seen as a Multiclass project in DataRobot, you can specify the target_type as seen below:\n\n```python\nproject = dr.Project.start(project_name='MyMulticlassClassificationProject',\n                        sourcedata= df,\n                        target='target',\n                        target_type = dr.enums.TARGET_TYPE.MULTICLASS)\n\nproject.wait_for_autopilot() #Wait for autopilot to complete\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Initiating Projects/Python/Starting a Multiclass Classification Project.ipynb",
      "tags": [
        "jupyter-notebook",
        "classification",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Multiclass Classification Project.ipynb",
        "size": 7306,
        "cell_count": 9,
        "code_cell_count": 4
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport numpy as np",
        "from sklearn.datasets import load_iris\ndata = load_iris()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()",
        "dr.Client(token='YOUR_API_KEY}', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')",
        "project = dr.Project.start(project_name='MyMulticlassClassificationProject',\n                        sourcedata= df,\n                        target='target',\n                        target_type = dr.enums.TARGET_TYPE.MULTICLASS)\n\nproject.wait_for_autopilot() #Wait for autopilot to complete"
      ],
      "api_methods": [
        "dr.project.start",
        "dr.enums.target_type",
        "project.start",
        "project.wait_for_autopilot"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "github_notebook_-1787881562393938696",
      "title": "Starting a Project with Selected Blueprints",
      "content": "## Starting a Project with Selected Blueprints\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Modeling Options\n\n### Scope\nThe scope of this notebook is to provide instructions on how to initiate a DataRobot project manually where the user has the option to choose which models/blueprints he wants to initiate. The procedure below should work for any type of problem you are trying to solve (regression, classification, time series, etc).\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport numpy as np\nimport time\n```\n\n#### Import Dataset\nWe will be loading the breast cancer dataset. A very simple binary classification dataset that is available through sk-learn.\n\n```python\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()\n```\n\n#### Connect to DataRobot\nConnect to DataRobot using your credentials and your endpoint. Change input below accordingly.\n\n```python\ndr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')\n```\n\n#### Initiate project\nWe will be initiating the project using <code>autopilot_on = False</code>. This way DataRobot will not start building models until we specify which ones we want to build\n\n```python\nproject = dr.Project.start(project_name='MyBinaryClassificationProject',\n                        sourcedata= df,\n                        autopilot_on = False,\n                        target='target')\n```\n\n#### Find all of the blueprints\nWe can use the <code>get_blueprints</code> method to see all of the blueprints DataRobot generated.\n\n```python\nblueprints = project.get_blueprints()\n\n#Now that we have the Blueprints, we can search for a specific blueprint. \n#For example all models that have \"Gradient\" in their name\n\nmodels_to_run = []\nfor blueprint in blueprints:\n    if 'Gradient' in blueprint.model_type:\n        models_to_run.append(blueprint)\n        \n```\n\n```python\nmodels_to_run\n```\n\n#### Lets now initiate these models\nWe can use the <code>train</code> method to initiate modeling for a specific blueprint. By default, the feature list used will be the <code>informative features </code> list produced by DataRobot but you can define your own feature list and pass it on the <code>featurelist_id</code> variable.\n\n```python\nfor model in models_to_run:\n    project.train(model, sample_pct = 80, featurelist_id=None)\n```\n\n#### Waiting for job completion\nWe can use the <code>get_all_jobs</code> method to wait for the models to finish running\n\n```python\nwhile len(project.get_all_jobs()) > 0:\n    time.sleep(1)\n    pass\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Initiating Projects/Python/Starting a Project with Selected Blueprints.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Project with Selected Blueprints.ipynb",
        "size": 15309,
        "cell_count": 16,
        "code_cell_count": 8
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport numpy as np\nimport time",
        "from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()",
        "dr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')",
        "project = dr.Project.start(project_name='MyBinaryClassificationProject',\n                        sourcedata= df,\n                        autopilot_on = False,\n                        target='target')",
        "blueprints = project.get_blueprints()\n\n#Now that we have the Blueprints, we can search for a specific blueprint. \n#For example all models that have \"Gradient\" in their name\n\nmodels_to_run = []\nfor blueprint in blueprints:\n    if 'Gradient' in blueprint.model_type:\n        models_to_run.append(blueprint)\n        ",
        "models_to_run",
        "for model in models_to_run:\n    project.train(model, sample_pct = 80, featurelist_id=None)",
        "while len(project.get_all_jobs()) > 0:\n    time.sleep(1)\n    pass"
      ],
      "api_methods": [
        "project.get_all_jobs",
        "project.start",
        "project.train",
        "dr.project.start",
        "project.get_blueprints",
        "datarobot.rest.restclientobject"
      ],
      "complexity_score": 0.8999999999999999,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-1957219602647467811",
      "title": "Starting a Regression Project",
      "content": "## Starting a Regression Project\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Modeling Options\n### Scope\nThe scope of this notebook is to provide instructions on how to initiate a DataRobot project for a numerical target using the R API.\n\n### Background\nRegression Analysis is the task of predicting the value of a continuous target column.\n\nExamples:\n\n- Predict Life Time Value (LTV) of customer.\n- Predicting player performance.\n- Predicting house price.\n\nThe target column will always be a continuous numeric variable even though regression could also be applicable a discreet high cardinality variable.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport numpy as np\n```\n\n#### Import Dataset\nWe will be loading the Boston Housing dataset. A very simple dataset for regression that is available through sk-learn.\n\n```python\nfrom sklearn.datasets import load_boston\ndata = load_boston()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()\n```\n\n#### Connect to DataRobot\nConnect to DataRobot using your credentials and your endpoint. Change input below accordingly.\n\n```python\ndr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')\n```\n\n#### Initiate Project\nI will be initiating a project calling the method <code>dr.Project.start</code>:\n* project_name: Name of project\n* source_data: Data source (Path to file or pandas dataframe)\n* target: String with target variable name\n* worker_count: Amount of workers to use\n* metric: Optimisation metric to use\n\n```python\nproject = dr.Project.start(project_name='MyRegressionProject',\n                        sourcedata= df,\n                        target='target')\nproject.wait_for_autopilot() #Wait for autopilot to complete\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Initiating Projects/Python/Starting a Regression Project.ipynb",
      "tags": [
        "jupyter-notebook",
        "regression",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Regression Project.ipynb",
        "size": 8552,
        "cell_count": 9,
        "code_cell_count": 4
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport numpy as np",
        "from sklearn.datasets import load_boston\ndata = load_boston()\n\ndf = pd.DataFrame(np.c_[data['data'], data['target']],\n                  columns= np.append(data['feature_names'], ['target']))\ndf.head()",
        "dr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')",
        "project = dr.Project.start(project_name='MyRegressionProject',\n                        sourcedata= df,\n                        target='target')\nproject.wait_for_autopilot() #Wait for autopilot to complete"
      ],
      "api_methods": [
        "dr.project.start",
        "project.start",
        "project.wait_for_autopilot"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_3563947241488722260",
      "title": "Starting a Time Series Project",
      "content": "## Starting a Time Series Project\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Modeling Options\n\n### Scope\nThe scope of this notebook is to provide instructions on how to initiate a DataRobot project for a Time Series problem using the R API. This notebook also covers calendars and feature settings for time series projects.\n\n### Background\nTime series modeling is the use of a machine learning model to predict future values of the target column based on previously observed values.\n\nExamples:\n\n- Demand Forecasting\n- Staffing\n- Sales Forecast\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport numpy as np\n```\n\n#### Dataset\nFor this tutorial, we are going to generate some simple time series data.\n\n```python\nintervals = 10 * 24 * 60 #10 Days, 24 hours, 60 minutes for each our\ntime_index = pd.date_range('2020-01-01', periods=intervals, freq='T')\ndata = np.random.randn(intervals)\n\ndf = pd.DataFrame(pd.Series(data=data, index=time_index)) #Create pandas dataframe with data\ndf.reset_index(inplace=True) #Reset index\ndf.columns = ['date','target'] #Rename columns\ndf.head()\n```\n\n#### Connect to DataRobot\nConnect to DataRobot using your credentials and your endpoint. Change input below accordingly.\n\n```python\ndr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')\n```\n\n#### Initiate Time Series Project\nTo set up a time series project, use the new time series specific parameters found in <code>dr.DatetimePartitioningSpecification </code>:\n\n* use_time_series - set this to True to enable time series for the project.\n* default_to_known_in_advance - set this to True to default to treating all features as known in advance features. Otherwise they will not be handled as known in advance features. See the prediction documentation for more information.\n* feature_derivation_window_start - the offset into the past to the start of the feature derivation window.\n* feature_derivation_window_end - the offset into the past to the end of the feature derivation window.\n* forecast_window_start - the offset into the future to the start of the forecast window.\n* forecast_window_end - the offset into the future to the end of the forecast window.\n* feature_settings - A list of settings. Can be used to set individual features to “known in advance”.\n* treat_as_exponential - Used to specify whether to treat the data as an exponential trend, which will apply a log-transform. By default, set as “auto”, this can be inferred automatically. See possible values in TreatAsExponential.\n* differencing_method - Used to specify a differencing method to apply if data is stationary. By default, set as “auto”, this can be inferred automatically. See possible values in DifferenicngMethod.\n* periodicities - A list of periodicities of different timestamps, specified in a list of lists.\n* windows_basis_units - The unit to use for feature derivation and forecast windows. Defaults to the inferred time step. If \"ROW\", will define the window with a number of rows.\n\n```python\ndr.DatetimePartitioningSpecification()\n```\n\n```python\n#First create the project\nproject = dr.Project.create(project_name='MySingleTimeSeriesProject',\n                        sourcedata= df)\n\n#Define a DatetimePartitioningSpecification object\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True)\n\n# start the project, specifying the partitioning method\nproject.set_target(\n    target='target',\n    partitioning_method=partition\n)\nproject.wait_for_autopilot() #Wait for autopilot to complete\n```\n\n#### Feature Derivation Window\n\nThe Feature Derivation window represents the rolling window that is used to derive time series features and lags, relative to the Forecast Point. It is defined in terms of <code>feature_derivation_window_start</code> and <code>feature_derivation_window_end</code> which are integer values representing datetime offsets in terms of the unit of time (e.g. hours or days).\nThe Feature Derivation Window start and end must be less than or equal to zero, indicating they are positioned before the forecast point. Additionally, the window must be specified as an integer multiple of the time step which defines the expected difference in time units between rows in the data.\nEnough rows of historical data must be provided to cover the span of the effective Feature Derivation Window (which may be longer than the project’s Feature Derivation Window depending on the differencing settings chosen). The effective Feature Derivation Window of any model can be checked via the <code>Project.effective_feature_derivation_window_start</code> and <code>effective_feature_derivation_window_end</code> attributes of a datetime model. See <code>Project.get_datetime_models</code>.\nThe window is closed, meaning the edges are considered to be inside the window.\nThis information is added to your <code>DatetimePartitioningSpecification</code> call like so:\n\n\n```python\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    feature_derivation_window_start = -24,\n    feature_derivation_window_end = -12,\n    use_time_series= True)\n```\n\n#### Forecast Window\nThe Forecast Window represents the rolling window of future values to predict, relative to the Forecast Point. It is defined in terms of the <code>forecast_window_start</code> and <code>forecast_window_end</code>, which are positive integer values indicating datetime offsets in terms of the time unit (e.g. hours or days).\nThe Forecast Window start and end must be positive integers, indicating they are positioned after the forecast point. Additionally, the window must be specified as an integer multiple of the time step which defines the expected difference in time units between rows in the data.\nThe window is closed, meaning the edges are considered to be inside the window.\nThis information is added to your <code>DatetimePartitioningSpecification</code> call like so:\n\n```python\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    forecast_window_start = 1,\n    forecast_window_end = 7,\n    use_time_series= True)\n```\n\n#### Feature Settings - Enable/Disable known in advance\n\nWhen setting up a time series project, input features could be identified as known in advance features. These features are not used to generate lags, and are expected to be known for the rows in the forecast window at predict time (e.g. “how much money will have been spent on marketing”, “is this a holiday”).\nTo start a time series project, use <code>DatetimePartitioningSpecification</code> and specify the <code>feature_settings</code>. (Note that this is for illustrative purposes only - this project will not actually build because the 10 data points are smaller than the 100 datapoint minimum required.)\n\n```python\nsettings = [dr.FeatureSettings('known_in_advance_feature', known_in_advance=True)]\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True,\n    feature_settings = settings)\n```\n\n#### Feature Settings - Enable/Disable feature derivation\n\nDataRobot does a lot of good work to automatically derive features that may be useful (e.g., lags). However, from time to time, it may be useful to disable DataRobot’s automatic feature engineering for a particular feature (e.g., so you can derive lags yourself manually). To do this, we can use the <code>feature_settings</code> to turn off derived features for a particular base feature:\n\n```python\nsettings = [dr.FeatureSettings('do_not_derive_feature', do_not_derive=True)]\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True,\n    feature_settings = settings)\n```\n\n#### Multiseries\nThe API also supports <strong>multiseries</strong>, or data with multiple time series delineated by multiseries ID columns. To create this, create a project, then create a datetime partition specification that specifies the <code>datetime_partition_column</code> (the column with your date in it) and the <code>multiseries_id_columns</code> (a list of columns specifying the ids that delineate the multiseries).\n\n```python\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True,\n    multiseries_id_columns=['multiseries_id'])\n```\n\n#### Prediction Intervals\nFor each model, prediction intervals estimate the range of values DataRobot expects actual values of the target to fall within. They are similar to a confidence interval of a prediction, but are based on the residual errors measured during the backtesting for the selected model.\nNote that because calculation depends on the backtesting values, prediction intervals are not available for predictions on models that have not had all backtests completed. Additionally, prediction intervals are not available when the number of points per forecast distance is less than 10, due to insufficient data.\nIn a prediction request, users can specify a prediction intervals size, which specifies the desired probability of actual values falling within the interval range. Larger values are less precise, but more conservative. For example, specifying a size of 80 will result in a lower bound of 10% and an upper bound of 90%. More generally, for a specific <code>prediction_intervals_size</code>, the upper and lower bounds will be calculated as follows:\n<ul>\n<li>predictionIntervalUpperBound = 50% + (<code>prediction_intervals_size</code> / 2)</li>\n<li>predictionIntervalLowerBound = 50% - (<code>prediction_intervals_size</code> / 2)</li>\n</ul>\nTo view prediction intervals data for a prediction, the prediction needs to have been created using <code>request_predictions</code> and specifying <code>include_prediction_intervals = True</code>. The size for the prediction interval can be specified with the <code>prediction_intervals_size</code> parameter for the same function, and will default to 80 if left unspecified. Specifying these fields will result in prediction interval bounds being included in the retrieved prediction data for that request. See <code>request_predictions</code> for more details.\n\n#### Calendars\nA calendar is an external file that lists events for a time series, such as holidays. For example, we might consider this calendar:\n\n```python\ncalendar_raw = \"\"\"01/01/2019;New Years,04/01/2019;National Spaghetti Day,29/11/2019;Black Friday\"\"\"\ncalendar = pd.DataFrame([x.split(';') for x in calendar_raw.split(',')],columns=['date','event'])\ncalendar.head()\n```\n\nTo create a calendar file from a csv file:\n\n```python\ncalendar.to_csv('calendar.csv',index=False) #Save Pandas dataframe as csv first\ncalendar = dr.CalendarFile.create('calendar.csv')\n```\n\n#### Making a Time Series Project using a Calendar\nThe main point of having calendars is not to admire them, but to use them for time series modeling! To do this, make a datetime partition like you usually would and pass the calendar using the calendar parameter.\n\n```python\n# specify the calendar_id in the partitioning specification\ndatetime_spec = dr.DatetimePartitioningSpecification(\n    use_time_series=True,\n    datetime_partition_column='date'\n    calendar_id=calendar.id\n)\n```\n\n#### Listing all available calendars\nTo list all of the available calendars use the dr.CalendarFile.list command\n\n```python\ndr.CalendarFile.list()\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Initiating Projects/Python/Starting a Time Series Project.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Initiating Projects/Python/Starting a Time Series Project.ipynb",
        "size": 17955,
        "cell_count": 29,
        "code_cell_count": 14
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport numpy as np",
        "intervals = 10 * 24 * 60 #10 Days, 24 hours, 60 minutes for each our\ntime_index = pd.date_range('2020-01-01', periods=intervals, freq='T')\ndata = np.random.randn(intervals)\n\ndf = pd.DataFrame(pd.Series(data=data, index=time_index)) #Create pandas dataframe with data\ndf.reset_index(inplace=True) #Reset index\ndf.columns = ['date','target'] #Rename columns\ndf.head()",
        "dr.Client(token='YOUR_API_KEY', \n          endpoint='YOUR_DATAROBOT_HOSTNAME')",
        "dr.DatetimePartitioningSpecification()",
        "#First create the project\nproject = dr.Project.create(project_name='MySingleTimeSeriesProject',\n                        sourcedata= df)\n\n#Define a DatetimePartitioningSpecification object\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True)\n\n# start the project, specifying the partitioning method\nproject.set_target(\n    target='target',\n    partitioning_method=partition\n)\nproject.wait_for_autopilot() #Wait for autopilot to complete",
        "partition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    feature_derivation_window_start = -24,\n    feature_derivation_window_end = -12,\n    use_time_series= True)",
        "partition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    forecast_window_start = 1,\n    forecast_window_end = 7,\n    use_time_series= True)",
        "settings = [dr.FeatureSettings('known_in_advance_feature', known_in_advance=True)]\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True,\n    feature_settings = settings)",
        "settings = [dr.FeatureSettings('do_not_derive_feature', do_not_derive=True)]\npartition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True,\n    feature_settings = settings)",
        "partition = dr.DatetimePartitioningSpecification(\n    datetime_partition_column = 'date',\n    use_time_series= True,\n    multiseries_id_columns=['multiseries_id'])",
        "calendar_raw = \"\"\"01/01/2019;New Years,04/01/2019;National Spaghetti Day,29/11/2019;Black Friday\"\"\"\ncalendar = pd.DataFrame([x.split(';') for x in calendar_raw.split(',')],columns=['date','event'])\ncalendar.head()",
        "calendar.to_csv('calendar.csv',index=False) #Save Pandas dataframe as csv first\ncalendar = dr.CalendarFile.create('calendar.csv')",
        "# specify the calendar_id in the partitioning specification\ndatetime_spec = dr.DatetimePartitioningSpecification(\n    use_time_series=True,\n    datetime_partition_column='date'\n    calendar_id=calendar.id\n)",
        "dr.CalendarFile.list()"
      ],
      "api_methods": [
        "project.set_target",
        "project.create",
        "project.wait_for_autopilot",
        "project.get_datetime_models",
        "dr.project.create",
        "dr.calendarfile.create",
        "dr.calendarfile.list",
        "project.effective_feature_derivation_window_start"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-9201351742354967397",
      "title": "Batch Prediction API",
      "content": "## Batch Prediction API\n\n### Scope\n\nThe scope of this notebook is to provide instructions on how to use DataRobot's Batch Prediction API to get predictions out of a DataRobot deployed model\n\n### Background\n\nThe Batch Prediction API provides flexible options for intake and output when scoring large datasets using the prediction servers you have already deployed. The API is exposed through the DataRobot Public API and can be consumed using a REST-enabled client or the DataRobot Python Public API bindings.\n\nThe main features of the API include:\n\n- Flexible options for intake and output.\n- Support for streaming local files and the ability to start scoring while still uploading—while simultaneously downloading the results.\n- Ability to score large datasets from, and to, Amazon S3 buckets.\n- Connection to external data sources using JDBC with bidirectional streaming of scoring data and results.\n- A mix of intake and output options; for example, scoring from a local file to an S3 target.\n- Protection against prediction server overload with a concurrency control level option.\n- Inclusion of Prediction Explanations (with an option to add thresholds).\n- Support for passthrough columns to correlate scored data with source data.\n- Addition of prediction warnings in the output.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.26.0. \n\nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://docs.datarobot.com/en/docs/predictions/batch/batch-prediction-api/index.html\n\nIt is assumed you already have a DataRobot <code>Deployment</code> object.\n\n### Step 1: Connecting to DataRobot\n\nTo inititate scoring jobs through the Batch Prediction API, you need two things:\n\n- Connect to DataRobot through the `datarobot.Client` command\n- Have your `DEPLOYMENT_ID` string. Easiest way to find that is to just go through the User Interface and Copy the ID from the URL. For example in the below example, everything after `deployments/` is the ID of the deployment: `https://app.eu.datarobot.com/deployments/232315iijdfsafw`\n\n```python\nimport datarobot as dr\n\ndr.Client(endpoint='YOUR_ENDPOINT/api/v2', token='YOUR_TOKEN')\ndeployment_id = \"YOUR_DEPLOYMENT_ID\"\n```\n\n### Step 2: Confirming Ingestion and Output \n\nDataRobot's Batch Prediction API allows you to score data from and to multiple sources. You should take advantage of the `credentials` and `data sources` you have already established previously through the UI for easy scoring. `Credentials` are basically usernames and passwords while `data sources` are the database that you have previously established a connection, like snowflake.\n\nBelow is some example code on how to query the `credentials` and `data sources`.\n\n\nFull list of [input options](https://docs.datarobot.com/en/docs/predictions/batch/batch-prediction-api/intake-options.html)\n\nFull list of [output options](https://docs.datarobot.com/en/docs/predictions/batch/batch-prediction-api/output-options.html)\n\n```python\n# List all credentials\ndr.Credential.list()\n```\n\nOn the above example, you can see that I have quite a few credentials. I have my `GitHub` Credentials, some `SnowflakeCredentials` and `s3 credentials. The alphanumerics on the left is just the ID of the credential. I can use that ID to access the credentials through the API.\n\n```python\n# List all datastores\ndr.DataStore.list()\nprint(dr.DataStore.list()[0].id)\n```\n\nOn the above example, you can see a list of all the datastores (I only have a snowflake connection), and with a little bit of manipulation, I can also access the ID of each datastore.\n\n## Examples\n\nBelow, we show some examples on how to use the Batch Prediction API Script. The `intake_settings` and `output_settings` can change to your needs. This means that you can *mix and match* as much as you want to to get to the outcome you prefer. Syntax only needs to change to one part of the equation to achieve this.\n\n### Scoring from CSV to CSV\n\n```python\n#Scoring without Prediction Explanations\ndr.BatchPredictionJob.score(\n    deployment_id,\n    intake_settings={\n        'type': 'localFile',\n        'file': 'inputfile.csv' #Path or Pandas or file-like object\n    },\n    output_settings={\n        'type': 'localFile',\n        'file': 'outputfile.csv'\n    }\n)\n\n#Scoring With Prediction Explanations\ndr.BatchPredictionJob.score(\n    deployment_id,\n    intake_settings={\n        'type': 'localFile',\n        'file': 'inputfile.csv' #Path or Pandas or file-like object\n    },\n    output_settings={\n        'type': 'localFile',\n        'file': 'outputfile.csv'\n    },\n    \n    max_explanations=3 #Compute prediction explanations for this amount of features\n    \n)\n```\n\n### Scoring from S3 to S3\n\n```python\ndr.BatchPredictionJob.score(\n    deployment_id,\n    intake_settings={\n        'type': 's3',\n        'url': 's3://theos-test-bucket/lending_club_scoring.csv',\n        'credential_id': 'YOUR_CREDENTIAL_ID_FROM_ABOVE',\n    },\n    output_settings={\n        'type': 's3',\n        'url': 's3://theos-test-bucket/lending_club_scored2.csv',\n        'credential_id': 'YOUR_CREDENTIAL_ID_FROM_ABOVE'\n    }\n)\n```\n\n### Scoring from JDBC to JDBC\n\n```python\ndr.BatchPredictionJob.score(\n    deployment_id,\n    \n    intake_settings = {\n    'type': 'jdbc',\n    'table': 'table_name',\n    'schema': 'public',\n    'dataStoreId': data_store.id, #Put the Id of the datastore you want\n    'credentialId': cred.credential_id #put the credentials you want\n    },\n    \n    output_settings = {\n        'type': 'jdbc',\n        'table': 'table_name',\n        'schema': 'public',\n        'statementType': 'insert',\n        'dataStoreId': data_store.id,\n        'credentialId': cred.credential_id\n    }\n)\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Making Predictions/Python/Batch Prediction API.ipynb",
      "tags": [
        "predictions",
        "jupyter-notebook",
        "tutorial",
        "api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Making Predictions/Python/Batch Prediction API.ipynb",
        "size": 9401,
        "cell_count": 18,
        "code_cell_count": 6
      },
      "code_examples": [
        "import datarobot as dr\n\ndr.Client(endpoint='YOUR_ENDPOINT/api/v2', token='YOUR_TOKEN')\ndeployment_id = \"YOUR_DEPLOYMENT_ID\"",
        "# List all credentials\ndr.Credential.list()",
        "# List all datastores\ndr.DataStore.list()\nprint(dr.DataStore.list()[0].id)",
        "#Scoring without Prediction Explanations\ndr.BatchPredictionJob.score(\n    deployment_id,\n    intake_settings={\n        'type': 'localFile',\n        'file': 'inputfile.csv' #Path or Pandas or file-like object\n    },\n    output_settings={\n        'type': 'localFile',\n        'file': 'outputfile.csv'\n    }\n)\n\n#Scoring With Prediction Explanations\ndr.BatchPredictionJob.score(\n    deployment_id,\n    intake_settings={\n        'type': 'localFile',\n        'file': 'inputfile.csv' #Path or Pandas or file-like object\n    },\n    output_settings={\n        'type': 'localFile',\n        'file': 'outputfile.csv'\n    },\n    \n    max_explanations=3 #Compute prediction explanations for this amount of features\n    \n)",
        "dr.BatchPredictionJob.score(\n    deployment_id,\n    intake_settings={\n        'type': 's3',\n        'url': 's3://theos-test-bucket/lending_club_scoring.csv',\n        'credential_id': 'YOUR_CREDENTIAL_ID_FROM_ABOVE',\n    },\n    output_settings={\n        'type': 's3',\n        'url': 's3://theos-test-bucket/lending_club_scored2.csv',\n        'credential_id': 'YOUR_CREDENTIAL_ID_FROM_ABOVE'\n    }\n)",
        "dr.BatchPredictionJob.score(\n    deployment_id,\n    \n    intake_settings = {\n    'type': 'jdbc',\n    'table': 'table_name',\n    'schema': 'public',\n    'dataStoreId': data_store.id, #Put the Id of the datastore you want\n    'credentialId': cred.credential_id #put the credentials you want\n    },\n    \n    output_settings = {\n        'type': 'jdbc',\n        'table': 'table_name',\n        'schema': 'public',\n        'statementType': 'insert',\n        'dataStoreId': data_store.id,\n        'credentialId': cred.credential_id\n    }\n)"
      ],
      "api_methods": [
        "dr.credential.list",
        "dr.batchpredictionjob.score",
        "dr.datastore.list"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-3803477907995492916",
      "title": "Getting Predictions and Prediction Explanations",
      "content": "## Getting Predictions and Prediction Explanations\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Model Deployment\n### Scope\n\nThe scope of this notebook is to provide instructions on how to get predictions and prediction explanations out of a trained model using the Python API.\n\n### Background\n\nThe main ways you can get predictions out of DataRobot using Python would be the modeling API and the prediction API.\n\n**Modeling API**: You can use the modelling API if you use Python or R and there are multiple ways you can interact with it.\n\n**Prediction API**: Any project can be called with the Prediction API if you have prediction servers. This is a simple REST API. Click on a model in the UI, then \"Deploy Model\" and \"Activate now\". You'll have access to a Python code snippet to help you interact with it. You can also deploy the model through the python API.\n\n\nFor the purposes of this tutorial, we will focus on the Modeling API. Note that this particular method of scoring utilizes modeling workers. This means that if someone is using these workers for modeling, your prediction is going to have to wait. This method of scoring is good for testing but not for deployment. For actual deployment, please deploy the model as a REST API through DataRobot's UI or through the API.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model </code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\n```\n\n#### Requesting Predictions\n\nBefore actually requesting predictions, you should upload the dataset you wish to predict via <code>Project.upload_dataset</code>. Previously uploaded datasets can be seen under <code>Project.get_datasets</code>. When uploading the dataset you can provide the path to a local file, a file object, raw file content, a pandas.DataFrame object, or the url to a publicly available dataset.\n\n```python\n#Uploading prediction dataset\ndataset_from_path = project.upload_dataset('path/file')\n\n#Request predictions\npredict_job = model.request_predictions(dataset_from_path.id)\n\n#Waiting for prediction calculations\npredictions = predict_job.get_result_when_complete()\n\npredictions.head()\n```\n\n#### Requesting Prediction Explanations\nIn order to create PredictionExplanations for a particular model and dataset, you must first Compute feature impact for the model via <code>dr.Model.get_or_request_feature_impact()</code>\n\n```python\nmodel.get_or_request_feature_impact()\n\npei = dr.PredictionExplanationsInitialization.create(project.id, model.id)\n\n#Wait for results of Prediction Explanations\npei.get_result_when_complete()\n\npe_job = dr.PredictionExplanations.create(project.id, model.id,  dataset_from_path.id)\n\n#Waiting for Job to Complete\npe = pe_job.get_result_when_complete()\n\ndf_pe = pe.get_all_as_dataframe()\ndf_pe.head()\n```\n\n#### Time Series Projects Caveats\nPrediction datasets are uploaded as normal predictions. However, when uploading a prediction dataset, a new parameter forecastPoint can be specified. The forecast point of a prediction dataset identifies the point in time relative which predictions should be generated, and if one is not specified when uploading a dataset, the server will choose the most recent possible forecast point. The forecast window specified when setting the partitioning options for the project determines how far into the future from the forecast point predictions should be calculated.\n\n**Important Note**:\nWhen uploading a dataset for Time Series projects scoring, you need to include the actual values from previous dates depending on the feature derivation setup. For example, if feature derivation window is -10 to -1 days and you want to forecast sales for the next 3 days, your dataset would look like this:\n\n| date       | sales | Known_in_advance_feature |\n|------------|-------|--------------------------|\n| 01/01/2019 | 130   | AAA                      |\n| 02/01/2019 | 123   | VVV                      |\n| 03/01/2019 | 412   | BBB                      |\n| 04/01/2019 | 321   | DDD                      |\n| 05/01/2019 | 512   | DDD                      |\n| 06/01/2019 | 623   | VVV                      |\n| 07/01/2019 | 356   | CCC                      |\n| 08/01/2019 | 133   | AAA                      |\n| 09/01/2019 | 356   | CCC                      |\n| 10/01/2019 | 654   | DDD                      |\n| 11/01/2019 |       | BBB                      |\n| 12/01/2019 |       | CCC                      |\n| 13/01/2019 |       | DDD                      |\n\nDataRobot will detect your forecast point as 10/01/2019 and then it will calculate lag features and make predictions for the missing dates.\n\n#### Getting Predictions from a DataRobot Deployment\nIf you have used MLOps to deploy a model (DataRobot or Custom), you will have access to an API which you can call using an API Client. Below is a python script of an API Client. You can create your own API Client in the language of your choice!\n\n```python\n\"\"\"\nUsage:\n    python datarobot-predict.py <input-file.csv>\n \nThis example uses the requests library which you can install with:\n    pip install requests\nWe highly recommend that you update SSL certificates with:\n    pip install -U urllib3[secure] certifi\n\"\"\"\nimport sys\nimport json\nimport requests\n \nAPI_URL = 'Find this in Deployment -> Overview -> Summary -> Endpoint'\nAPI_KEY = 'YOUR_API_KEY'\nDATAROBOT_KEY = 'Find this in Deployment -> Predictions -> Prediction API -> Single mode -> on top of the code sample'\n \nDEPLOYMENT_ID = 'YOUR_DEPLOYMENT_ID'\nMAX_PREDICTION_FILE_SIZE_BYTES = 52428800  # 50 MB\n \n \nclass DataRobotPredictionError(Exception):\n    \"\"\"Raised if there are issues getting predictions from DataRobot\"\"\"\n \n \ndef make_datarobot_deployment_predictions(data, deployment_id):\n    \"\"\"\n    Make predictions on data provided using DataRobot deployment_id provided.\n    See docs for details:\n         https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html\n \n    Parameters\n    ----------\n    data : str\n        Feature1,Feature2\n        numeric_value,string\n    deployment_id : str\n        The ID of the deployment to make predictions with.\n \n    Returns\n    -------\n    Response schema:\n        https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html#response-schema\n \n    Raises\n    ------\n    DataRobotPredictionError if there are issues getting predictions from DataRobot\n    \"\"\"\n    # Set HTTP headers. The charset should match the contents of the file.\n    headers = {\n        'Content-Type': 'text/plain; charset=UTF-8',\n        'Authorization': 'Bearer {}'.format(API_KEY),\n        'DataRobot-Key': DATAROBOT_KEY,\n    }\n \n    url = API_URL.format(deployment_id=deployment_id)\n    # Make API request for predictions\n    predictions_response = requests.post(\n        url,\n        data=data,\n        headers=headers,\n    )\n    _raise_dataroboterror_for_status(predictions_response)\n    # Return a Python dict following the schema in the documentation\n    return predictions_response.json()\n \n \ndef _raise_dataroboterror_for_status(response):\n    \"\"\"Raise DataRobotPredictionError if the request fails along with the response returned\"\"\"\n    try:\n        response.raise_for_status()\n    except requests.exceptions.HTTPError:\n        err_msg = '{code} Error: {msg}'.format(\n            code=response.status_code, msg=response.text)\n        raise DataRobotPredictionError(err_msg)\n \n \ndef main(filename, deployment_id):\n    \"\"\"\n    Return an exit code on script completion or error. Codes > 0 are errors to the shell.\n    Also useful as a usage demonstration of\n    `make_datarobot_deployment_predictions(data, deployment_id)`\n    \"\"\"\n    if not filename:\n        print(\n            'Input file is required argument. '\n            'Usage: python datarobot-predict.py <input-file.csv>')\n        return 1\n    data = open(filename, 'rb').read()\n    data_size = sys.getsizeof(data)\n    if data_size >= MAX_PREDICTION_FILE_SIZE_BYTES:\n        print(\n            'Input file is too large: {} bytes. '\n            'Max allowed size is: {} bytes.'\n        ).format(data_size, MAX_PREDICTION_FILE_SIZE_BYTES)\n        return 1\n    try:\n        predictions = make_datarobot_deployment_predictions(data, deployment_id)\n    except DataRobotPredictionError as exc:\n        print(exc)\n        return 1\n    print(json.dumps(predictions, indent=4))\n    return 0\n \n \nif __name__ == \"__main__\":\n    filename = sys.argv[1]\n    sys.exit(main(filename, DEPLOYMENT_ID))\n \n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Making Predictions/Python/Getting Predictions and Prediction Explanations.ipynb",
      "tags": [
        "predictions",
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Making Predictions/Python/Getting Predictions and Prediction Explanations.ipynb",
        "size": 11833,
        "cell_count": 10,
        "code_cell_count": 4
      },
      "code_examples": [
        "import datarobot as dr",
        "#Uploading prediction dataset\ndataset_from_path = project.upload_dataset('path/file')\n\n#Request predictions\npredict_job = model.request_predictions(dataset_from_path.id)\n\n#Waiting for prediction calculations\npredictions = predict_job.get_result_when_complete()\n\npredictions.head()",
        "model.get_or_request_feature_impact()\n\npei = dr.PredictionExplanationsInitialization.create(project.id, model.id)\n\n#Wait for results of Prediction Explanations\npei.get_result_when_complete()\n\npe_job = dr.PredictionExplanations.create(project.id, model.id,  dataset_from_path.id)\n\n#Waiting for Job to Complete\npe = pe_job.get_result_when_complete()\n\ndf_pe = pe.get_all_as_dataframe()\ndf_pe.head()",
        "\"\"\"\nUsage:\n    python datarobot-predict.py <input-file.csv>\n \nThis example uses the requests library which you can install with:\n    pip install requests\nWe highly recommend that you update SSL certificates with:\n    pip install -U urllib3[secure] certifi\n\"\"\"\nimport sys\nimport json\nimport requests\n \nAPI_URL = 'Find this in Deployment -> Overview -> Summary -> Endpoint'\nAPI_KEY = 'YOUR_API_KEY'\nDATAROBOT_KEY = 'Find this in Deployment -> Predictions -> Prediction API -> Single mode -> on top of the code sample'\n \nDEPLOYMENT_ID = 'YOUR_DEPLOYMENT_ID'\nMAX_PREDICTION_FILE_SIZE_BYTES = 52428800  # 50 MB\n \n \nclass DataRobotPredictionError(Exception):\n    \"\"\"Raised if there are issues getting predictions from DataRobot\"\"\"\n \n \ndef make_datarobot_deployment_predictions(data, deployment_id):\n    \"\"\"\n    Make predictions on data provided using DataRobot deployment_id provided.\n    See docs for details:\n         https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html\n \n    Parameters\n    ----------\n    data : str\n        Feature1,Feature2\n        numeric_value,string\n    deployment_id : str\n        The ID of the deployment to make predictions with.\n \n    Returns\n    -------\n    Response schema:\n        https://app.eu.datarobot.com/docs/users-guide/predictions/api/new-prediction-api.html#response-schema\n \n    Raises\n    ------\n    DataRobotPredictionError if there are issues getting predictions from DataRobot\n    \"\"\"\n    # Set HTTP headers. The charset should match the contents of the file.\n    headers = {\n        'Content-Type': 'text/plain; charset=UTF-8',\n        'Authorization': 'Bearer {}'.format(API_KEY),\n        'DataRobot-Key': DATAROBOT_KEY,\n    }\n \n    url = API_URL.format(deployment_id=deployment_id)\n    # Make API request for predictions\n    predictions_response = requests.post(\n        url,\n        data=data,\n        headers=headers,\n    )\n    _raise_dataroboterror_for_status(predictions_response)\n    # Return a Python dict following the schema in the documentation\n    return predictions_response.json()\n \n \ndef _raise_dataroboterror_for_status(response):\n    \"\"\"Raise DataRobotPredictionError if the request fails along with the response returned\"\"\"\n    try:\n        response.raise_for_status()\n    except requests.exceptions.HTTPError:\n        err_msg = '{code} Error: {msg}'.format(\n            code=response.status_code, msg=response.text)\n        raise DataRobotPredictionError(err_msg)\n \n \ndef main(filename, deployment_id):\n    \"\"\"\n    Return an exit code on script completion or error. Codes > 0 are errors to the shell.\n    Also useful as a usage demonstration of\n    `make_datarobot_deployment_predictions(data, deployment_id)`\n    \"\"\"\n    if not filename:\n        print(\n            'Input file is required argument. '\n            'Usage: python datarobot-predict.py <input-file.csv>')\n        return 1\n    data = open(filename, 'rb').read()\n    data_size = sys.getsizeof(data)\n    if data_size >= MAX_PREDICTION_FILE_SIZE_BYTES:\n        print(\n            'Input file is too large: {} bytes. '\n            'Max allowed size is: {} bytes.'\n        ).format(data_size, MAX_PREDICTION_FILE_SIZE_BYTES)\n        return 1\n    try:\n        predictions = make_datarobot_deployment_predictions(data, deployment_id)\n    except DataRobotPredictionError as exc:\n        print(exc)\n        return 1\n    print(json.dumps(predictions, indent=4))\n    return 0\n \n \nif __name__ == \"__main__\":\n    filename = sys.argv[1]\n    sys.exit(main(filename, DEPLOYMENT_ID))\n "
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "model.id",
        "dr.predictionexplanationsinitialization.create",
        "project.id",
        "dr.predictionexplanations.create",
        "project.get_datasets",
        "model.request_predictions",
        "project.upload_dataset",
        "dr.model.get_or_request_feature_impact"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-2522314687480337125",
      "title": "Getting Confusion Chart",
      "content": "## Getting Confusion Chart\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Evaluating Models\n\n### Scope\nThe scope of this notebook is to provide instructions on how to get the Confusion Matrix Chart using the Python API. The Code below will work **only for Multiclass Classification Projects**. For Binary Classification Projects, use the <code>get_roc_curve</code> command.\n\n### Background\n\nThe confusion matrix for a multiclass classification project will be a 𝑁×𝑁 matrix, with the left axis showing the true class of an observation and the top axis showing the class assigned to the observation by the model. Each element 𝑖,𝑗 of the matrix would be the number of items with true class 𝑖 that were classified as being in class 𝑗.\n\nSome important definitions: \n\n**F1**: The F1 score for each class.\n\n**Precision**: The precision statistic for each class.\n\n**Recall**: The recall statistic for each class.\n\n**Actual Count**: The number of records for each class that actually are that class.\n\n**Predicted Count**: The number of times each class was predicted.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\n```\n\n#### Requesting Confusion Chart Data\n\n```python\ncc = model.get_confusion_chart(source='validation')\ncc.raw_data\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Getting Confusion Chart.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Getting Confusion Chart.ipynb",
        "size": 5427,
        "cell_count": 5,
        "code_cell_count": 2
      },
      "code_examples": [
        "import datarobot as dr",
        "cc = model.get_confusion_chart(source='validation')\ncc.raw_data"
      ],
      "api_methods": [
        "model.get_confusion_chart"
      ],
      "complexity_score": 0.385,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_856247675677913610",
      "title": "Getting Feature Impact",
      "content": "## Getting Feature Impact\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Evaluating Models\n\n### Scope\nThe scope of this notebook is to provide instructions on how to get Feature Impact scores of a specific model using the Python API.\n\n### Background\nFeature Impact informs us of how important a feature is in a model.  This is calcualted using a model agnostic approach. Feature Impact is normalised so that the most impactful feature will always have a feature impact score of 100%, while the other features impact scores are set relative to that top predictor.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n\n#### Requesting Feature Impact\n\n```python\n#Get Feature Impact\nfeature_impact = model.get_or_request_feature_impact()\n\n#Save feature impact in pandas dataframe\nfi_df = pd.DataFrame(feature_impact)\n```\n\n#### Plotting Feature Impact\n\n```python\nfig, ax = plt.subplots(figsize = (12,5))\n\n#Plot feature impact\nsns.barplot(x='featureName', y='impactNormalized', data=fi_df[0:5], color='g')\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Getting Feature Impact.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Getting Feature Impact.ipynb",
        "size": 16047,
        "cell_count": 7,
        "code_cell_count": 3
      },
      "code_examples": [
        "import datarobot as dr\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt",
        "#Get Feature Impact\nfeature_impact = model.get_or_request_feature_impact()\n\n#Save feature impact in pandas dataframe\nfi_df = pd.DataFrame(feature_impact)",
        "fig, ax = plt.subplots(figsize = (12,5))\n\n#Plot feature impact\nsns.barplot(x='featureName', y='impactNormalized', data=fi_df[0:5], color='g')"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact"
      ],
      "complexity_score": 0.6,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-2035808231199888097",
      "title": "Getting Lift Chart",
      "content": "## Getting Lift Chart\n\n**Author**: Matthew Cohen\n\n**Label**: Evaluating Models\n\n### Scope\nThe scope of this notebook is to provide instructions on how to retrieve the Lift Chart of a specific model using the Python API.\n\n### Background\nInsights provided by the Lift Chart are helpful in checking the performance of machine learning models. \n\nThe lift chart depicts how well a model segments the target population and how capable it is of predicting the target across the prediction distribution.  This allows you to evaluate the performance of a model at both the high end (very likely) and low end (very unlikely) end of the spectrum. \n\nFor example—so you can see how well the model performs for different ranges of values of the target variable. Looking at the Lift chart, the left side of the curve indicates where the model predicted a low score on one section of the population while the right side of the curve indicates where the model predicted a high score. In general, the steeper the actual line is, and the more closely the predicted line matches the actual line, the better the model is. A consistently increasing line is another good indicator.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n#Some color\ndr_dark_blue = '#08233F'\ndr_blue = '#1F77B4'\ndr_orange = '#FF7F0E'\ndr_red = '#BE3C28'\n```\n\n#### Requesting Lift Chart Data\n\nLift chart data can be retrieved for a specific data partition (validation, cross-validation, or holdout) using <code>get_lift_chart</code>. To retrieve the data for holdout partition, it needs to be unlocked first.\n\nLet’s retrieve the validation partition data for top model using <code>get_lift_chart</code>. The <code>get_lift_chart</code> method returns data for validation partition by default. We can retrieve data for specific data partition by passing value to source parameter in <code>get_lift_chart</code>.\n\n```python\nlc = model.get_lift_chart(source='validation')\n\n#Saving to pandas dataframe\nbins_df = pd.DataFrame(lc.bins)\nbins_df.head()\n```\n\nLet’s define our rebinning and plotting functions.\n\n```python\ndef rebin_df(raw_df, number_of_bins):\n    cols = ['bin', 'actual_mean', 'predicted_mean', 'bin_weight']\n    new_df = pd.DataFrame(columns=cols)\n    current_prediction_total = 0\n    current_actual_total = 0\n    current_row_total = 0\n    x_index = 1\n    bin_size = 60 / number_of_bins\n    for rowId, data in raw_df.iterrows():\n        current_prediction_total += data['predicted'] * data['bin_weight']\n        current_actual_total += data['actual'] * data['bin_weight']\n        current_row_total += data['bin_weight']\n\n        if ((rowId + 1) % bin_size == 0):\n            x_index += 1\n            bin_properties = {\n                'bin': ((round(rowId + 1) / 60) * number_of_bins),\n                'actual_mean': current_actual_total / current_row_total,\n                'predicted_mean': current_prediction_total / current_row_total,\n                'bin_weight': current_row_total\n            }\n\n            new_df = new_df.append(bin_properties, ignore_index=True)\n            current_prediction_total = 0\n            current_actual_total = 0\n            current_row_total = 0\n    return new_df\n\n\ndef matplotlib_lift(bins_df, bin_count, ax):\n    grouped = rebin_df(bins_df, bin_count)\n    ax.plot(range(1, len(grouped) + 1), grouped['predicted_mean'],\n            marker='+', lw=1, color=dr_blue)\n    ax.plot(range(1, len(grouped) + 1), grouped['actual_mean'],\n            marker='*', lw=1, color=dr_orange)\n    ax.set_xlim([0, len(grouped) + 1])\n    ax.set_facecolor(dr_dark_blue)\n    ax.legend(loc='best')\n    ax.set_title('Lift chart {} bins'.format(bin_count))\n    ax.set_xlabel('Sorted Prediction')\n    ax.set_ylabel('Value')\n    return grouped\n```\n\nNow we can show all lift charts we propose in DataRobot web application.\n\n- Note 1 : While this method will work for any bin count less then 60 - the most reliable result will be achieved when the number of bins is a divisor of 60.\n\n- Note 2 : This visualization method will NOT work for bin count > 60 because DataRobot does not provide enough information for a larger resolution.\n\n#### Plotting Lift Chart\n\n```python\nbin_counts = [10, 12, 15, 20, 30, 60]\nf, axarr = plt.subplots(len(bin_counts))\nf.set_size_inches((8, 4 * len(bin_counts)))\n\nrebinned_dfs = []\nfor i in range(len(bin_counts)):\n    rebinned_dfs.append(matplotlib_lift(bins_df, bin_counts[i], axarr[i]))\nplt.tight_layout()\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Getting Lift Chart.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Getting Lift Chart.ipynb",
        "size": 174806,
        "cell_count": 11,
        "code_cell_count": 4
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n#Some color\ndr_dark_blue = '#08233F'\ndr_blue = '#1F77B4'\ndr_orange = '#FF7F0E'\ndr_red = '#BE3C28'",
        "lc = model.get_lift_chart(source='validation')\n\n#Saving to pandas dataframe\nbins_df = pd.DataFrame(lc.bins)\nbins_df.head()",
        "def rebin_df(raw_df, number_of_bins):\n    cols = ['bin', 'actual_mean', 'predicted_mean', 'bin_weight']\n    new_df = pd.DataFrame(columns=cols)\n    current_prediction_total = 0\n    current_actual_total = 0\n    current_row_total = 0\n    x_index = 1\n    bin_size = 60 / number_of_bins\n    for rowId, data in raw_df.iterrows():\n        current_prediction_total += data['predicted'] * data['bin_weight']\n        current_actual_total += data['actual'] * data['bin_weight']\n        current_row_total += data['bin_weight']\n\n        if ((rowId + 1) % bin_size == 0):\n            x_index += 1\n            bin_properties = {\n                'bin': ((round(rowId + 1) / 60) * number_of_bins),\n                'actual_mean': current_actual_total / current_row_total,\n                'predicted_mean': current_prediction_total / current_row_total,\n                'bin_weight': current_row_total\n            }\n\n            new_df = new_df.append(bin_properties, ignore_index=True)\n            current_prediction_total = 0\n            current_actual_total = 0\n            current_row_total = 0\n    return new_df\n\n\ndef matplotlib_lift(bins_df, bin_count, ax):\n    grouped = rebin_df(bins_df, bin_count)\n    ax.plot(range(1, len(grouped) + 1), grouped['predicted_mean'],\n            marker='+', lw=1, color=dr_blue)\n    ax.plot(range(1, len(grouped) + 1), grouped['actual_mean'],\n            marker='*', lw=1, color=dr_orange)\n    ax.set_xlim([0, len(grouped) + 1])\n    ax.set_facecolor(dr_dark_blue)\n    ax.legend(loc='best')\n    ax.set_title('Lift chart {} bins'.format(bin_count))\n    ax.set_xlabel('Sorted Prediction')\n    ax.set_ylabel('Value')\n    return grouped",
        "bin_counts = [10, 12, 15, 20, 30, 60]\nf, axarr = plt.subplots(len(bin_counts))\nf.set_size_inches((8, 4 * len(bin_counts)))\n\nrebinned_dfs = []\nfor i in range(len(bin_counts)):\n    rebinned_dfs.append(matplotlib_lift(bins_df, bin_counts[i], axarr[i]))\nplt.tight_layout()"
      ],
      "api_methods": [
        "model.get_lift_chart"
      ],
      "complexity_score": 0.65,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_1843178667778606913",
      "title": "Getting Partial Dependence Plot",
      "content": "## Getting Partial Dependence Plot\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Evaluating Models\n\n### Scope\nThe scope of this notebook is to provide instructions on how to get the Partial Dependence Plot of a specific model using the Python API.\n\n### Background\nPartial dependence conveys how changes to the value of each feature change model predictions if everything else remained unchanged. You can find the partial dependence plot in the \"Feature Effects\" tab interface.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\nimport pandas as pd\n```\n\n#### Requesting Feature Effects (Partial Dependence)\n\n```python\nfeature_effects = model.request_feature_effect()\nfeature_effects.wait_for_completion() #Wait till feature effects computes\n```\n\n#### Plot Partial Dependence for a Specific Feature\n\n```python\ndef plot_partial_dependence_column(feature_effects, column):\n     \n    temp_pd = list(feature_effects.get_result())\n    for item in temp_pd:\n        if item['feature_name'] == column:\n            partial_dependence_data = item['partial_dependence']['data']\n            partial_dependence_data_df = pd.DataFrame(partial_dependence_data) #Save results into a pandas dataframe\n            partial_dependence_data_df.columns = ['target_probability','value']\n    plt.figure(figsize=(16, 6))\n    ax = sns.lineplot(x=\"value\", y=\"target_probability\", data=partial_dependence_data_df)#.set_title('Partial Dependence plot for %s'%column)\n    return partial_dependence_data_df\n    \n```\n\n```python\nplot_partial_dependence_column(feature_effects,'admission_type_id') #Fill in your own column\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Getting Partial Dependence Plot.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Getting Partial Dependence Plot.ipynb",
        "size": 50476,
        "cell_count": 8,
        "code_cell_count": 4
      },
      "code_examples": [
        "import datarobot as dr\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\nimport pandas as pd",
        "feature_effects = model.request_feature_effect()\nfeature_effects.wait_for_completion() #Wait till feature effects computes",
        "def plot_partial_dependence_column(feature_effects, column):\n     \n    temp_pd = list(feature_effects.get_result())\n    for item in temp_pd:\n        if item['feature_name'] == column:\n            partial_dependence_data = item['partial_dependence']['data']\n            partial_dependence_data_df = pd.DataFrame(partial_dependence_data) #Save results into a pandas dataframe\n            partial_dependence_data_df.columns = ['target_probability','value']\n    plt.figure(figsize=(16, 6))\n    ax = sns.lineplot(x=\"value\", y=\"target_probability\", data=partial_dependence_data_df)#.set_title('Partial Dependence plot for %s'%column)\n    return partial_dependence_data_df\n    ",
        "plot_partial_dependence_column(feature_effects,'admission_type_id') #Fill in your own column"
      ],
      "api_methods": [
        "model.request_feature_effect"
      ],
      "complexity_score": 0.65,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_6560183849374468923",
      "title": "Getting ROC Curve",
      "content": "## Getting ROC Curve\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Evaluating Models\n\n### Scope\nThe scope of this notebook is to provide instructions on how to get ROC Curve data of a specific model using the Python API.\n\n### Background\nInsights provided by the ROC Curve are helpful in evaluating the performance of machine learning models. \n\nThe receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n\n#### Requesting ROC Curve Data\n\nROC curve data can be generated for a specific data partition (validation, cross validation, or holdout) or for all the data partition using the <code>get_roc_curve</code> method.\n\nTo retrieve ROC curve information:\n\n```python\nroc = model.get_roc_curve('validation')\n\n#Save the result into a pandas dataframe\ndf = pd.DataFrame(roc.roc_points)\ndf.head()\n```\n\n#### Plotting ROC Curve\n\n```python\ndr_roc_green = '#03c75f'\nwhite = '#ffffff'\ndr_purple = '#65147D'\ndr_dense_green = '#018f4f'\ndr_dark_blue = '#08233F'\n\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=dr_dark_blue)\n\nplt.scatter(df.false_positive_rate, df.true_positive_rate, color=dr_roc_green)\nplt.plot(df.false_positive_rate, df.true_positive_rate, color=dr_roc_green)\nplt.plot([0, 1], [0, 1], color=white, alpha=0.25)\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate (Fallout)')\nplt.xlim([0, 1])\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.ylim([0, 1])\n```\n\n#### Threshold Operations\n\nYou can get the recommended threshold value with maximal F1 score. That is the same threshold that is preselected in DataRobot when you open the “ROC curve” tab.\n\n```python\nthreshold = roc.get_best_f1_threshold()\nthreshold\n```\n\nYou can also estimate metrics for different threshold values. This will produce the same results as updating the threshold on the DataRobot “ROC curve” tab.\n\n```python\nmetrics = roc.estimate_threshold(threshold)\nmetrics\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Getting ROC Curve.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Getting ROC Curve.ipynb",
        "size": 44124,
        "cell_count": 12,
        "code_cell_count": 5
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport matplotlib.pyplot as plt",
        "roc = model.get_roc_curve('validation')\n\n#Save the result into a pandas dataframe\ndf = pd.DataFrame(roc.roc_points)\ndf.head()",
        "dr_roc_green = '#03c75f'\nwhite = '#ffffff'\ndr_purple = '#65147D'\ndr_dense_green = '#018f4f'\ndr_dark_blue = '#08233F'\n\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=dr_dark_blue)\n\nplt.scatter(df.false_positive_rate, df.true_positive_rate, color=dr_roc_green)\nplt.plot(df.false_positive_rate, df.true_positive_rate, color=dr_roc_green)\nplt.plot([0, 1], [0, 1], color=white, alpha=0.25)\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate (Fallout)')\nplt.xlim([0, 1])\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.ylim([0, 1])",
        "threshold = roc.get_best_f1_threshold()\nthreshold",
        "metrics = roc.estimate_threshold(threshold)\nmetrics"
      ],
      "api_methods": [
        "model.get_roc_curve"
      ],
      "complexity_score": 0.65,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-6852719625834640063",
      "title": "Getting SHAP Values",
      "content": "",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Getting SHAP Values.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Getting SHAP Values.ipynb",
        "size": 4465684,
        "cell_count": 0,
        "code_cell_count": 0
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-8389008604716988425",
      "title": "Getting Word Cloud",
      "content": "## Getting Word Cloud\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Evaluating Models\n\n### Scope\nThe scope of this notebook is to provide instructions on how to get the Word Cloud of a specific model using the Python API.\n\n### Background\nThe word cloud is a type of insight available for some text-processing models for datasets containing text columns. You can get information about how the appearance of each ngram (word or sequence of words) in the text field affects the predicted target value.\n\nThis example will show you how to obtain word cloud data and visualize it, similar to how DataRobot visualizes the word cloud in the “Model Insights” tab interface.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nfrom colour import Color\nimport wordcloud\nimport numpy as np\nfrom datarobot.errors import ClientError\nimport matplotlib.pyplot as plt\n```\n\n#### Define some Colors\n\n```python\ncolors = [Color('#2458EB')]\ncolors.extend(list(Color('#2458EB').range_to(Color('#31E7FE'), 81))[1:])\ncolors.extend(list(Color('#31E7FE').range_to(Color('#8da0a2'), 21))[1:])\ncolors.extend(list(Color('#a18f8c').range_to(Color('#ffad9e'), 21))[1:])\ncolors.extend(list(Color('#ffad9e').range_to(Color('#d80909'), 81))[1:])\nwebcolors = [c.get_web() for c in colors]\n```\n\n#### Requesting Word Cloud\nRequest word cloud with <code>get_word_cloud</code> method.\n\n```python\nmodels = project.get_models()\n\nmodel_with_word_cloud = None\nfor model in models:\n    try:\n        model.get_word_cloud()\n        model_with_word_cloud = model\n        break\n    except ClientError as e:\n        if e.json['message'] and 'No word cloud data' in e.json['message']:\n            pass\n        else:\n            raise\n\nmodel_with_word_cloud\n```\n\n```python\nwc = model_with_word_cloud.get_word_cloud(exclude_stop_words=True)\n```\n\n#### Plotting Word Cloud\n\n```python\ndef word_cloud_plot(wc, font_path=None):\n    # Stopwords usually dominate any word cloud, so we will filter them out\n    dict_freq = {wc_word['ngram']: wc_word['frequency']\n                 for wc_word in wc.ngrams\n                 if not wc_word['is_stopword']}\n    dict_coef = {wc_word['ngram']: wc_word['coefficient']\n                 for wc_word in wc.ngrams}\n\n    def color_func(*args, **kwargs):\n        word = args[0]\n        palette_index = int(round(dict_coef[word] * 100)) + 100\n        r, g, b = colors[palette_index].get_rgb()\n        return 'rgb({:.0f}, {:.0f}, {:.0f})'.format(int(r * 255),\n                                                    int(g * 255),\n                                                    int(b * 255))\n\n    wc_image = wordcloud.WordCloud(stopwords=set(),\n                                   width=1024, height=1024,\n                                   relative_scaling=0.5,\n                                   prefer_horizontal=1,\n                                   color_func=color_func,\n                                   background_color=(0, 10, 29),\n                                   font_path=font_path).fit_words(dict_freq)\n    plt.imshow(wc_image, interpolation='bilinear')\n    plt.axis('off')\n```\n\n```python\nword_cloud_plot(wc)\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Getting Word Cloud.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Getting Word Cloud.ipynb",
        "size": 111267,
        "cell_count": 12,
        "code_cell_count": 6
      },
      "code_examples": [
        "import datarobot as dr\nfrom colour import Color\nimport wordcloud\nimport numpy as np\nfrom datarobot.errors import ClientError\nimport matplotlib.pyplot as plt",
        "colors = [Color('#2458EB')]\ncolors.extend(list(Color('#2458EB').range_to(Color('#31E7FE'), 81))[1:])\ncolors.extend(list(Color('#31E7FE').range_to(Color('#8da0a2'), 21))[1:])\ncolors.extend(list(Color('#a18f8c').range_to(Color('#ffad9e'), 21))[1:])\ncolors.extend(list(Color('#ffad9e').range_to(Color('#d80909'), 81))[1:])\nwebcolors = [c.get_web() for c in colors]",
        "models = project.get_models()\n\nmodel_with_word_cloud = None\nfor model in models:\n    try:\n        model.get_word_cloud()\n        model_with_word_cloud = model\n        break\n    except ClientError as e:\n        if e.json['message'] and 'No word cloud data' in e.json['message']:\n            pass\n        else:\n            raise\n\nmodel_with_word_cloud",
        "wc = model_with_word_cloud.get_word_cloud(exclude_stop_words=True)",
        "def word_cloud_plot(wc, font_path=None):\n    # Stopwords usually dominate any word cloud, so we will filter them out\n    dict_freq = {wc_word['ngram']: wc_word['frequency']\n                 for wc_word in wc.ngrams\n                 if not wc_word['is_stopword']}\n    dict_coef = {wc_word['ngram']: wc_word['coefficient']\n                 for wc_word in wc.ngrams}\n\n    def color_func(*args, **kwargs):\n        word = args[0]\n        palette_index = int(round(dict_coef[word] * 100)) + 100\n        r, g, b = colors[palette_index].get_rgb()\n        return 'rgb({:.0f}, {:.0f}, {:.0f})'.format(int(r * 255),\n                                                    int(g * 255),\n                                                    int(b * 255))\n\n    wc_image = wordcloud.WordCloud(stopwords=set(),\n                                   width=1024, height=1024,\n                                   relative_scaling=0.5,\n                                   prefer_horizontal=1,\n                                   color_func=color_func,\n                                   background_color=(0, 10, 29),\n                                   font_path=font_path).fit_words(dict_freq)\n    plt.imshow(wc_image, interpolation='bilinear')\n    plt.axis('off')",
        "word_cloud_plot(wc)"
      ],
      "api_methods": [
        "model.get_word_cloud",
        "project.get_models"
      ],
      "complexity_score": 0.75,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-1080788017867674547",
      "title": "Plotting Prediction Intervals for Time Series Projects",
      "content": "## Plotting Prediction Intervals for Time Series Projects\n\n**Author**: Lukas Innig\n\n**Label**: TIme Series\n\n### Scope\nThe scope of this notebook is to provide instructions on how to plot prediction intervals for time series projects. This script will work for both multi and single time series projects.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot Time Series <code>Project</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n%matplotlib inline\n```\n\n#### Connect to DR, Define Project and Model\n\nMake sure you pick a model other than \"Recommended for Deployment\" as there is no backtesting calculations for that model. You will also need to **load the dataset you used to create the project** in a pandas dataframe called <code>data</code>\n\n```python\ndr.Client(token = 'YOUR_API_KEY',\n         endpoint = 'YOUR_HOSTNAME')\n\ndata = pd.read_excel('path/to/file') #The data that was used for the project. change pd.read_ method according to file type\n\nproject = project = dr.Project.get('YOUR_PROJECT_ID') #Use this with project ID to get your project.\nmodel = [m for m in project.get_models()][1] #You can choose whichever model you wish other than the recommended for deployment\n```\n\n#### Define function that returns training predictions\nBelow function might take a while to finish running\n\n```python\ndef get_training_predictions(model):\n    print(f\"calculating training predictions for {model.model_type}\")\n    try:\n        backtests_pred_job = model.request_training_predictions(data_subset=dr.enums.DATA_SUBSET.ALL_BACKTESTS)\n        backtests_pred_job.wait_for_completion()\n        backtest_predictions = backtests_pred_job.get_result_when_complete().get_all_as_dataframe()\n    except:\n        pass\n    try:  \n        holdout_pred_job = model.request_training_predictions(data_subset=dr.enums.DATA_SUBSET.HOLDOUT)\n        holdout_pred_job.wait_for_completion()\n        holdout_predictions = holdout_pred_job.get_result_when_complete().get_all_as_dataframe()\n    except:\n        pass\n    all_predictions = dr.TrainingPredictions.list(model.project_id)\n\n    my_model_training_predictions = [p for p in all_predictions if p.model_id == model.id]\n    all_prediction_frames = [p.get_all_as_dataframe() for p in my_model_training_predictions]\n    if 'Holdout' in all_prediction_frames[0].partition_id.unique():\n        holdout_predictions = all_prediction_frames[0]\n        backtest_predictions = all_prediction_frames[1]\n    else:\n        holdout_predictions = all_prediction_frames[1]\n        backtest_predictions = all_prediction_frames[0]\n    return backtest_predictions, holdout_predictions\n```\n\n```python\nbacktest_predictions, holdout_predictions = get_training_predictions(model)\n```\n\n#### Get partitioning information\nThe script will figure out automatically if this is a single series or a multiseries project.\n\n```python\npartitioning = dr.DatetimePartitioning.get(project.id)\n\ntry:\n    series_id_column = partitioning.multiseries_id_columns[0].replace(' (actual)','')\n    is_multiseries = True\nexcept:\n    is_multiseries = False\n    print('This seems to be a single Series Project')\n    \ndate_column = partitioning.datetime_partition_column.replace(' (actual)','')\ntarget_column = project.target.replace(' (actual)','')\n```\n\n#### Define function that calculates errors\nBetween predictions made by DataRobot and actual values (with intervals).\n\n```python\ndef calculate_errors(backtest_preds, holdout_preds):\n    holdout_preds['timestamp'] = pd.to_datetime(holdout_preds['timestamp'], utc=False).dt.tz_localize(None)\n    holdout_preds['forecast_point'] = pd.to_datetime(holdout_preds['forecast_point'], utc=False).dt.tz_localize(None)\n    \n    if is_multiseries == True:\n        holdout_preds.rename(columns={'series_id':series_id_column, 'timestamp':date_column}, inplace=True)\n    else:\n        holdout_preds.rename(columns={'timestamp':date_column}, inplace=True)\n        \n    backtest_preds['timestamp'] = pd.to_datetime(backtest_preds['timestamp'], utc=False).dt.tz_localize(None)\n    backtest_preds['forecast_point'] = pd.to_datetime(backtest_preds['forecast_point'], utc=False).dt.tz_localize(None)\n    \n    if is_multiseries == True:\n        backtest_preds.rename(columns={'series_id':series_id_column, 'timestamp':date_column}, inplace=True)\n    else:\n        backtest_preds.rename(columns={'timestamp':date_column}, inplace=True)\n    \n    if is_multiseries == True:\n        holdout_preds_joined = holdout_preds.set_index([date_column, series_id_column]).join(data.set_index([date_column, series_id_column])).reset_index()\n        backtest_preds_joined = backtest_preds.set_index([date_column, series_id_column]).join(data.set_index([date_column, series_id_column])).reset_index()\n    else:\n        holdout_preds_joined = holdout_preds.set_index([date_column]).join(data.set_index([date_column])).reset_index()\n        backtest_preds_joined = backtest_preds.set_index([date_column]).join(data.set_index([date_column])).reset_index()\n    \n    if is_multiseries == True:\n        errors = backtest_preds_joined.groupby(['forecast_distance', series_id_column])[['prediction', target_column]].apply(lambda df: pd.Series(np.abs(df['prediction'] - df[target_column]).std()))\n\n        errors.columns=['prediction_interval']\n        holdout_preds_joined = holdout_preds_joined.set_index(['forecast_distance',series_id_column]).join(errors).reset_index()\n    else:\n        errors = backtest_preds_joined.groupby(['forecast_distance'])[['prediction', target_column]].apply(lambda df: pd.Series(np.abs(df['prediction'] - df[target_column]).std()))\n\n        errors.columns=['prediction_interval']\n        holdout_preds_joined = holdout_preds_joined.set_index(['forecast_distance']).join(errors).reset_index()\n\n    holdout_preds_joined['error_high'] = holdout_preds_joined['prediction'] + 2*holdout_preds_joined['prediction_interval']\n    holdout_preds_joined['error_low'] = holdout_preds_joined['prediction'] - 2*holdout_preds_joined['prediction_interval']\n    return holdout_preds_joined\n```\n\n```python\nprediction_data = calculate_errors(backtest_predictions, holdout_predictions)\n```\n\n#### Define function to plot prediction intervals\n\n```python\ndef plot_predictions(prediction_data, data, forecast_point, history):\n    if is_multiseries == True:\n        n_cols = data[series_id_column].drop_duplicates().shape[0]//2\n        fig, axs = plt.subplots(n_cols,2, figsize=(20,30))\n        plot_counter = 1\n        for c, df in prediction_data.groupby(series_id_column):\n            ax = plt.subplot(n_cols,2,plot_counter)\n            plot_counter += 1\n            pred_data_plot = df[df.forecast_point == forecast_point]\n            all_data_plot = data.loc[(data[series_id_column] == c) & (data.Date > pd.to_datetime(forecast_point) - history)]\n\n            g = sns.lineplot(data=pred_data_plot, x='Date', y='prediction', label='Prediction', color='green', linewidth=3)\n            ax.fill_between(pred_data_plot.Date.values, pred_data_plot.error_low, pred_data_plot.error_high, alpha=0.3, zorder=-10, color='lightgreen')\n\n            #sns.lineplot(data=df, x='Date', y='target')\n            g = sns.lineplot(data=all_data_plot[all_data_plot.Date <= forecast_point], x='Date', y=target_column, linewidth=3, color='blue', label='Actual')\n            g = sns.lineplot(data=all_data_plot[all_data_plot.Date > forecast_point], x='Date', y=target_column, linewidth=3, color='blue')\n            g.lines[2].set_linestyle(\"--\")\n            ax.axvline(forecast_point, linestyle='--', c='gray')\n\n            plt.xticks(rotation=45, ha='right')\n            plt.title(c)\n            #plt.show()\n        plt.tight_layout()\n        return fig\n    else:\n        df = prediction_data\n        fig, axs = plt.subplots(figsize=(15,7))\n        ax = plt.subplot(1,1,1)\n        pred_data_plot = df[df.forecast_point == forecast_point]\n        all_data_plot = data.loc[(data.Date > pd.to_datetime(forecast_point) - history)]\n\n        g = sns.lineplot(data=pred_data_plot, x='Date', y='prediction', label='Prediction', color='green', linewidth=3)\n        ax.fill_between(pred_data_plot.Date.values, pred_data_plot.error_low, pred_data_plot.error_high, alpha=0.3, zorder=-10, color='lightgreen')\n\n        #sns.lineplot(data=df, x='Date', y='target')\n        g = sns.lineplot(data=all_data_plot[all_data_plot.Date <= forecast_point], x='Date', y=target_column, linewidth=3, color='blue', label='Actual')\n        g = sns.lineplot(data=all_data_plot[all_data_plot.Date > forecast_point], x='Date', y=target_column, linewidth=3, color='blue')\n        g.lines[2].set_linestyle(\"--\")\n        ax.axvline(forecast_point, linestyle='--', c='gray')\n\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n```\n\n```python\nplot_predictions(prediction_data, data, forecast_point='2014-06-07', history=pd.Timedelta(1, 'M'))\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Evaluation/Python/Plotting Prediction Intervals for Time Series Projects.ipynb",
      "tags": [
        "jupyter-notebook",
        "evaluation",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Evaluation/Python/Plotting Prediction Intervals for Time Series Projects.ipynb",
        "size": 752351,
        "cell_count": 17,
        "code_cell_count": 9
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n%matplotlib inline",
        "dr.Client(token = 'YOUR_API_KEY',\n         endpoint = 'YOUR_HOSTNAME')\n\ndata = pd.read_excel('path/to/file') #The data that was used for the project. change pd.read_ method according to file type\n\nproject = project = dr.Project.get('YOUR_PROJECT_ID') #Use this with project ID to get your project.\nmodel = [m for m in project.get_models()][1] #You can choose whichever model you wish other than the recommended for deployment",
        "def get_training_predictions(model):\n    print(f\"calculating training predictions for {model.model_type}\")\n    try:\n        backtests_pred_job = model.request_training_predictions(data_subset=dr.enums.DATA_SUBSET.ALL_BACKTESTS)\n        backtests_pred_job.wait_for_completion()\n        backtest_predictions = backtests_pred_job.get_result_when_complete().get_all_as_dataframe()\n    except:\n        pass\n    try:  \n        holdout_pred_job = model.request_training_predictions(data_subset=dr.enums.DATA_SUBSET.HOLDOUT)\n        holdout_pred_job.wait_for_completion()\n        holdout_predictions = holdout_pred_job.get_result_when_complete().get_all_as_dataframe()\n    except:\n        pass\n    all_predictions = dr.TrainingPredictions.list(model.project_id)\n\n    my_model_training_predictions = [p for p in all_predictions if p.model_id == model.id]\n    all_prediction_frames = [p.get_all_as_dataframe() for p in my_model_training_predictions]\n    if 'Holdout' in all_prediction_frames[0].partition_id.unique():\n        holdout_predictions = all_prediction_frames[0]\n        backtest_predictions = all_prediction_frames[1]\n    else:\n        holdout_predictions = all_prediction_frames[1]\n        backtest_predictions = all_prediction_frames[0]\n    return backtest_predictions, holdout_predictions",
        "backtest_predictions, holdout_predictions = get_training_predictions(model)",
        "partitioning = dr.DatetimePartitioning.get(project.id)\n\ntry:\n    series_id_column = partitioning.multiseries_id_columns[0].replace(' (actual)','')\n    is_multiseries = True\nexcept:\n    is_multiseries = False\n    print('This seems to be a single Series Project')\n    \ndate_column = partitioning.datetime_partition_column.replace(' (actual)','')\ntarget_column = project.target.replace(' (actual)','')",
        "def calculate_errors(backtest_preds, holdout_preds):\n    holdout_preds['timestamp'] = pd.to_datetime(holdout_preds['timestamp'], utc=False).dt.tz_localize(None)\n    holdout_preds['forecast_point'] = pd.to_datetime(holdout_preds['forecast_point'], utc=False).dt.tz_localize(None)\n    \n    if is_multiseries == True:\n        holdout_preds.rename(columns={'series_id':series_id_column, 'timestamp':date_column}, inplace=True)\n    else:\n        holdout_preds.rename(columns={'timestamp':date_column}, inplace=True)\n        \n    backtest_preds['timestamp'] = pd.to_datetime(backtest_preds['timestamp'], utc=False).dt.tz_localize(None)\n    backtest_preds['forecast_point'] = pd.to_datetime(backtest_preds['forecast_point'], utc=False).dt.tz_localize(None)\n    \n    if is_multiseries == True:\n        backtest_preds.rename(columns={'series_id':series_id_column, 'timestamp':date_column}, inplace=True)\n    else:\n        backtest_preds.rename(columns={'timestamp':date_column}, inplace=True)\n    \n    if is_multiseries == True:\n        holdout_preds_joined = holdout_preds.set_index([date_column, series_id_column]).join(data.set_index([date_column, series_id_column])).reset_index()\n        backtest_preds_joined = backtest_preds.set_index([date_column, series_id_column]).join(data.set_index([date_column, series_id_column])).reset_index()\n    else:\n        holdout_preds_joined = holdout_preds.set_index([date_column]).join(data.set_index([date_column])).reset_index()\n        backtest_preds_joined = backtest_preds.set_index([date_column]).join(data.set_index([date_column])).reset_index()\n    \n    if is_multiseries == True:\n        errors = backtest_preds_joined.groupby(['forecast_distance', series_id_column])[['prediction', target_column]].apply(lambda df: pd.Series(np.abs(df['prediction'] - df[target_column]).std()))\n\n        errors.columns=['prediction_interval']\n        holdout_preds_joined = holdout_preds_joined.set_index(['forecast_distance',series_id_column]).join(errors).reset_index()\n    else:\n        errors = backtest_preds_joined.groupby(['forecast_distance'])[['prediction', target_column]].apply(lambda df: pd.Series(np.abs(df['prediction'] - df[target_column]).std()))\n\n        errors.columns=['prediction_interval']\n        holdout_preds_joined = holdout_preds_joined.set_index(['forecast_distance']).join(errors).reset_index()\n\n    holdout_preds_joined['error_high'] = holdout_preds_joined['prediction'] + 2*holdout_preds_joined['prediction_interval']\n    holdout_preds_joined['error_low'] = holdout_preds_joined['prediction'] - 2*holdout_preds_joined['prediction_interval']\n    return holdout_preds_joined",
        "prediction_data = calculate_errors(backtest_predictions, holdout_predictions)",
        "def plot_predictions(prediction_data, data, forecast_point, history):\n    if is_multiseries == True:\n        n_cols = data[series_id_column].drop_duplicates().shape[0]//2\n        fig, axs = plt.subplots(n_cols,2, figsize=(20,30))\n        plot_counter = 1\n        for c, df in prediction_data.groupby(series_id_column):\n            ax = plt.subplot(n_cols,2,plot_counter)\n            plot_counter += 1\n            pred_data_plot = df[df.forecast_point == forecast_point]\n            all_data_plot = data.loc[(data[series_id_column] == c) & (data.Date > pd.to_datetime(forecast_point) - history)]\n\n            g = sns.lineplot(data=pred_data_plot, x='Date', y='prediction', label='Prediction', color='green', linewidth=3)\n            ax.fill_between(pred_data_plot.Date.values, pred_data_plot.error_low, pred_data_plot.error_high, alpha=0.3, zorder=-10, color='lightgreen')\n\n            #sns.lineplot(data=df, x='Date', y='target')\n            g = sns.lineplot(data=all_data_plot[all_data_plot.Date <= forecast_point], x='Date', y=target_column, linewidth=3, color='blue', label='Actual')\n            g = sns.lineplot(data=all_data_plot[all_data_plot.Date > forecast_point], x='Date', y=target_column, linewidth=3, color='blue')\n            g.lines[2].set_linestyle(\"--\")\n            ax.axvline(forecast_point, linestyle='--', c='gray')\n\n            plt.xticks(rotation=45, ha='right')\n            plt.title(c)\n            #plt.show()\n        plt.tight_layout()\n        return fig\n    else:\n        df = prediction_data\n        fig, axs = plt.subplots(figsize=(15,7))\n        ax = plt.subplot(1,1,1)\n        pred_data_plot = df[df.forecast_point == forecast_point]\n        all_data_plot = data.loc[(data.Date > pd.to_datetime(forecast_point) - history)]\n\n        g = sns.lineplot(data=pred_data_plot, x='Date', y='prediction', label='Prediction', color='green', linewidth=3)\n        ax.fill_between(pred_data_plot.Date.values, pred_data_plot.error_low, pred_data_plot.error_high, alpha=0.3, zorder=-10, color='lightgreen')\n\n        #sns.lineplot(data=df, x='Date', y='target')\n        g = sns.lineplot(data=all_data_plot[all_data_plot.Date <= forecast_point], x='Date', y=target_column, linewidth=3, color='blue', label='Actual')\n        g = sns.lineplot(data=all_data_plot[all_data_plot.Date > forecast_point], x='Date', y=target_column, linewidth=3, color='blue')\n        g.lines[2].set_linestyle(\"--\")\n        ax.axvline(forecast_point, linestyle='--', c='gray')\n\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()",
        "plot_predictions(prediction_data, data, forecast_point='2014-06-07', history=pd.Timedelta(1, 'M'))"
      ],
      "api_methods": [
        "model.request_training_predictions",
        "model.id",
        "project.target",
        "model.project_id",
        "project.id",
        "dr.enums.data_subset",
        "dr.project.get",
        "model.model_type",
        "project.get",
        "dr.datetimepartitioning.get",
        "project.get_models",
        "dr.trainingpredictions.list"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_6019121443223588925",
      "title": "Manage-models-in-production",
      "content": "# Manage models in production\n\nThis code example shows how to manage and monitor models deployed to production environments using DataRobot's Python client to accomplish various tasks: model deployment, replacement, deletion, and monitoring.\n\nDownload this notebook from the [code examples home page](index).\n\n## Requirements\n\n* Python version 3.7.3+\n* DataRobot API version 2.19.0+\n* A DataRobot `Project` object\n* A DataRobot `Model` object\n\n### Import libraries\n\n```python\nimport datarobot as dr\n```\n\n### Create a deployment\n\nWhen creating a new deployment, you must provide a DataRobot model ID. The model ID represents a single instance of a model type, feature list, and sample size, used to differentiate models from the blueprint ID. The model ID is generated during Autopilot. Obtain the model ID from the UI by selecting the model to deploy from the Leaderboard and copying the string after `/models/` in the URL.\n\nA prediction server makes predictions against a deployment and is required for each deployment. Use the default prediction server unless you are an Enterprise user, in which case you should use a preconfigured prediction server instead.\n\n```python\n# Get a list of prediction servers\nprediction_server = dr.PredictionServer.list()[0]\n\nproject = dr.Project.get('<project_id>') # Provide your project object here\nmodel = project.get_models()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model.id, label='New Deployment', description='A new deployment',\n    default_prediction_server_id=prediction_server.id)\ndeployment\n```\n\n### List available deployments\n\nUse the following command to list all available deployments.\n\n```python\ndeployments = dr.Deployment.list()\ndeployments\n```\n\nYou can filter the returned deployments by passing an instance of the `DeploymentListFilters` class to the `filters` keyword argument:\n\n```python\nfilters = dr.models.deployment.DeploymentListFilters(\n    role='OWNER',\n    accuracy_health=dr.enums.DEPLOYMENT_ACCURACY_HEALTH_STATUS.FAILING\n)\ndeployments = dr.Deployment.list(filters=filters)\ndeployments\n```\n\n### Retrieve a deployment\n\nRetrieve a specific deployment by specifying its [deployment ID](https://docs.datarobot.com/en/docs/predictions/predapi/dep-pred.html#predictions-for-deployments), rather than listing all deployments.\n\n```python\ndeployment = dr.Deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID\n```\n\n### Update a deployment\n\nUse the command below to update the label and description for a deployment.\n\n```python\ndeployment = dr.Deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID\ndeployment.update(label='New label')\n```\n\n### Delete a deployment\n\n```python\ndeployment = dr.Deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID\ndeployment.delete()\n```\n\n### Model replacement\n\nYou can replace a deployment's model without interrupting predictions. Model replacement is an asynchronous process which requires prepatory steps to complete the process. However, predictions made against the deployment will use the new model as soon as you initiate the replacement process. Note that the <code>replace_model()</code> function will not return until this process is fully completed.\n\nAlongside the new model's model ID, you must provide a reason for replacement, stored in the model history of the deployment for auditing purposes. An enum, <code>MODEL_REPLACEMENT_REASON</code>, is provided for this purpose, and all possible values are listed below:\n\n- MODEL_REPLACEMENT_REASON.ACCURACY\n- MODEL_REPLACEMENT_REASON.DATA_DRIFT\n- MODEL_REPLACEMENT_REASON.ERRORS\n- MODEL_REPLACEMENT_REASON.SCHEDULED_REFRESH\n- MODEL_REPLACEMENT_REASON.SCORING_SPEED\n- MODEL_REPLACEMENT_REASON.OTHER\n\nThe code below demonstrates an example of model replacement:\n\n```python\nfrom datarobot.enums import MODEL_REPLACEMENT_REASON\n\ndeployment=deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID\ndeployment.model['id'], deployment.model['type']\n\ndeployment.replace_model('<deployment_id>', MODEL_REPLACEMENT_REASON.ACCURACY) #Provide the new model ID\ndeployment.model['id'], deployment.model['type']\n```\n\n#### Model validation (pre-replacement)\n\nBefore initiating a model replacement request, DataRobot recommends using the <code>validate_replacement_model()</code> function to verify if the new model can be used as a replacement.\n\nThe <code>validate_replacement_model()</code> function returns the validation status, a message, and a checks dictionary. If the status is `passing` or `warning`, use <code>replace_model()</code> to replace the model. If the status is <code>failing</code>, refer to the checks dictionary for information as to why the new model cannot be used as a replacement.\n\n\n```python\ndeployment = dr.Deployment.get(deployment_id='<deployment_id>')\nstatus, message, checks = deployment.validate_replacement_model(new_model_id=model.id)\nstatus\n\n# `checks` can be inspected for detail, showing two examples here:\nchecks['target']\nchecks['permission']\n```\n\n## Model monitoring\n\nDeployment monitoring can be summarized by its three major components: [service health](https://docs.datarobot.com/en/docs/mlops/monitor/service-health.html), [data drift](https://docs.datarobot.com/en/docs/mlops/monitor/data-drift.html#data-drift-tab), and [accuracy](https://docs.datarobot.com/en/docs/mlops/monitor/deploy-accuracy.html). For a Deployment object, DataRobot provides `get` functions that allows you to query all of the monitoring data. Alternatively, you can retrieve monitoring data directly using a deployment ID.\n\n\n```python\nfrom datarobot.models import Deployment, ServiceStats\n\ndeployment_id = '<deployment_id>'\n\n# call `get` functions on a `Deployment` object\ndeployment = Deployment.get(deployment_id)\nservice_stats = deployment.get_service_stats()\n\n# directly fetch without a `Deployment` object\nservice_stats = ServiceStats.get(deployment_id)\n```\n\nWhen querying monitoring data, you can optionally provide a start and end time. DataRobot accepts either a datetime object or a string. Note that only top of the hour datetimes are accepted, for example: `2019-08-01T00:00:00Z`. By default, the end time of the query will be the next top of the hour, and the start time will be 7 days before the end time.\n\nIn the \"over time\" variants, an optional `bucket_size` can be provided to specify the resolution of time buckets. For example, if the start time is `2019-08-01T00:00:00Z`, then the end time is `2019-08-02T00:00:00Z` and the `bucket_size` is `T1H`. In this case 24 time buckets are generated, each providing data calculated over one hour. Use `construct_duration_string()` to help construct a bucket size string.\n\n### Service health\n\nService health metrics capture a deployment’s ability to respond to prediction requests quickly and reliably. This helps identify bottlenecks and assess capacity, which is critical to proper provisioning. Use `SERVICE_STAT_METRIC.ALL` to retrieve a list of supported metrics.\n\n`ServiceStats` retrieves values for all service stats metrics; `ServiceStatsOverTime` can be used to fetch how one single metric changes over time.\n\n```python\nfrom datetime import datetime\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\ndeployment = Deployment.get(deployment_id='<deployment_id>')\nservice_stats = deployment.get_service_stats(\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15)\n)\nservice_stats[SERVICE_STAT_METRIC.TOTAL_PREDICTIONS]\n\ntotal_predictions = deployment.get_service_stats_over_time(\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15),\n    bucket_size=construct_duration_string(days=1),\n    metric=SERVICE_STAT_METRIC.TOTAL_PREDICTIONS\n)\ntotal_predictions.bucket_values\n```\n\n### Data drift\n\nAs training and production data change over time, a deployed model loses predictive power. The data surrounding the model is said to be drifting. By leveraging the training data and prediction data (also known as inference data) that is added to your deployment, data drift helps you to analyze a model's performance after it has been deployed.\n\nDeployment’s target drift and feature drift can be retrieved separately using `datarobot.models.TargetDrift` and `datarobot.models.FeatureDrift`. Use `DATA_DRIFT_METRIC.ALL` to retrieve a list of supported metrics.\n\n```python\nfrom datetime import datetime\nfrom datarobot.enums import DATA_DRIFT_METRIC\nfrom datarobot.models import Deployment, FeatureDrift\n\ndeployment = Deployment.get(deployment_id='<deployment_id>')\ntarget_drift = deployment.get_target_drift(\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15)\n)\nfeature_drift_data = FeatureDrift.list(\n    deployment_id='<deployment_id>',\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15),\n    metric=DATA_DRIFT_METRIC.HELLINGER\n)\nfeature_drift = feature_drift_data[0]\nfeature_drift.name\nfeature_drift.drift_score\n```\n\n### Accuracy\n\nAccuracy metrics help determine whether a model's quality is decaying and if you should consider replacing it. A collection of metrics are provided to measure the accuracy of a deployment’s predictions. For deployments with classification models, use `ACCURACY_METRIC.ALL_CLASSIFICATION` for all supported metrics. For deployments with regression models, use `ACCURACY_METRIC.ALL_REGRESSION` instead.\n\n`Accuracy` and `AccuracyOverTime` are provided to retrieve all default accuracy metrics and how one single metric change over time.\n\n```python\nfrom datetime import datetime\nfrom datarobot.enums import ACCURACY_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\ndeployment = Deployment.get(deployment_id='<deployment_id>')\naccuracy = deployment.get_accuracy(\n    start_time=datetime(2021, 8, 1, hour=15),\n    end_time=datetime(2021, 8, 1, 15, 0)\n)\naccuracy[ACCURACY_METRIC.RMSE]\n\nrmse = deployment.get_accuracy_over_time(\n    start_time=datetime(2021, 12, 1),\n    end_time=datetime(2021, 12, 3),\n    bucket_size=construct_duration_string(days=1),\n    metric=ACCURACY_METRIC.RMSE\n)\nrmse.bucket_values\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Management/Python/Manage-models-in-production.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Management/Python/Manage-models-in-production.ipynb",
        "size": 14769,
        "cell_count": 28,
        "code_cell_count": 13
      },
      "code_examples": [
        "import datarobot as dr",
        "# Get a list of prediction servers\nprediction_server = dr.PredictionServer.list()[0]\n\nproject = dr.Project.get('<project_id>') # Provide your project object here\nmodel = project.get_models()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model.id, label='New Deployment', description='A new deployment',\n    default_prediction_server_id=prediction_server.id)\ndeployment",
        "deployments = dr.Deployment.list()\ndeployments",
        "filters = dr.models.deployment.DeploymentListFilters(\n    role='OWNER',\n    accuracy_health=dr.enums.DEPLOYMENT_ACCURACY_HEALTH_STATUS.FAILING\n)\ndeployments = dr.Deployment.list(filters=filters)\ndeployments",
        "deployment = dr.Deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID",
        "deployment = dr.Deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID\ndeployment.update(label='New label')",
        "deployment = dr.Deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID\ndeployment.delete()",
        "from datarobot.enums import MODEL_REPLACEMENT_REASON\n\ndeployment=deployment.get(deployment_id='<deployment_id>') # Provide your own deployment ID\ndeployment.model['id'], deployment.model['type']\n\ndeployment.replace_model('<deployment_id>', MODEL_REPLACEMENT_REASON.ACCURACY) #Provide the new model ID\ndeployment.model['id'], deployment.model['type']",
        "deployment = dr.Deployment.get(deployment_id='<deployment_id>')\nstatus, message, checks = deployment.validate_replacement_model(new_model_id=model.id)\nstatus\n\n# `checks` can be inspected for detail, showing two examples here:\nchecks['target']\nchecks['permission']",
        "from datarobot.models import Deployment, ServiceStats\n\ndeployment_id = '<deployment_id>'\n\n# call `get` functions on a `Deployment` object\ndeployment = Deployment.get(deployment_id)\nservice_stats = deployment.get_service_stats()\n\n# directly fetch without a `Deployment` object\nservice_stats = ServiceStats.get(deployment_id)",
        "from datetime import datetime\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\ndeployment = Deployment.get(deployment_id='<deployment_id>')\nservice_stats = deployment.get_service_stats(\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15)\n)\nservice_stats[SERVICE_STAT_METRIC.TOTAL_PREDICTIONS]\n\ntotal_predictions = deployment.get_service_stats_over_time(\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15),\n    bucket_size=construct_duration_string(days=1),\n    metric=SERVICE_STAT_METRIC.TOTAL_PREDICTIONS\n)\ntotal_predictions.bucket_values",
        "from datetime import datetime\nfrom datarobot.enums import DATA_DRIFT_METRIC\nfrom datarobot.models import Deployment, FeatureDrift\n\ndeployment = Deployment.get(deployment_id='<deployment_id>')\ntarget_drift = deployment.get_target_drift(\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15)\n)\nfeature_drift_data = FeatureDrift.list(\n    deployment_id='<deployment_id>',\n    start_time=datetime(2019, 8, 1, hour=15),\n    end_time=datetime(2019, 8, 8, hour=15),\n    metric=DATA_DRIFT_METRIC.HELLINGER\n)\nfeature_drift = feature_drift_data[0]\nfeature_drift.name\nfeature_drift.drift_score",
        "from datetime import datetime\nfrom datarobot.enums import ACCURACY_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\ndeployment = Deployment.get(deployment_id='<deployment_id>')\naccuracy = deployment.get_accuracy(\n    start_time=datetime(2021, 8, 1, hour=15),\n    end_time=datetime(2021, 8, 1, 15, 0)\n)\naccuracy[ACCURACY_METRIC.RMSE]\n\nrmse = deployment.get_accuracy_over_time(\n    start_time=datetime(2021, 12, 1),\n    end_time=datetime(2021, 12, 3),\n    bucket_size=construct_duration_string(days=1),\n    metric=ACCURACY_METRIC.RMSE\n)\nrmse.bucket_values"
      ],
      "api_methods": [
        "deployment.get",
        "dr.models.deployment",
        "deployment.create_from_learning_model",
        "deployment.validate_replacement_model",
        "deployment.list",
        "dr.deployment.get",
        "deployment.replace_model",
        "deployment.get_accuracy",
        "model.id",
        "dr.predictionserver.list",
        "deployment.model",
        "datarobot.models.featuredrift",
        "dr.deployment.create_from_learning_model",
        "deployment.deploymentlistfilters",
        "deployment.get_service_stats",
        "project.get",
        "datarobot.helpers.partitioning_methods",
        "deployment.delete",
        "deployment.get_accuracy_over_time",
        "deployment.get_service_stats_over_time",
        "dr.deployment.list",
        "deployment.get_target_drift",
        "datarobot.models.targetdrift",
        "dr.project.get",
        "project.get_models",
        "dr.enums.deployment_accuracy_health_status",
        "deployment.update"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_2890942784265982988",
      "title": "Reuse Retrain Deploy",
      "content": "## REUSING BLUEPRINTS, HYPERPARAMETERS and DEPLOYING MODELS via API\n\n**Author**: Tim Whittaker\n\nThe point of this script is to illustrate the following\n<a id=\"toc\"></a>\n1. [Pull blueprint for a model from an existing project](#ebp)\n2. [Train that blueprint in a new project with a new data set](#tbp)\n3. [Deploy the model (or replace in a current deployment)](#deploy)\n4. [Keep the hyper parameters for step 2](#savehp)\n\n## Requirements\n* DataRobot Modeling API\nPlease us `pip install datarobot --upgrade` to get latest and greatest.  \n\n__This example assumes that you have built a project using the wine quality dataset, and the project id and a specific model id are available.  If not, see wine_autopilot.py__\n\n```python\n# # DataRobot upgrade command below if needed\n# !pip install datarobot --upgrade\n```\n\n```python\nimport pandas as pd\nimport datarobot as dr\nfrom config import *\nfrom datetime import datetime\nimport numpy as np\nimport yaml\n```\n\n## Get Data\n\n```python\n## get data\n## we are actually going to break this up and use half as new data\n## the project we are pulling from was built on the entire wine-quality dataset.\nwine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", delimiter=\";\")\nnp.random.seed(1)\nmsk = np.random.rand(len(wine)) < 0.75\nold_data = wine[msk]\nnew_data = wine[~msk]\n```\n\n```python\nnew_data.describe()\n```\n\n<a id=\"ebp\"></a>\n## Get an existing blueprint\n[Table of Contents](#toc) \n\n```python\n## config datarobot Client\n## Don't keep api token in script.  Place your token in a config file\n## so there is no concern of accidentally sharing.  See config.py\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n\n## original project id and original model id are accessible from the gui url\n## just click on desired model in leaderboard and pull approapriate ids from url\n## for example.\n## https://app.datarobot.com/projects/<project_id>/models/<model_id>/blueprint\noriginal_pid = \"5cf71ab5d9436e2c4d0c7a7b\"\noriginal_mid = \"5cf71c005ff3772856c2a81b\"\n\n## ============================================================================#\n## 1. Pull blueprint for a model from an existing project\n## all we now at this point is the original project id\n## as well as the model id we want to use.\n## the project id and model id are available in gui by clicking on the model\n## and pulling info from url, for example\n## https://app.datarobot.com/projects/<project_id>/models/<model_id>/blueprint\n\nre_orig_project = dr.Project.get(project_id=original_pid)\nblueprints = re_orig_project.get_blueprints()\nmodels = re_orig_project.get_models()\n\nxgb_model = [m for m in models if m.id == original_mid].pop()\n\nxgb_blueprint = [bp for bp in blueprints if bp.id == xgb_model.blueprint_id].pop()\n## instead of finding the particular blueprint, we could just use\n# xgb_blueprint = xgb_model.blueprint_id\n## be advised this returns a string and not an actual Blueprint object.\n## ============================================================================#\n\n```\n\n<a id=\"tbp\"></a>\n## Train that blueprint in a new project with a new data set\n[Table of Contents](#toc)\n\n```python\n## ============================================================================#\n## 2. Train on that blueprint in a new project with a new data set\n## one thing to consider - do we want the same exact set of hyperparameters and blueprint\n## used in the previous project (case a), or do we just want the same blue print (case b)\n## and let DataRobot figure out the new set of best hyperparameters for the data?\n## there is a chance it will learn the same hyperparameters on the new data.  \n\n## create a new project\nnew_project = dr.Project.create(sourcedata=new_data,\n                           project_name='new wine data {}'.format(datetime.now()))\nnew_project.set_target(target=\"quality\", mode=\"manual\")\n\n## here we are using the blueprint only.\n## as DataRobot runs the model is will select the best hyperparameters based on the\n## data.  It is entirely possible that DataRobot will select the same hyperparameters \n## as in the original project.  \nnew_project.train(xgb_blueprint, source_project_id=original_pid, sample_pct=64)\n## the following would also have worked\n# new_project.train(xgb_model.blueprint_id, source_project_id=original_pid, sample_pct=43)\nmodel_job = new_project.get_model_jobs()\ndone = model_job[0].get_result_when_complete()\n```\n\n```python\nnew_features = new_project.get_features()\norig_features = re_orig_project.get_features()\n\nnew_features = set( [ (f.name, f.feature_type) for f in new_features])\norig_features = set( [ (f.name, f.feature_type) for f in orig_features])\n\n```\n\n```python\nif orig_features.difference(new_features) != set():\n    print(\"features in new project are different from old project on basis of name and type\")\n```\n\n### Reuse hyperparameters from original project\n\n```python\n## now suppose that I wanted to use the same exact set of hyperparameters as used\n## in the original project.\nhyper_params = xgb_model.get_advanced_tuning_parameters()\n## PLEASE BE ADVISED: as of DR Python API 2.17, the \"default_value\" key contains\n## best of searched parameters.  This may change in a later version.\n## PLEASE BE ADVISED: some models aren't tunable, thus an exception will be tossed.\n## If `get_advanced_tuning_parameters` tosses an exception with a 500 internal\n## server error message, please reach out to support.\n\n## the best of searched hyperparameters.\nbest_hyper_params = dict([(param[\"parameter_id\"], param[\"default_value\"]) for param in hyper_params[\"tuning_parameters\"]])\nnew_xgb = new_project.get_models()[0]\nmodel_job = new_xgb.advanced_tune(best_hyper_params, description=\"hyperparameters from original project {}\".format(original_pid))\n## PLEASE BE ADVISED: an exception will be tossed if a model on the leaderboard with sameblueprint has the\n## same set of hyperparameters.  This means that when we trainined on the blueprint, and let DR learn they \n## hyperparameters, it learned the same set.  \nnew_xgb_tuned = model_job.get_result_when_complete()\n\n## next open the leaderboard browser to view the models.\n## the model with the old hyperparameters will have a description as set above.\nnew_project.open_leaderboard_browser()\n```\n\n<a id=\"deploy\"></a>\n## Deploy\n[Table of Contents](#toc)\n\n```python\n## 3. Deploy the model (or replace in a current deployment)\n## model deployment is available in python api 2.17.0 and this script will be\n## updated soon.\nprint(\"=\")\nprediction_server = dr.PredictionServer.list()[0]\nprediction_server.id\n\n## grab current deployments\ndeployments = dr.Deployment.list()\n\n## let's deploy the xgboost from the original project\ndeployment = dr.Deployment.create_from_learning_model(\n    xgb_model.id, label='xgBoost Model', description='A new deployment',\n    default_prediction_server_id=prediction_server.id)\n\nprint(deployment.id)  ## this is also available via gui url\ndeployment_id = deployment.id\n## clean up\ndel deployment, deployments\n```\n\n### Replace Deployment\n\n```python\n## oops, we should have deployed the new xgBoost model tuned with the\n## original models hyperparameters\nfrom datarobot.enums import MODEL_REPLACEMENT_REASON\n\ndeployment = dr.Deployment.get(deployment_id=deployment_id)\n\nprint(\"current deployment details\\n\\tmodel type:{}\\n\\tmodel id:{}\".format(deployment.model['type'],deployment.model['id']))\n\ndeployment.replace_model(new_xgb_tuned.id, MODEL_REPLACEMENT_REASON.OTHER)\n\nprint(\"new deployment details\\n\\tmodel type:{}\\n\\tmodel id:{}\".format(deployment.model['type'],deployment.model['id']))\n```\n\nExample of the new model on the leaderboard as well as a description.\n<img src=\"img/scree-grab.png\"></img>\n\n<a id=\"savehp\"></a>\n## Stash hyperparameters\n[Table of Contents](#toc)\n\n```python\ntry:\n    assert(yaml.__version__ == \"5.1\")\nexcept:\n    print(\"loading hyperparameters from yaml may throw and exception.  Try setting Loader=None\")\n```\n\n```python\n## 4. Keep the hyper parameters for step 2\n## In any event, regardless of which model we want to keep we can easily store hyperparameters on disk\n## options include yaml, pickle, etc.  Yaml example below.\n\nwith open(\"model_hyperparameters.yaml\", \"w\") as f:\n    f.write( yaml.dump(hyper_params))\n\n## load hyperparameters back into python dictionary.\nwith open(\"model_hyperparameters.yaml\", \"r\") as f:\n    hyperparams_dict = yaml.load(f, Loader=yaml.FullLoader)\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Management/Python/Reuse Retrain Deploy.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Management/Python/Reuse Retrain Deploy.ipynb",
        "size": 21249,
        "cell_count": 24,
        "code_cell_count": 13
      },
      "code_examples": [
        "# # DataRobot upgrade command below if needed\n# !pip install datarobot --upgrade",
        "import pandas as pd\nimport datarobot as dr\nfrom config import *\nfrom datetime import datetime\nimport numpy as np\nimport yaml",
        "## get data\n## we are actually going to break this up and use half as new data\n## the project we are pulling from was built on the entire wine-quality dataset.\nwine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", delimiter=\";\")\nnp.random.seed(1)\nmsk = np.random.rand(len(wine)) < 0.75\nold_data = wine[msk]\nnew_data = wine[~msk]",
        "new_data.describe()",
        "## config datarobot Client\n## Don't keep api token in script.  Place your token in a config file\n## so there is no concern of accidentally sharing.  See config.py\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n\n## original project id and original model id are accessible from the gui url\n## just click on desired model in leaderboard and pull approapriate ids from url\n## for example.\n## https://app.datarobot.com/projects/<project_id>/models/<model_id>/blueprint\noriginal_pid = \"5cf71ab5d9436e2c4d0c7a7b\"\noriginal_mid = \"5cf71c005ff3772856c2a81b\"\n\n## ============================================================================#\n## 1. Pull blueprint for a model from an existing project\n## all we now at this point is the original project id\n## as well as the model id we want to use.\n## the project id and model id are available in gui by clicking on the model\n## and pulling info from url, for example\n## https://app.datarobot.com/projects/<project_id>/models/<model_id>/blueprint\n\nre_orig_project = dr.Project.get(project_id=original_pid)\nblueprints = re_orig_project.get_blueprints()\nmodels = re_orig_project.get_models()\n\nxgb_model = [m for m in models if m.id == original_mid].pop()\n\nxgb_blueprint = [bp for bp in blueprints if bp.id == xgb_model.blueprint_id].pop()\n## instead of finding the particular blueprint, we could just use\n# xgb_blueprint = xgb_model.blueprint_id\n## be advised this returns a string and not an actual Blueprint object.\n## ============================================================================#\n",
        "## ============================================================================#\n## 2. Train on that blueprint in a new project with a new data set\n## one thing to consider - do we want the same exact set of hyperparameters and blueprint\n## used in the previous project (case a), or do we just want the same blue print (case b)\n## and let DataRobot figure out the new set of best hyperparameters for the data?\n## there is a chance it will learn the same hyperparameters on the new data.  \n\n## create a new project\nnew_project = dr.Project.create(sourcedata=new_data,\n                           project_name='new wine data {}'.format(datetime.now()))\nnew_project.set_target(target=\"quality\", mode=\"manual\")\n\n## here we are using the blueprint only.\n## as DataRobot runs the model is will select the best hyperparameters based on the\n## data.  It is entirely possible that DataRobot will select the same hyperparameters \n## as in the original project.  \nnew_project.train(xgb_blueprint, source_project_id=original_pid, sample_pct=64)\n## the following would also have worked\n# new_project.train(xgb_model.blueprint_id, source_project_id=original_pid, sample_pct=43)\nmodel_job = new_project.get_model_jobs()\ndone = model_job[0].get_result_when_complete()",
        "new_features = new_project.get_features()\norig_features = re_orig_project.get_features()\n\nnew_features = set( [ (f.name, f.feature_type) for f in new_features])\norig_features = set( [ (f.name, f.feature_type) for f in orig_features])\n",
        "if orig_features.difference(new_features) != set():\n    print(\"features in new project are different from old project on basis of name and type\")",
        "## now suppose that I wanted to use the same exact set of hyperparameters as used\n## in the original project.\nhyper_params = xgb_model.get_advanced_tuning_parameters()\n## PLEASE BE ADVISED: as of DR Python API 2.17, the \"default_value\" key contains\n## best of searched parameters.  This may change in a later version.\n## PLEASE BE ADVISED: some models aren't tunable, thus an exception will be tossed.\n## If `get_advanced_tuning_parameters` tosses an exception with a 500 internal\n## server error message, please reach out to support.\n\n## the best of searched hyperparameters.\nbest_hyper_params = dict([(param[\"parameter_id\"], param[\"default_value\"]) for param in hyper_params[\"tuning_parameters\"]])\nnew_xgb = new_project.get_models()[0]\nmodel_job = new_xgb.advanced_tune(best_hyper_params, description=\"hyperparameters from original project {}\".format(original_pid))\n## PLEASE BE ADVISED: an exception will be tossed if a model on the leaderboard with sameblueprint has the\n## same set of hyperparameters.  This means that when we trainined on the blueprint, and let DR learn they \n## hyperparameters, it learned the same set.  \nnew_xgb_tuned = model_job.get_result_when_complete()\n\n## next open the leaderboard browser to view the models.\n## the model with the old hyperparameters will have a description as set above.\nnew_project.open_leaderboard_browser()",
        "## 3. Deploy the model (or replace in a current deployment)\n## model deployment is available in python api 2.17.0 and this script will be\n## updated soon.\nprint(\"=\")\nprediction_server = dr.PredictionServer.list()[0]\nprediction_server.id\n\n## grab current deployments\ndeployments = dr.Deployment.list()\n\n## let's deploy the xgboost from the original project\ndeployment = dr.Deployment.create_from_learning_model(\n    xgb_model.id, label='xgBoost Model', description='A new deployment',\n    default_prediction_server_id=prediction_server.id)\n\nprint(deployment.id)  ## this is also available via gui url\ndeployment_id = deployment.id\n## clean up\ndel deployment, deployments",
        "## oops, we should have deployed the new xgBoost model tuned with the\n## original models hyperparameters\nfrom datarobot.enums import MODEL_REPLACEMENT_REASON\n\ndeployment = dr.Deployment.get(deployment_id=deployment_id)\n\nprint(\"current deployment details\\n\\tmodel type:{}\\n\\tmodel id:{}\".format(deployment.model['type'],deployment.model['id']))\n\ndeployment.replace_model(new_xgb_tuned.id, MODEL_REPLACEMENT_REASON.OTHER)\n\nprint(\"new deployment details\\n\\tmodel type:{}\\n\\tmodel id:{}\".format(deployment.model['type'],deployment.model['id']))",
        "try:\n    assert(yaml.__version__ == \"5.1\")\nexcept:\n    print(\"loading hyperparameters from yaml may throw and exception.  Try setting Loader=None\")",
        "## 4. Keep the hyper parameters for step 2\n## In any event, regardless of which model we want to keep we can easily store hyperparameters on disk\n## options include yaml, pickle, etc.  Yaml example below.\n\nwith open(\"model_hyperparameters.yaml\", \"w\") as f:\n    f.write( yaml.dump(hyper_params))\n\n## load hyperparameters back into python dictionary.\nwith open(\"model_hyperparameters.yaml\", \"r\") as f:\n    hyperparams_dict = yaml.load(f, Loader=yaml.FullLoader)"
      ],
      "api_methods": [
        "deployment.get",
        "deployment.create_from_learning_model",
        "dr.project.create",
        "deployment.list",
        "project.get_blueprints",
        "dr.deployment.get",
        "project.get_model_jobs",
        "deployment.replace_model",
        "model.id",
        "dr.predictionserver.list",
        "deployment.model",
        "dr.deployment.create_from_learning_model",
        "project.get",
        "project.train",
        "deployment.id",
        "project.set_target",
        "project.create",
        "model.blueprint_id",
        "model.get_advanced_tuning_parameters",
        "project.get_features",
        "dr.deployment.list",
        "project.open_leaderboard_browser",
        "dr.project.get",
        "project.get_models"
      ],
      "complexity_score": 1.0,
      "use_case_category": "deployment"
    },
    {
      "id": "github_notebook_-3431891966884131268",
      "title": "Sharing Projects",
      "content": "## Sharing Projects\n\n**Author**: Peter Simon\n\n### Scope\n\nThe scope of this notebook is to provide instructions on how to share projects with colleagues through the Python API.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object.\n\n#### Import libraries\n\n```python\nimport datarobot as dr\n```\n\n#### Define people to share projects with\n\n```python\n# who to share the project(s). Make sure the emails are valid.\nshare_targets = ['name.surname@datarobot.com', 'name.surname_2@datarobot.com']\n```\n\n#### Share project\nYou can share the project using the <code>datarobot.SharingAccess></code> command. There are multiple sharing roles:\n\n- <code> dr.enums.SHARING_ROLE.CONSUMER</code>\n- <code> dr.enums.SHARING_ROLE.OWNER</code>\n- <code> dr.enums.SHARING_ROLE.USER</code>\n\nso pick one according to your needs.\n\n```python\ntry:\n    for u in share_targets:\n        new_access = dr.SharingAccess(username=u, role=dr.enums.SHARING_ROLE.OWNER) #Multiple sharing roles exist\n        project.share([new_access])\n\nexcept dr.errors.ClientError as e:\n    print('skipped')\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Management/Python/Sharing Projects.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Management/Python/Sharing Projects.ipynb",
        "size": 2736,
        "cell_count": 7,
        "code_cell_count": 3
      },
      "code_examples": [
        "import datarobot as dr",
        "# who to share the project(s). Make sure the emails are valid.\nshare_targets = ['name.surname@datarobot.com', 'name.surname_2@datarobot.com']",
        "try:\n    for u in share_targets:\n        new_access = dr.SharingAccess(username=u, role=dr.enums.SHARING_ROLE.OWNER) #Multiple sharing roles exist\n        project.share([new_access])\n\nexcept dr.errors.ClientError as e:\n    print('skipped')"
      ],
      "api_methods": [
        "project.share",
        "dr.enums.sharing_role",
        "dr.errors.clienterror"
      ],
      "complexity_score": 0.6499999999999999,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-7000026332288451886",
      "title": "Uploading Actuals to a DataRobot Deployment",
      "content": "## Uploading Actuals to a DataRobot Deployment\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Model Deployment\n\n### Scope\nThe scope of this notebook is to provide instructions on how to upload actuals into the DataRobot platform in order to calculate accuracy metrics.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a deployed model and you know the id of the deployment.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nimport pandas as pd\n```\n\n#### Data Structure\nThe data that will be uploaded should have the below format:\n\n| association_id | actual_value |\n|----------------|--------------|\n| ΑΑΑ            | 0            |\n| AAB            | 1            |\n| AAC            | 0            |\n| AAD            | 0            |\n| AAE            | 0            |\n| AAF            | 1            |\n| AAG            | 1            |\n| AAH            | 0            |\n\nWith the **exact** column names.\n\n#### Uploading data\nWe will be using the <code>submit_actuals</code> method of the DataRobot <code>deployment</code> object to upload the actuals into the platform. DataRobot will calculate accuracy using the predicted values and the actual values just uploaded.\n\nFill in the deployment of your model\n\n```python\ndeployment_id = 'YOUR_DEPLOYMENT_ID'\nactuals = pd.read_csv('.path_to_file/file.csv')\n\ndeployment = dr.Deployment.get(deployment_id=deployment_id)\ndeployment.submit_actuals(actuals)\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Model Management/Python/Uploading Actuals to a DataRobot Deployment.ipynb",
      "tags": [
        "deployment",
        "jupyter-notebook",
        "datarobot",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Model Management/Python/Uploading Actuals to a DataRobot Deployment.ipynb",
        "size": 3032,
        "cell_count": 6,
        "code_cell_count": 2
      },
      "code_examples": [
        "import datarobot as dr\nimport pandas as pd",
        "deployment_id = 'YOUR_DEPLOYMENT_ID'\nactuals = pd.read_csv('.path_to_file/file.csv')\n\ndeployment = dr.Deployment.get(deployment_id=deployment_id)\ndeployment.submit_actuals(actuals)"
      ],
      "api_methods": [
        "deployment.submit_actuals",
        "dr.deployment.get",
        "deployment.get"
      ],
      "complexity_score": 0.8720000000000001,
      "use_case_category": "deployment"
    },
    {
      "id": "github_notebook_5808407769938195621",
      "title": "Advanced Tuning",
      "content": "## Advanced Tuning\n**Author**: Thodoris Petropoulos\n\n**Label**: Modeling Options\n### Scope\nThe scope of this notebook is to provide instructions on how to do advanced tuning using the Python API.\n\n### Background\n\nDataRobot is very good at choosing optimal hyperparameters for models to maximize speed and accuracy. However, sometimes we wish to change those hyperparameters ourselves.  This could be because we know something that DataRobot does not, we want to experiment with different approaches, or we have some other reason to use a particular parameter.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.19.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model</code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\n```\n\n### Advanced Tuning Interface\n\nThe easiest way to do advanced tuning is to set up a model and use the <code>start_advanced_tunning_session</code> method.\n\n```python\ntune = model.start_advanced_tuning_session()\n```\n\nThis function returns an object that you can use to see the default, current value, and possible values for each one of the parameters you can change.\n\n### Get Data on Parameters Available for Tuning\nIf you wish to see the underlying data of which parameters are available for tuning for a model and what their default, current, and possible values are, then you can turn to <code>get_parameters</code>:\n\n```python\ntune.get_parameters()\n```\n\nEach model’s blueprint consists of a series of tasks including both preprocessing steps and the model itself. Each task contains tunable parameters. Let’s take a look at the available (tunable) tasks:\n\n```python\ntune.get_task_names()\n```\n\nTo see all of the available hyperparameter options for the particular task, use the get_parameter_names method on the tune object:\n\n```python\ntask_name = tune.get_task_names()[2] #Save a task name by chance\ntune.get_parameter_names(task_name) # Get all of the hypermarameter options\n```\n\nset_parameter is a method used on the tune object that lets you fill in all the hyperparameters for your particular model. If you pass an XGB model, RunTune will have the hyperparameters to tune XGB (e.g., colsample_bytree), whereas if you pass an Elastic Net model, the function will instead have hyperparameters for Elastic Nets (e.g., lambda) instead.\n\n```python\nparameter_name = tune.get_parameter_names(task_name)[0] #Save a parameter name by chance\n\ntune.set_parameter(\n    task_name=task_name,\n    parameter_name=parameter_name,\n    value=1)\n```\n\nWhen you are finished setting all the different parameters you want to chance, start tuning with the run method\n\n```python\ntune.run()\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Advanced Tuning and Partitioning/Python/Advanced Tuning.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/Advanced Tuning.ipynb",
        "size": 20737,
        "cell_count": 18,
        "code_cell_count": 7
      },
      "code_examples": [
        "import datarobot as dr",
        "tune = model.start_advanced_tuning_session()",
        "tune.get_parameters()",
        "tune.get_task_names()",
        "task_name = tune.get_task_names()[2] #Save a task name by chance\ntune.get_parameter_names(task_name) # Get all of the hypermarameter options",
        "parameter_name = tune.get_parameter_names(task_name)[0] #Save a parameter name by chance\n\ntune.set_parameter(\n    task_name=task_name,\n    parameter_name=parameter_name,\n    value=1)",
        "tune.run()"
      ],
      "api_methods": [
        "model.start_advanced_tuning_session"
      ],
      "complexity_score": 0.7,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-5208697523384110929",
      "title": "AdvancedOptions object",
      "content": "### Using the AdvancedOptions object\n**Author:** Thodoris Petropoulos\n\n**Label:** Modeling Options\n\nScope\nThe scope of this notebook is to provide instructions on how to initiate a DataRobot project taking advantage of the `AdvancedOptions` object. The object has multiple options so refer to the official API documentation for a full overview.\n\nDataRobot API version >= 2.22.0. \n\nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\n```python\n#Import datarobot library (make sure >=2.22.0)\nimport datarobot as dr\nimport pandas as pd\n\n#Connect to DataRobot\ndr.Client(token='YOUR_API_TOKEN',\n          endpoint = 'YOUR_API_ENDPOINT')\n```\n\n```python\n#Create an advanced options object\nadvanced_options = dr.AdvancedOptions(accuracy_optimized_mb=True,\n                                      consider_blenders_in_recommendation=True)\n```\n\n```python\n#Load data\ndataset = pd.read_excel('readmissions.xlsx')\n```\n\n```python\n#Create project\nproject = dr.Project.create(project_name='MyBinaryClassificationProject',\n                        sourcedata= dataset)\n\n#Set target and pass on the AdvancedOptions object\nproject.set_target('readmitted', advanced_options=advanced_options)\n\n\nproject.wait_for_autopilot() #Wait for autopilot to complete\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Advanced Tuning and Partitioning/Python/AdvancedOptions object.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/AdvancedOptions object.ipynb",
        "size": 3743,
        "cell_count": 5,
        "code_cell_count": 4
      },
      "code_examples": [
        "#Import datarobot library (make sure >=2.22.0)\nimport datarobot as dr\nimport pandas as pd\n\n#Connect to DataRobot\ndr.Client(token='YOUR_API_TOKEN',\n          endpoint = 'YOUR_API_ENDPOINT')",
        "#Create an advanced options object\nadvanced_options = dr.AdvancedOptions(accuracy_optimized_mb=True,\n                                      consider_blenders_in_recommendation=True)",
        "#Load data\ndataset = pd.read_excel('readmissions.xlsx')",
        "#Create project\nproject = dr.Project.create(project_name='MyBinaryClassificationProject',\n                        sourcedata= dataset)\n\n#Set target and pass on the AdvancedOptions object\nproject.set_target('readmitted', advanced_options=advanced_options)\n\n\nproject.wait_for_autopilot() #Wait for autopilot to complete"
      ],
      "api_methods": [
        "project.set_target",
        "project.create",
        "project.wait_for_autopilot",
        "dr.project.create",
        "datarobot.rest.restclientobject"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-3836585663428662117",
      "title": "Optimizing Through API",
      "content": "## Controlling DataRobot Optimizer Through Python\n\nThis code shows how to interact to DataRobot Optimizer App using the example Lending Club dataset **\"Lending Club Sample 30.csv\"**.\nIn this example, we are trying to find the best combination of values for revol_util, inq_last_6mths, loan_amnt, and dti that minimizes the probability of a loan going bad.  \n\n### Result\n\nA dataframe with the original features, prediction, and the optimized features.  The optimized features have the prefix **opt_**\n\n\n### Assumptions\n\n1. A model has been deployed \n2. An Optimization App has been created\n3. The dataset and Jupyter notebook are in the same folder\n4. The outcome of the optimization is added to the original dataframe\n\n### Steps\n\n1. Change key_dict \n    1. Get the URL from the application (see figure below)\n    2. Put the values in key_dict\n    3. Put the name of the dataset in ts_settings[\"filename\"]\n2. Read file into a dataframe\n3. result_df = perform_optimization(data_df)\n\n\n### Functions\n\n*get_optimization* : makes a post request to perform optimization.  It returns optimized values for the constraint features, and the predicted target\n\n*get_constraints*  : accesses the constraint features and their ranges from the Optimizer App\n\n*create_constrain_from_df* : if you want to decide which features to constrain, change the  \"cfeatures\" list in ts_settings and provide a file to estimate the min and the max for these features\n\n*set_optimizer* : prepares the elements required by the Optimizer App\n\n\n\n### Get the URL with application id, and the token id from\nThe URL is available from the share icon in the top menu of the Optimization App.\n\n<img src=\"Picture1.gif\">\n\n```python\nimport requests  # you could also use aiohttp instead of requests to make calls asynchronously\nimport json\nimport pandas as pd\n\n'''\nYou get the ID and Token from the Optimizer App by clicking the share triangle (in the top menu of the UI)\n'''\n#https://606c32b7462514a41c85c492.apps.datarobot.com/settings?token=xMdWLVsq2C2dNvMCTMlbDzbZ6IcF_GwA4E76iYtJTGs&lang=en&theme=light\n\nkey_dict = {\n    'Complete_data': (\n                 '',  # ID\n                 '',  # Token\n                ),\n}\n#Set constraint features, and filename\n\nts_settings = {\"cfeatures\" :['revol_util','inq_last_6mths','loan_amnt','dti'],\n              \"filename\":\"Lending Club Sample 30.csv\"}\n\n```\n\n```python\ndef get_optimization(app, constraints):\n    '''\n    Given the credentials and data, get the best target under the constrained features\n    :param app: 'Complete_data' in key_dict\n    :param constraints: a dict containing the constrained features, the data, and the optimization type\n    :return: optimization performance, and values tried for each constrained feature\n    '''\n    app_id, token = key_dict[app]\n    url = f'https://{app_id}.apps.datarobot.com/api/optimize'\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    return requests.post(url, json=constraints, headers=headers).json()\n\n```\n\n```python\ndef get_constraints(key_dict):\n    '''\n    Get a list of constraint features and their minimum and maximum values from the Optimization App  \n    :param key_dict: the dictionary with the required app id and token\n    :return: a list of constraint features and their min and max\n    '''\n    app_id, token = key_dict['Complete_data']\n    url = f'https://{app_id}.apps.datarobot.com/api/application'\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    constraints = requests.get(url,headers=headers).json()\n    constrain_list = constraints['constraints']\n    [x.pop('feature_type') for x in constrain_list]\n    return constrain_list\n```\n\n```python\n#You can build your constraint list from full dataset\n#The min and the max are calculated from the full dataset\ndef create_constrain_from_df(df_tmp,cfeatures):\n    '''\n    Create a list of constraint features and their minimum and maximum values.  We assume we get the minimum and maximum \n    of the constraint features from a dataset\n    :param df_tmp: dataframe \n    :param cfeatures: the name of the constrained features\n    :return: a list of constraint features and their min and max\n    '''\n    constrain_list = []\n    for c_name in cfeatures:\n        c_dict = {'feature': c_name,'info':{'min': df_tmp[c_name].min(),'max':df_tmp[c_name].max(),'is_int':False}}\n        constrain_list.append(c_dict)\n    return constrain_list\n```\n\n```python\ndef set_optimizer(constrain_list,tmp1):\n    '''\n    Create the constraints dictionary:  It has to have the constraint list, and a row that need to be optimized\n    :param constrain_list: list of features and their constraints\n    :param tmp1: a row that has been changed to a JSON format\n    :return: a dict\n    '''\n    constraints = {\n        'settings': {\n        # Specify if you want to maximize or minimize\n            \"targetDirection\": 'max',\n\n            # Specify your flex feature ranges\n            'constraints': constrain_list,\n        },\n    #    'optimization': {'method': 'exhaust'},  # optional\n\n        # Specify your fixed values\n        'datapoint': tmp1,\n    }\n    return constraints\n```\n\n```python\ndef perform_optimization(df):\n    '''\n    Perform a batch optimization.  Add the results to the original dataframe\n    :param df: dataframe with the dataset to be optimized \n    :return: a dataframe with the original dataframe with the predicted and optimized features.  The optimized features  \n             name are prefixed with opt_ \n    '''\n    #create constrain_list\n    constrain_list = get_constraints(key_dict)\n    #create new features and set them to 0.00\n    df[\"opt_prediction\"] = 0.00\n    for feature in ts_settings[\"cfeatures\"]:\n        df[\"opt_\"+feature] = 0.00\n\n    #For create a constraints for each row and request optimization from DataRobot Optimizer App\n    for index, row in df.iterrows():\n        tmp1 = json.loads(row.to_json())\n        constraints =set_optimizer(constrain_list,tmp1)\n        for app in key_dict:  # if using aiohttp, you could start multiple optimizations and have them run simultaneously\n            results = get_optimization(app, constraints)\n            #Get the optimized target: prediction, and the values of the constrained features that resulted in the optimized target\n            tp = results['optimized_simulation']\n            df.loc[index,\"opt_prediction\"] = tp['prediction']\n            for feature in tp['features']:\n                f_name = feature[\"name\"]\n                df.loc[index,\"opt_\"+f_name] = feature[\"value\"]\n    return df\n\n```\n\n# RUN THIS \n\n```python\ndata_df = pd.read_csv(ts_settings[\"filename\"])\nresult_df = perform_optimization(data_df)\n```\n\n```python\nresult_df[['opt_prediction','opt_revol_util','opt_inq_last_6mths','opt_loan_amnt','opt_dti']]\n\n```\n\n```python\n# See example of output from the optimizer\nresults\n```\n\n```python\ndf.loc[3,\"opt_prediction\"]\n```\n\n```python\nresuld_df\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Advanced Tuning and Partitioning/Python/DataRobot_Optimizer/Optimizing Through API.ipynb",
      "tags": [
        "jupyter-notebook",
        "datarobot",
        "tutorial",
        "api"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/DataRobot_Optimizer/Optimizing Through API.ipynb",
        "size": 74676,
        "cell_count": 16,
        "code_cell_count": 11
      },
      "code_examples": [
        "import requests  # you could also use aiohttp instead of requests to make calls asynchronously\nimport json\nimport pandas as pd\n\n'''\nYou get the ID and Token from the Optimizer App by clicking the share triangle (in the top menu of the UI)\n'''\n#https://606c32b7462514a41c85c492.apps.datarobot.com/settings?token=xMdWLVsq2C2dNvMCTMlbDzbZ6IcF_GwA4E76iYtJTGs&lang=en&theme=light\n\nkey_dict = {\n    'Complete_data': (\n                 '',  # ID\n                 '',  # Token\n                ),\n}\n#Set constraint features, and filename\n\nts_settings = {\"cfeatures\" :['revol_util','inq_last_6mths','loan_amnt','dti'],\n              \"filename\":\"Lending Club Sample 30.csv\"}\n",
        "def get_optimization(app, constraints):\n    '''\n    Given the credentials and data, get the best target under the constrained features\n    :param app: 'Complete_data' in key_dict\n    :param constraints: a dict containing the constrained features, the data, and the optimization type\n    :return: optimization performance, and values tried for each constrained feature\n    '''\n    app_id, token = key_dict[app]\n    url = f'https://{app_id}.apps.datarobot.com/api/optimize'\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    return requests.post(url, json=constraints, headers=headers).json()\n",
        "def get_constraints(key_dict):\n    '''\n    Get a list of constraint features and their minimum and maximum values from the Optimization App  \n    :param key_dict: the dictionary with the required app id and token\n    :return: a list of constraint features and their min and max\n    '''\n    app_id, token = key_dict['Complete_data']\n    url = f'https://{app_id}.apps.datarobot.com/api/application'\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    constraints = requests.get(url,headers=headers).json()\n    constrain_list = constraints['constraints']\n    [x.pop('feature_type') for x in constrain_list]\n    return constrain_list",
        "#You can build your constraint list from full dataset\n#The min and the max are calculated from the full dataset\ndef create_constrain_from_df(df_tmp,cfeatures):\n    '''\n    Create a list of constraint features and their minimum and maximum values.  We assume we get the minimum and maximum \n    of the constraint features from a dataset\n    :param df_tmp: dataframe \n    :param cfeatures: the name of the constrained features\n    :return: a list of constraint features and their min and max\n    '''\n    constrain_list = []\n    for c_name in cfeatures:\n        c_dict = {'feature': c_name,'info':{'min': df_tmp[c_name].min(),'max':df_tmp[c_name].max(),'is_int':False}}\n        constrain_list.append(c_dict)\n    return constrain_list",
        "def set_optimizer(constrain_list,tmp1):\n    '''\n    Create the constraints dictionary:  It has to have the constraint list, and a row that need to be optimized\n    :param constrain_list: list of features and their constraints\n    :param tmp1: a row that has been changed to a JSON format\n    :return: a dict\n    '''\n    constraints = {\n        'settings': {\n        # Specify if you want to maximize or minimize\n            \"targetDirection\": 'max',\n\n            # Specify your flex feature ranges\n            'constraints': constrain_list,\n        },\n    #    'optimization': {'method': 'exhaust'},  # optional\n\n        # Specify your fixed values\n        'datapoint': tmp1,\n    }\n    return constraints",
        "def perform_optimization(df):\n    '''\n    Perform a batch optimization.  Add the results to the original dataframe\n    :param df: dataframe with the dataset to be optimized \n    :return: a dataframe with the original dataframe with the predicted and optimized features.  The optimized features  \n             name are prefixed with opt_ \n    '''\n    #create constrain_list\n    constrain_list = get_constraints(key_dict)\n    #create new features and set them to 0.00\n    df[\"opt_prediction\"] = 0.00\n    for feature in ts_settings[\"cfeatures\"]:\n        df[\"opt_\"+feature] = 0.00\n\n    #For create a constraints for each row and request optimization from DataRobot Optimizer App\n    for index, row in df.iterrows():\n        tmp1 = json.loads(row.to_json())\n        constraints =set_optimizer(constrain_list,tmp1)\n        for app in key_dict:  # if using aiohttp, you could start multiple optimizations and have them run simultaneously\n            results = get_optimization(app, constraints)\n            #Get the optimized target: prediction, and the values of the constrained features that resulted in the optimized target\n            tp = results['optimized_simulation']\n            df.loc[index,\"opt_prediction\"] = tp['prediction']\n            for feature in tp['features']:\n                f_name = feature[\"name\"]\n                df.loc[index,\"opt_\"+f_name] = feature[\"value\"]\n    return df\n",
        "data_df = pd.read_csv(ts_settings[\"filename\"])\nresult_df = perform_optimization(data_df)",
        "result_df[['opt_prediction','opt_revol_util','opt_inq_last_6mths','opt_loan_amnt','opt_dti']]\n",
        "# See example of output from the optimizer\nresults",
        "df.loc[3,\"opt_prediction\"]",
        "resuld_df"
      ],
      "api_methods": [],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-8779150786094924938",
      "title": "Datetime Partitioning",
      "content": "## Datetime Partitioning\n\n**Author**: Thodoris Petropoulos\n\n**Label**: Modeling Options\n\n### Scope\n\nThe scope of this notebook is to provide instructions on how to do datetime partitioning using the Python API.\n\n### Background\n\nWhen dividing your data for model training and validation, DataRobot will randomly choose a set of rows from your dataset to assign amongst different cross validation folds. This will verify that you have not overfit your model to the training set and that the model can perform well on new data. However when your data has an intrinsic time based component, then you have to be even more careful about target leakage.  DataRobot now posseses datetime partitioning which will be diligent within model training & validation to guard against this, but you should always use your domain exerptise to evaluate your features prior to modeling.\n\nLet’s look at how we would frame a problem with a time component within DataRobot. This project basically simulated what you would get if you tried \"Out of Time Validation\" within DataRobot interface which is **not the same as Time Series projects**, even though the way we define backtests is very similar.\n\n### Requirements\n\n- Python version 3.7.3\n-  DataRobot API version 2.20.0. \nSmall adjustments might be needed depending on the Python version and DataRobot API version you are using.\n\nFull documentation of the Python package can be found here: https://datarobot-public-api-client.readthedocs-hosted.com\n\nIt is assumed you already have a DataRobot <code>Project</code> object and a DataRobot <code>Model </code> object.\n\n#### Import Libraries\n\n```python\nimport datarobot as dr\nfrom datetime import datetime\n```\n\n#### Running a DataRobot Project with a datetime partition\nThe durations need to be specified in string using the format of the <code>dr.helpers.partitioning_methods.construct_duration_string()</code> method.\n\n```python\nspec = dr.DatetimePartitioningSpecification(datetime_partition_column = 'Date',\n                                            holdout_start_date=datetime(2017,1,2),\n                                            holdout_duration='P1Y0M0DT0H0M0S',\n                                            number_of_backtests = 2,\n                                            use_time_series = False)\n```\n\nWe took advantage of DataRobot’s automated partition date selection after we specified the number of backtests to use. DataRobot allows further control, where we can further specify the validation start date as well as duration. Let’s look at an example below.\n\n#### Create Backtest Specifications\nThe below method would work for both Time Series and Out of Time Validation projects. Currently, we have specified that <code>use_time_series = False</code> in the <code>dr.DatetimePartitioningSpecification()</code> method so this would be initiated as an OTV project.\n\n```python\n# Dates are not project specific but rather example dates\nspec.backtests=[dr.BacktestSpecification(0,gap_duration = 'P0Y0M0DT0H0M0S',\n                                         validation_start_date = datetime(2016,1,2), \n                                         validation_duration = 'P1Y0M0DT0H0M0S'),\n\n\n                dr.BacktestSpecification(1,gap_duration = 'P0Y0M0DT0H0M0S',\n                                         validation_start_date = datetime(2015,1,2), \n                                         validation_duration = 'P1Y0M0DT0H0M0S')]\n\n#To start the project\nproject = dr.Project.create(sourcedata = df, project_name = 'Project Name')\nproject.set_target('target_column',partitioning_method = spec)\n```\n\nThe above methods will change the backtest specification for the first and second backtests.",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "Advanced Tuning and Partitioning/Python/Datetime Partitioning.ipynb",
      "tags": [
        "jupyter-notebook",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "examples-for-data-scientists",
        "file_path": "Advanced Tuning and Partitioning/Python/Datetime Partitioning.ipynb",
        "size": 5329,
        "cell_count": 9,
        "code_cell_count": 3
      },
      "code_examples": [
        "import datarobot as dr\nfrom datetime import datetime",
        "spec = dr.DatetimePartitioningSpecification(datetime_partition_column = 'Date',\n                                            holdout_start_date=datetime(2017,1,2),\n                                            holdout_duration='P1Y0M0DT0H0M0S',\n                                            number_of_backtests = 2,\n                                            use_time_series = False)",
        "# Dates are not project specific but rather example dates\nspec.backtests=[dr.BacktestSpecification(0,gap_duration = 'P0Y0M0DT0H0M0S',\n                                         validation_start_date = datetime(2016,1,2), \n                                         validation_duration = 'P1Y0M0DT0H0M0S'),\n\n\n                dr.BacktestSpecification(1,gap_duration = 'P0Y0M0DT0H0M0S',\n                                         validation_start_date = datetime(2015,1,2), \n                                         validation_duration = 'P1Y0M0DT0H0M0S')]\n\n#To start the project\nproject = dr.Project.create(sourcedata = df, project_name = 'Project Name')\nproject.set_target('target_column',partitioning_method = spec)"
      ],
      "api_methods": [
        "project.set_target",
        "project.create",
        "dr.project.create",
        "dr.helpers.partitioning_methods"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_9002795185719928829",
      "title": "AWS_Athena_End_to_End",
      "content": "<center><H1>DataRobot AutoML end-to-end with Amazon Athena</H1></center>\n\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<td><img src=\"https://www.datarobot.com/wp-content/uploads/2021/08/DataRobot-logo-color.svg\" height=200px width=200px>\n</td>\n<td><font size=10> + </font> </td>\n<td> <img src=\"https://vectorwiki.com/images/1BalA__aws-athena.svg\" height=100px width=100px> </td>\n\nAuthor: Biju Krishnan\n\n[API reference documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n<font>\nThis example notebook outlines the following tasks: <p>\n<ol>\n<li> Read in an Amazon Athena table and upload it to DataRobot's AI Catalog </li>\n<li> Create a project with the dataset</li>\n<li> Deploy the top performing model to a DataRobot prediction server </li>\n<li> Make batch predictions with a test dataset </li>\n</ol>\n<p>\n</font>\n\n## Setup\n\n### Import libraries\n\n```python\nimport datarobot as dr\n```\n\n### Bind variables\n\n```python\n# These variables can aso be fetched from a secret store or config files\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-16\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret\n```\n\n### Connect to DataRobot\n\nYou can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n```python\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT, ssl_verify=False)\n```\n\n```python\n# This line reads the driver object needed for creating a datastore\nathena_driver = [\n    drv for drv in dr.DataDriver.list() if drv.canonical_name == \"AWS Athena (v5)\"\n][-1]\n```\n\n## Import data\n\n### Create a data connection\n\nUse the cell below to define the parameters required to make a connector.\n\n```python\nathena_s3_bucket = \"e2eaccelerator09122022\"  # Specifythe name of the bucket followed by any prefix, later you format it as an S3 URI\n\njdbc_url = \"jdbc:awsathena://athena.eu-west-1.amazonaws.com;AwsRegion=eu-west-1;S3OutputLocation=s3://{}/\".format(\n    athena_s3_bucket\n)\n\nquery = 'SELECT * FROM \"new_york_taxi\".\"input\" limit 10000;'\n```\n\n```python\n# Create a data connection (AKA a datastore)\n\nDR_DATASTORE_NAME = \"ATHENA Data Connection\"  # This name can be altered\n\n# Checking if datastore already exists\nfor dstore in dr.DataStore.list():\n    if dstore.canonical_name == DR_DATASTORE_NAME:\n        datastore_flag = False\n        datastore = dstore\n        break\n    else:\n        datastore_flag = True\n\nif datastore_flag:\n    datastore = dr.DataStore.create(\n        data_store_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Connection\",  # This name can be replaced\n        driver_id=athena_driver.id,\n        jdbc_url=jdbc_url,\n    )\n```\n\n```python\n# Create a data connection based on a query\n# The Athena JDBC driver only supports query-based ingestion\n\nparams = dr.DataSourceParameters(data_store_id=datastore.id, query=query)\n\nDR_DATASOURCE_NAME = \"ATHENA Data Source\"  # This name can be altered\n\nfor dsource in dr.DataSource.list():\n    if dsource.canonical_name == DR_DATASOURCE_NAME:\n        datasource_flag = False\n        datasource = dsource\n        break\n    else:\n        datasource_flag = True\n\nif datasource_flag:\n    datasource = dr.DataSource.create(\n        data_source_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Source\",  # This name can be altered\n        params=params,\n    )\n```\n\n```python\n# This code snippet creates a snapshot of the Athena table and stores it in the AI Catalog\n\ndatarobot_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=datasource.id, username=AWS_KEY, password=AWS_SECRET\n)\n```\n\n## Create a project and initiate Autopilot\n\n```python\n# This cell will take several minutes to complete execution\n# An AutoML project named \"E2E Demo Amazon Athena\" is created with \"tip_amount\" as the target column\n# Quick mode is designated, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon Athena\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"tip_amount\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)\n```\n\n### Get the top-performing model\n\nOnce the AutoML project is complete, select the top-performing model on the Leaderboard based on the chosen metric for deployment.\n\n```python\ndef sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)\n```\n\n### Deploy a model\n\nNote that steps in the following sections require DataRobot MLOps licensed features. Contact your DataRobot account representatives if you are missing some licensed MLOps features.\n\n```python\n# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon Athena Test\",\n    description=\"Model trained on New York Taxi trips dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id\n```\n\n### Make predictions\n\nDataRobot's batch predictions API is capable of directly reading and writing to Amazon S3 storage. \n\n```python\n# To run a batch prediction job you need to store the Amazon Athena Credentials in the DataRobot credentials manager\n\nDR_CREDENTIAL_NAME = \"Amazon Athena Credentials\"  # Choose a name\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        athena_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if they do not exist\nif cred_flag:\n    credential = dr.Credential.create_basic(\n        name=DR_CREDENTIAL_NAME,  # The username and password is the AWS KEY and SECRET respectively\n        user=AWS_KEY,\n        password=AWS_SECRET,\n    )\n    athena_credential_id = credential.credential_id\n\nprint(athena_credential_id)\n```\n\n```python\nDR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        s3_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if it does not exist\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    s3_credential_id = credential.credential_id\n\nprint(s3_credential_id)\n```\n\n### Batch predictions snippet\n\nThe snippet below provides sample code to demonstratehow to make batch predictions to and from Amazon S3.\n\n```python\n# This example scores the training data but there needs to be an Athena table with test data.\n\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": \"select * from new_york_taxi.input limit 1000\",  # This has to be a query, since the JDBC driver does not seem to understand table schema structure\n        \"data_store_id\": datastore.id,  # The ID of the datastore you want\n        \"credential_id\": athena_credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\",  # Note this has to be a filename and not just a bucket name\n        \"credential_id\": s3_credential_id,\n    },\n)\njob.wait_for_completion()\njob.get_status()\n```\n\n<font family=verdana>\n<p>\nThe output of the batch predictions is available under the path s3://e2eaccelerator09122022/predictions/output/\n<pre><code><font color=grey size=1>\naws s3 ls s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\n2022-12-12 17:47:49      22725 new_york_taxi_predictions.csv\n</font></code></pre>\n</font>\n",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/AWS_Athena_template/AWS_Athena_End_to_End.ipynb",
      "tags": [
        "integration",
        "athena",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "aws",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/AWS_Athena_template/AWS_Athena_End_to_End.ipynb",
        "size": 14525,
        "cell_count": 27,
        "code_cell_count": 14
      },
      "code_examples": [
        "import datarobot as dr",
        "# These variables can aso be fetched from a secret store or config files\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-16\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret",
        "dr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT, ssl_verify=False)",
        "# This line reads the driver object needed for creating a datastore\nathena_driver = [\n    drv for drv in dr.DataDriver.list() if drv.canonical_name == \"AWS Athena (v5)\"\n][-1]",
        "athena_s3_bucket = \"e2eaccelerator09122022\"  # Specifythe name of the bucket followed by any prefix, later you format it as an S3 URI\n\njdbc_url = \"jdbc:awsathena://athena.eu-west-1.amazonaws.com;AwsRegion=eu-west-1;S3OutputLocation=s3://{}/\".format(\n    athena_s3_bucket\n)\n\nquery = 'SELECT * FROM \"new_york_taxi\".\"input\" limit 10000;'",
        "# Create a data connection (AKA a datastore)\n\nDR_DATASTORE_NAME = \"ATHENA Data Connection\"  # This name can be altered\n\n# Checking if datastore already exists\nfor dstore in dr.DataStore.list():\n    if dstore.canonical_name == DR_DATASTORE_NAME:\n        datastore_flag = False\n        datastore = dstore\n        break\n    else:\n        datastore_flag = True\n\nif datastore_flag:\n    datastore = dr.DataStore.create(\n        data_store_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Connection\",  # This name can be replaced\n        driver_id=athena_driver.id,\n        jdbc_url=jdbc_url,\n    )",
        "# Create a data connection based on a query\n# The Athena JDBC driver only supports query-based ingestion\n\nparams = dr.DataSourceParameters(data_store_id=datastore.id, query=query)\n\nDR_DATASOURCE_NAME = \"ATHENA Data Source\"  # This name can be altered\n\nfor dsource in dr.DataSource.list():\n    if dsource.canonical_name == DR_DATASOURCE_NAME:\n        datasource_flag = False\n        datasource = dsource\n        break\n    else:\n        datasource_flag = True\n\nif datasource_flag:\n    datasource = dr.DataSource.create(\n        data_source_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Source\",  # This name can be altered\n        params=params,\n    )",
        "# This code snippet creates a snapshot of the Athena table and stores it in the AI Catalog\n\ndatarobot_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=datasource.id, username=AWS_KEY, password=AWS_SECRET\n)",
        "# This cell will take several minutes to complete execution\n# An AutoML project named \"E2E Demo Amazon Athena\" is created with \"tip_amount\" as the target column\n# Quick mode is designated, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon Athena\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"tip_amount\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)",
        "def sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)",
        "# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon Athena Test\",\n    description=\"Model trained on New York Taxi trips dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id",
        "# To run a batch prediction job you need to store the Amazon Athena Credentials in the DataRobot credentials manager\n\nDR_CREDENTIAL_NAME = \"Amazon Athena Credentials\"  # Choose a name\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        athena_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if they do not exist\nif cred_flag:\n    credential = dr.Credential.create_basic(\n        name=DR_CREDENTIAL_NAME,  # The username and password is the AWS KEY and SECRET respectively\n        user=AWS_KEY,\n        password=AWS_SECRET,\n    )\n    athena_credential_id = credential.credential_id\n\nprint(athena_credential_id)",
        "DR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        s3_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if it does not exist\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    s3_credential_id = credential.credential_id\n\nprint(s3_credential_id)",
        "# This example scores the training data but there needs to be an Athena table with test data.\n\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": \"select * from new_york_taxi.input limit 1000\",  # This has to be a query, since the JDBC driver does not seem to understand table schema structure\n        \"data_store_id\": datastore.id,  # The ID of the datastore you want\n        \"credential_id\": athena_credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\",  # Note this has to be a filename and not just a bucket name\n        \"credential_id\": s3_credential_id,\n    },\n)\njob.wait_for_completion()\njob.get_status()"
      ],
      "api_methods": [
        "dr.datasource.list",
        "project.wait_for_autopilot",
        "model.metrics",
        "dr.datasource.create",
        "dr.datastore.create",
        "deployment.create_from_learning_model",
        "dr.client._global_client",
        "dr.autopilot_mode.quick",
        "dr.credential.list",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "dr.credential.create_s3",
        "project.get",
        "dr.dataset.create_from_data_source",
        "deployment.id",
        "dr.datadriver.list",
        "dr.datastore.list",
        "dr.credential.create_basic",
        "dr.batchpredictionjob.score",
        "dr.project.get",
        "project.analyze_and_model",
        "project.get_models",
        "project.metric",
        "project.create_from_dataset",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-8257183951654074456",
      "title": "Zero Shot Text Classification for Error Analysis",
      "content": "# Zero-shot text classification for error analysis\n\n**Author:** Glen Koundry  \n**Date:** June 6, 2023  \n\n## Overview\n\nThis notebook outlines how to use zero-shot text classification with large language models (LLMs), focusing on its application in error analysis of supervised text classification models.\n\nThis notebook's primary goal is to equip you with practical knowledge of using LLMs for zero-shot text classification and its application in performing error analysis of supervised text classification models. This understanding is becoming an increasingly valuable skill in the era of big data and dynamically evolving categories.\n\n### Core concepts\n\n**Text Classification** is the process of attributing predefined categories (or labels) to text. For example, identifying emails as spam or not spam is a classic text classification task.\n\n**Zero-shot learning** represents an approach in machine learning where a model can tackle a task it hasn't explicitly seen during its training phase. This implies that the text classifier can discern and attribute labels that it hasn't been specifically trained on, a critical ability in real-world situations where labeled data is scarce or when new categories surface post-training.\n\n**Error analysis** involves examining and understanding the mistakes made by a supervised learning model. In the context of text classification, this entails identifying the cases where a model has misclassified the text and understanding why those misclassifications occurred. Error analysis can provide insights into how to improve the model, whether by gathering more or different data, modifying the model architecture, or adjusting the training process.\n\nZero-shot classification exploits the power of **large language models (LLMs)** such as GPT-3, GPT-4, and others by OpenAI. These models, trained on extensive and diverse internet text, can produce human-like text, answer queries, translate languages, and more. A collateral benefit of this wide-ranging training is their ability to comprehend context and make intelligent predictions, a quality you can utilize for zero-shot classification and error analysis.\n\n### Approaches for zero-shot text classification\n\nIn this notebook, you will delve into three distinctive approaches for zero-shot text classification using large language models:\n\n* Embeddings-based classification: Extract high-dimensional vector representations (embeddings) of both text and labels, then measure their similarity.\n* Natural Language Inference-based (NLI) Classification: This method capitalizes on the model's capability to ascertain whether a given statement is true or false within a specific context.\n* Direct classification: This method involves framing the task as a question to a conversational model and interpreting its generated response.\n\nEach of these techniques comes with its unique strengths and challenges, and are explored throughout the notebook.\n\n## Setup\n\n### Import libraries\n\n```python\nimport datetime\nimport json\n\nfrom IPython.display import display, HTML, Markdown\nimport datarobot as dr\nfrom datasets import load_dataset\nimport numpy as np\nimport openai\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\nimport pandas as pd\nfrom transformers import pipeline\n```\n\n### Credentials\n\n**IMPORTANT**: Before running this cell, you need to provide your personal DataRobot API key and your OpenAI API key. Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n\nTo find your OpenAI API key, log in to your OpenAI account, click on your name in the upper right corner and select \"View API Keys\".\n\nIn the cell, replace the existing string within the quotes (`\"your_openapi_api_key\"`) with your personal OpenAI API Key. Be sure to keep your key within the quotes.\n        \nDon't forget to keep your API keys secure and avoid sharing them publicly.\n\n```python\n# Place your DataRobot API key and URL here\n# DATAROBOT_API_KEY = \"<insert DataRobot API key here>\"\nDATAROBOT_API_ENDPOINT = \"https://app.datarobot.com/api/v2/\"\n\n# Place you OpenAI API key here\nOPENAI_API_KEY = \"<insert OpenAI API key here>\"\nopenai.api_key = OPENAI_API_KEY\n```\n\n### Import data\n\nFor experiments with text classification. use the `financial_phrasebank` dataset from Huggingface. This dataset comprises sentences from financial news, each labeled with a sentiment: negative, neutral, or positive, as determined by human annotators.\n\n```python\n# Dataset to load from Huggingface dataset hub\nDATASET_INFO = {\n    \"path\": \"financial_phrasebank\",\n    \"name\": \"sentences_50agree\",\n    \"split\": \"train\",\n}\nTEXT_FIELD_NAME = \"sentence\"\nLABEL_FIELD_NAME = \"label\"\n\ndataset = load_dataset(**DATASET_INFO)\n\n# Financial phrasebank labels\n# 0 = negative, 1 = neutral, 2 = positive\nlabels = [\n    \"negative\",\n    \"neutral\",\n    \"positive\",\n]\n\ndataset.select(range(5)).to_pandas()\n```\n\n### Create a training and testing split\n\nPrepare the data for your project. Divide the dataset into two subsets: one for training a supervised model and one for testing its performance. After the model is trained, the test subset will be used to evaluate the model's accuracy and to conduct error analysis, exploring where and why the model makes mistakes.\n\n```python\nTEST_SIZE = 1000\n\n# Create a training/testing split with TEST_SIZE test_split\ndataset = dataset.train_test_split(test_size=TEST_SIZE)\n\n# Make DataFrames from the dataset\n# `train_df` is only used for DataRobot supervised model training\ntrain_df = dataset[\"train\"].to_pandas()\ntest_df = dataset[\"test\"].to_pandas()\n```\n\n## Modeling\n\n### Create a project for multi-class supervised learning\n\nThis code creates and trains a supervised learning model using DataRobot's AutoML functionality. It first establishes a connection to the DataRobot application. It then initiates a project and starts the AutoML process, which automatically selects the best model and retrains it on the full training dataset. Once the best model is identified and trained, the code generates predictions for the test dataset. These predictions are then converted into a more readable format, facilitating subsequent error analysis. This model will serve as the primary tool for understanding the strengths and weaknesses of supervised learning in text classification.\n\n```python\n# Connect to DataRobot\ndr.Client(\n    token=DATAROBOT_API_KEY,\n    endpoint=DATAROBOT_API_ENDPOINT,\n)\n\n# Create a project and start Autopilot\nproject = dr.Project.start(\n    train_df,\n    target=LABEL_FIELD_NAME,\n    target_type=\"Multiclass\",\n    project_name=f\"{DATASET_INFO['path']}_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n)\nproject.wait_for_autopilot()\n\n# Get the best-performing model (highest scoring model retrained on 100% of training data)\nmodels = project.get_models()\nfinal_model = [model for model in models if model.sample_pct == 100][0]\nprint(f\"Best model: {final_model.model_type}\\n\")\n\n# Get predictions for the testing dataset\npredict_job = final_model.request_predictions(dataframe=test_df)\nprediction_df = predict_job.get_result_when_complete()\n\n# Change the label to `int` to make indexing the `labels` list easier\nprediction_df[\"prediction\"] = prediction_df[\"prediction\"].astype(int)\n\n# Show predictions\nprediction_df = prediction_df.rename(\n    columns={f\"class_{i}\": label for i, label in enumerate(labels)}\n)\nprediction_df\n```\n\n### Supervised learning model results\n\nThis code evaluates the accuracy of the supervised learning model and collects details about misclassified examples. It first determines the model's accuracy by comparing predictions against actual labels. Then, it creates a list of details about instances where the model's predictions were incorrect. Each entry includes the actual and predicted labels and the text that was misclassified. For demo purposes, this process stops after collecting details for a maximum of ten misclassified instances. This information is crucial for understanding and diagnosing the types of errors our model makes.\n\n```python\n# Show model accuracy\ncorrect_predictions = (\n    prediction_df[\"prediction\"].astype(int) == test_df[LABEL_FIELD_NAME]\n)\nprint(\n    f\"Model accuracy: {correct_predictions.sum()} correct out of {correct_predictions.shape[0]}\"\n)\n\n# Limit the number of mistakes to classify since this is just a demo\nMAX_ERRORS = 20\n\n# Create list of misclassification details\nresult_df = pd.concat((prediction_df, test_df), axis=1)\nerror_details = []\nfor _, result_row in result_df.iterrows():\n    if result_row[LABEL_FIELD_NAME] != result_row[\"prediction\"]:\n        error_details.append(\n            {\n                \"actual_label\": labels[result_row[LABEL_FIELD_NAME]],\n                \"predicted_label\": labels[result_row[\"prediction\"]],\n                \"text\": result_row[TEXT_FIELD_NAME],\n            }\n        )\n        if len(error_details) == MAX_ERRORS:\n            break\n```\n\n## Error analysis of misclassified text\n\n### Define error classes\n\nThe first step is to decide on what types of misclassification you are interested in. This piece of code declares a list, `error_class_templates`, containing potential classes of errors that may occur during text sentiment classification. These error classes are expressed in the form of descriptive strings:\n\n1. \"The sentiment of this statement is ambiguous.\" This error class indicates that the sentiment expressed in the given statement cannot be clearly determined. It could be due to the use of both positive and negative language, sarcasm, or complex language structure.\n\n2. \"The sentiment is unclear and requires more context\" - This error class implies that the given statement lacks enough context to make an accurate sentiment classification. This can occur if the statement is too short, vague, or depends heavily on preceding or following information not included in the text.\n\n3. \"The sentiment of this statement is not {}\" - This error class represents situations where the classifier incorrectly assigns a sentiment. The `{}` is a placeholder that will be replaced with the actual sentiment label predicted by the classifier, allowing this message to dynamically reflect the specific misclassification that occurred.\n\n4. \"The sentiment of this statement is {}\" - This means that the example and label are valid and that\nthe supervised classifier made a mistake.\n\nThese classes represent common types of errors that the classifier might make, and they serve as a starting point for error analysis. During this analysis, misclassified examples will be categorized according to these templates.\n\n```python\n# List of possible classification errors. Replace \"{}\" with the actual label.\nerror_class_templates = [\n    \"The sentiment of this statement is ambiguous.\",\n    \"The sentiment is unclear and requires more context\",\n    \"The sentiment of this statement is not {}\",\n    \"The sentiment of this statement is {}\",\n]\n```\n\n### Helper function for visualizing results\n\nThis function takes a DataFrame containing the results of the error analysis and displays the results using markdown formatting.\n\n```python\ndef show_error_class_results(result_df):\n    for _, result_row in result_df.iterrows():\n        display(Markdown(f\"## Sentence\\n{result_row['sentence']}\"))\n        display(Markdown(f\"**Actual label:** {result_row['actual_label']}\"))\n        display(Markdown(f\"**Predicted label:** {result_row['predicted_label']}\"))\n        probabilities = result_row[error_class_templates].tolist()\n        error_classes = [\n            class_template.format(result_row[\"actual_label\"])\n            for class_template in error_class_templates\n        ]\n        display(\n            pd.Series(probabilities, index=error_classes).to_frame(name=\"probability\")\n        )\n```\n\n## Zero-shot classification\n\nWith a text classification model and list of errors, you can now venture into the exciting domain of zero-shot learning. Zero-shot learning allows you to apply the model to text categories it has never seen before during training, broadening the applicability and versatility of the classifier. In the next sections, explore three distinct methods for implementing zero-shot text classification. These methods are:\n\n* Embeddings-based classification. \n* Natural Language Inference-based (NLI) classification\n* Direct classification \n\nEach technique offers a unique approach to the problem and is based on different principles, with its own benefits and considerations. The aim is to equip you with a variety of tools and perspectives on how to leverage large language models for text classification in a zero-shot setting. Let's delve into each of these techniques and understand how they enhance text classification capabilities.\n\n### Embeddings-based classification\n\nEmbeddings-based classification is a technique for zero-shot text classification that leverages the semantic representations of text encoded as high-dimensional vectors, also known as embeddings. Large language models (LLMs), like GPT-3, GPT-4, or BERT, are excellent at generating these embeddings as they have been trained on a diverse range of internet text, learning the contextual and semantic nuances of the language in the process.\n\nHere's a simplified breakdown of how the process works:\n\n1. **Generate embeddings**: For each text input and each possible label, generate embeddings. This is done by passing the text and labels through the LLM. The LLM then produces a high-dimensional vector for each, encapsulating their semantic information.\n\n2. **Calculate cosine similarity**: Calculate the cosine similarity between the text's embedding and each of the label embeddings. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The higher the cosine similarity, the smaller the angle and greater the match between vectors.\n\n3. **Classify**: The label whose embedding has the highest cosine similarity to the text's embedding is chosen as the predicted label for that text.\n\nThe power of this method lies in its ability to capture semantic similarity between text and labels, allowing it to categorize text into labels that it wasn't explicitly trained on. This method is especially useful when you don't have a lot of labeled data for training, a common scenario in real-world applications.\n\n#### Implement embeddings-based zero-shot classification\n\nThe cell below performs zero-shot text classification using the embeddings-based method. It starts by importing the necessary functions from OpenAI's `embeddings_utils` module. The `get_embedding` function is used to generate embeddings for each label and sentence in the test dataset using OpenAI's `text-embedding-ada-002` engine.\n\nNext, the cosine similarity between the embeddings of each sentence and each label is computed. These similarities are then scaled and converted into pseudo-probabilities representing the likelihood of each label being the correct one for each sentence.\n\nFinally, the predictions and pseudo-probabilities are stored in a DataFrame, which is then passed to the `show_error_class_results` function to display the results of the classification.\n\n\n```python\n# This method doesn't estimate probabilities but you can approximate them by using a scaled softmax\nCS_SCALE = 32\n\nembedding_results = []\nfor error_detail in error_details:\n    # Get embeddings for the misclassified examples\n    sentence_embedding = get_embedding(\n        error_detail[\"text\"], engine=\"text-embedding-ada-002\"\n    )\n\n    # Add the label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Get embeddings for each error class\n    label_embeddings = [\n        get_embedding(error_class, engine=\"text-embedding-ada-002\")\n        for error_class in error_classes\n    ]\n\n    # Compute the similarity of the sentence with each label\n    similarities = [\n        cosine_similarity(sentence_embedding, label_embedding)\n        for label_embedding in label_embeddings\n    ]\n\n    # Make pseudo-probabilities from similarity scores\n    probs = np.exp(CS_SCALE * np.array(similarities))\n    probs = probs / probs.sum()\n\n    embedding_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probs)),\n        )\n    )\n\nembedding_results_df = pd.DataFrame(embedding_results)\nshow_error_class_results(embedding_results_df)\n```\n\n### Use an Natural Language Inference (NLI) model\n\nZero-shot classification using a Natural Language Inference (NLI) model is a method where the model is not trained directly to predict the desired classes. Instead, the task is framed as an inference problem. For example, if you want to classify a sentence into one of several categories, you would pair the sentence with a category in the form of a hypothesis (e.g., \"The sentiment of this sentence is positive\"). The NLI model then predicts whether this hypothesis is true or false. You repeat this for each category, essentially asking the model to infer the most likely category. This is a powerful technique that allows you to classify text into categories that the model was never explicitly trained on, hence the term \"zero-shot\".\n\n#### Implement Natural Language Inference (NLI) zero-shot classification\n\nThis code performs error analysis on misclassified instances using a Natural Language Inference (NLI) model. It creates an NLI-based classifier using the HuggingFace transformers library. For each misclassified example, it generates custom error classes using the actual label and error class templates. The NLI model is then used to determine the probabilities of each error class given the misclassified text. Finally, the code displays the sentence, actual label, predicted label, and a table of probabilities for each error class. This analysis helps in understanding why a particular misclassification occurred.\n\n```python\n# Create the classifier using HuggingFace's zero-shot-classification pipeline\n# NOTE: Change `device=0` to `device=-1` if you do not have a GPU\nnli_classifier = pipeline(\n    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0\n)\n\nnli_results = []\nfor error_detail in error_details:\n    # Add actual label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Suppress warnings from the pipeline\n    nli_classifier.call_count = 0\n\n    # Get error class probabilities\n    predictions = nli_classifier(error_detail[\"text\"], error_classes)\n\n    # Get probabilities and change the order to match error class order\n    probabilites = pd.Series(\n        predictions[\"scores\"],\n        index=[\n            error_classes.index(error_class) for error_class in predictions[\"labels\"]\n        ],\n    ).sort_index()\n\n    nli_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilites)),\n        )\n    )\n\nnli_results_df = pd.DataFrame(nli_results)\nshow_error_class_results(nli_results_df)\n```\n\n### Conversational model prompting\n\nZero-shot classification using a conversational model is a method in which you frame the classification task as a question and answer conversation. Instead of providing an explicit label, the input text is presented to the model in the form of a question, and the model generates a response. \n\nFor example, if you want to classify a piece of text as either 'positive', 'negative', or 'neutral', you can ask the model a question like \"What is the sentiment of this text?\" The model will then generate a response based on its understanding of the text and the question, predicting one of the classes 'positive', 'negative', or 'neutral'. \n\nThis approach leverages the ability of conversational models to understand and generate human-like text. As these models have been trained on diverse and vast amounts of data, they can often generate surprisingly accurate predictions for classes they were not explicitly trained on, hence the term \"zero-shot\".\n\n#### Implement conversational model prompting\n\nThis code analyzes misclassification errors using a conversational AI model. For each error, it provides the AI model with a set of instructions to assess why the error might have occurred based on four possible reasons. The AI model is then asked to assign a probability to each reason for the given misclassified statement. The model's response is processed and displayed as a DataFrame, providing insights into the potential causes of misclassification. By incorporating human-like understanding and context awareness, this method provides an additional layer of depth to your error analysis.\n\n```python\n# Instructions for the conversational model\n# Note that the instructions ask for JSON formatted output so you can have Python process the results\nsystem_prompt_template = (\n    \"ChatGPT, I would like you to analyze a series of misclassified statements from a financial \"\n    \"news sentiment classification model and estimate the probabilites of reasons for the misclassification. \"\n    \"Please review each statement, and based on your understanding, determine whether the misclassification \"\n    \"likely occurred because:\\n\"\n    \"    1. There was not enough context to accurately determine the sentiment.\\n\"\n    \"    2. The statement itself is ambiguous, making it hard to assign a clear sentiment.\\n\"\n    \"    3. The sentiment label ({label}) provided in the dataset is incorrect.\\n\"\n    \"    4. The sentiment label ({label}) provided in the dataset is correct and the model is wrong.\\n\"\n    \"Remember, your task is to analyze the statement and give your best guess for the probability \"\n    \"of each misclassification reason from the above four numbered options.\\n\"\n    \"Please respond in JSON format with only the reason numbers and probability estimates. \"\n    \"Use the numbers (1, 2, 3 and 4) for the reasons.\\n\"\n    \"IMPORTANT: Don't include any explanations or anything that cannot be parsed as JSON.\"\n)\n\nchat_results = []\nfor error_detail in error_details:\n    # Create message with the instructions and the text you want to classify\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt_template.format(\n                label=error_detail[\"actual_label\"]\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f'Statement: \"{error_detail[\"text\"]}\"\\n'\n                f'Correct Sentiment: {error_detail[\"actual_label\"]}\\n'\n                f'Predicted Sentiment: {error_detail[\"predicted_label\"]}'\n            ),\n        },\n    ]\n\n    # Send your request to a conversational model (GPT3.5)\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0\n    )\n\n    # Turn ChatGPT response into a DataFrame\n    result = completion.choices[0].message.content.strip()\n    result_json = json.loads(result)\n\n    # Get probabilities and make sure order matches error class order\n    probabilities = pd.Series(\n        result_json.values(), index=result_json.keys()\n    ).sort_index()\n\n    chat_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilities)),\n        )\n    )\n\n# Convert results to DataFrame and set missing probabilities to zero\nchat_results_df = pd.DataFrame(chat_results).fillna(0)\n\nshow_error_class_results(chat_results_df)\n```\n\n## Compare results from error analysis methods\n\n### Create a table of results for the three methods\n\nThis following cell synthesizes the results of the three zero-shot learning methods – embedding-based, NLI-based, and chat-based – to analyze misclassified examples from the supervised model. It assigns simplified labels to the most probable error class for each method, resulting in a clear and concise summary. This information is then used to create a comparison table, presenting a side-by-side view of each sentence and the respective error analyses from each zero-shot method. The table provides an intuitive way to understand and compare the insights gained from different techniques.\n\n```python\n# Short names to use as column headers\nshort_error_classes = [\n    \"Ambiguous\",\n    \"No Context\",\n    \"Mislabeled\",\n    \"Model Misclassification\",\n]\n\n# For each method, get the highest probability and assign a short label\nembedding_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(\n        embedding_results_df[error_class_templates].values, axis=1\n    )\n]\nnli_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(nli_results_df[error_class_templates].values, axis=1)\n]\nchat_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(chat_results_df[error_class_templates].values, axis=1)\n]\n\n# Show summary of results in a table\nhtml = \"<table><tr><th>Sentence</th><th>Embedding</th><th>NLI</th><th>Chat</th></tr>\"\nfor sentence, embedding_error_class, nli_error_class, chat_error_class in zip(\n    embedding_results_df[\"sentence\"],\n    embedding_results_short,\n    nli_results_short,\n    chat_results_short,\n):\n    html += (\n        f\"<tr><td>{sentence}</td><td>{embedding_error_class}</td><td>{nli_error_class}</td>\"\n        f\"<td>{chat_error_class}</td></tr>\"\n    )\ndisplay(HTML(html + \"</table>\"))\n```\n\n### Compare Zero-Shot Techniques\n\nThe table presents a comparative analysis of the outcomes from three different methods of zero-shot classification— Embeddings-based, Natural Language Inference (NLI), and Conversational model prompting (Chat)—applied on a set of sentences. The goal is to identify the primary cause of misclassification for each sentence as determined by the prior trained model. The potential error classes include \"Ambiguous\", \"No Context\", \"Mislabeled\", and \"Model Misclassification\".\n\nOne key observation from the results is the variability in error classification among the three methods. Each method often interprets and analyzes the same sentence differently, leading to diverse classifications of the source of the error. This indicates that the choice of method can significantly influence the outcomes and subsequent interpretation of error analysis.\n\nHowever, there are instances where all three methods agree on the error classification for a sentence. These instances can provide robust evidence for a particular type of misclassification, suggesting that these might be areas where the original supervised model struggled most.\n\nConversely, sentences that elicit widely varied error classifications across the three methods might indicate particularly challenging instances for automatic text classification. These cases could provide valuable insights into potential areas for improvement in model training or data annotation.\n\n## Conclusion\n\nThis notebook demonstrates the use of zero-shot text classification for error analysis in a supervised learning model. After training the model and identifying its errors, you employed various zero-shot techniques, such as embeddings-based, NLI-based, and conversational AI models, to analyze these misclassifications.\n\nThrough this approach, you gained valuable insights into the reasons behind the model's mistakes, helping to understand its limitations and potential areas of improvement. This showcases how zero-shot classification can enrich the error analysis process, ultimately leading to more robust and accurate models.",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "generative_ai/zero_shot_LMM_error_analysis_NLP/Zero Shot Text Classification for Error Analysis.ipynb",
      "tags": [
        "generative-ai",
        "genai",
        "jupyter-notebook",
        "llm",
        "predictions",
        "classification",
        "ai-accelerators",
        "openai",
        "datarobot",
        "nlp",
        "tutorial",
        "zero-shot"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "generative_ai/zero_shot_LMM_error_analysis_NLP/Zero Shot Text Classification for Error Analysis.ipynb",
        "size": 212479,
        "cell_count": 33,
        "code_cell_count": 12
      },
      "code_examples": [
        "import datetime\nimport json\n\nfrom IPython.display import display, HTML, Markdown\nimport datarobot as dr\nfrom datasets import load_dataset\nimport numpy as np\nimport openai\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\nimport pandas as pd\nfrom transformers import pipeline",
        "# Place your DataRobot API key and URL here\n# DATAROBOT_API_KEY = \"<insert DataRobot API key here>\"\nDATAROBOT_API_ENDPOINT = \"https://app.datarobot.com/api/v2/\"\n\n# Place you OpenAI API key here\nOPENAI_API_KEY = \"<insert OpenAI API key here>\"\nopenai.api_key = OPENAI_API_KEY",
        "# Dataset to load from Huggingface dataset hub\nDATASET_INFO = {\n    \"path\": \"financial_phrasebank\",\n    \"name\": \"sentences_50agree\",\n    \"split\": \"train\",\n}\nTEXT_FIELD_NAME = \"sentence\"\nLABEL_FIELD_NAME = \"label\"\n\ndataset = load_dataset(**DATASET_INFO)\n\n# Financial phrasebank labels\n# 0 = negative, 1 = neutral, 2 = positive\nlabels = [\n    \"negative\",\n    \"neutral\",\n    \"positive\",\n]\n\ndataset.select(range(5)).to_pandas()",
        "TEST_SIZE = 1000\n\n# Create a training/testing split with TEST_SIZE test_split\ndataset = dataset.train_test_split(test_size=TEST_SIZE)\n\n# Make DataFrames from the dataset\n# `train_df` is only used for DataRobot supervised model training\ntrain_df = dataset[\"train\"].to_pandas()\ntest_df = dataset[\"test\"].to_pandas()",
        "# Connect to DataRobot\ndr.Client(\n    token=DATAROBOT_API_KEY,\n    endpoint=DATAROBOT_API_ENDPOINT,\n)\n\n# Create a project and start Autopilot\nproject = dr.Project.start(\n    train_df,\n    target=LABEL_FIELD_NAME,\n    target_type=\"Multiclass\",\n    project_name=f\"{DATASET_INFO['path']}_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n)\nproject.wait_for_autopilot()\n\n# Get the best-performing model (highest scoring model retrained on 100% of training data)\nmodels = project.get_models()\nfinal_model = [model for model in models if model.sample_pct == 100][0]\nprint(f\"Best model: {final_model.model_type}\\n\")\n\n# Get predictions for the testing dataset\npredict_job = final_model.request_predictions(dataframe=test_df)\nprediction_df = predict_job.get_result_when_complete()\n\n# Change the label to `int` to make indexing the `labels` list easier\nprediction_df[\"prediction\"] = prediction_df[\"prediction\"].astype(int)\n\n# Show predictions\nprediction_df = prediction_df.rename(\n    columns={f\"class_{i}\": label for i, label in enumerate(labels)}\n)\nprediction_df",
        "# Show model accuracy\ncorrect_predictions = (\n    prediction_df[\"prediction\"].astype(int) == test_df[LABEL_FIELD_NAME]\n)\nprint(\n    f\"Model accuracy: {correct_predictions.sum()} correct out of {correct_predictions.shape[0]}\"\n)\n\n# Limit the number of mistakes to classify since this is just a demo\nMAX_ERRORS = 20\n\n# Create list of misclassification details\nresult_df = pd.concat((prediction_df, test_df), axis=1)\nerror_details = []\nfor _, result_row in result_df.iterrows():\n    if result_row[LABEL_FIELD_NAME] != result_row[\"prediction\"]:\n        error_details.append(\n            {\n                \"actual_label\": labels[result_row[LABEL_FIELD_NAME]],\n                \"predicted_label\": labels[result_row[\"prediction\"]],\n                \"text\": result_row[TEXT_FIELD_NAME],\n            }\n        )\n        if len(error_details) == MAX_ERRORS:\n            break",
        "# List of possible classification errors. Replace \"{}\" with the actual label.\nerror_class_templates = [\n    \"The sentiment of this statement is ambiguous.\",\n    \"The sentiment is unclear and requires more context\",\n    \"The sentiment of this statement is not {}\",\n    \"The sentiment of this statement is {}\",\n]",
        "def show_error_class_results(result_df):\n    for _, result_row in result_df.iterrows():\n        display(Markdown(f\"## Sentence\\n{result_row['sentence']}\"))\n        display(Markdown(f\"**Actual label:** {result_row['actual_label']}\"))\n        display(Markdown(f\"**Predicted label:** {result_row['predicted_label']}\"))\n        probabilities = result_row[error_class_templates].tolist()\n        error_classes = [\n            class_template.format(result_row[\"actual_label\"])\n            for class_template in error_class_templates\n        ]\n        display(\n            pd.Series(probabilities, index=error_classes).to_frame(name=\"probability\")\n        )",
        "# This method doesn't estimate probabilities but you can approximate them by using a scaled softmax\nCS_SCALE = 32\n\nembedding_results = []\nfor error_detail in error_details:\n    # Get embeddings for the misclassified examples\n    sentence_embedding = get_embedding(\n        error_detail[\"text\"], engine=\"text-embedding-ada-002\"\n    )\n\n    # Add the label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Get embeddings for each error class\n    label_embeddings = [\n        get_embedding(error_class, engine=\"text-embedding-ada-002\")\n        for error_class in error_classes\n    ]\n\n    # Compute the similarity of the sentence with each label\n    similarities = [\n        cosine_similarity(sentence_embedding, label_embedding)\n        for label_embedding in label_embeddings\n    ]\n\n    # Make pseudo-probabilities from similarity scores\n    probs = np.exp(CS_SCALE * np.array(similarities))\n    probs = probs / probs.sum()\n\n    embedding_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probs)),\n        )\n    )\n\nembedding_results_df = pd.DataFrame(embedding_results)\nshow_error_class_results(embedding_results_df)",
        "# Create the classifier using HuggingFace's zero-shot-classification pipeline\n# NOTE: Change `device=0` to `device=-1` if you do not have a GPU\nnli_classifier = pipeline(\n    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0\n)\n\nnli_results = []\nfor error_detail in error_details:\n    # Add actual label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Suppress warnings from the pipeline\n    nli_classifier.call_count = 0\n\n    # Get error class probabilities\n    predictions = nli_classifier(error_detail[\"text\"], error_classes)\n\n    # Get probabilities and change the order to match error class order\n    probabilites = pd.Series(\n        predictions[\"scores\"],\n        index=[\n            error_classes.index(error_class) for error_class in predictions[\"labels\"]\n        ],\n    ).sort_index()\n\n    nli_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilites)),\n        )\n    )\n\nnli_results_df = pd.DataFrame(nli_results)\nshow_error_class_results(nli_results_df)",
        "# Instructions for the conversational model\n# Note that the instructions ask for JSON formatted output so you can have Python process the results\nsystem_prompt_template = (\n    \"ChatGPT, I would like you to analyze a series of misclassified statements from a financial \"\n    \"news sentiment classification model and estimate the probabilites of reasons for the misclassification. \"\n    \"Please review each statement, and based on your understanding, determine whether the misclassification \"\n    \"likely occurred because:\\n\"\n    \"    1. There was not enough context to accurately determine the sentiment.\\n\"\n    \"    2. The statement itself is ambiguous, making it hard to assign a clear sentiment.\\n\"\n    \"    3. The sentiment label ({label}) provided in the dataset is incorrect.\\n\"\n    \"    4. The sentiment label ({label}) provided in the dataset is correct and the model is wrong.\\n\"\n    \"Remember, your task is to analyze the statement and give your best guess for the probability \"\n    \"of each misclassification reason from the above four numbered options.\\n\"\n    \"Please respond in JSON format with only the reason numbers and probability estimates. \"\n    \"Use the numbers (1, 2, 3 and 4) for the reasons.\\n\"\n    \"IMPORTANT: Don't include any explanations or anything that cannot be parsed as JSON.\"\n)\n\nchat_results = []\nfor error_detail in error_details:\n    # Create message with the instructions and the text you want to classify\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt_template.format(\n                label=error_detail[\"actual_label\"]\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f'Statement: \"{error_detail[\"text\"]}\"\\n'\n                f'Correct Sentiment: {error_detail[\"actual_label\"]}\\n'\n                f'Predicted Sentiment: {error_detail[\"predicted_label\"]}'\n            ),\n        },\n    ]\n\n    # Send your request to a conversational model (GPT3.5)\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0\n    )\n\n    # Turn ChatGPT response into a DataFrame\n    result = completion.choices[0].message.content.strip()\n    result_json = json.loads(result)\n\n    # Get probabilities and make sure order matches error class order\n    probabilities = pd.Series(\n        result_json.values(), index=result_json.keys()\n    ).sort_index()\n\n    chat_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilities)),\n        )\n    )\n\n# Convert results to DataFrame and set missing probabilities to zero\nchat_results_df = pd.DataFrame(chat_results).fillna(0)\n\nshow_error_class_results(chat_results_df)",
        "# Short names to use as column headers\nshort_error_classes = [\n    \"Ambiguous\",\n    \"No Context\",\n    \"Mislabeled\",\n    \"Model Misclassification\",\n]\n\n# For each method, get the highest probability and assign a short label\nembedding_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(\n        embedding_results_df[error_class_templates].values, axis=1\n    )\n]\nnli_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(nli_results_df[error_class_templates].values, axis=1)\n]\nchat_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(chat_results_df[error_class_templates].values, axis=1)\n]\n\n# Show summary of results in a table\nhtml = \"<table><tr><th>Sentence</th><th>Embedding</th><th>NLI</th><th>Chat</th></tr>\"\nfor sentence, embedding_error_class, nli_error_class, chat_error_class in zip(\n    embedding_results_df[\"sentence\"],\n    embedding_results_short,\n    nli_results_short,\n    chat_results_short,\n):\n    html += (\n        f\"<tr><td>{sentence}</td><td>{embedding_error_class}</td><td>{nli_error_class}</td>\"\n        f\"<td>{chat_error_class}</td></tr>\"\n    )\ndisplay(HTML(html + \"</table>\"))"
      ],
      "api_methods": [
        "project.wait_for_autopilot",
        "model.sample_pct",
        "model.model_type",
        "project.start",
        "model.request_predictions",
        "project.get_models",
        "dr.project.start"
      ],
      "complexity_score": 1.0,
      "use_case_category": "modeling"
    },
    {
      "id": "github_notebook_-8057759570243150651",
      "title": "FP&A",
      "content": "# Financial Planning and Analysis Workflow\n\n## Summary\nThis notebook illustrates an end-to-end FP&A workflow in DataRobot. Time series forecasting in DataRobot has a huge suite of tools and approaches to handle highly complex multiseries problems. \n\nDataRobot will be used for the model training, selection, deployment, and creating forecasts. While this example will leverage a snapshot file as a datasource this workflow applies to any data source, e.g. Redshift, S3, Big Query, Synapse, etc. \n\n\nThis notebook will demonstrate how to use the Python API client to:\n\n1.  Connect to DataRobot\n2.  Import and preparation of data for time series modeling\n3.  Create a time series forecasting project and run Autopilot\n4.  Retrieve and evaluate model performance and insights\n5.  Making forward looking forecasts\n6.  Evaluating forecasts vs. historical trends\n7.  Deploy a model\n\n## Setup - Import libraries\n\n```python\nfrom datetime import datetime as dt\nfrom platform import python_version\n\nimport datarobot as dr\nfrom datarobot.models.data_engine_query_generator import (\n    QueryGeneratorDataset,\n    QueryGeneratorSettings,\n)\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nprint(\"Python version:\", python_version())\nprint(\"Client version:\", dr.__version__)\n```\n\n## Connect to DataRobot\nTo connect to DataRobot, you need to provide your API token and the endpoint. For more information, please refer to the following documentation:\n\n - [Create and manage API keys via developer tools in the GUI](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management)\n - [Different options to connect to DataRobot from the API client](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n   \nYour API token can be found in the DataRobot UI in the Developer tools section, accessed from the profile menu in the top right corner. Copy the API token and paste in the cell below.\n\n```python\n# Instantiate the DataRobot connection\n\n# Get the token from the Developer Tools page in the DataRobot UI\nDATAROBOT_API_TOKEN = \"\"\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # This should be the URL you use to access the DataRobot UI\n\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n```\n\n## Import Data\n\nWe will be importing 4 years of raw transactional sales data and then wrangling that data, for each product segment, into a time series format for modeling\n\n\n```python\n# Read in csv file to dataframe\ndf = pd.read_csv(\"storage/sales.csv\")\n\n# Convert 'Order Date' columns to datetime format\ndf[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n\n# Display first few rows of data\ndf.head()\n```\n\n## Data Preparation\nWe now need to transform our transactional data into a time series dataset with evenly spaced intervals. We will leverage DataRobot's [Data Prep for Time Series](https://docs.datarobot.com/en/docs/modeling/time/ts-modeling-data/ts-data-prep.html#set-manual-options) to transform the dataset and aggregate each product segment to the monthly level.\n<br/>\n<br/>\nGenerally speaking, DataRobot's Data Prep for Time Series will aggregates the dataset to the selected time step, and, if missing rows are detected will impute the target value. Data Prep for Time Series allows you to choose aggregation methods for numeric, categorical, and text values. You can also use it to explore modeling at different time scales. The resulting dataset is then published to DataRobot's AI Catalog.\n\n```python\n# Upload the dataset to the AI Catalog\ndataset = dr.Dataset.upload(df)\n\n# Rename the entry in AI Catalog\ndataset.modify(name=\"Transactional_Sales_Data\", categories=dataset.categories)\n\n# Create a time series data prep query generator from the dataset we just uploaded to AI Catalog\nquery_generator_dataset = QueryGeneratorDataset(\n    alias=\"Transactional_Sales_Data\",\n    dataset_id=dataset.id,\n    dataset_version_id=dataset.version_id,\n)\n\n# Set the parameters for our time series Data Prep\nquery_generator_settings = QueryGeneratorSettings(\n    datetime_partition_column=\"Order Date\",  # Date/time feature used as the basis for partitioning\n    time_unit=\"MONTH\",  # Time unit (seconds, days, months, etc.) that comprise the time step\n    time_step=1,  # Number of (time) units that comprise the time step.\n    default_numeric_aggregation_method=\"sum\",  # Aggregate the target using either mean & most recent or sum & zero\n    default_categorical_aggregation_method=\"last\",  # Aggregate categorical features using the most frequent value or the last value within the aggregation time step.\n    target=\"Sales\",  # Numeric column in the dataset to predict.\n    multiseries_id_columns=[\n        \"Segment\"\n    ],  # Column containing the series identifier, which allows DataRobot to process the dataset as a separate time series.\n    default_text_aggregation_method=\"meanLength\",  # Choose ignore to skip handling of text features or aggregate by: 'concat', 'last', 'meanLength', 'mostFrequent', 'totalLength'\n    start_from_series_min_datetime=True,  # Basis for the series start date, either the earliest date for each series (per series) or the earliest date found for any series (global).\n    end_to_series_max_datetime=True,  # Basis for the series end date, either the last entry date for each series (per series) or the latest date found for any series (global).\n)\nquery_generator = dr.DataEngineQueryGenerator.create(\n    generator_type=\"TimeSeries\",\n    datasets=[query_generator_dataset],\n    generator_settings=query_generator_settings,\n)\n\n# Prep the training dataset\ntraining_dataset = query_generator.create_dataset()\n\n# Rename the entry in AI Catalog\ntraining_dataset.modify(\n    name=\"ts_monthly_training\", categories=training_dataset.categories\n)\n```\n\n## Exploratory Data Analysis\n\nLet's visualize the monthly sales for each segment in order to quickly identify anomalies, evaluate patterns and seasonality, and identify anything that may warrant further expoloration.\n\n```python\n# Load the dataset into a pandas dataframe\ntraining_df = training_dataset.get_as_dataframe()\n\n# Convert 'Order Date' to datetime format and sort\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\n\n# Adding Total Sales as an additional segment\ntotal_sales = training_df.groupby(\"Order Date\").agg({\"Sales\": \"sum\"}).reset_index()\ntotal_sales[\"Segment\"] = \"Total\"\ntraining_df = pd.concat([training_df, total_sales], ignore_index=True)\n\n# Visualize our data:\nfig = go.Figure()\n\n# Line chart for monthly sales by segment\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment A\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment B\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment C\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Total\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Total\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Total\",\n    )\n)\n\nfig.update_layout(\n    title=\"Monthly Sales by Segment and Total\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    template=\"plotly_dark\",\n)\nfig.show()\n```\n\n![ExploratoryDataAnalysis.png](attachment:ExploratoryDataAnalysis.png)\n\n## Create a time series forecasting project and run Autopilot\n\nWe can create a project using our dataset in the AI Catalog:\n\n```python\n# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"Monthly_Sales_Forecast\", dataset_id=training_dataset.id\n)\n```\n\n```python\n# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)\n```\n\nConfigure time-series modeling settings\nTime-series projects have a number of parameters we can adjust. This includes:\n\n- Multi-series (i.e. Series ID column)\n- Backtest partitioning\n- Feature Derivation Window\n- Forecast Window\n- Known-in-advance (KA) Variables\n- Do not derive (DND) Variables\n- Calendars\n\n```python\n# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-6, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 12)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = []\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Order Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Order Date\"]),\n)  # Last date in calendar\n```\n\nWe pass all our settings to a [DatetimePartitioningSpecification](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/datetime_partition.html?highlight=datetimepartitioningspecification#setting-up-a-datetime-partitioned-project) object which will then be passed to our Autopilot process.\n\n```python\n# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Order Date\",  # Date column\n    multiseries_id_columns=[\"Segment\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)\n```\n\n## Start modeling with autopilot\nTo start the Autopilot process, call the analyze_and_model function. Provide the prediction target and our DatetimePartitioningSpecification as part of the function call. We have several modes to spin up Autopilot - in this demo, we will use the default \"Quick\" mode.\n\n```python\n# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)\n```\n\n```python\n# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)\n```\n\n## Retrieve and evaluate model performances and insights\nAfter Autopilot completes, you can easily evaluate your model results. Evaluation can include compiling the Leaderboard as a dataframe, measuring performances across different backtest partitions with different metrics, visualizing the accuracy across series, analyzing Feature Impact and Feature Effects to understand each models' behaviors, and more. This can be done for every single model created by DataRobot.\n\nAs a simple example in this notebook, <b>we identify the best model created by Autopilot and evaluate</b>:\n\n- RMSE performance\n- MASE performance\n- Accuracy for Time for various Forecast Distance and Series combinations\n- Feature Impact of Top 10 features\n- Compare Accuracy across Series\n\n```python\n# Unlock the holdout set within the project\nproject.unlock_holdout()\n```\n\n```python\n# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\ndf_list = []  # This will store each individual DataFrame to concatenate at the end\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    df_list.append(model_performances)  # Append the DataFrame to the list\n\n# Concatenate all DataFrames in the list\nscores = pd.concat(df_list, ignore_index=True)\n\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores\n```\n\n```python\n# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)\n```\n\n```python\nprint(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)\n```\n\n## Get Accuracy Over Time\nDataRobot provides two helpful views of our forecasts out-of-the-box:\n\n- [Accuracy Over Time](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/aot.html) fixes the forecast distance and visualizes the corresponding forecast for each forecasted date.\n- [Forecast vs Actual](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/fore-act.html) sets a specific forecast point and visualizes the corresponding forecasts for the entire forecast window.\nWe can pull the results out for either analysis. As a demonstration, we will generate the Accuracy Over Time plots for forecast distances of 1 day and 7 day.\n\n```python\n# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=1, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=1\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n)\n\n# Display the plot\n\n# Update layout\nfig.update_layout(template=\"plotly_dark\")\nfig.show()\n```\n\n![AccuracyOverTime_All.png](attachment:AccuracyOverTime_All.png)\n\n```python\n# Get Accuracy over Time for FD=6, For just Segment A\nacc_plot_FD6_Consumer = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=6, series_id=\"Segment A\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD6_Consumer.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=6 (Segment A)\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()\n```\n\n![AccuracyOverTimeSegmentA.png](attachment:AccuracyOverTimeSegmentA.png)\n\n## Retrieve Feature Impact\nAs an example of model explainability, calculate the Feature Impact values of the model using the get_or_request_feature_impact function.\n\n```python\n# get model\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][2])\n\n# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:5]\n\n# Plotly Graph\nfig = go.Figure()\n\n# Add bar trace\nfig.add_trace(\n    go.Bar(y=FI_df[\"featureName\"], x=FI_df[\"impactNormalized\"], orientation=\"h\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Feature Impact\",\n    xaxis=dict(title=\"Normalized Feature Impact\", range=[0, 1.1]),\n    yaxis=dict(title=\"Feature\", autorange=\"reversed\"),\n    margin=dict(\n        l=200\n    ),  # this is to ensure the y-labels (feature names) are not cut off\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()\n```\n\n![FeatureImpact.png](attachment:FeatureImpact.png)\n\n## Analyze Accuracy for each Series\nThe [Series Insight](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/series-insights-multi.html) tool provides the ability to compute the accuracy for each indivudal series. This is especially powerful to help us identify which series the model is doing particularly better or worse in forecasting.\n\nIn this demonstration, we see that the model has particularly high RMSE for the Savannah and Louisville store forecasts. We may consider refining our model by splitting those two series into a separate model as a future modeling experiment.\n\n```python\n# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing\n```\n\n```python\n# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight\n```\n\n```python\n# Create a scatter plot with Plotly\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x=model_series_insight[\"Series\"],\n        y=model_series_insight[\"backtestingScore\"],\n        mode=\"markers\",\n    )\n)\n\n# Update the layout\nfig.update_layout(\n    title=\"Performance by Segment\",\n    xaxis=dict(title=\"Series\", tickangle=45),\n    yaxis=dict(title=\"RMSE\"),\n)\n\n# Display the plot\nfig.update_layout(template=\"plotly_dark\")\nfig.show()\n```\n\n![PerformanceBySegment.png](attachment:PerformanceBySegment.png)\n\n## Make new predictions with a test dataset\nWe can make new predictions directly on the leaderboard by uploading new test datasets to the project. We can then score the test dataset with any model on the leaderboard and retrieve the results.\n\nIn this notebook, we will load the data into the notebook and upload to the project. As with the training dataset, you can also use the JDBC connector to ingest the test dataset into the AI catalog and then directly use the dataset ID in the request_predictions call. The JDBC path provides more efficient upload of large datasets to DataRobot.\n\nFor time-series predictions, DataRobot expects new prediction datasets to have rows for each new date we want to forecast. These rows should have values for the known-in-advance features and NA everywhere else. For example, since the model is forecasting 1-7 days out from each forecast point, we will have 7 new rows (corresponding from June 15 to June 21, 2014).\n\n```python\n# Upload data to modeling project\ndf = training_dataset.get_as_dataframe()\ntest_dataset = project.upload_dataset(df)\n\n# Get frozen model\nfrozen_model = all_models[0]\n\n# Request Predictions\npred_job = frozen_model.request_predictions(\n    dataset_id=test_dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=85,\n)\n\npreds = pred_job.get_result_when_complete()\n\npreds.head(5)\n```\n\n```python\n# Step 1: Rename columns in preds to match training_df\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Step 2: Drop columns from preds that are not in training_df\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]\n```\n\n## Join Forecasts to Actuals\n\n```python\n# Function to evaluate date time values\n\n\ndef convert_or_localize_to_utc(series):\n    if series.dt.tz is not None:  # If it's already timezone-aware\n        return series.dt.tz_convert(\"UTC\")\n    return series.dt.tz_localize(\"UTC\")  # If it's naive, then localize it\n\n\n# Convert or localize 'Order Date' in training_df to UTC\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\ntraining_df[\"Order Date\"] = convert_or_localize_to_utc(training_df[\"Order Date\"])\n\n# Rename columns in prediction data\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Drop columns from prediction data that is not in training data\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]\n\n# Convert or localize 'Order Date' in preds_subset to UTC\npreds_subset = modified_preds.copy()  # To avoid SettingWithCopyWarning\npreds_subset[\"Order Date\"] = pd.to_datetime(preds_subset[\"Order Date\"])\npreds_subset[\"Order Date\"] = convert_or_localize_to_utc(preds_subset[\"Order Date\"])\n\n# Concatenate the DataFrames\ncombined_df = pd.concat([training_df, preds_subset], ignore_index=True)\n\n# Sort by Segment and Order Date\ncombined_df.sort_values(by=[\"Segment\", \"Order Date\"], inplace=True)\n```\n\n## Visualize Forecasts With a Line Chart\n\n```python\n# Initialize the figure\nfig = go.Figure()\n\n# Define a color mapping for segments (you can extend or modify this as needed)\ncolor_map = {\"Segment A\": \"blue\", \"Segment B\": \"green\", \"Segment C\": \"orange\"}\n\n# For each segment, plot actual sales and forecasted sales\nfor segment, color in color_map.items():\n    segment_df = combined_df[combined_df[\"Segment\"] == segment]\n    actuals = segment_df[:-12]  # Adjust based on your actual data\n    forecast = segment_df[-12:]\n\n    fig.add_trace(\n        go.Scatter(\n            x=actuals[\"Order Date\"],\n            y=actuals[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Actuals\",\n            line=dict(color=color),  # Use the color from the color_map\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=forecast[\"Order Date\"],\n            y=forecast[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Forecast\",\n            line=dict(\n                dash=\"dot\", color=color\n            ),  # Use the color from the color_map and make the line dotted\n        )\n    )\n\nfig.update_layout(\n    title=\"Actual + Forecasted Sales by Segment\",\n    xaxis_title=\"Order Date\",\n    yaxis_title=\"Sales\",\n    legend_title=\"Segments\",\n    template=\"plotly_dark\",\n)\n\nfig.show()\n```\n\n![ActualsAndForecasts.png](attachment:ActualsAndForecasts.png)\n\n## Evaluate Forecasted Growth Metrics\n\nFP&A teams typically use a variety of growth metrics to gain a comprehensive understanding of sales performance. Different metrics can provide unique insights into how a financial metrics is trending and what underlying factors might be driving those trends. \n\n1. **Month Over Month Growth (MoM Growth)**: evaluates the growth of sales (or any other metric) from one month to the next. MoM Growth can provide a short-term perspective on trends to quickly identify any sudden changes or spikes which might be due to seasonality, promotions, or other short-term factors. Useful for identifying short-term operational challenges or successes.\n\n2. **Monthly Year Over Year Growth (YoY Growth)**: evaluates a specific month to the sales in the same month the previous year. This view accounts for seasonality, as it compares the same month across years and can helps identify longer-term trends and determine if a particular month's sales are genuinely growing or declining over the years.\n\n3. **Year to Date Year over Year Growth (YTD YoY Growth)**: evaluates the cumulative sum from the beginning of the year up to a specific month as compared to the same time period in the previous year. This provides an aggregated view of performance over a year, making it easier to see if the company is on track to meet annual targets. This can also mitigate any month-to-month volatility, providing a more consistent perspective on how the year is progressing.\n\n4. **Rolling 12 Month Year Over Year Growth (R12M YoY Growth)**: evaluates the most recent 12-month period to sales in the 12 months before that. This offers a continuously updated yearly perspective, irrespective of where you are in the calendar year and smoothens out short-term fluctuations and seasonality, as it always encompasses a full year of data.\n\n### Calculate the forecasted year over year growth\n\n```python\ndef calculate_growth(segment_data):\n    forecast_sum = segment_data.tail(12)[\"Sales\"].sum()\n    prior_year_sum = segment_data.iloc[-24:-12][\"Sales\"].sum()\n    two_years_ago_sum = segment_data.iloc[-36:-24][\"Sales\"].sum()\n\n    forecasted_growth = (forecast_sum - prior_year_sum) / prior_year_sum * 100\n    prior_year_growth = (prior_year_sum - two_years_ago_sum) / two_years_ago_sum * 100\n\n    return forecast_sum, prior_year_sum, forecasted_growth, prior_year_growth\n\n\nsegments = combined_df[\"Segment\"].unique()\ngrowth_data = [\n    (*calculate_growth(combined_df[combined_df[\"Segment\"] == segment]), segment)\n    for segment in segments\n]\n\ngrowth_df = pd.DataFrame(\n    growth_data,\n    columns=[\n        \"Forecasted Total\",\n        \"Prior Year Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n        \"Segment\",\n    ],\n)\n\n# Calculate aggregate level\naggregate_prior_year = growth_df[\"Prior Year Total\"].sum()\naggregate_forecast = growth_df[\"Forecasted Total\"].sum()\naggregate_growth = (\n    (aggregate_forecast - aggregate_prior_year) / aggregate_prior_year * 100\n)\n\nadjusted_prior_year_growth = (\n    (\n        aggregate_prior_year\n        - growth_df[\"Prior Year Total\"].sum()\n        + growth_df.iloc[-1][\"Prior Year Total\"]\n    )\n    / (growth_df[\"Prior Year Total\"].sum() - growth_df.iloc[-1][\"Prior Year Total\"])\n    * 100\n)\n\naggregate_row = pd.DataFrame(\n    [\n        {\n            \"Segment\": \"Total\",\n            \"Prior Year Total\": aggregate_prior_year,\n            \"Forecasted Total\": aggregate_forecast,\n            \"Forecasted Growth\": aggregate_growth,\n            \"Prior Year Growth\": adjusted_prior_year_growth,\n        }\n    ]\n)\n\ngrowth_df = pd.concat([growth_df, aggregate_row], ignore_index=True)\n```\n\n## Visualize the year over year growth in a bar chart\n\n```python\ndef plot_sales_data_with_table(growth_df):\n    # Filter out the 'Total' row for the bar chart\n    chart_df = growth_df[growth_df[\"Segment\"] != \"Total\"]\n\n    # Create subplots: one row for bar chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.15,\n        subplot_titles=(\n            \"Prior Year vs Forecasted Sales Totals by Segment\",\n            \"Data Table\",\n        ),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Add bar chart to the first row of subplot\n    fig.add_trace(\n        go.Bar(\n            name=\"Prior Year Total\",\n            x=chart_df[\"Segment\"],\n            y=chart_df[\"Prior Year Total\"],\n        ),\n        row=1,\n        col=1,\n    )\n    forecasted_total_bar = go.Bar(\n        name=\"Forecasted Total\", x=chart_df[\"Segment\"], y=chart_df[\"Forecasted Total\"]\n    )\n    fig.add_trace(forecasted_total_bar, row=1, col=1)\n\n    # Overlay forecast growth on top of the Forecasted Total bars\n    for i, segment in enumerate(chart_df[\"Segment\"]):\n        fig.add_annotation(\n            x=segment,\n            y=chart_df.loc[chart_df[\"Segment\"] == segment, \"Forecasted Total\"].values[\n                0\n            ],\n            text=f\"{chart_df.loc[chart_df['Segment'] == segment, 'Forecasted Growth'].values[0]:.2f}%\",\n            showarrow=False,\n            font=dict(color=\"white\"),\n            row=1,\n            col=1,\n        )\n\n    # Change the bar mode to 'group'\n    fig.update_layout(barmode=\"group\")\n\n    # Specify the order of columns for the table\n    ordered_columns = [\n        \"Segment\",\n        \"Prior Year Total\",\n        \"Forecasted Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n    ]\n\n    # Format the numeric values with commas and two decimal points\n    formatted_data = []\n    for col in ordered_columns:\n        if growth_df[col].dtype in [float, \"float64\"]:\n            # Format float columns\n            formatted_data.append(growth_df[col].map(\"{:,.2f}\".format).tolist())\n        elif growth_df[col].dtype in [int, \"int64\"]:\n            # Format integer columns\n            formatted_data.append(growth_df[col].map(\"{:,}\".format).tolist())\n        else:\n            # Keep non-numeric columns as is\n            formatted_data.append(growth_df[col].tolist())\n\n    table_header = ordered_columns\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=formatted_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\")\n\n    fig.show()\n\n\n# Example usage with your growth_df dataframe\nplot_sales_data_with_table(growth_df)\n```\n\n![PriorYearVsForecast.png](attachment:PriorYearVsForecast.png)\n\n## Calculate additional metrics to evaluate forecasted growth\n\n```python\n# Create a copy of training_df with only the necessary columns\ndf_copy = combined_df[[\"Segment\", \"Order Date\", \"Sales\"]].copy()\n\n# Convert \"Order Date\" column of df_copy to datetime and set as index\ndf_copy[\"Order Date\"] = pd.to_datetime(df_copy[\"Order Date\"])\ndf_copy.set_index(\"Order Date\", inplace=True)\n\n\ndef calculate_growth_metrics(segment_data):\n    # Using .loc to set values to avoid SettingWithCopyWarning\n    segment_data = segment_data.copy()\n    segment_data.loc[:, \"Year\"] = segment_data.index.year\n    segment_data.loc[:, \"Month\"] = segment_data.index.month\n\n    # Group by year and month to get monthly sales and specify numeric_only=True\n    monthly_sales = segment_data.groupby([\"Year\", \"Month\"]).sum(numeric_only=True)\n\n    # Calculate growth metrics\n    monthly_sales[\"MoM Growth\"] = monthly_sales[\"Sales\"].pct_change() * 100\n    monthly_sales[\"Monthly YoY Growth\"] = (\n        monthly_sales[\"Sales\"].pct_change(periods=12) * 100\n    )\n    monthly_sales[\"Rolling 12 Mth YoY Growth\"] = (\n        monthly_sales[\"Sales\"].rolling(window=12).sum().pct_change(periods=12) * 100\n    )\n\n    # New YTD Growth logic\n    monthly_sales[\"YTD\"] = monthly_sales[\"Sales\"].groupby(level=0).cumsum()\n    previous_ytd = monthly_sales[\"YTD\"].shift(12)\n    monthly_sales[\"YTD YoY Growth\"] = (\n        (monthly_sales[\"YTD\"] - previous_ytd) / previous_ytd\n    ) * 100\n\n    # Convert 'Year' and 'Month' back to actual date (the first day of each month)\n    monthly_sales.reset_index(inplace=True)\n    monthly_sales[\"Order Date\"] = pd.to_datetime(\n        monthly_sales[[\"Year\", \"Month\"]].assign(DAY=1)\n    )\n    monthly_sales.drop([\"Year\", \"Month\"], axis=1, inplace=True)\n\n    return monthly_sales\n\n\n# Filter data based on segments and calculate metrics\nsegmenta_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment A\"])\nsegmentb_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment B\"])\nsegmentc_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment C\"])\ntotal_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Total\"])\n```\n\n## Visualize the additional metrics with line charts\n\n```python\n# Function to plot a specific growth metric for all segments and show table below\n\n\ndef plot_metric_across_segments_with_table(metric_name, datasets, yaxis_range=None):\n    # Create subplots: one row for line chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.1,\n        subplot_titles=(f\"Comparison of {metric_name} Across Segments\", \"Data Table\"),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Lists to store filtered data for the table\n    table_dates = None\n    table_metric_values = []\n\n    # Add line plots to the first row of subplot\n    for segment_name, segment_df in datasets.items():\n        # Filter DataFrame to remove null values in the metric column and keep data from January 2014 onwards\n        segment_df = segment_df[\n            pd.notnull(segment_df[metric_name])\n            & (segment_df[\"Order Date\"] >= \"2014-01-01\")\n        ]\n\n        # Store filtered data for table\n        if table_dates is None:\n            table_dates = segment_df[\"Order Date\"].tolist()\n\n        # Format metric values to two decimal points\n        formatted_metric_values = [\n            round(val, 2) for val in segment_df[metric_name].tolist()\n        ]\n        table_metric_values.append(formatted_metric_values)\n\n        fig.add_trace(\n            go.Scatter(\n                x=segment_df[\"Order Date\"],\n                y=segment_df[metric_name],\n                mode=\"lines\",\n                name=segment_name,\n            ),\n            row=1,\n            col=1,\n        )\n\n    # Create a table with the filtered data\n    table_data = [table_dates] + table_metric_values\n    table_header = [\"Order Date\"] + list(datasets.keys())\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=table_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\", yaxis_range=yaxis_range)\n\n    fig.show()\n\n\n# Datasets for all segments\ndatasets = {\n    \"Segment A\": segmenta_df,\n    \"Segment B\": segmentb_df,\n    \"Segment C\": segmentc_df,\n    \"Total\": total_df,\n}\n\n# Create the plots for each specific metric:\nplot_metric_across_segments_with_table(\"MoM Growth\", datasets, yaxis_range=[-200, 500])\nplot_metric_across_segments_with_table(\n    \"Monthly YoY Growth\", datasets, yaxis_range=[-200, 600]\n)\nplot_metric_across_segments_with_table(\n    \"YTD YoY Growth\", datasets, yaxis_range=[-200, 300]\n)\nplot_metric_across_segments_with_table(\"Rolling 12 Mth YoY Growth\", datasets)\n```\n\n![ComparisonOfMOM.png](attachment:ComparisonOfMOM.png)\n\n![MonthlyYoY.png](attachment:MonthlyYoY.png)\n\n![YTDYoY.png](attachment:YTDYoY.png)\n\n![Rolling12YoY.png](attachment:Rolling12YoY.png)\n\n## Deploy Model to DataRobot ML Production for Monitoring and Governance\n\n```python\n# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=frozen_model.id,\n    label=\"FP&A - Segment Forecasting Model\",\n    description=\"FP&A - Segment Forecasting Model\",\n    default_prediction_server_id=prediction_server_id,\n)\n```\n\n## Request Predictions from Deployment\n\n```python\n# Score the dataset using the given deployment ID\njob, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, df\n)  # Deployment ID and Scoring dataset\n\n# Print a message to indicate that scoring has started\nprint(\"Started scoring...\", job)\n\n# Wait for the job to complete\njob.wait_for_completion()\n```\n\n## Clean Up\n\n```python\n# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\ndeployment.delete()\n# project.delete()\n```\n\n# Next Steps and Additional Resources\n\n## Improve Accuracy\n\nThere are a number of approaches that we could apply to improve model accuracy. We can try:\n\n\n-  Running DataRobot's entire blueprint repository\n-  Evaluating different feature derivation windows across projects\n-  Evaluating different training lengths\n-  Evaluating different blenders / ensemble models\n-  Adding in additional data or other data sources (e.g. macroeconomic data)\n\n## Other Use Cases\n- [Scenario analysis](https://docs.datarobot.com/en/docs/app-builder/ts-app.html#what-if-widget) - evaluate what happens to our sales forecasts under certain conditions by leveraging [known in advance variables](https://docs.datarobot.com/en/docs/modeling/build-models/adv-opt/time-series-adv-opt.html#set-known-in-advance-ka) in DataRobot\n- Long term strategic planning where we can forecast over longer time horizons than annual planning requires.\n- Risk management - model potential risks that have financial impacts, such as: churn, write-downs, etc.\n\n## Additional Resources\n- [The DataRobot AI Accelerator Library](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library) has similar accelerators for other [Ecosystem Integrations](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library/label-name/ecosystem%20integration%20templates) to use DataRobot with other tools (e.g. AWS, GCP, Azure, etc) as well as accelerators for more advanced time-series applications.\n- [The DataRobot API user guide](https://docs.datarobot.com/en/docs/api/guide/python/index.html) provides code examples covering topics such as model factories, classification problems, feature impact rank ensembling, and more.\n\nTo learn more about advanced workflows for handling complex and large scale time series problems:\n\n- [Time series clustering](https://docs.datarobot.com/en/docs/modeling/time/ts-clustering.html#time-series-clustering)\n- [Segmented modeling](https://docs.datarobot.com/en/docs/modeling/time/ts-segmented.html)",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "use_cases_and_horizontal_approaches/FP&A/FP&A.ipynb",
      "tags": [
        "planning",
        "use-cases",
        "deployment",
        "jupyter-notebook",
        "tutorial",
        "predictions",
        "time-series",
        "ai-accelerators",
        "openai",
        "datarobot",
        "business-applications",
        "analysis",
        "finance"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "use_cases_and_horizontal_approaches/FP&A/FP&A.ipynb",
        "size": 1557853,
        "cell_count": 72,
        "code_cell_count": 32
      },
      "code_examples": [
        "from datetime import datetime as dt\nfrom platform import python_version\n\nimport datarobot as dr\nfrom datarobot.models.data_engine_query_generator import (\n    QueryGeneratorDataset,\n    QueryGeneratorSettings,\n)\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nprint(\"Python version:\", python_version())\nprint(\"Client version:\", dr.__version__)",
        "# Instantiate the DataRobot connection\n\n# Get the token from the Developer Tools page in the DataRobot UI\nDATAROBOT_API_TOKEN = \"\"\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # This should be the URL you use to access the DataRobot UI\n\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)",
        "# Read in csv file to dataframe\ndf = pd.read_csv(\"storage/sales.csv\")\n\n# Convert 'Order Date' columns to datetime format\ndf[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n\n# Display first few rows of data\ndf.head()",
        "# Upload the dataset to the AI Catalog\ndataset = dr.Dataset.upload(df)\n\n# Rename the entry in AI Catalog\ndataset.modify(name=\"Transactional_Sales_Data\", categories=dataset.categories)\n\n# Create a time series data prep query generator from the dataset we just uploaded to AI Catalog\nquery_generator_dataset = QueryGeneratorDataset(\n    alias=\"Transactional_Sales_Data\",\n    dataset_id=dataset.id,\n    dataset_version_id=dataset.version_id,\n)\n\n# Set the parameters for our time series Data Prep\nquery_generator_settings = QueryGeneratorSettings(\n    datetime_partition_column=\"Order Date\",  # Date/time feature used as the basis for partitioning\n    time_unit=\"MONTH\",  # Time unit (seconds, days, months, etc.) that comprise the time step\n    time_step=1,  # Number of (time) units that comprise the time step.\n    default_numeric_aggregation_method=\"sum\",  # Aggregate the target using either mean & most recent or sum & zero\n    default_categorical_aggregation_method=\"last\",  # Aggregate categorical features using the most frequent value or the last value within the aggregation time step.\n    target=\"Sales\",  # Numeric column in the dataset to predict.\n    multiseries_id_columns=[\n        \"Segment\"\n    ],  # Column containing the series identifier, which allows DataRobot to process the dataset as a separate time series.\n    default_text_aggregation_method=\"meanLength\",  # Choose ignore to skip handling of text features or aggregate by: 'concat', 'last', 'meanLength', 'mostFrequent', 'totalLength'\n    start_from_series_min_datetime=True,  # Basis for the series start date, either the earliest date for each series (per series) or the earliest date found for any series (global).\n    end_to_series_max_datetime=True,  # Basis for the series end date, either the last entry date for each series (per series) or the latest date found for any series (global).\n)\nquery_generator = dr.DataEngineQueryGenerator.create(\n    generator_type=\"TimeSeries\",\n    datasets=[query_generator_dataset],\n    generator_settings=query_generator_settings,\n)\n\n# Prep the training dataset\ntraining_dataset = query_generator.create_dataset()\n\n# Rename the entry in AI Catalog\ntraining_dataset.modify(\n    name=\"ts_monthly_training\", categories=training_dataset.categories\n)",
        "# Load the dataset into a pandas dataframe\ntraining_df = training_dataset.get_as_dataframe()\n\n# Convert 'Order Date' to datetime format and sort\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\n\n# Adding Total Sales as an additional segment\ntotal_sales = training_df.groupby(\"Order Date\").agg({\"Sales\": \"sum\"}).reset_index()\ntotal_sales[\"Segment\"] = \"Total\"\ntraining_df = pd.concat([training_df, total_sales], ignore_index=True)\n\n# Visualize our data:\nfig = go.Figure()\n\n# Line chart for monthly sales by segment\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment A\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment B\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment C\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Total\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Total\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Total\",\n    )\n)\n\nfig.update_layout(\n    title=\"Monthly Sales by Segment and Total\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    template=\"plotly_dark\",\n)\nfig.show()",
        "# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"Monthly_Sales_Forecast\", dataset_id=training_dataset.id\n)",
        "# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)",
        "# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-6, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 12)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = []\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Order Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Order Date\"]),\n)  # Last date in calendar",
        "# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Order Date\",  # Date column\n    multiseries_id_columns=[\"Segment\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)",
        "# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)",
        "# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)",
        "# Unlock the holdout set within the project\nproject.unlock_holdout()",
        "# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\ndf_list = []  # This will store each individual DataFrame to concatenate at the end\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    df_list.append(model_performances)  # Append the DataFrame to the list\n\n# Concatenate all DataFrames in the list\nscores = pd.concat(df_list, ignore_index=True)\n\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores",
        "# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)",
        "print(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)",
        "# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=1, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=1\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n)\n\n# Display the plot\n\n# Update layout\nfig.update_layout(template=\"plotly_dark\")\nfig.show()",
        "# Get Accuracy over Time for FD=6, For just Segment A\nacc_plot_FD6_Consumer = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=6, series_id=\"Segment A\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD6_Consumer.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=6 (Segment A)\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()",
        "# get model\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][2])\n\n# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:5]\n\n# Plotly Graph\nfig = go.Figure()\n\n# Add bar trace\nfig.add_trace(\n    go.Bar(y=FI_df[\"featureName\"], x=FI_df[\"impactNormalized\"], orientation=\"h\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Feature Impact\",\n    xaxis=dict(title=\"Normalized Feature Impact\", range=[0, 1.1]),\n    yaxis=dict(title=\"Feature\", autorange=\"reversed\"),\n    margin=dict(\n        l=200\n    ),  # this is to ensure the y-labels (feature names) are not cut off\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()",
        "# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing",
        "# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight",
        "# Create a scatter plot with Plotly\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x=model_series_insight[\"Series\"],\n        y=model_series_insight[\"backtestingScore\"],\n        mode=\"markers\",\n    )\n)\n\n# Update the layout\nfig.update_layout(\n    title=\"Performance by Segment\",\n    xaxis=dict(title=\"Series\", tickangle=45),\n    yaxis=dict(title=\"RMSE\"),\n)\n\n# Display the plot\nfig.update_layout(template=\"plotly_dark\")\nfig.show()",
        "# Upload data to modeling project\ndf = training_dataset.get_as_dataframe()\ntest_dataset = project.upload_dataset(df)\n\n# Get frozen model\nfrozen_model = all_models[0]\n\n# Request Predictions\npred_job = frozen_model.request_predictions(\n    dataset_id=test_dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=85,\n)\n\npreds = pred_job.get_result_when_complete()\n\npreds.head(5)",
        "# Step 1: Rename columns in preds to match training_df\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Step 2: Drop columns from preds that are not in training_df\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]",
        "# Function to evaluate date time values\n\n\ndef convert_or_localize_to_utc(series):\n    if series.dt.tz is not None:  # If it's already timezone-aware\n        return series.dt.tz_convert(\"UTC\")\n    return series.dt.tz_localize(\"UTC\")  # If it's naive, then localize it\n\n\n# Convert or localize 'Order Date' in training_df to UTC\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\ntraining_df[\"Order Date\"] = convert_or_localize_to_utc(training_df[\"Order Date\"])\n\n# Rename columns in prediction data\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Drop columns from prediction data that is not in training data\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]\n\n# Convert or localize 'Order Date' in preds_subset to UTC\npreds_subset = modified_preds.copy()  # To avoid SettingWithCopyWarning\npreds_subset[\"Order Date\"] = pd.to_datetime(preds_subset[\"Order Date\"])\npreds_subset[\"Order Date\"] = convert_or_localize_to_utc(preds_subset[\"Order Date\"])\n\n# Concatenate the DataFrames\ncombined_df = pd.concat([training_df, preds_subset], ignore_index=True)\n\n# Sort by Segment and Order Date\ncombined_df.sort_values(by=[\"Segment\", \"Order Date\"], inplace=True)",
        "# Initialize the figure\nfig = go.Figure()\n\n# Define a color mapping for segments (you can extend or modify this as needed)\ncolor_map = {\"Segment A\": \"blue\", \"Segment B\": \"green\", \"Segment C\": \"orange\"}\n\n# For each segment, plot actual sales and forecasted sales\nfor segment, color in color_map.items():\n    segment_df = combined_df[combined_df[\"Segment\"] == segment]\n    actuals = segment_df[:-12]  # Adjust based on your actual data\n    forecast = segment_df[-12:]\n\n    fig.add_trace(\n        go.Scatter(\n            x=actuals[\"Order Date\"],\n            y=actuals[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Actuals\",\n            line=dict(color=color),  # Use the color from the color_map\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=forecast[\"Order Date\"],\n            y=forecast[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Forecast\",\n            line=dict(\n                dash=\"dot\", color=color\n            ),  # Use the color from the color_map and make the line dotted\n        )\n    )\n\nfig.update_layout(\n    title=\"Actual + Forecasted Sales by Segment\",\n    xaxis_title=\"Order Date\",\n    yaxis_title=\"Sales\",\n    legend_title=\"Segments\",\n    template=\"plotly_dark\",\n)\n\nfig.show()",
        "def calculate_growth(segment_data):\n    forecast_sum = segment_data.tail(12)[\"Sales\"].sum()\n    prior_year_sum = segment_data.iloc[-24:-12][\"Sales\"].sum()\n    two_years_ago_sum = segment_data.iloc[-36:-24][\"Sales\"].sum()\n\n    forecasted_growth = (forecast_sum - prior_year_sum) / prior_year_sum * 100\n    prior_year_growth = (prior_year_sum - two_years_ago_sum) / two_years_ago_sum * 100\n\n    return forecast_sum, prior_year_sum, forecasted_growth, prior_year_growth\n\n\nsegments = combined_df[\"Segment\"].unique()\ngrowth_data = [\n    (*calculate_growth(combined_df[combined_df[\"Segment\"] == segment]), segment)\n    for segment in segments\n]\n\ngrowth_df = pd.DataFrame(\n    growth_data,\n    columns=[\n        \"Forecasted Total\",\n        \"Prior Year Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n        \"Segment\",\n    ],\n)\n\n# Calculate aggregate level\naggregate_prior_year = growth_df[\"Prior Year Total\"].sum()\naggregate_forecast = growth_df[\"Forecasted Total\"].sum()\naggregate_growth = (\n    (aggregate_forecast - aggregate_prior_year) / aggregate_prior_year * 100\n)\n\nadjusted_prior_year_growth = (\n    (\n        aggregate_prior_year\n        - growth_df[\"Prior Year Total\"].sum()\n        + growth_df.iloc[-1][\"Prior Year Total\"]\n    )\n    / (growth_df[\"Prior Year Total\"].sum() - growth_df.iloc[-1][\"Prior Year Total\"])\n    * 100\n)\n\naggregate_row = pd.DataFrame(\n    [\n        {\n            \"Segment\": \"Total\",\n            \"Prior Year Total\": aggregate_prior_year,\n            \"Forecasted Total\": aggregate_forecast,\n            \"Forecasted Growth\": aggregate_growth,\n            \"Prior Year Growth\": adjusted_prior_year_growth,\n        }\n    ]\n)\n\ngrowth_df = pd.concat([growth_df, aggregate_row], ignore_index=True)",
        "def plot_sales_data_with_table(growth_df):\n    # Filter out the 'Total' row for the bar chart\n    chart_df = growth_df[growth_df[\"Segment\"] != \"Total\"]\n\n    # Create subplots: one row for bar chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.15,\n        subplot_titles=(\n            \"Prior Year vs Forecasted Sales Totals by Segment\",\n            \"Data Table\",\n        ),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Add bar chart to the first row of subplot\n    fig.add_trace(\n        go.Bar(\n            name=\"Prior Year Total\",\n            x=chart_df[\"Segment\"],\n            y=chart_df[\"Prior Year Total\"],\n        ),\n        row=1,\n        col=1,\n    )\n    forecasted_total_bar = go.Bar(\n        name=\"Forecasted Total\", x=chart_df[\"Segment\"], y=chart_df[\"Forecasted Total\"]\n    )\n    fig.add_trace(forecasted_total_bar, row=1, col=1)\n\n    # Overlay forecast growth on top of the Forecasted Total bars\n    for i, segment in enumerate(chart_df[\"Segment\"]):\n        fig.add_annotation(\n            x=segment,\n            y=chart_df.loc[chart_df[\"Segment\"] == segment, \"Forecasted Total\"].values[\n                0\n            ],\n            text=f\"{chart_df.loc[chart_df['Segment'] == segment, 'Forecasted Growth'].values[0]:.2f}%\",\n            showarrow=False,\n            font=dict(color=\"white\"),\n            row=1,\n            col=1,\n        )\n\n    # Change the bar mode to 'group'\n    fig.update_layout(barmode=\"group\")\n\n    # Specify the order of columns for the table\n    ordered_columns = [\n        \"Segment\",\n        \"Prior Year Total\",\n        \"Forecasted Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n    ]\n\n    # Format the numeric values with commas and two decimal points\n    formatted_data = []\n    for col in ordered_columns:\n        if growth_df[col].dtype in [float, \"float64\"]:\n            # Format float columns\n            formatted_data.append(growth_df[col].map(\"{:,.2f}\".format).tolist())\n        elif growth_df[col].dtype in [int, \"int64\"]:\n            # Format integer columns\n            formatted_data.append(growth_df[col].map(\"{:,}\".format).tolist())\n        else:\n            # Keep non-numeric columns as is\n            formatted_data.append(growth_df[col].tolist())\n\n    table_header = ordered_columns\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=formatted_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\")\n\n    fig.show()\n\n\n# Example usage with your growth_df dataframe\nplot_sales_data_with_table(growth_df)",
        "# Create a copy of training_df with only the necessary columns\ndf_copy = combined_df[[\"Segment\", \"Order Date\", \"Sales\"]].copy()\n\n# Convert \"Order Date\" column of df_copy to datetime and set as index\ndf_copy[\"Order Date\"] = pd.to_datetime(df_copy[\"Order Date\"])\ndf_copy.set_index(\"Order Date\", inplace=True)\n\n\ndef calculate_growth_metrics(segment_data):\n    # Using .loc to set values to avoid SettingWithCopyWarning\n    segment_data = segment_data.copy()\n    segment_data.loc[:, \"Year\"] = segment_data.index.year\n    segment_data.loc[:, \"Month\"] = segment_data.index.month\n\n    # Group by year and month to get monthly sales and specify numeric_only=True\n    monthly_sales = segment_data.groupby([\"Year\", \"Month\"]).sum(numeric_only=True)\n\n    # Calculate growth metrics\n    monthly_sales[\"MoM Growth\"] = monthly_sales[\"Sales\"].pct_change() * 100\n    monthly_sales[\"Monthly YoY Growth\"] = (\n        monthly_sales[\"Sales\"].pct_change(periods=12) * 100\n    )\n    monthly_sales[\"Rolling 12 Mth YoY Growth\"] = (\n        monthly_sales[\"Sales\"].rolling(window=12).sum().pct_change(periods=12) * 100\n    )\n\n    # New YTD Growth logic\n    monthly_sales[\"YTD\"] = monthly_sales[\"Sales\"].groupby(level=0).cumsum()\n    previous_ytd = monthly_sales[\"YTD\"].shift(12)\n    monthly_sales[\"YTD YoY Growth\"] = (\n        (monthly_sales[\"YTD\"] - previous_ytd) / previous_ytd\n    ) * 100\n\n    # Convert 'Year' and 'Month' back to actual date (the first day of each month)\n    monthly_sales.reset_index(inplace=True)\n    monthly_sales[\"Order Date\"] = pd.to_datetime(\n        monthly_sales[[\"Year\", \"Month\"]].assign(DAY=1)\n    )\n    monthly_sales.drop([\"Year\", \"Month\"], axis=1, inplace=True)\n\n    return monthly_sales\n\n\n# Filter data based on segments and calculate metrics\nsegmenta_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment A\"])\nsegmentb_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment B\"])\nsegmentc_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment C\"])\ntotal_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Total\"])",
        "# Function to plot a specific growth metric for all segments and show table below\n\n\ndef plot_metric_across_segments_with_table(metric_name, datasets, yaxis_range=None):\n    # Create subplots: one row for line chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.1,\n        subplot_titles=(f\"Comparison of {metric_name} Across Segments\", \"Data Table\"),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Lists to store filtered data for the table\n    table_dates = None\n    table_metric_values = []\n\n    # Add line plots to the first row of subplot\n    for segment_name, segment_df in datasets.items():\n        # Filter DataFrame to remove null values in the metric column and keep data from January 2014 onwards\n        segment_df = segment_df[\n            pd.notnull(segment_df[metric_name])\n            & (segment_df[\"Order Date\"] >= \"2014-01-01\")\n        ]\n\n        # Store filtered data for table\n        if table_dates is None:\n            table_dates = segment_df[\"Order Date\"].tolist()\n\n        # Format metric values to two decimal points\n        formatted_metric_values = [\n            round(val, 2) for val in segment_df[metric_name].tolist()\n        ]\n        table_metric_values.append(formatted_metric_values)\n\n        fig.add_trace(\n            go.Scatter(\n                x=segment_df[\"Order Date\"],\n                y=segment_df[metric_name],\n                mode=\"lines\",\n                name=segment_name,\n            ),\n            row=1,\n            col=1,\n        )\n\n    # Create a table with the filtered data\n    table_data = [table_dates] + table_metric_values\n    table_header = [\"Order Date\"] + list(datasets.keys())\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=table_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\", yaxis_range=yaxis_range)\n\n    fig.show()\n\n\n# Datasets for all segments\ndatasets = {\n    \"Segment A\": segmenta_df,\n    \"Segment B\": segmentb_df,\n    \"Segment C\": segmentc_df,\n    \"Total\": total_df,\n}\n\n# Create the plots for each specific metric:\nplot_metric_across_segments_with_table(\"MoM Growth\", datasets, yaxis_range=[-200, 500])\nplot_metric_across_segments_with_table(\n    \"Monthly YoY Growth\", datasets, yaxis_range=[-200, 600]\n)\nplot_metric_across_segments_with_table(\n    \"YTD YoY Growth\", datasets, yaxis_range=[-200, 300]\n)\nplot_metric_across_segments_with_table(\"Rolling 12 Mth YoY Growth\", datasets)",
        "# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=frozen_model.id,\n    label=\"FP&A - Segment Forecasting Model\",\n    description=\"FP&A - Segment Forecasting Model\",\n    default_prediction_server_id=prediction_server_id,\n)",
        "# Score the dataset using the given deployment ID\njob, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, df\n)  # Deployment ID and Scoring dataset\n\n# Print a message to indicate that scoring has started\nprint(\"Started scoring...\", job)\n\n# Wait for the job to complete\njob.wait_for_completion()",
        "# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\ndeployment.delete()\n# project.delete()"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "project.wait_for_autopilot",
        "model.metrics",
        "project.get_datetime_models",
        "model.get_series_accuracy_as_dataframe",
        "deployment.create_from_learning_model",
        "dr.dataenginequerygenerator.create",
        "dr.dataset.upload",
        "model.request_predictions",
        "project.get_uri",
        "model.id",
        "dr.predictionserver.list",
        "project.project_name",
        "datarobot.models.data_engine_query_generator",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "dr.batchpredictionjob.score_pandas",
        "model.compute_series_accuracy",
        "project.upload_dataset",
        "project.unlock_holdout",
        "deployment.delete",
        "deployment.id",
        "project.delete",
        "model.get_uri",
        "model.get",
        "dr.calendarfile.create_calendar_from_country_code",
        "dr.model.get",
        "model.model_type",
        "project.analyze_and_model",
        "project.metric",
        "model.get_accuracy_over_time_plot",
        "project.create_from_dataset",
        "datarobot.rest.restclientobject",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-4662061732685148295",
      "title": "Amazon_S3_End_to_End",
      "content": "<center><H1>End to End DataRobot AutoML workflow with Amazon S3</H1></center>\n\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<td><img src=\"https://www.datarobot.com/wp-content/uploads/2021/08/DataRobot-logo-color.svg\" height=200px width=200px>\n</td>\n<td><font size=10> + </font> </td>\n<td> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Amazon-S3-Logo.svg/1712px-Amazon-S3-Logo.svg.png\" height=100px width=100px> </td>\n\nAuthor: Biju Krishnan\n\n[API reference documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n\n<font>\nThis example notebook outlines the following tasks: <p>\n<ol>\n<li> Read PARQUET files from an Amazon S3 bucket into a pandas dataframe using AWS Wrangler Python library </li>\n<li> Upload a dataset in a dataframe to DataRobot's AI Catalog </li>\n<li> Initiate a DataRobot AutoML project with the dataset</li>\n<li> Deploy the top performing model to a DataRobot prediction server. </li>\n<li> Make batch predictions with a test dataset. </li>\n</ol>\n<p>\nThe files stored in S3 used for training can be in any format supported by the AWS Wrangler Python library. For batch predictions, DataRobot supports Parquet and CSV.\n</font>\n\n## Setup\n\n### Import libraries\n\n```python\nfrom io import StringIO\n\nimport awswrangler as wr  # This notebooks uses AWS Wrangler because its easy to read multiple files from the S3 bucket\nimport boto3\nimport datarobot as dr\nimport pandas as pd\n```\n\n### Bind variables\n\n```python\n# Bind variables\n# These variables can aso be fetched from a secret store or config files\n\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-14\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret\n```\n\n### Connect to DataRobot\n\nYou can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n```python\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n```\n\n```python\n# Instantiate a BOTO3 connection for connection to AWS\n# This session will be used in the next cell to read files from S3\n\nmy_session = boto3.Session(\n    aws_access_key_id=AWS_KEY,\n    aws_secret_access_key=AWS_SECRET,\n    # aws_session_token = <Optional>\n)\n```\n\n## Import data\n\n<font>\n<p>\nFor illustration purposes, the training dataset containing patient visits to a hospital is stored in an S3 bucket named e2eaccelerator09122022 under the path <code>s3://e2eaccelerator09122022/training/input/</code> .\n<pre><code><font color=grey size=1>\naws s3 ls s3://e2eaccelerator09122022/training/input/\n2022-12-09 09:55:47          0\n2022-12-09 09:56:15     267017 10k_diabetes.parquet\n</font></code></pre>\n<p>\nThe input folder contains only one file in this scenario, however the code will also work in case of multiple files.\n</font>\n\n```python\n# Read parquet files from an S3 bucket into a pandas dataframe using AWS Wrangler\n\ns3_training_input = \"s3://e2eaccelerator09122022/training/input/\"\ndf = wr.s3.read_parquet(path=s3_training_input, dataset=True, boto3_session=my_session)\n# Specifying dataset=True allows reading multiple files\ndf.head()\n```\n\n### Create a dataset\n\nCreate a dataset in the AI Catalog to use it for project creation.\n\n```python\ndatarobot_dataset = dr.Dataset.create_from_in_memory_data(\n    data_frame=df, fname=\"10K diabetes E2E accelerator\"\n)\ndatarobot_dataset.id\n```\n\n### Create a project and initiate Autopilot\n\n```python\n# This cell will take several minutes to complete execution\n# Creates an AutoML project named \"E2E Demo Amazon S3\" with \"readmitted\" as the target column\n# Quick mode is the designated training mode in this example, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon S3\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"readmitted\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)\n```\n\nOnce the AutoML project is complete, select the top-performing model on the Leaderboard based on the chosen metric for deployment.\n\n```python\ndef sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)\n```\n\n### Deploy a model\n\nNote that steps in the following sections require DataRobot MLOps licensed features. Contact your DataRobot account representatives if you are missing some licensed MLOps features.\n\n```python\n# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon S3 Test\",\n    description=\"Model trained on 10k diabetes dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id\n```\n\n### Make predictions\n\n<font family=verdana>\nDataRobot's batch predictions API is capable of directly reading and writing to Amazon S3 storage. \n<p>\n<i>Note: Parquet support for batch predictions is still in preview mode. Contact your DataRobot representative to enable the feature flags for trial.</i>\n</font>\n\n```python\n# To run a batch prediction job you need to store the AWS Credentials in the DataRobot credentials manager\n# The AWS key and secret should be unique\n# If they are already stored in the Credentials manager this code will throw an error\n\nDR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    credential_id = credential.credential_id\n\nprint(credential_id)\n```\n\n### Batch predictions snippet\n\nThe snippet below provides sample code to demonstratehow to make batch predictions to and from Amazon S3\n\n```python\ndr.BatchPredictionJob._s3_settings = dr.BatchPredictionJob._s3_settings.allow_extra(\"*\")\n\n# Use the manipulated batch job class to score:\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"csv\",  # Can also be Parquet\n        \"url\": \"s3://e2eaccelerator09122022/predictions/input/10k_diabetes_test.csv\",  ## This can be a path or a file depending on the format chosen\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"parquet\",  # Can also be CSV\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/10k_diabetes_test.parquet\",  ## This should point to a file not a path\n    },\n)\n\njob.wait_for_completion()\njob.get_status()\n```\n\n<font family=verdana>\n<p>\nThe output of the batch predictions is thus available under the path s3://e2eaccelerator09122022/predictions/output/\n<pre><code><font color=grey size=1>\naws s3 ls s3://e2eaccelerator09122022/predictions/output/\n2022-12-09 11:35:32          0\n2022-12-09 14:09:28      21244 10k_diabetes_test.parquet\n</font></code></pre>\n</font>\n",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/AWS_S3_template/Amazon_S3_End_to_End.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "aws",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/AWS_S3_template/Amazon_S3_End_to_End.ipynb",
        "size": 13146,
        "cell_count": 24,
        "code_cell_count": 11
      },
      "code_examples": [
        "from io import StringIO\n\nimport awswrangler as wr  # This notebooks uses AWS Wrangler because its easy to read multiple files from the S3 bucket\nimport boto3\nimport datarobot as dr\nimport pandas as pd",
        "# Bind variables\n# These variables can aso be fetched from a secret store or config files\n\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-14\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret",
        "dr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)",
        "# Instantiate a BOTO3 connection for connection to AWS\n# This session will be used in the next cell to read files from S3\n\nmy_session = boto3.Session(\n    aws_access_key_id=AWS_KEY,\n    aws_secret_access_key=AWS_SECRET,\n    # aws_session_token = <Optional>\n)",
        "# Read parquet files from an S3 bucket into a pandas dataframe using AWS Wrangler\n\ns3_training_input = \"s3://e2eaccelerator09122022/training/input/\"\ndf = wr.s3.read_parquet(path=s3_training_input, dataset=True, boto3_session=my_session)\n# Specifying dataset=True allows reading multiple files\ndf.head()",
        "datarobot_dataset = dr.Dataset.create_from_in_memory_data(\n    data_frame=df, fname=\"10K diabetes E2E accelerator\"\n)\ndatarobot_dataset.id",
        "# This cell will take several minutes to complete execution\n# Creates an AutoML project named \"E2E Demo Amazon S3\" with \"readmitted\" as the target column\n# Quick mode is the designated training mode in this example, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon S3\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"readmitted\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)",
        "def sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)",
        "# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon S3 Test\",\n    description=\"Model trained on 10k diabetes dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id",
        "# To run a batch prediction job you need to store the AWS Credentials in the DataRobot credentials manager\n# The AWS key and secret should be unique\n# If they are already stored in the Credentials manager this code will throw an error\n\nDR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    credential_id = credential.credential_id\n\nprint(credential_id)",
        "dr.BatchPredictionJob._s3_settings = dr.BatchPredictionJob._s3_settings.allow_extra(\"*\")\n\n# Use the manipulated batch job class to score:\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"csv\",  # Can also be Parquet\n        \"url\": \"s3://e2eaccelerator09122022/predictions/input/10k_diabetes_test.csv\",  ## This can be a path or a file depending on the format chosen\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"parquet\",  # Can also be CSV\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/10k_diabetes_test.parquet\",  ## This should point to a file not a path\n    },\n)\n\njob.wait_for_completion()\njob.get_status()"
      ],
      "api_methods": [
        "project.wait_for_autopilot",
        "model.metrics",
        "deployment.create_from_learning_model",
        "dr.client._global_client",
        "dr.autopilot_mode.quick",
        "dr.credential.list",
        "dr.predictionserver.list",
        "dr.batchpredictionjob._s3_settings",
        "dr.deployment.create_from_learning_model",
        "dr.credential.create_s3",
        "project.get",
        "deployment.id",
        "dr.dataset.create_from_in_memory_data",
        "dr.batchpredictionjob.score",
        "dr.project.get",
        "project.analyze_and_model",
        "project.metric",
        "project.create_from_dataset",
        "dr.project.create_from_dataset",
        "project.get_models"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_2987579447386754782",
      "title": "AWS_SageMaker_DataRobot_MLOps",
      "content": "# Integrate Amazon SageMaker with DataRobot MLOps\n\nAuthors: Oleksandr Saienko, DataRobot\nMao Shun, AWS\n\nVersion 1.3 (04/03/2022)\n\nWith Amazon SageMaker, you can package your own algorithms that can than be trained and deployed in the SageMaker environment. [DataRobot MLOps](https://docs.datarobot.com/en/docs/mlops/index.html) monitoring provides service health, data drift, accuracy monitoring, reports, and alerts about machine learning performance. This notebook is modified based on a [SageMaker example notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb) to show integration capabilities of DataRobot MLOps. \n\nTo integrate with SageMaker, you must first build and register a SageMaker container.\n\nThe README demonstrates how to build a custom SageMaker container in your local environment by including custom Python libraries for both training and inference. The README also includes DataRobot-related libraries useful for model monitoring.\n\nAdditionally, modify the Dockerfile as you need and follow the command instructions.\n\nOnce you have your container packaged, you can use it to train models and use the model for hosting.\n\nDataRobot recommends running the cells below in a SageMaker notebook instance for simplicity. If you want to run it locally, some settings need be added.\n\n## Setup\n\nSpecify a bucket to use and the role used when working with SageMaker.\n\n```python\n# S3 prefix\nprefix = \"DEMO-scikit-byo-iris-v3\"\n\nimport json\nimport os\nimport re\n\n# Define IAM role\nimport boto3\nimport numpy as np\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)\n```\n\n### Create a session\n\nThe session remembers your connection parameters to SageMaker. Use it to perform all of the SageMaker operations.\n\n```python\nfrom time import gmtime, strftime\n\nimport sagemaker as sage\n\nsess = sage.Session()\n```\n\n### Import data\n\nThis example workflow uses the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) and is included in the notebook folder.\n\nUse the tools provided by the SageMaker Python SDK to upload the data to a default bucket. \n\n```python\nWORK_DIRECTORY = \"data\"\n\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n```\n\n## Create an estimator and fit the model\n\nIn order to use SageMaker to fit your algorithm, create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n\n* The __container name__. This is constructed in the shell commands above.\n* The __role__. Defined above.\n* The __instance count__ is the number of machines to use for training.\n* The __instance type__ is the type of machine to use for training.\n* The __output path__ determines where the model artifact is written.\n* The __session__ is the SageMaker session object that you defined above.\n\nUse fit() on the estimator to train against the data uploaded above.\n\n```python\naccount = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\nregion = sess.boto_session.region_name\nimage = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-datarobot-decision-trees:latest\".format(\n    account, region\n)\n\nprint(image)\n\nprint(\"data_location\")\nprint(data_location)\n\ntree = sage.estimator.Estimator(\n    image,\n    role,\n    1,\n    \"ml.c4.2xlarge\",\n    output_path=\"s3://{}/output\".format(sess.default_bucket()),\n    sagemaker_session=sess,\n)\n\ntree.fit(data_location)\n```\n\n## Configure DataRobot MLOps\n\nBefore proceeding with the workflow, install a pip package in the current kernel.\n\n```python\nimport sys\n\n# installing DataRobot MLOps client\n!{sys.executable} -m pip install datarobot-mlops\n\n# installing mlops-cli tool\n!{sys.executable} -m pip install datarobot-mlops-connected-client\n```\n\n### Connect to DataRobot\n\nTo use the DataRobot API, you first need to [create an API key](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#create-a-datarobot-api-key).\n\nThen, add `MLOPS_SERVICE_URL` and `MLOPS_API_TOKEN` as environment variables.\n\n```python\n%env MLOPS_SERVICE_URL=https://app.datarobot.com\n# PUT Your DataRobot API Key here:\n%env MLOPS_API_TOKEN=PUT_YOUR_API_TOKEN\n```\n\n### Upload a training dataset to DataRobot AI Catalog\n\nIn the UI, you can [import a dataset via the AI catalog](https://app.datarobot.com/docs/data/ai-catalog/catalog.html#add-data).\n\nAlternatively, you can use `mlops-cli` from the command line as demonstrated in the cells below.\n\n```python\n%%capture cap --no-stderr\n# ^^^^ Just to catch mlops-cli commands output to process it programmatically, comment it for cell output\n# Load the training dataset using mlops-cli,\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n!mlops-cli dataset upload --input \"data/iris_with_header.csv\" --timeout 600 --json --quiet\n```\n\n```python\n# Output of this command will contain uploaded Dataset ID in 'id' field that needs to be used on the next steps:\nprint(cap.stdout)\nif \"ERROR\" not in cap.stdout:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Training dataset uploaded successfully, TRAINING_DATASET_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"TRAINING_DATASET_ID\"] = stdout_json[\"id\"]\nelse:\n    # Print output of mlops-cli in case of error:\n    print(\"Training dataset uploading failed:\")\n    print(cap.stdout)\n```\n\n## Create a model package\n\nIn the UI, you can view existing model packages or add a new one by navigating to [**Model Registry > Model Packages**](https://app.datarobot.com/docs/mlops/deployment/registry/reg-create.html#create-model-packages).\n\nAlternatively, you can use `mlops-cli` as shown in the following cells:\n\n```python\nMODEL_PACKAGE_NAME = \"SageMaker_MLOps_Demo_v2\"\n\n# Set model type\nprediction_type = \"Multiclass\"\n# Set traget column\nmodel_target = \"variety\"\n# Set traget classes\nclass_names = [\"setosa\", \"versicolor\", \"virginica\"]\n\nmodel_config = {\n    \"name\": MODEL_PACKAGE_NAME,\n    \"modelDescription\": {\n        \"modelName\": \"Iris classification model\",\n        \"description\": \"Classification on iris dataset\",\n    },\n    \"target\": {\n        \"type\": prediction_type,\n        \"name\": model_target,\n        \"classNames\": class_names,\n    },\n}\n\n# write model configuration json to a file:\nwith open(\"demo_model.json\", \"w\") as model_json_file:\n    model_json_file.write(json.dumps(model_config, indent=4))\n```\n\n```python\n%%capture cap --no-stderr\n# Create model package\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n# Using Dataset ID from previouse step as a training-dataset-id argument:\n!mlops-cli model create --json-config \"demo_model.json\" --training-dataset-id $TRAINING_DATASET_ID  --json --quiet\n# Output of this command will contain json with created model package ID that needs to be used on the next steps:\n```\n\n```python\nprint(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # catch Model Package ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model package created successfully, MODEL_PACKAGE_ID=\" + stdout_json[\"id\"])\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"MODEL_PACKAGE_ID\"] = stdout_json[\"id\"]\n    # set Model Package ID corresponding variable:\n    model_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model package creation failed:\")\n    print(cap.stdout)\n```\n\n## Create a DataRobot prediction environment\n\nModels that run on your own infrastructure (outside of DataRobot) may be run in different environments and can have differing deployment permissions and approval processes. \nTo deploy models on external infrastructure, you need create a custom external prediction environment using the UI or the DataRobot API and copying the prediction environment ID.\n\nFor more information, reference the documentation for [creating external prediction environments](https://app.datarobot.com/docs/mlops/deployment/ext-model-prep/pred-env.html).\n\nTo create a prediction environment from `mlops-cli` you can use the following cells:\n\n```python\n# Create a  configuration\ndemo_pe_config = {\n    \"name\": \"MLOps SageMaker Demo v2\",\n    \"description\": \"AWS Sagemaker DataRobot MLOps Demo\",\n    \"platform\": \"aws\",\n    \"supportedModelFormats\": [\"externalModel\"],\n}\n\n# Write the configuration json to a file\nwith open(\"demo_pe.json\", \"w\") as demo_pe_file:\n    demo_pe_file.write(json.dumps(demo_pe_config, indent=4))\n```\n\n```python\n%%capture cap --no-stderr\n# Used to catch mlops-cli commands output\n# Run this only once, or at least clean up after so you don't end up with a lot of deployments\n!mlops-cli prediction-environment create --json-config \"demo_pe.json\"  --json --quiet\n# The output of this command will contain a prediction environment ID that is required in the following cells\n```\n\n```python\n# The output of this command contains a prediction environment ID that is required in the following cells\nprint(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch prediction environment corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Prediction environment created successfully, PREDICTION_ENVIRONMENT_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Set the PREDICTION_ENVIRONMENT_ID environment variable to use it in the following cells\n    os.environ[\"PREDICTION_ENVIRONMENT_ID\"] = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Prediction environment creation failed:\")\n    print(cap.stdout)\n```\n\n```python\n%%capture cap --no-stderr\n# In the UI, you can create a deployment from a model package under Model Registry -> {model package} -> Deployments.\n# Set --deployment-label with name that you choose\n# --model-package-id from previous step\n# --prediction-environment-id from previous step\n!mlops-cli model deploy --model-package-id $MODEL_PACKAGE_ID --prediction-environment-id $PREDICTION_ENVIRONMENT_ID --deployment-label \"SageMaker_MLOps_Demo\"  --json --quiet\n```\n\n```python\n# The output of this command contains a deployment ID that is required in the following cells\nprint(cap.stdout)  # Used to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch deployment ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model deployment created successfully, DEPLOYMENT_ID=\" + stdout_json[\"id\"])\n    # Set the deployment_id variable to use it in the next steps:\n    deployment_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model deployment creation failed:\")\n    print(cap.stdout)\n```\n\n## Create an SQS queue as a spooler channel\n\nThe MLOps library communicates to the MLOps agent using a spooler. This workflow uses AWS SQS as a spooler channel, more details how to create SQS queue:\nYou can read more about how to [create an SQS queue using the cloud console UI](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/step-create-queue.html) or [by using the AWS CLI](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sqs/create-queue.html).\n\n```python\n#!aws sqs create-queue --queue-name datarobot-mlops-demo-v2\n# MLOps spooler channel SQS queue\n# Put your SQS queue URL here:\nMLOPS_SQS_QUEUE = \"https://sqs.us-east-1.amazonaws.com/12345678/aws-mlops-blogpost-demo\"\n```\n\n## Host your model\n\nYou can use a trained model to get real time predictions using an HTTP endpoint.\n\n### Deploy the model\n\nDeploying a model to SageMaker hosting just requires a `deploy` call with the fitted model. This call requires an instance count, instance type, and optional serializer and deserializer functions. These functions are used when the resulting predictor is created on the endpoint.\n\n```python\nimport json\n\nfrom sagemaker.serializers import CSVSerializer\n\n# Pass all required environment variables to the SageMaker deployment\nenv_vars = {\n    \"MLOPS_DEPLOYMENT_ID\": deployment_id,\n    \"MLOPS_MODEL_ID\": model_id,\n    \"MLOPS_SQS_QUEUE\": MLOPS_SQS_QUEUE,\n    \"prediction_type\": prediction_type,\n    \"CLASS_NAMES\": json.dumps(class_names),\n}\n\nprint(env_vars)\n\npredictor = tree.deploy(1, \"ml.m4.xlarge\", serializer=CSVSerializer(), env=env_vars)\n```\n\n### Get prediction data\n\nIn order to make predictions, extract the data used for training to make predictions against it. This is is strictly for demo purposes as it is bad statistical practice. However it is a good demonstration of how the mechanism works.\n\n```python\nshape = pd.read_csv(\"data/iris_with_header.csv\", header=None)\nshape.sample(3)\n```\n\n```python\n# Drop the label column in the training set\nshape.drop(shape.columns[[0]], axis=1, inplace=True)\nshape.sample(3)\n```\n\n```python\nimport itertools\n\na = [50 * i for i in range(3)]\nb = [40 + i for i in range(10)]\nindices = [i + j for i, j in itertools.product(a, b)]\n\ntest_data = shape.iloc[indices[:-1]]\n\ntest_data\n```\n\nMaking prediction is as easy as calling `predict` with the predictor you get back from `deploy` and the data you want to make predictions with. The serializers convert the data for you.\n\n```python\nimport io\n\nprint(test_data)\nout = io.StringIO()\npd.DataFrame(test_data).to_csv(out, header=True, index=False)\nprint(predictor.predict(out.getvalue()).decode(\"utf-8\"))\n```\n\n### Cleanup\n\nOptional. When you're done with the endpoint, you can clean it up.\n\n```python\nsess.delete_endpoint(predictor.endpoint)\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/AWS_monitor_sagemaker_model_in_DataRobot/AWS_SageMaker_DataRobot_MLOps.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "aws",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/AWS_monitor_sagemaker_model_in_DataRobot/AWS_SageMaker_DataRobot_MLOps.ipynb",
        "size": 42513,
        "cell_count": 41,
        "code_cell_count": 23
      },
      "code_examples": [
        "# S3 prefix\nprefix = \"DEMO-scikit-byo-iris-v3\"\n\nimport json\nimport os\nimport re\n\n# Define IAM role\nimport boto3\nimport numpy as np\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)",
        "from time import gmtime, strftime\n\nimport sagemaker as sage\n\nsess = sage.Session()",
        "WORK_DIRECTORY = \"data\"\n\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)",
        "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\nregion = sess.boto_session.region_name\nimage = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-datarobot-decision-trees:latest\".format(\n    account, region\n)\n\nprint(image)\n\nprint(\"data_location\")\nprint(data_location)\n\ntree = sage.estimator.Estimator(\n    image,\n    role,\n    1,\n    \"ml.c4.2xlarge\",\n    output_path=\"s3://{}/output\".format(sess.default_bucket()),\n    sagemaker_session=sess,\n)\n\ntree.fit(data_location)",
        "import sys\n\n# installing DataRobot MLOps client\n!{sys.executable} -m pip install datarobot-mlops\n\n# installing mlops-cli tool\n!{sys.executable} -m pip install datarobot-mlops-connected-client",
        "%env MLOPS_SERVICE_URL=https://app.datarobot.com\n# PUT Your DataRobot API Key here:\n%env MLOPS_API_TOKEN=PUT_YOUR_API_TOKEN",
        "%%capture cap --no-stderr\n# ^^^^ Just to catch mlops-cli commands output to process it programmatically, comment it for cell output\n# Load the training dataset using mlops-cli,\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n!mlops-cli dataset upload --input \"data/iris_with_header.csv\" --timeout 600 --json --quiet",
        "# Output of this command will contain uploaded Dataset ID in 'id' field that needs to be used on the next steps:\nprint(cap.stdout)\nif \"ERROR\" not in cap.stdout:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Training dataset uploaded successfully, TRAINING_DATASET_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"TRAINING_DATASET_ID\"] = stdout_json[\"id\"]\nelse:\n    # Print output of mlops-cli in case of error:\n    print(\"Training dataset uploading failed:\")\n    print(cap.stdout)",
        "MODEL_PACKAGE_NAME = \"SageMaker_MLOps_Demo_v2\"\n\n# Set model type\nprediction_type = \"Multiclass\"\n# Set traget column\nmodel_target = \"variety\"\n# Set traget classes\nclass_names = [\"setosa\", \"versicolor\", \"virginica\"]\n\nmodel_config = {\n    \"name\": MODEL_PACKAGE_NAME,\n    \"modelDescription\": {\n        \"modelName\": \"Iris classification model\",\n        \"description\": \"Classification on iris dataset\",\n    },\n    \"target\": {\n        \"type\": prediction_type,\n        \"name\": model_target,\n        \"classNames\": class_names,\n    },\n}\n\n# write model configuration json to a file:\nwith open(\"demo_model.json\", \"w\") as model_json_file:\n    model_json_file.write(json.dumps(model_config, indent=4))",
        "%%capture cap --no-stderr\n# Create model package\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n# Using Dataset ID from previouse step as a training-dataset-id argument:\n!mlops-cli model create --json-config \"demo_model.json\" --training-dataset-id $TRAINING_DATASET_ID  --json --quiet\n# Output of this command will contain json with created model package ID that needs to be used on the next steps:",
        "print(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # catch Model Package ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model package created successfully, MODEL_PACKAGE_ID=\" + stdout_json[\"id\"])\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"MODEL_PACKAGE_ID\"] = stdout_json[\"id\"]\n    # set Model Package ID corresponding variable:\n    model_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model package creation failed:\")\n    print(cap.stdout)",
        "# Create a  configuration\ndemo_pe_config = {\n    \"name\": \"MLOps SageMaker Demo v2\",\n    \"description\": \"AWS Sagemaker DataRobot MLOps Demo\",\n    \"platform\": \"aws\",\n    \"supportedModelFormats\": [\"externalModel\"],\n}\n\n# Write the configuration json to a file\nwith open(\"demo_pe.json\", \"w\") as demo_pe_file:\n    demo_pe_file.write(json.dumps(demo_pe_config, indent=4))",
        "%%capture cap --no-stderr\n# Used to catch mlops-cli commands output\n# Run this only once, or at least clean up after so you don't end up with a lot of deployments\n!mlops-cli prediction-environment create --json-config \"demo_pe.json\"  --json --quiet\n# The output of this command will contain a prediction environment ID that is required in the following cells",
        "# The output of this command contains a prediction environment ID that is required in the following cells\nprint(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch prediction environment corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Prediction environment created successfully, PREDICTION_ENVIRONMENT_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Set the PREDICTION_ENVIRONMENT_ID environment variable to use it in the following cells\n    os.environ[\"PREDICTION_ENVIRONMENT_ID\"] = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Prediction environment creation failed:\")\n    print(cap.stdout)",
        "%%capture cap --no-stderr\n# In the UI, you can create a deployment from a model package under Model Registry -> {model package} -> Deployments.\n# Set --deployment-label with name that you choose\n# --model-package-id from previous step\n# --prediction-environment-id from previous step\n!mlops-cli model deploy --model-package-id $MODEL_PACKAGE_ID --prediction-environment-id $PREDICTION_ENVIRONMENT_ID --deployment-label \"SageMaker_MLOps_Demo\"  --json --quiet",
        "# The output of this command contains a deployment ID that is required in the following cells\nprint(cap.stdout)  # Used to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch deployment ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model deployment created successfully, DEPLOYMENT_ID=\" + stdout_json[\"id\"])\n    # Set the deployment_id variable to use it in the next steps:\n    deployment_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model deployment creation failed:\")\n    print(cap.stdout)",
        "#!aws sqs create-queue --queue-name datarobot-mlops-demo-v2\n# MLOps spooler channel SQS queue\n# Put your SQS queue URL here:\nMLOPS_SQS_QUEUE = \"https://sqs.us-east-1.amazonaws.com/12345678/aws-mlops-blogpost-demo\"",
        "import json\n\nfrom sagemaker.serializers import CSVSerializer\n\n# Pass all required environment variables to the SageMaker deployment\nenv_vars = {\n    \"MLOPS_DEPLOYMENT_ID\": deployment_id,\n    \"MLOPS_MODEL_ID\": model_id,\n    \"MLOPS_SQS_QUEUE\": MLOPS_SQS_QUEUE,\n    \"prediction_type\": prediction_type,\n    \"CLASS_NAMES\": json.dumps(class_names),\n}\n\nprint(env_vars)\n\npredictor = tree.deploy(1, \"ml.m4.xlarge\", serializer=CSVSerializer(), env=env_vars)",
        "shape = pd.read_csv(\"data/iris_with_header.csv\", header=None)\nshape.sample(3)",
        "# Drop the label column in the training set\nshape.drop(shape.columns[[0]], axis=1, inplace=True)\nshape.sample(3)",
        "import itertools\n\na = [50 * i for i in range(3)]\nb = [40 + i for i in range(10)]\nindices = [i + j for i, j in itertools.product(a, b)]\n\ntest_data = shape.iloc[indices[:-1]]\n\ntest_data",
        "import io\n\nprint(test_data)\nout = io.StringIO()\npd.DataFrame(test_data).to_csv(out, header=True, index=False)\nprint(predictor.predict(out.getvalue()).decode(\"utf-8\"))",
        "sess.delete_endpoint(predictor.endpoint)"
      ],
      "api_methods": [
        "model.json"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_658341266384200599",
      "title": "dr_model_sagemaker",
      "content": "## Setup\n\n### Import libraries\n\n```python\nfrom datetime import datetime\nimport json\nimport logging\nimport os.path\nimport sys\nimport tarfile\nimport time\n\nimport boto3\nfrom botocore.exceptions import ClientError\nimport datarobot as dr\nimport pandas as pd\nimport requests\n```\n\n### Set some logging paramaters\n\n```python\n# Configure formatting for logging\nlog = logging.getLogger()\nlog.setLevel(logging.INFO)\n\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"[%(asctime)s][%(name)s][%(levelname)s]: %(message)s\")\nhandler.setFormatter(formatter)\nlog.addHandler(handler)\n```\n\n### Variable configuration\nIn this cell, define all of the variables and access tokens.\n  \n#### DataRobot\n`dr_host`: The DataRobot cluster you are connecting to. Please ensure that the `/api/v2` part of the URL is still in the string.  \n`api_key`: The API key of the DataRobot user used to interact with the platform.\n  \n#### AWS\n`aws_region`: The AWS region that everything will be deployed to.  \n`aws_access_key_id`: AWS Access Key for authentication.  \n`aws_secret_access_key`: AWS Secret Access Key for authentication.  \n`aws_session_token`: AWS Session Token for authentication.  \n  \n`s3_bucket`: The name of the S3 bucket that will be created for uploading your Scoring Code JAR file into.  \n`aws_ecr_repo`: The name of the ECR repo into which you upload your runtime docker image.  \n`sagemaker_execution_role_name`: The name of the IAM Role that will be created to allow SageMaker to interact with S3 and other SageMaker services.  \n\n```python\n# DataRobot Variables\n\n# DataRobot Host\ndr_host = \"https://app.datarobot.com/api/v2\"\n\n# DataRobot API Key\napi_key = \"<API_TOKEN>\"\n\n\n# AWS Variables\ns3_bucket = \"<YOUR_S3_BUCKET_NAME>\"\naws_ecr_repo = \"<YOUR_ECR_REPO_NAME>\"\nsagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-Demo\"\naws_region = \"us-east-1\"\n\naws_access_key_id = \"\"\naws_secret_access_key = \"\"\naws_session_token = \"\"\n```\n\n### Connect to DataRobot\n\n```python\nclient = dr.Client(\n    token=api_key,\n    endpoint=dr_host,\n    user_agent_suffix=\"AIA-E2E-AWS-7\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')\n```\n\nRead more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n## Modeling\n\nThis section of the notebook focueses on the steps for creating and exporting an ML model developed within DataRobot.\n\n### Create a project and initiate Autopilot\n\nIn the following snippet you will upload your training data to DataRobot. This example uses a dataset of Lending Club loans to predict if a loan will default or not.\n\nThis example sets an advanced option for the project to include only models that are compatible with Scoring Code Export. Java Scoring Code can be downloaded as a binary file or compiled, and contains all of the data transformations, feature engineering and final model parameters from the DataRobot Model . Since the data and feature engineering pipeline are completely contained in the portable JAR file, predictions can be made outside of DataRobot, as long as the scoring data is in the same format as the training data. More information can be [found in the DataRobot documentation](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/index.html#model-support).\n\nNext, you will initiate Autopilot to build models.\n\nIf you already have a model that you want to deploy, then this part can be skipped, but you must manually define the project and model ID below to continue using the notebook.\n\n```python\n# Create a project, kick off Autopilot, and wait for completion\ndf = pd.read_csv(\"training_data/10K_Lending_Club_Loans.csv\")\n\nadvanced_options = dr.AdvancedOptions(\n    blend_best_models=False, scoring_code_only=True, prepare_model_for_deployment=True\n)\n\nproject = dr.Project.create(\n    sourcedata=df,\n    project_name=\"DR_Demo_Sagemaker_{}\".format(\n        datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    ),\n)\nproject.analyze_and_model(\n    target=\"is_bad\", worker_count=-1, advanced_options=advanced_options\n)\nproject.wait_for_autopilot(verbosity=1)\n```\n\n### Get our Project ID and the ID of the top model in the leaderboard for export.\n\n```python\n# Get your Project ID and Model ID of the top rated model on the leaderboard\nproject_id = project.id\ntop_model = project.get_top_model()\nmodel_id = top_model.id\n\nlog.info(\"Project ID: {} | Model ID: {}\".format(project_id, model_id))\n```\n\n## Export a DataRobot model\n\nUse the following cells to download the model as a Scoring Code JAR file (in a local directory called model) and then compress that file into a .tar.gz archive to upload to S3.\n\n```python\n# Below is a helper function that downloads your JAR file to your local system from a target server\n# The output returns the model path name if the file has been downloaded and returns None if not\n\n\ndef get_scoring_code(session, host, project_id, model_id):\n    apiEndpoint = format(\n        \"{}/projects/{}/models/{}/scoringCode/\".format(host, project_id, model_id)\n    )\n\n    try:\n        r = session.get(apiEndpoint)\n        r.raise_for_status()\n        return r\n    except requests.exceptions.HTTPError as err:\n        log.error(err)\n        return None\n```\n\n```python\nheaders = {}\nheaders[\"Authorization\"] = \"Bearer {}\".format(api_key)\n\nsession = requests.Session()\nsession.headers.update(headers)\n\nlog.info(\"Getting scoring code jar file from DataRobot location: \" + dr_host)\n# Get scoring code jar\noutput = get_scoring_code(session, dr_host, project_id, model_id)\nif output is None:\n    log.error(\"download failed\")\nelse:\n    # Model name is grabbed from Content-Disposition header, which provides a dynamically generated suggested name for the model (usually model_id.jar)\n    modeldir = \"model/\"\n\n    # cCeate local model directory if it doesn't exist already\n    if not os.path.exists(modeldir):\n        os.mkdir(modeldir)\n\n    fd = output.headers.get(\"Content-Disposition\")\n    modelname = fd.split(\";\")[1].strip().split(\"=\")[1]\n    modelpath = modeldir + modelname\n\n    with open(modelpath, \"wb\") as f:\n        f.write(output.content)\n\n    log.info(\"Scoring Code jar downloaded to {}\".format(modelpath))\n\n    # Compress the jar file into a tar.gz as required by SageMaker\n    log.info(\"Compressing jar file into tar.gz\")\n    tgz_name = modelname + \".tar.gz\"\n    tgz_path = modeldir + tgz_name\n\n    with tarfile.open(tgz_path, \"w:gz\") as tar:\n        tar.add(modelpath, arcname=os.path.basename(modelpath))\n\n    log.info(\"COMPLETE!\")\n```\n\n## Import to AWS\n\nThis section of the notebook focuses on the steps required to prepare AWS for hosting a DataRobot model within SageMaker. It includes examples for how to make predictions against the model for both real-time and batch use cases.\n\n### Download docker runtime image\n\nThis step will pull down the scoring-inference-code-sagemaker docker image that will be used to run the Scoring Code JAR file in SageMaker.\n\n```python\n%%bash\n# Pull down the scoring-inference-code-sagemaker image that will be pushed to AWS ECR for hosting our Scoring Code models in Sagemaker\ndocker pull datarobot/scoring-inference-code-sagemaker:latest\n```\n\n### Create an AWS session connection\n\n```python\n# Create an AWS Boto3 Session\nsession = boto3.Session(\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key,\n    aws_session_token=aws_session_token,\n    region_name=aws_region,\n)\n```\n\nNext, create an AWS ECR repo to hold the `scoring-inference-code-sagemaker` docker image.\n\n```python\n# Create the AWS ECR repo\nlog.info(\"Creating ECR Repo to hold our base image for running scoring code jar file.\")\necr_client = session.client(\"ecr\")\n\necr_response = ecr_client.create_repository(repositoryName=aws_ecr_repo)\n\nlog.info(\"ECR Name: {}\".format(ecr_response.get(\"repository\").get(\"repositoryName\")))\nlog.info(\"ECR ARN: {}\".format(ecr_response.get(\"repository\").get(\"repositoryArn\")))\nlog.info(\"ECR URI: {}\".format(ecr_response.get(\"repository\").get(\"repositoryUri\")))\n\necr_repo_uri = ecr_response.get(\"repository\").get(\"repositoryUri\")\necr_registry_id = ecr_response.get(\"repository\").get(\"registryId\")\nregistry_url = ecr_registry_id + \".dkr.ecr.\" + aws_region + \".amazonaws.com\"\n\nlog.info(\"ECR Repo created!\")\n```\n\nNow you can push `scoring-inference-code-sagemaker` to the ECR repo.\n\n```python\n%%bash -s \"$ecr_repo_uri\" \"$registry_url\" \"$aws_access_key_id\" \"$aws_secret_access_key\" \"$aws_session_token\" \"$aws_region\"\n# Push datarobot/scoring-inference-code-sagemaker:latest to ECR Repo\n\nexport AWS_ACCESS_KEY_ID=$3\nexport AWS_SECRET_ACCESS_KEY=$4\nexport AWS_SESSION_TOKEN=$5\n\ndocker login -u AWS -p $(aws ecr get-login-password --region $6) $2\ndocker tag datarobot/scoring-inference-code-sagemaker:latest $1:latest\ndocker push $1:latest\n```\n\n### Create an S3 Bucket\n\nThis S3 bucket stores your DataRobot model.\n\n```python\n# Create S3 Bucket\nlog.info(\"Creating S3 Bucket {}\".format(s3_bucket))\n\ns3 = session.resource(\"s3\")\ntry:\n    s3.create_bucket(Bucket=s3_bucket)\n    log.info(\"S3 Bucket Creation Complete!\")\nexcept ClientError as e:\n    log.error(e)\n```\n\nNext, upload the Scoring Code JAR file to S3.\n\n```python\n# Upload scoring code jar tarball to AWS S3\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(tgz_name, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_model = \"sagemaker/models/\" + tgz_name\ntry:\n    s3.meta.client.upload_file(tgz_path, s3_bucket, s3_obj_name_model)\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)\n```\n\n### Upload sample data to S3\n\nIn this cell, you upload a sample dataset to make batch predictions in SageMaker. This dataset is specifically designed for the model that was created earlier in this notebook.\n\n```python\n# Upload Batch Scoring Data to S3\nbatch_path = \"scoring_data/10K_Lending_Club_Loans_scoring.csv\"\n\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(batch_path, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_csv = \"sagemaker/\" + batch_path\ntry:\n    s3.meta.client.upload_file(batch_path, s3_bucket, s3_obj_name_csv)\n    batch_input_file = \"s3://\" + s3_bucket + \"/\" + s3_obj_name_csv\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)\n```\n\nThis cell will create an IAM role for SageMaker that will grant access to run things within SageMaker itself, and to allow for access to the S3 bucket contianing the uploaded Scoring Code model file.\n\n```python\n# Create IAM Role for Sagemaker to use\nlog.info(\"Creating Execution IAM Role for Sagemaker to use\")\niam = session.client(\"iam\")\niamr = session.resource(\"iam\")\n\nrole_policy = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Action\": [\"s3:ListBucket\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket],\n            },\n            {\n                \"Action\": [\"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket + \"/*\"],\n            },\n        ],\n    }\n)\n\nlog.info(\"Creating Exeuction S3 Access Policy\")\npolicy = iam.create_policy(\n    PolicyName=sagemaker_execution_role_name + \"-policy\", PolicyDocument=role_policy\n)\n\npolicy_arn = policy.get(\"Policy\").get(\"Arn\")\n\nassume_role_policy_document = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n                \"Action\": \"sts:AssumeRole\",\n            }\n        ],\n    }\n)\n\nlog.info(\"Creating actual role\")\nrole = iam.create_role(\n    RoleName=sagemaker_execution_role_name,\n    AssumeRolePolicyDocument=assume_role_policy_document,\n)\n\n# Attach our execution Policy\nlog.info(\"Attaching Execution Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name, PolicyArn=policy_arn\n)\n\n# Attach the AmazonSageMakerFullAccess Policy\nlog.info(\"Attaching AmazonSageMakerFullAccess Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name,\n    PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n)\n\nrole_resource = iamr.Role(sagemaker_execution_role_name)\nlog.info(\"IAM Role Info:\")\nlog.info(\"IAM Role Name: {}\".format(role_resource.name))\nlog.info(\"IAM Role ARN: {}\".format(role_resource.arn))\nlog.info(\"IAM Role Policies:\")\nfor p in role_resource.attached_policies.all():\n    log.info(p)\n\nlog.info(\"COMPLETE!\")\n```\n\n### Create a SageMaker model\n\n```python\n# Create model in sagemaker\nlog.info(\"Creating Sagemaker Model\")\nsm_client = session.client(\"sagemaker\")\naws_model_name = modelname.split(\".\")[0]\n\nresponse = sm_client.create_model(\n    ModelName=aws_model_name,\n    PrimaryContainer={\n        \"Image\": ecr_repo_uri + \":latest\",\n        \"ImageConfig\": {\"RepositoryAccessMode\": \"Platform\"},\n        \"Mode\": \"SingleModel\",\n        \"ModelDataUrl\": \"s3://\" + s3_bucket + \"/\" + s3_obj_name_model,\n    },\n    ExecutionRoleArn=role_resource.arn,\n)\n\nif response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Created!\")\n    log.info(\"model name: {}\".format(aws_model_name))\n    log.info(\"model arn: {}\".format(response.get(\"ModelArn\")))\n    log.info(\"COMPLETE!\")\n```\n\n### SageMaker endpoint configuration\n\nThis is used as part of the assembly of a SageMaker Endpoint that is required for real time API prediction requests.\n\n```python\n# Create Sagemaker Endpoint Configuration\nlog.info(\"Creating Sagemaker Model Endpoint Configuration\")\naws_endpoint_config_name = aws_model_name + \"-ec\"\n\nec_response = sm_client.create_endpoint_config(\n    EndpointConfigName=aws_endpoint_config_name,\n    ProductionVariants=[\n        {\n            \"VariantName\": \"variant-1\",\n            \"ModelName\": aws_model_name,\n            \"InitialInstanceCount\": 1,\n            \"InstanceType\": \"ml.m4.xlarge\",\n        }\n    ],\n)\n\nif ec_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Endpoint Configuration Created!\")\n    log.info(\"endpoint configuration name: {}\".format(aws_endpoint_config_name))\n    log.info(\n        \"endpoint configuration arn: {}\".format(ec_response.get(\"EndpointConfigArn\"))\n    )\n    log.info(\"COMPLETE!\")\n```\n\nUse the cell below to create a Sagemaker endpoint.\n\n```python\n# Create Sagemaker Endpoint\nlog.info(\"Creating Sagemaker Model Endpoint... This process can take a few minutes\")\naws_endpoint_name = aws_model_name + \"-ep\"\n\nep_response = sm_client.create_endpoint(\n    EndpointName=aws_endpoint_name,\n    EndpointConfigName=aws_endpoint_config_name,\n)\n\nif ep_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when sending endpoint creation request to Sagemaker\")\n    log.error(ep_response)\nelse:\n    i = 0\n    while i < 30:\n        status_r = sm_client.describe_endpoint(EndpointName=aws_endpoint_name)\n        status = status_r.get(\"EndpointStatus\")\n        log.info(\"Endpoint Creation Status: {}\".format(status_r.get(\"EndpointStatus\")))\n\n        if status == \"InService\":\n            break\n        else:\n            time.sleep(20)\n            i = i + 1\n\n    if status == \"InService\":\n        log.info(\"Sagemaker Model Endpoint Created!\")\n        log.info(\"Endpoint Name: {}\".format(status_r.get(\"EndpointName\")))\n        log.info(\"Endpoint ARN: {}\".format(status_r.get(\"EndpointArn\")))\n        invocation_url = \"https://runtime.sagemaker.{}.amazonaws.com/endpoints/{}/invocations\".format(\n            aws_region, status_r.get(\"EndpointName\")\n        )\n        log.info(\"Endpoint API URL: {}\".format(invocation_url))\n        log.info(\"COMPLETE!\")\n    else:\n        log.error(\"Sagemaker did not return an 'InService' status in time!\")\n        log.error(\"Last status received: {}\".format(status))\n        log.error(status_r)\n```\n\n## Predicion examples\nThe following cells will show how to make predictions against the deployed model using both batch and real-time methods.\n  \n### Create SageMaker Batch Transform Job\n\nUse this cell to programatically create a batch transform job in SageMaker that can be used for batch predictions.  This job reads in a CSV that you previously uploaded to an S3 bucket. The output of the job will then be written to another folder (`scoring_output`) that will exist in the S3 bucket that you previously created.\n\n```python\n# Create a batch transform job for batch predictions\nlog.info(\"Creating Sagemaker Batch Transform Job\")\nbtj_client = session.client(\"sagemaker\")\n\njob_name = (\n    aws_model_name + \"-batch-transform-job-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n)\n\nbatch_output_folder = \"s3://\" + s3_bucket + \"/scoring_output/\"\nresponse = btj_client.create_transform_job(\n    TransformJobName=job_name,\n    ModelName=aws_model_name,\n    TransformInput={\n        \"DataSource\": {\n            \"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input_file}\n        },\n        \"ContentType\": \"string\",\n        \"CompressionType\": \"None\",\n        \"SplitType\": \"None\",\n    },\n    TransformOutput={\n        \"S3OutputPath\": batch_output_folder,\n        \"Accept\": \"string\",\n        \"AssembleWith\": \"None\",\n    },\n    TransformResources={\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n)\n\n# Response\nlog.info(\"Running Sagemaker Batch Transform Job {}\".format(job_name))\ni = 0\nwhile i < 30:\n    status_r = btj_client.describe_transform_job(TransformJobName=job_name)\n    status = status_r.get(\"TransformJobStatus\")\n    log.info(\"Batch Job Status: {}\".format(status_r.get(\"TransformJobStatus\")))\n\n    if status not in [\"InProgress\", \"Stopping\"]:\n        break\n    else:\n        time.sleep(20)\n        i = i + 1\n```\n\n### View batch transform Job results\n\nIn this cell you download the results file from the batch transform job that you just ran in SageMaker and output the contents of the dataframe to show what was scored.  \n\nIn this case, you are scoring a binary classification model, so your output will be two columns that contain the scores of our positive and negative classes, which will translate into whether a potential loan will default or not.\n\n```python\ns3 = session.client(\"s3\")\noutput_dir = \"scoring_output\"\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nfor key in s3.list_objects(Bucket=s3_bucket)[\"Contents\"]:\n    if \".out\" in key[\"Key\"]:\n        s3.download_file(s3_bucket, key[\"Key\"], \"scoring_output/output.csv\")\n\ndf = pd.read_csv(output_dir + \"/output.csv\")\ndf\n```\n\n### Real-time predictions with SageMaker\n\nThis cell shows how to interact with the SageMaker endpoint that you previously created for your model to use with real time prediction workloads.  \n\nYou will be using the AWS boto3 client and making a call to the SageMaker endpoint to score a row of data from a CSV file and then print out the result.\n\n```python\ns_client = session.client(\"sagemaker-runtime\")\n\nbuffer = open(\"scoring_data/1_row_Lending_Club_Loans_scoring.csv\")\npayload = buffer.read()\n\nresponse = s_client.invoke_endpoint(\n    EndpointName=aws_endpoint_name, ContentType=\"text/csv\", Body=payload\n)\n\ndata = response.get(\"Body\").read()\nlog.info(\"Scoring output:\\n{}\".format(data.decode(\"utf-8\")))\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/AWS_sagemaker_deployment/dr_model_sagemaker.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "aws",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/AWS_sagemaker_deployment/dr_model_sagemaker.ipynb",
        "size": 28903,
        "cell_count": 46,
        "code_cell_count": 22
      },
      "code_examples": [
        "from datetime import datetime\nimport json\nimport logging\nimport os.path\nimport sys\nimport tarfile\nimport time\n\nimport boto3\nfrom botocore.exceptions import ClientError\nimport datarobot as dr\nimport pandas as pd\nimport requests",
        "# Configure formatting for logging\nlog = logging.getLogger()\nlog.setLevel(logging.INFO)\n\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"[%(asctime)s][%(name)s][%(levelname)s]: %(message)s\")\nhandler.setFormatter(formatter)\nlog.addHandler(handler)",
        "# DataRobot Variables\n\n# DataRobot Host\ndr_host = \"https://app.datarobot.com/api/v2\"\n\n# DataRobot API Key\napi_key = \"<API_TOKEN>\"\n\n\n# AWS Variables\ns3_bucket = \"<YOUR_S3_BUCKET_NAME>\"\naws_ecr_repo = \"<YOUR_ECR_REPO_NAME>\"\nsagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-Demo\"\naws_region = \"us-east-1\"\n\naws_access_key_id = \"\"\naws_secret_access_key = \"\"\naws_session_token = \"\"",
        "client = dr.Client(\n    token=api_key,\n    endpoint=dr_host,\n    user_agent_suffix=\"AIA-E2E-AWS-7\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')",
        "# Create a project, kick off Autopilot, and wait for completion\ndf = pd.read_csv(\"training_data/10K_Lending_Club_Loans.csv\")\n\nadvanced_options = dr.AdvancedOptions(\n    blend_best_models=False, scoring_code_only=True, prepare_model_for_deployment=True\n)\n\nproject = dr.Project.create(\n    sourcedata=df,\n    project_name=\"DR_Demo_Sagemaker_{}\".format(\n        datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    ),\n)\nproject.analyze_and_model(\n    target=\"is_bad\", worker_count=-1, advanced_options=advanced_options\n)\nproject.wait_for_autopilot(verbosity=1)",
        "# Get your Project ID and Model ID of the top rated model on the leaderboard\nproject_id = project.id\ntop_model = project.get_top_model()\nmodel_id = top_model.id\n\nlog.info(\"Project ID: {} | Model ID: {}\".format(project_id, model_id))",
        "# Below is a helper function that downloads your JAR file to your local system from a target server\n# The output returns the model path name if the file has been downloaded and returns None if not\n\n\ndef get_scoring_code(session, host, project_id, model_id):\n    apiEndpoint = format(\n        \"{}/projects/{}/models/{}/scoringCode/\".format(host, project_id, model_id)\n    )\n\n    try:\n        r = session.get(apiEndpoint)\n        r.raise_for_status()\n        return r\n    except requests.exceptions.HTTPError as err:\n        log.error(err)\n        return None",
        "headers = {}\nheaders[\"Authorization\"] = \"Bearer {}\".format(api_key)\n\nsession = requests.Session()\nsession.headers.update(headers)\n\nlog.info(\"Getting scoring code jar file from DataRobot location: \" + dr_host)\n# Get scoring code jar\noutput = get_scoring_code(session, dr_host, project_id, model_id)\nif output is None:\n    log.error(\"download failed\")\nelse:\n    # Model name is grabbed from Content-Disposition header, which provides a dynamically generated suggested name for the model (usually model_id.jar)\n    modeldir = \"model/\"\n\n    # cCeate local model directory if it doesn't exist already\n    if not os.path.exists(modeldir):\n        os.mkdir(modeldir)\n\n    fd = output.headers.get(\"Content-Disposition\")\n    modelname = fd.split(\";\")[1].strip().split(\"=\")[1]\n    modelpath = modeldir + modelname\n\n    with open(modelpath, \"wb\") as f:\n        f.write(output.content)\n\n    log.info(\"Scoring Code jar downloaded to {}\".format(modelpath))\n\n    # Compress the jar file into a tar.gz as required by SageMaker\n    log.info(\"Compressing jar file into tar.gz\")\n    tgz_name = modelname + \".tar.gz\"\n    tgz_path = modeldir + tgz_name\n\n    with tarfile.open(tgz_path, \"w:gz\") as tar:\n        tar.add(modelpath, arcname=os.path.basename(modelpath))\n\n    log.info(\"COMPLETE!\")",
        "%%bash\n# Pull down the scoring-inference-code-sagemaker image that will be pushed to AWS ECR for hosting our Scoring Code models in Sagemaker\ndocker pull datarobot/scoring-inference-code-sagemaker:latest",
        "# Create an AWS Boto3 Session\nsession = boto3.Session(\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key,\n    aws_session_token=aws_session_token,\n    region_name=aws_region,\n)",
        "# Create the AWS ECR repo\nlog.info(\"Creating ECR Repo to hold our base image for running scoring code jar file.\")\necr_client = session.client(\"ecr\")\n\necr_response = ecr_client.create_repository(repositoryName=aws_ecr_repo)\n\nlog.info(\"ECR Name: {}\".format(ecr_response.get(\"repository\").get(\"repositoryName\")))\nlog.info(\"ECR ARN: {}\".format(ecr_response.get(\"repository\").get(\"repositoryArn\")))\nlog.info(\"ECR URI: {}\".format(ecr_response.get(\"repository\").get(\"repositoryUri\")))\n\necr_repo_uri = ecr_response.get(\"repository\").get(\"repositoryUri\")\necr_registry_id = ecr_response.get(\"repository\").get(\"registryId\")\nregistry_url = ecr_registry_id + \".dkr.ecr.\" + aws_region + \".amazonaws.com\"\n\nlog.info(\"ECR Repo created!\")",
        "%%bash -s \"$ecr_repo_uri\" \"$registry_url\" \"$aws_access_key_id\" \"$aws_secret_access_key\" \"$aws_session_token\" \"$aws_region\"\n# Push datarobot/scoring-inference-code-sagemaker:latest to ECR Repo\n\nexport AWS_ACCESS_KEY_ID=$3\nexport AWS_SECRET_ACCESS_KEY=$4\nexport AWS_SESSION_TOKEN=$5\n\ndocker login -u AWS -p $(aws ecr get-login-password --region $6) $2\ndocker tag datarobot/scoring-inference-code-sagemaker:latest $1:latest\ndocker push $1:latest",
        "# Create S3 Bucket\nlog.info(\"Creating S3 Bucket {}\".format(s3_bucket))\n\ns3 = session.resource(\"s3\")\ntry:\n    s3.create_bucket(Bucket=s3_bucket)\n    log.info(\"S3 Bucket Creation Complete!\")\nexcept ClientError as e:\n    log.error(e)",
        "# Upload scoring code jar tarball to AWS S3\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(tgz_name, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_model = \"sagemaker/models/\" + tgz_name\ntry:\n    s3.meta.client.upload_file(tgz_path, s3_bucket, s3_obj_name_model)\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)",
        "# Upload Batch Scoring Data to S3\nbatch_path = \"scoring_data/10K_Lending_Club_Loans_scoring.csv\"\n\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(batch_path, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_csv = \"sagemaker/\" + batch_path\ntry:\n    s3.meta.client.upload_file(batch_path, s3_bucket, s3_obj_name_csv)\n    batch_input_file = \"s3://\" + s3_bucket + \"/\" + s3_obj_name_csv\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)",
        "# Create IAM Role for Sagemaker to use\nlog.info(\"Creating Execution IAM Role for Sagemaker to use\")\niam = session.client(\"iam\")\niamr = session.resource(\"iam\")\n\nrole_policy = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Action\": [\"s3:ListBucket\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket],\n            },\n            {\n                \"Action\": [\"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket + \"/*\"],\n            },\n        ],\n    }\n)\n\nlog.info(\"Creating Exeuction S3 Access Policy\")\npolicy = iam.create_policy(\n    PolicyName=sagemaker_execution_role_name + \"-policy\", PolicyDocument=role_policy\n)\n\npolicy_arn = policy.get(\"Policy\").get(\"Arn\")\n\nassume_role_policy_document = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n                \"Action\": \"sts:AssumeRole\",\n            }\n        ],\n    }\n)\n\nlog.info(\"Creating actual role\")\nrole = iam.create_role(\n    RoleName=sagemaker_execution_role_name,\n    AssumeRolePolicyDocument=assume_role_policy_document,\n)\n\n# Attach our execution Policy\nlog.info(\"Attaching Execution Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name, PolicyArn=policy_arn\n)\n\n# Attach the AmazonSageMakerFullAccess Policy\nlog.info(\"Attaching AmazonSageMakerFullAccess Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name,\n    PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n)\n\nrole_resource = iamr.Role(sagemaker_execution_role_name)\nlog.info(\"IAM Role Info:\")\nlog.info(\"IAM Role Name: {}\".format(role_resource.name))\nlog.info(\"IAM Role ARN: {}\".format(role_resource.arn))\nlog.info(\"IAM Role Policies:\")\nfor p in role_resource.attached_policies.all():\n    log.info(p)\n\nlog.info(\"COMPLETE!\")",
        "# Create model in sagemaker\nlog.info(\"Creating Sagemaker Model\")\nsm_client = session.client(\"sagemaker\")\naws_model_name = modelname.split(\".\")[0]\n\nresponse = sm_client.create_model(\n    ModelName=aws_model_name,\n    PrimaryContainer={\n        \"Image\": ecr_repo_uri + \":latest\",\n        \"ImageConfig\": {\"RepositoryAccessMode\": \"Platform\"},\n        \"Mode\": \"SingleModel\",\n        \"ModelDataUrl\": \"s3://\" + s3_bucket + \"/\" + s3_obj_name_model,\n    },\n    ExecutionRoleArn=role_resource.arn,\n)\n\nif response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Created!\")\n    log.info(\"model name: {}\".format(aws_model_name))\n    log.info(\"model arn: {}\".format(response.get(\"ModelArn\")))\n    log.info(\"COMPLETE!\")",
        "# Create Sagemaker Endpoint Configuration\nlog.info(\"Creating Sagemaker Model Endpoint Configuration\")\naws_endpoint_config_name = aws_model_name + \"-ec\"\n\nec_response = sm_client.create_endpoint_config(\n    EndpointConfigName=aws_endpoint_config_name,\n    ProductionVariants=[\n        {\n            \"VariantName\": \"variant-1\",\n            \"ModelName\": aws_model_name,\n            \"InitialInstanceCount\": 1,\n            \"InstanceType\": \"ml.m4.xlarge\",\n        }\n    ],\n)\n\nif ec_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Endpoint Configuration Created!\")\n    log.info(\"endpoint configuration name: {}\".format(aws_endpoint_config_name))\n    log.info(\n        \"endpoint configuration arn: {}\".format(ec_response.get(\"EndpointConfigArn\"))\n    )\n    log.info(\"COMPLETE!\")",
        "# Create Sagemaker Endpoint\nlog.info(\"Creating Sagemaker Model Endpoint... This process can take a few minutes\")\naws_endpoint_name = aws_model_name + \"-ep\"\n\nep_response = sm_client.create_endpoint(\n    EndpointName=aws_endpoint_name,\n    EndpointConfigName=aws_endpoint_config_name,\n)\n\nif ep_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when sending endpoint creation request to Sagemaker\")\n    log.error(ep_response)\nelse:\n    i = 0\n    while i < 30:\n        status_r = sm_client.describe_endpoint(EndpointName=aws_endpoint_name)\n        status = status_r.get(\"EndpointStatus\")\n        log.info(\"Endpoint Creation Status: {}\".format(status_r.get(\"EndpointStatus\")))\n\n        if status == \"InService\":\n            break\n        else:\n            time.sleep(20)\n            i = i + 1\n\n    if status == \"InService\":\n        log.info(\"Sagemaker Model Endpoint Created!\")\n        log.info(\"Endpoint Name: {}\".format(status_r.get(\"EndpointName\")))\n        log.info(\"Endpoint ARN: {}\".format(status_r.get(\"EndpointArn\")))\n        invocation_url = \"https://runtime.sagemaker.{}.amazonaws.com/endpoints/{}/invocations\".format(\n            aws_region, status_r.get(\"EndpointName\")\n        )\n        log.info(\"Endpoint API URL: {}\".format(invocation_url))\n        log.info(\"COMPLETE!\")\n    else:\n        log.error(\"Sagemaker did not return an 'InService' status in time!\")\n        log.error(\"Last status received: {}\".format(status))\n        log.error(status_r)",
        "# Create a batch transform job for batch predictions\nlog.info(\"Creating Sagemaker Batch Transform Job\")\nbtj_client = session.client(\"sagemaker\")\n\njob_name = (\n    aws_model_name + \"-batch-transform-job-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n)\n\nbatch_output_folder = \"s3://\" + s3_bucket + \"/scoring_output/\"\nresponse = btj_client.create_transform_job(\n    TransformJobName=job_name,\n    ModelName=aws_model_name,\n    TransformInput={\n        \"DataSource\": {\n            \"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input_file}\n        },\n        \"ContentType\": \"string\",\n        \"CompressionType\": \"None\",\n        \"SplitType\": \"None\",\n    },\n    TransformOutput={\n        \"S3OutputPath\": batch_output_folder,\n        \"Accept\": \"string\",\n        \"AssembleWith\": \"None\",\n    },\n    TransformResources={\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n)\n\n# Response\nlog.info(\"Running Sagemaker Batch Transform Job {}\".format(job_name))\ni = 0\nwhile i < 30:\n    status_r = btj_client.describe_transform_job(TransformJobName=job_name)\n    status = status_r.get(\"TransformJobStatus\")\n    log.info(\"Batch Job Status: {}\".format(status_r.get(\"TransformJobStatus\")))\n\n    if status not in [\"InProgress\", \"Stopping\"]:\n        break\n    else:\n        time.sleep(20)\n        i = i + 1",
        "s3 = session.client(\"s3\")\noutput_dir = \"scoring_output\"\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nfor key in s3.list_objects(Bucket=s3_bucket)[\"Contents\"]:\n    if \".out\" in key[\"Key\"]:\n        s3.download_file(s3_bucket, key[\"Key\"], \"scoring_output/output.csv\")\n\ndf = pd.read_csv(output_dir + \"/output.csv\")\ndf",
        "s_client = session.client(\"sagemaker-runtime\")\n\nbuffer = open(\"scoring_data/1_row_Lending_Club_Loans_scoring.csv\")\npayload = buffer.read()\n\nresponse = s_client.invoke_endpoint(\n    EndpointName=aws_endpoint_name, ContentType=\"text/csv\", Body=payload\n)\n\ndata = response.get(\"Body\").read()\nlog.info(\"Scoring output:\\n{}\".format(data.decode(\"utf-8\")))"
      ],
      "api_methods": [
        "model.id",
        "project.create",
        "project.get_top_model",
        "project.wait_for_autopilot",
        "project.id",
        "dr.project.create",
        "project.analyze_and_model",
        "dr.client._global_client"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-588466627783698926",
      "title": "Azure_End_to_End",
      "content": "# End to end modeling workflow with Azure\n\nAuthor: Brent Hinks (2023-01-27)\n\n## Overview\n\nThis notebook illustrates an end-to-end data science workflow using DataRobot. The workflow ingests a dataset hosted in an Azure blob container, trains a series of models using DataRobot's AutoML capabilities, deploys a recommended model, and sets up a batch prediction job that writes predictions back to the original container.\n\nIn this notebook you'll cover the following steps:\n\n- Acquiring a training dataset from an Azure storage container\n- Building a new DataRobot project\n- Deploying a recommended model\n- Scoring via batch prediction API\n- Writing results back to a source Azure container\n\n## Setup\n\nPrior to execution, ensure that the following dependencies are available in your notebook environment:\n\n- **datarobot**, provided via PyPi (Python library used to communicate with the DataRobot platform)\n- **azure.storage.blob**, provided via PyPi (Python library used to access Azure storage services)\n- **pandas**, provided via PyPi (common data science library)\n- **Azure CLI**, used to authenticate to Azure. You can reference [installation instructions](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) for more information.\n\n### Import libraries\n\nThe first cell of the notebook imports necessary packages, and sets up the connection to the DataRobot platform. There are also optional values that can be provided to use an existing project and deployment - if they are omitted then a new autopilot session will be kicked off and a new deployment will be created using DataRobot's recommended model.\n\n```python\nfrom io import StringIO\n\nfrom azure.storage.blob import BlobServiceClient\nimport datarobot as dr\nimport pandas as pd\n```\n\n### Connect to DataRobot\n\n```python\n# Set DataRobot connection info here\nDATAROBOT_API_TOKEN = \"\"\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AZURE-78\",  # Optional but helps DataRobot improve this workflow\n)\n```\n\n### Bind credentials\n\n```python\n# Set Azure connection blob info here\nAZURE_STORAGE_ACCOUNT = \"\"\nAZURE_STORAGE_CONTAINER = \"\"\n\n# Find this value by following the \"Access keys\" link from your storage account in the Azure console\nAZURE_STORAGE_ACCESS_KEY = \"\"\n\n# Provide dataset filenames and the modeling target feature\nAZURE_INPUT_FILE = \"input.csv\"\nAZURE_OUTPUT_FILE = \"scored.csv\"\nAZURE_INPUT_TARGET = \"target\"\n\n# Set name for Azure credentials in DataRobot\nDR_CREDENTIAL_NAME = \"Azure_{}\".format(AZURE_STORAGE_ACCOUNT)\n\nproject_id = None\ndeployment_id = None\n```\n\nBefore running the next cell, which creates the storage service client, you should run `az login` from your terminal to establish an authenticated session to Azure.\n\n```python\naccount_url = \"https://{}.blob.core.windows.net\".format(AZURE_STORAGE_ACCOUNT)\nblob_service_client = BlobServiceClient(account_url)\n```\n\n### Import data\n\nLoad the dataset stored in your Azure container into a pandas dataframe.\n\n```python\ncontainer_client = blob_service_client.get_container_client(\n    container=AZURE_STORAGE_CONTAINER\n)\ndownloaded_blob = container_client.download_blob(AZURE_INPUT_FILE)\n\ndf = pd.read_csv(StringIO(downloaded_blob.content_as_text()))\n```\n\nEnsure that proper Azure credentials are stored in DataRobot. This credential can be used in the future to automate data reads and writes in scoring jobs. Check for an existing credential matching the name we provided above. If none is found, then create a new one.\n\n```python\n# Use this code to look up the ID of the credential object created.\ncredential = None\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        credential = cred\n\nif credential == None:\n    credential = dr.Credential.create_azure(\n        name=DR_CREDENTIAL_NAME,\n        azure_connection_string=\"DefaultEndpointsProtocol=https;AccountName={};AccountKey={};\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_ACCESS_KEY\n        ),\n    )\n\ncredential\n```\n\n## Modeling\n\n### Create a project\n\nCreate a new project in DataRobot and upload the data stored in your dataframe. After that you will set the target and start the AutoML process.\n\nIf a `project_id` was supplied above, skip these steps.\n\n```python\n# Create a project without setting the target\nif project_id == None:\n    project = dr.Project.create(project_name=\"New Test Project (Azure)\", sourcedata=df)\n    print(project.id)\n```\n\n### Initate Autopilot\n\n```python\nif project_id == None:\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=AZURE_INPUT_TARGET,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight\n```\n\n```python\nif project_id == None:\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current Autopilot status:\n    # project.stage\n    # project.get_model_jobs()\n```\n\n## Select and deploy a model\n\nReview DataRobot's model recommendations and select one for deployment. If `deployment_id` was supplied above, skip this step.\n\n```python\nprint(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()\n```\n\nWhen you are happy with your model you can automate deployment.\n\n```python\nif deployment_id == None:\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment (Azure)\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)\n```\n\n## Make batch predictions\n\nCreate a batch prediction job that will read in your training dataset, produce scores with optional explanations, and write the results back to the original container. If any errors occur along the way, get details from `job.get_status()` to assist in troubleshooting.\n\n```python\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_INPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_OUTPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    # Uncomment the next line to include prediction explanations.\n    # max_explanations=3,\n    passthrough_columns_set=\"all\",\n)\njob.wait_for_completion()\njob.get_status()\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/Azure_template/Azure_End_to_End.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/Azure_template/Azure_End_to_End.ipynb",
        "size": 12013,
        "cell_count": 26,
        "code_cell_count": 12
      },
      "code_examples": [
        "from io import StringIO\n\nfrom azure.storage.blob import BlobServiceClient\nimport datarobot as dr\nimport pandas as pd",
        "# Set DataRobot connection info here\nDATAROBOT_API_TOKEN = \"\"\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AZURE-78\",  # Optional but helps DataRobot improve this workflow\n)",
        "# Set Azure connection blob info here\nAZURE_STORAGE_ACCOUNT = \"\"\nAZURE_STORAGE_CONTAINER = \"\"\n\n# Find this value by following the \"Access keys\" link from your storage account in the Azure console\nAZURE_STORAGE_ACCESS_KEY = \"\"\n\n# Provide dataset filenames and the modeling target feature\nAZURE_INPUT_FILE = \"input.csv\"\nAZURE_OUTPUT_FILE = \"scored.csv\"\nAZURE_INPUT_TARGET = \"target\"\n\n# Set name for Azure credentials in DataRobot\nDR_CREDENTIAL_NAME = \"Azure_{}\".format(AZURE_STORAGE_ACCOUNT)\n\nproject_id = None\ndeployment_id = None",
        "account_url = \"https://{}.blob.core.windows.net\".format(AZURE_STORAGE_ACCOUNT)\nblob_service_client = BlobServiceClient(account_url)",
        "container_client = blob_service_client.get_container_client(\n    container=AZURE_STORAGE_CONTAINER\n)\ndownloaded_blob = container_client.download_blob(AZURE_INPUT_FILE)\n\ndf = pd.read_csv(StringIO(downloaded_blob.content_as_text()))",
        "# Use this code to look up the ID of the credential object created.\ncredential = None\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        credential = cred\n\nif credential == None:\n    credential = dr.Credential.create_azure(\n        name=DR_CREDENTIAL_NAME,\n        azure_connection_string=\"DefaultEndpointsProtocol=https;AccountName={};AccountKey={};\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_ACCESS_KEY\n        ),\n    )\n\ncredential",
        "# Create a project without setting the target\nif project_id == None:\n    project = dr.Project.create(project_name=\"New Test Project (Azure)\", sourcedata=df)\n    print(project.id)",
        "if project_id == None:\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=AZURE_INPUT_TARGET,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight",
        "if project_id == None:\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current Autopilot status:\n    # project.stage\n    # project.get_model_jobs()",
        "print(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()",
        "if deployment_id == None:\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment (Azure)\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)",
        "job = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_INPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_OUTPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    # Uncomment the next line to include prediction explanations.\n    # max_explanations=3,\n    passthrough_columns_set=\"all\",\n)\njob.wait_for_completion()\njob.get_status()"
      ],
      "api_methods": [
        "deployment.get",
        "project.wait_for_autopilot",
        "deployment.create_from_learning_model",
        "dr.project.create",
        "deployment.update_drift_tracking_settings",
        "project.stage",
        "dr.enums.autopilot_mode",
        "dr.deployment.get",
        "project.get_model_jobs",
        "dr.enums.recommended_model_type",
        "dr.credential.list",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "dr.modelrecommendation.get_all",
        "deployment.update_association_id_settings",
        "deployment.id",
        "project.create",
        "dr.modelrecommendation.get",
        "dr.credential.create_azure",
        "dr.batchpredictionjob.score",
        "project.analyze_and_model"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_7394199518252556440",
      "title": "Custom Model End-to-End With Compliance Docs",
      "content": "",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/Custom Model End-to-End With Compliance Docs/Custom Model End-to-End With Compliance Docs.ipynb",
      "tags": [
        "integration",
        "jupyter-notebook",
        "ecosystem",
        "ai-accelerators",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/Custom Model End-to-End With Compliance Docs/Custom Model End-to-End With Compliance Docs.ipynb",
        "size": 1697363,
        "cell_count": 0,
        "code_cell_count": 0
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_927313794518775080",
      "title": "Databricks & Datarobot - Large Scale Forecasting",
      "content": "# End-to-end demand forecasting workflow with DataRobot and Databricks \n\nAuthors: Austin Chou, Andrew Mathis\n\nReference: [DataRobot API documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n## Summary\n\nThis notebook outlines a use case forecasting future sales for multiple stores via multiseries modeling. Multiseries modeling allows you to model datasets that contain multiple time series based on a common set of input features. As in this example, multiseries forecasting is often useful for large chain businesses that want to more accurately manage their inventory and staffing by predicting the sales volume in the future. \n\nThis notebook focuses on working with Databricks and walks through how to use the Python API client to:\n\n1. Connect to DataRobot\n2. Import data from Databricks into the AI Catalog\n3. Create a time series forecasting project and run Autopilot\n4. Retrieve and evaluate model performances and insights\n5. Make new predictions with a test dataset\n6. Deploy a model with monitoring in MLOps\n7. Forecast predictions via the Prediction API\n\nFor this walkthrough, you can use the following publicly available dataset from the public DataRobot S3 bucket. Datasets for the excercise can be downloaded from here:\n\n* [Multiseries sales forecasting - Training data](https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_training.csv)\n* [Multiseries sales forecasting - Predictions data](https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_prediction.csv)\n\n## Setup\n\n### Import libraries\n\n```python\n# DataRobot Python library\nimport datetime as dt\n\nimport datarobot as dr\nimport matplotlib.pyplot as plt\n\n# The following are libraries used in this notebook for data and model evaluation\nimport pandas as pd\n```\n\n#### Optional: Import public training data for walkthrough (If data not already available in Databricks)\n\nFor this walkthrough, you can usea publicly available dataset ('DR_Demo_Sales_Multiseries_training.csv') from the public DataRobot S3 bucket to create a temporary table in Databricks. This will let you run the cells in this notebook and follow along.\n\n```python\n# Pull data from public DataRobot datasets\n# Training Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_training.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_training\")\n\n\n# Test Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_prediction.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_prediction\")\n```\n\n## Connect to DataRobot\n\nTo connect to DataRobot, you need to provide your API token and the endpoint. For more information, please refer to the following documentation:\n\n- [Create and manage API keys via developer tools in the GUI](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management)\n- [Different options to connect to DataRobot from the API client](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n\nYour API token can be found in the DataRobot UI in the **Developer tools** section, accessed from the profile menu in the top right corner. Copy the API token and paste in the cell below.\n\n```python\n# Connect to the DataRobot client\n# API Token\nDATAROBOT_API_TOKEN = \"INSERT YOUR DATAROBOT API TOKEN\"  # You can find the API token under the Developer Tools in the UI\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # If you have another endpoint (e.g. on-prem installs), change this accordingly\n\n# Connect to client\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n```\n\n## Import data from Databricks into the AI Catalog\n\nCurrently, DataRobot supports ingesting data via (1) pulling the data into a notebook, converting to a pandas dataframe, and then ingesting to DataRobot OR (2) directly ingesting data via JDBC connection to your Databricks data source.\n\nWe will demonstrate the first approach here. For large datasets, we recommend the second approach in order to bypass the need to convert the PySpark dataframe into a Pandas dataframe. ([Click here for documentation on setting up data connections and data sources for direct ingest.](https://docs.datarobot.com/en/docs/data/connect-data/data-conn.html))\n\n```python\n# Query and prepare data for ingest\ntraining_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_training \"\"\")\n\n# Convert to pandas df\ntraining_df = training_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# Prepare our dataframe accordingly:\ntraining_df[[\"Date\"]] = training_df[[\"Date\"]].apply(\n    pd.to_datetime\n)  # Convert Date to datetime\ntraining_df = training_df.sort_values(by=[\"Store\", \"Date\"])  # Order\n\ntraining_df.head(5)\n```\n\n```python\n# Let's also visualize our data:\ntraining_df.pivot(index=\"Date\", columns=\"Store\", values=\"Sales\").plot(figsize=(18, 8))\nplt.show()\n```\n\n```python\n# Upload data to DataRobot AI Catalog\nnew_dataset = dr.Dataset.upload(source=training_df)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Sales_Multiseries_training\")\n```\n\nDatasets in the AI Catalog are assigned a **dataset ID** which you can use to reference/get the dataset via the API.\n\n```python\n# Quick link to the AI Catalog dataset you just created\nprint(\"AI Catalog URL: \" + new_dataset.get_uri())\nprint(\"Dataset ID: \" + new_dataset.id)\n```\n\n## Create a time series forecasting project and run Autopilot\n\nYou can **create DataRobot projects** directly from:\n\n* A dataset in AI Catalog (using the dataset's ID in DataRobot)\n* A pandas dataframe (you do not need to write back to a data source or disk)\n* Directly from data sources\n\nOnce a project is created, you can tune and iterate on various modeling options (time series modeling, partition details, accuracy metrics, feature lists, etc.) and start modeling. For time series, these models can range from integrated models (Arima, RNNs), per forecast distance models (XGboost, elastic net), Trends and decomposition models, and more. In addition to this, the modeling process will also create and explore various time-series features - baseline features, rolling statistics, seasonal features based on date time, etc.\n\nYou can actively watch the project in action in the DataRobot UI after initiating Autopilot. The API and UI are parallel gateways to working on the same project which allows for cross-functional collaboration.\n\nEach created project is associated with a unique project ID. You can use the project ID to retrieve the project of interest via the API later on.\n\nReference the following resources for more information about time series modeling projects:\n* [Python API reference](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/time_series.html)\n* [Time Series modeling framework](https://docs.datarobot.com/en/docs/modeling/time/ts-reference/ts-framework.html)\n\n### Create a project\n\nWe create a project using our dataset in the AI Catalog:\n\n```python\n# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"[AIA] Multi_Store_Sales_Forecast\", dataset_id=new_dataset.id\n)\n```\n\nDataRobot Projects are assigned a **project ID** which you can use to reference/get the dataset via the API.\n\n```python\n# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)\n```\n\n### Configure time-series modeling settings\n\nTime-series projects have a number of parameters we can adjust. This includes:\n\n* Multi-series (i.e. Series ID column)\n* Backtest partitioning\n* Feature Derivation Window\n* Forecast Window\n* Known-in-advance (KA) Variables\n* Do not derive (DND) Variables\n* Calendars\n\nWe will set a number of these parameters. Additional information can be referenced in the [Time series modeling documentation](https://docs.datarobot.com/en/docs/modeling/time/ts-flow-overview.html). Here are some initial parameters to get started:\n\n**What rolling window should DataRobot use to derive features?** \n\nThe Feature Derivation window (FDW) represents the rolling window that is used to derive time series features and lags, relative to the Forecast Point (i.e. date of prediction). For example, we will build features within the rolling window of last 35 days in this accelerator.\n\n**Which future values do you want to forecast?**\n\nThe Forecast Window (FW) represents the rolling window of future values to predict, relative to the Forecast Point. For example, to predict sales for next 7 days, we enter 1 to 7 days. ([Click here for additional documentation on FDW and FW.](https://docs.datarobot.com/en/docs/modeling/time/ts-adv-modeling/ts-customization.html#set-window-values))\n\n**What are [\"Known-in-advance\" (KA) features](https://docs.datarobot.com/en/docs/modeling/time/ts-adv-modeling/ts-adv-opt.html#set-known-in-advance-ka)?**\n\nKA features are variables that you know in advance and can use as inputs for that row at the time of prediction. For example, our marketing team will already set the marketing campaign ahead of each date, so we do not need to lag the Marketing feature (i.e. Marketing is a KA feature).\n\n**How can we include special calendar events during feature engineering?**\n\nYou can [generate or upload a customized calendar](https://docs.datarobot.com/en/docs/modeling/time/ts-adv-modeling/ts-adv-opt.html#calendar-files) to the project. DataRobot will include additional feature engineering based on the calendar as part of the time-series feature engineering process.\n\n```python\n# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-35, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 7)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = [\"Store_Size\", \"Marketing\", \"TouristEvent\"]\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Date\"]),\n)  # Last date in calendar\n```\n\nWe pass all our settings to a [DatetimePartitioningSpecification](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/datetime_partition.html?highlight=datetimepartitioningspecification#setting-up-a-datetime-partitioned-project) object which will then be passed to our Autopilot process.\n\n```python\n# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Date\",  # Date column\n    multiseries_id_columns=[\"Store\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)\n```\n\n### Start modeling with autopilot\n\nTo start the Autopilot process, call the `analyze_and_model` function. Provide the prediction target and our DatetimePartitioningSpecification as part of the function call. We have several modes to spin up Autopilot - in this demo, we will use the default \"Quick\" mode.\n\n```python\n# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)\n```\n\n```python\n# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)\n```\n\n## Retrieve and evaluate model performances and insights\n\nAfter Autopilot completes, you can easily evaluate your model results. Evaluation can include compiling the Leaderboard as a dataframe, measuring performances across different backtest partitions with different metrics, visualizing the accuracy across series, analyzing Feature Impact and Feature Effects to understand each models' behaviors, and more. This can be done for every single model created by DataRobot.\n\nAs a simple example in this notebook, **we identify the best model created by Autopilot and evaluate:**\n\n* RMSE performance\n* MASE performance\n* Accuracy for Time for various Forecast Distance and Series combinations\n* Feature Impact of Top 10 features\n* Compare Accuracy across Series\n\n```python\n# # For convenience, we can access the project directly with the project ID:\n# project = dr.Project.get(project_id='65021b8e737ea778d21a78d8')\n```\n\n```python\n# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    scores = scores.append(model_performances, sort=False).reset_index(drop=True)\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores\n```\n\n```python\n# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)\n```\n\n### Model performance by metric\n\nAs a demonstration, we will get the performance of our model for:\n\n* **RMSE**: The optimization metric used by Autopilot in this project. We could have used another optimization metric when we kicked off the autopilot process with \"analyze_and_model\").\n* [**MASE (Mean Absolute Scaled Error)**](https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html#mase): Autopilot automatically uses a naive baseline model (e.g. using most recent value as the forecasted value) and scales the error to the naive model. MASE < 1 means an improvement over baseline, whereas MASE > 1 means the model performs worse than the naive approach (e.g. just taking the most recent value).\n\n```python\nprint(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)\n```\n\n### Get Accuracy Over Time\n\nDataRobot provides two helpful views of our forecasts out-of-the-box:\n\n* [**Accuracy Over Time**](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/aot.html) fixes the forecast distance and visualizes the corresponding forecast for each forecasted date.\n* [**Forecast vs Actual**](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/fore-act.html) sets a specific forecast point and visualizes the corresponding forecasts for the entire forecast window.\n\nWe can pull the results out for either analysis. As a demonstration, we will generate the Accuracy Over Time plots for forecast distances of 1 day and 7 day.\n\n```python\n# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()\n```\n\n```python\n# Get Accuracy over Time for FD=7, For just the Baltimore store series\nacc_plot_FD7_Baltimore = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=2, series_id=\"Baltimore\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD7_Baltimore.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()\n```\n\n### Retrieve Feature Impact\n\nAs an example of model explainability, calculate the Feature Impact values of the model using the `get_or_request_feature_impact` function.\n\n```python\n# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:10]\n\n# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()\n```\n\n### Analyze Accuracy for each Series\n\nThe [**Series Insight**](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/series-insights-multi.html) tool provides the ability to compute the accuracy for each indivudal series. This is especially powerful to help us identify which series the model is doing particularly better or worse in forecasting.\n\nIn this demonstration, we see that the model has particularly high RMSE for the Savannah and Louisville store forecasts. We may consider refining our model by splitting those two series into a separate model as a future modeling experiment.\n\n```python\n# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing\n```\n\n```python\n# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight\n```\n\n```python\n# Visualize the performance by stores\nplt.scatter(model_series_insight[\"Series\"], model_series_insight[\"backtestingScore\"])\np = plt.xticks(rotation=45, horizontalalignment=\"right\")\np = plt.ylabel(\"RMSE\")\n```\n\n## Make new predictions with a test dataset\n\nWe can make new predictions directly on the leaderboard by uploading new test datasets to the project. We can then score the test dataset with any model on the leaderboard and retrieve the results.\n\nIn this notebook, we will load the data into the notebook and upload to the project. As with the training dataset, you can also use the JDBC connector to ingest the test dataset into the AI catalog and then directly use the dataset ID in the `request_predictions` call. The JDBC path provides more efficient upload of large datasets to DataRobot.\n\nFor time-series predictions, DataRobot expects new prediction datasets to have rows for each new date we want to forecast. These rows should have values for the known-in-advance features and NA everywhere else. For example, since the model is forecasting 1-7 days out from each forecast point, we will have 7 new rows (corresponding from June 15 to June 21, 2014).\n\n**Note: We assume the prediction data is already available within Databricks. If not, please refer to the optional setup section to download and create a temporary view of the prediction dataset.**\n\n```python\n# Query and prepare data for ingest\ntest_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_prediction \"\"\")\n\n# Convert to pandas df\ntest_df = test_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# We prepare our dataframe accordingly:\ntest_df[[\"Date\"]] = test_df[[\"Date\"]].apply(pd.to_datetime)  # Convert Date to datetime\ntest_df = test_df.sort_values(by=[\"Store\", \"Date\"])  # Order\n```\n\n```python\n# Upload data to modeling project\ndataset = project.upload_dataset(test_df)\n\n# Make test predictions on the top model\npred_job = top_model.request_predictions(\n    dataset_id=dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=80,\n)\n\n# Get prediction results\npreds = pred_job.get_result_when_complete()\npreds.head(10)\n```\n\n## Deploy a model with monitoring in MLOps\n\nWith a single function call or click on the UI, DataRobot can quickly deploy models into production while fully reproducing the entire modeling pipeline including the necessary data preprocessing steps utilized by the blueprints and any advanced feature engineering that are part of the project. Once deployed, you can call the DataRobot REST or Python API to make batch and real-time predictions. You can also configure and schedule recurring batch prediction jobs that write back into a database.\n\nOnce a model is deployed, you can access MLOps monitoring capabilities such as:\n\n* Service health\n* Data drift\n* Prediction accuracy\n* Model retraining\n\nTo deploy a model, call the `create_from_learning_model` function and provide the ID of the model you want to deploy and the ID of the prediction server you want to deploy into.\n\nFor additional information, please see documentation for:\n\n1. [**MLOps monitoring**](https://docs.datarobot.com/en/docs/mlops/mlops-overview.html) \n2. [**Available prediction methods**](https://docs.datarobot.com/en/docs/predictions/index.html)\n3. [**Other deployment workflows with DataRobot**](https://docs.datarobot.com/en/docs/mlops/deployment/deploy-workflows/index.html)\n\n**Note: This demo assumes you have a prediction server available, such as in the DataRobot managed cloud instance. Please check that you have an accessible prediction server for your account and that you have available deployment slots before continuing.**\n\n```python\n# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    label=\"[AIA] Multi-Store Forecasting Deployment\",\n    description=\"[AIA] Multi Store Forecasting Deployment\",\n    default_prediction_server_id=prediction_server_id,\n)\n```\n\nEvery deployment in DataRobot is assigned a **deployment ID** which you can use to reference/retrieve the deployment via the API.\n\n```python\n# Quick link to the deployment you just created\nprint(\"DataRobot Deployment URL: \" + deployment.get_uri())\nprint(\"Deployment ID: \" + deployment.id)\n```\n\n### Configure model monitoring\n\nIn this example, we set up the deployment to monitor data drift. We use the following API call:\n\n* Data drift: `update_drift_tracking_settings`\n\nWe additionally enable and set the deployment to return (alongside the forecast):\n* Prediction intervals: `update_prediction_intervals_settings`\n* Prediction explanations: `PredictionExplanationsInitialization.create`\n\n```python\n# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n```\n\n```python\n# For time-series deployments, we can also set the prediction intervals for each forecast.\ndeployment.update_prediction_intervals_settings(percentiles=[95], enabled=True)\n```\n\n```python\n# In order to compute prediction explanations at time of predictions, initialize it for the best model\ndr.PredictionExplanationsInitialization.create(\n    project_id=project.id, model_id=top_model.id\n)\n```\n\n## Forecast predictions via the Prediction API\n\nThe [Batch Prediction API](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/index.html) provides flexible options for intake and output when scoring large datasets using the prediction servers you have already deployed. The API is exposed through the DataRobot Public API and can be consumed using  any REST-enabled client or the DataRobot Python Public API bindings.\n\nTo ensure the Batch Prediction API can process your time series dataset, you must configure the following:\n* Sort prediction rows by their timestamps, with the earliest row first.\n* If using multiseries, the prediction rows must be sorted by series ID then timestamp.\n* There is no limit on the number of series DataRobot supports. For more information, see [Time series data requirements for batch predictions via API](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/batch-pred-ts.html).\n\nIn this notebook, we will perform batch predictions with the 'test_df' dataset by calling the API endpoint and getting the results out as a pandas dataframe.\n\nAdditional methods to score DataRobot models include:\n* Reading from and writing to different data sources ([General Batch Prediction documentation](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/index.html)). For example, we can leverage JDBC connectors to read from Databricks and write to S3 with [score](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/batch_predictions.html?highlight=score#wiring-a-batch-prediction-job-manually).\n* Using the model's scoring code in a Databricks notebook for distributed spark scoring ([AI Accelerator demonstrating spark scoring here](https://community.datarobot.com/t5/ai-accelerators-library/end-to-end-workflows-with-datarobot-and-databricks/ta-p/16461)). Note that not every model will have scoring code available. For example, the top model in this project does not. You can identify scoring-code-capable models on the leaderboard by whether there is a Scoring Code tag.\n\n```python\njob, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, test_df, max_explanations=3  # Deployment ID  # Scoring dataset\n)  # If prediction explanations are required\n\nprint(\"Started scoring...\", job)\njob.wait_for_completion()\n```\n\n```python\n# Take a look at our prediction results\n# Format the prediction output. Select specific columns we want to view\nselect_predictions_df = predictions[\n    [\n        \"Store_y\",\n        \"FORECAST_POINT\",\n        \"Date_y\",\n        \"Sales (actual)_PREDICTION\",\n        \"PREDICTION_95_PERCENTILE_LOW\",\n        \"PREDICTION_95_PERCENTILE_HIGH\",\n        \"Marketing\",\n        \"EXPLANATION_1_FEATURE_NAME\",\n        \"EXPLANATION_2_FEATURE_NAME\",\n    ]\n]\nselect_predictions_df = select_predictions_df.rename(\n    columns={\"Store_y\": \"Store\", \"Date_y\": \"Date\"}\n)\n\n# Output predictions for the next seven days for two stores\nselect_predictions_df.head(14)\n```\n\n## Clean up\n\nTo remove everything added in the DataRobot platform as part of this notebook, run the following cell. If you want to delete any tables created in Databricks as a result of running this notebook, you will need to manually do so in Databricks.\n\n```python\n# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# deployment.delete()\n# project.delete()\n# CALENDAR.delete(CALENDAR.id)\n# new_dataset.delete(new_dataset.id)\n```\n\n## Additional Resources\n\n* The [DataRobot AI Accelerator Library](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library) has similar accelerators for other [Ecosystem Integrations](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library/label-name/ecosystem%20integration%20templates) to use DataRobot with other tools (e.g. AWS, GCP, Azure, etc) as well as accelerators for more advanced time-series applications.\n* The [DataRobot API user guide](https://docs.datarobot.com/en/docs/api/guide/python/index.html) provides code examples covering topics such as model factories, classification problems, feature impact rank ensembling, and more.\n* Learn how to [schedule notebooks in Databricks](https://docs.databricks.com/notebooks/schedule-notebook-jobs.html).\n\nTo learn more about advanced workflows for handling complex and large scale time series problems:\n* [Time series clustering](https://docs.datarobot.com/en/docs/modeling/time/ts-clustering.html#time-series-clustering)\n* [Segmented modeling](https://docs.datarobot.com/en/docs/modeling/time/ts-segmented.html)",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/Databricks & Datarobot - Large Scale Forecasting/Databricks & Datarobot - Large Scale Forecasting.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "time-series",
        "ai-accelerators",
        "openai",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/Databricks & Datarobot - Large Scale Forecasting/Databricks & Datarobot - Large Scale Forecasting.ipynb",
        "size": 972162,
        "cell_count": 57,
        "code_cell_count": 33
      },
      "code_examples": [
        "# DataRobot Python library\nimport datetime as dt\n\nimport datarobot as dr\nimport matplotlib.pyplot as plt\n\n# The following are libraries used in this notebook for data and model evaluation\nimport pandas as pd",
        "# Pull data from public DataRobot datasets\n# Training Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_training.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_training\")\n\n\n# Test Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_prediction.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_prediction\")",
        "# Connect to the DataRobot client\n# API Token\nDATAROBOT_API_TOKEN = \"INSERT YOUR DATAROBOT API TOKEN\"  # You can find the API token under the Developer Tools in the UI\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # If you have another endpoint (e.g. on-prem installs), change this accordingly\n\n# Connect to client\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)",
        "# Query and prepare data for ingest\ntraining_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_training \"\"\")\n\n# Convert to pandas df\ntraining_df = training_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# Prepare our dataframe accordingly:\ntraining_df[[\"Date\"]] = training_df[[\"Date\"]].apply(\n    pd.to_datetime\n)  # Convert Date to datetime\ntraining_df = training_df.sort_values(by=[\"Store\", \"Date\"])  # Order\n\ntraining_df.head(5)",
        "# Let's also visualize our data:\ntraining_df.pivot(index=\"Date\", columns=\"Store\", values=\"Sales\").plot(figsize=(18, 8))\nplt.show()",
        "# Upload data to DataRobot AI Catalog\nnew_dataset = dr.Dataset.upload(source=training_df)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Sales_Multiseries_training\")",
        "# Quick link to the AI Catalog dataset you just created\nprint(\"AI Catalog URL: \" + new_dataset.get_uri())\nprint(\"Dataset ID: \" + new_dataset.id)",
        "# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"[AIA] Multi_Store_Sales_Forecast\", dataset_id=new_dataset.id\n)",
        "# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)",
        "# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-35, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 7)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = [\"Store_Size\", \"Marketing\", \"TouristEvent\"]\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Date\"]),\n)  # Last date in calendar",
        "# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Date\",  # Date column\n    multiseries_id_columns=[\"Store\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)",
        "# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)",
        "# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)",
        "# # For convenience, we can access the project directly with the project ID:\n# project = dr.Project.get(project_id='65021b8e737ea778d21a78d8')",
        "# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    scores = scores.append(model_performances, sort=False).reset_index(drop=True)\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores",
        "# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)",
        "print(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)",
        "# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()",
        "# Get Accuracy over Time for FD=7, For just the Baltimore store series\nacc_plot_FD7_Baltimore = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=2, series_id=\"Baltimore\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD7_Baltimore.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()",
        "# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:10]\n\n# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()",
        "# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing",
        "# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight",
        "# Visualize the performance by stores\nplt.scatter(model_series_insight[\"Series\"], model_series_insight[\"backtestingScore\"])\np = plt.xticks(rotation=45, horizontalalignment=\"right\")\np = plt.ylabel(\"RMSE\")",
        "# Query and prepare data for ingest\ntest_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_prediction \"\"\")\n\n# Convert to pandas df\ntest_df = test_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# We prepare our dataframe accordingly:\ntest_df[[\"Date\"]] = test_df[[\"Date\"]].apply(pd.to_datetime)  # Convert Date to datetime\ntest_df = test_df.sort_values(by=[\"Store\", \"Date\"])  # Order",
        "# Upload data to modeling project\ndataset = project.upload_dataset(test_df)\n\n# Make test predictions on the top model\npred_job = top_model.request_predictions(\n    dataset_id=dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=80,\n)\n\n# Get prediction results\npreds = pred_job.get_result_when_complete()\npreds.head(10)",
        "# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    label=\"[AIA] Multi-Store Forecasting Deployment\",\n    description=\"[AIA] Multi Store Forecasting Deployment\",\n    default_prediction_server_id=prediction_server_id,\n)",
        "# Quick link to the deployment you just created\nprint(\"DataRobot Deployment URL: \" + deployment.get_uri())\nprint(\"Deployment ID: \" + deployment.id)",
        "# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)",
        "# For time-series deployments, we can also set the prediction intervals for each forecast.\ndeployment.update_prediction_intervals_settings(percentiles=[95], enabled=True)",
        "# In order to compute prediction explanations at time of predictions, initialize it for the best model\ndr.PredictionExplanationsInitialization.create(\n    project_id=project.id, model_id=top_model.id\n)",
        "job, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, test_df, max_explanations=3  # Deployment ID  # Scoring dataset\n)  # If prediction explanations are required\n\nprint(\"Started scoring...\", job)\njob.wait_for_completion()",
        "# Take a look at our prediction results\n# Format the prediction output. Select specific columns we want to view\nselect_predictions_df = predictions[\n    [\n        \"Store_y\",\n        \"FORECAST_POINT\",\n        \"Date_y\",\n        \"Sales (actual)_PREDICTION\",\n        \"PREDICTION_95_PERCENTILE_LOW\",\n        \"PREDICTION_95_PERCENTILE_HIGH\",\n        \"Marketing\",\n        \"EXPLANATION_1_FEATURE_NAME\",\n        \"EXPLANATION_2_FEATURE_NAME\",\n    ]\n]\nselect_predictions_df = select_predictions_df.rename(\n    columns={\"Store_y\": \"Store\", \"Date_y\": \"Date\"}\n)\n\n# Output predictions for the next seven days for two stores\nselect_predictions_df.head(14)",
        "# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# deployment.delete()\n# project.delete()\n# CALENDAR.delete(CALENDAR.id)\n# new_dataset.delete(new_dataset.id)"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "project.wait_for_autopilot",
        "model.metrics",
        "project.get_datetime_models",
        "model.get_series_accuracy_as_dataframe",
        "deployment.create_from_learning_model",
        "deployment.update_drift_tracking_settings",
        "dr.dataset.upload",
        "model.request_predictions",
        "project.get_uri",
        "model.id",
        "dr.predictionserver.list",
        "project.project_name",
        "dr.predictionexplanationsinitialization.create",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "dr.batchpredictionjob.score_pandas",
        "model.compute_series_accuracy",
        "project.get",
        "project.upload_dataset",
        "deployment.delete",
        "deployment.id",
        "deployment.get_uri",
        "project.delete",
        "model.get_uri",
        "model.get",
        "dr.calendarfile.create_calendar_from_country_code",
        "deployment.update_prediction_intervals_settings",
        "dr.model.get",
        "dr.project.get",
        "model.model_type",
        "project.analyze_and_model",
        "project.metric",
        "model.get_accuracy_over_time_plot",
        "project.create_from_dataset",
        "datarobot.rest.restclientobject",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "time_series"
    },
    {
      "id": "github_notebook_-4662679503130224263",
      "title": "Databricks_End_To_End",
      "content": "# End to end ML workflow with Databricks\n\nThis notebook illustrates and end-to-end data science workflow using features of both DataRobot and Databricks. You will leverage DataRobot for model training, selection, and MLOps monitoring while using Databricks to facilitate datasource access and utilize the Spark backbone to perform distributed scoring to support large-scale use cases.\n\nThis notebook covers the following steps:\n- Acquiring a training dataset from a data table\n- Building a new DataRobot project\n- Deploying a recommended model\n- Scoring via Spark using DataRobot's exportable Java scoring code\n- Scoring via prediction API\n- Reporting monitoring data to DataRobot's MLOps agent framework\n- Writing results back to a new table\n\nPrior to execution, you need to install a few dependencies to the Databricks cluster:\n- **datarobot**, provided via PyPI (Python library used to communicate with the DataRobot platform)\n- **com.datarobot:datarobot-prediction:2.2.1**, provided via Maven Central (Java library used to establish interface with DataRobot scoring code)\n- **com.datarobot:scoring-code-spark-api_3.0.0:0.0.4**, provided via Maven Central (Java library used to wrap scoring code with Spark functionality)\n- **mlops_utils_for_spark_3_2_0_8_1_0-4c992.jar**, provided via downloadable MLOps package which is available on the Developer Tools page in the DataRobot UI (Java library used to report monitoring statistics to MLOps Agent)\n\n## Setup\n\n### Import libraries\n\nThe first cell of the notebook imports necessary packages, and sets up the connection to the DataRobot platform. There are also optional values that can be provided to use an existing project and deployment - if they are omitted then a new Autopilot session will be kicked off and a new deployment will be created using DataRobot's recommended model.\n\n```python\nfrom io import StringIO\nimport time\n\nimport datarobot as dr\nimport pandas as pd\nfrom py4j.java_gateway import java_import\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import col\nimport requests\n\napi_key = \"\"  # Get this from the Developer Tools page in the DataRobot UI\nendpoint = \"https://app.datarobot.com/\"  # This should be the URL you use to access the DataRobot UI\n\nclient = dr.Client(\n    token=api_key,\n    endpoint=endpoint,\n    user_agent_suffix=\"AIA-E2E-DBX-8\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\n# Set these to empty strings to create a new project and/or deployment\nproject_id = \"\"\ndeployment_id = \"\"\n```\n\n### Connect to DataRobot\n\n```python\ndr.Client()\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')\n```\n\nRead more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n### Import data\n\nHere you'll pull in some data to work with. If a data table is available, you can provide the input table name, destination table name, and target feature in this cell. If none of those are provided, load the sample dataset provided by Databricks. This is also where any necessary data preparation would occur before sending the dataset to DataRobot. Note that DataRobot does not currently ingest Spark dataframes directly, so the dataframe will need to be converted to a Pandas dataframe prior to upload.\n\n```python\ntraining_table = \"\"\nscoring_table = \"\"\ntarget = \"\"\n\nif training_table == \"\":\n    scoring_table = \"white_wine_scored\"\n    target = \"quality\"\n    input_df = (\n        spark.read.option(\"header\", True)\n        .option(\"delimiter\", \";\")\n        .csv(\"dbfs:/databricks-datasets/wine-quality/winequality-white.csv\")\n    )\n    input_df = input_df.select(\n        [col(column).alias(column.replace(\" \", \"_\")) for column in input_df.columns]\n    )\nelse:\n    input_df = sql(\"select * from %s\" % (training_table))\n\ndf = input_df.toPandas()\ndisplay(input_df)\n```\n\n### Create a project\n\nThe Pandas dataframe is uploaded to the DataRobot platform and a name is given to the project.\n\n```python\n# Create a project wothout setting the target\nif project_id == \"\":\n    project = dr.Project.create(\n        project_name=\"New Test Project (Databricks)\", sourcedata=df\n    )\n    print(project.id)\n```\n\n## Modeling\n\n### Set the target feature\n\nHere you can define any advanced options needed for your project, including the Autopilot mode you wish to run (Standard Autopilot, Quick Mode, Comprehensive Mode, Manual). This API call will set our desired target feature and then kick off the EDA2 process, followed immediately by model training.\n\n```python\nif project_id == \"\":\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=target,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight\n```\n\n### Start Autopilot\n\nThis optional API call will block execution of the notebook until the full autopilot process has completed. This can take several minutes or hours, depending on the autopilot mode selected, the size of the dataset, and the type of problem we're trying to solve.\n\n```python\nif project_id == \"\":\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current autopilot status:\n    # project.stage\n    # project.get_model_jobs()\n```\n\n### List models\n\nThis API call outputs a list of all the models trained in the project, sorted by the selected validation metric.\n\n```python\n# Optionally, skip Autopilot and start here:\nif project_id != \"\":\n    project = dr.Project.get(project_id)\n\n# Pull the list of all models. You can iterate over these and examine them.\nproject.get_models()\n```\n\n### Retrieve the recommended model\n\nDataRobot provides a recommendation for an accurate and performant model at the end of Autopilot. This API call will fetch that recommendation.\n\n```python\nprint(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()\n```\n\n## Deploy a model\n\nIf no deployment ID was specified during setup, deploy DataRobot's recommended model. This will make the model available via the dedicated prediction API, and will wrap the model in our MLOps monitoring framework. Optional monitoring features are also enabled here, including accuracy tracking and data drift monitoring.\n\n```python\n# When you are happy with your model you can automate deployment\nif deployment_id == \"\":\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)\n```\n\n## Score a Spark Dataframe\n\nThe Spark wrapper that you imported into your cluster allows you to use the distributed power of the Spark cluster to quickly score large datasets. The following cells provide examples of scoring a Spark dataframe using Python or Scala.\n\n### Score with Python\n\nPython can be used to invoke the Java methods you provide to score with DataRobot models. The method call on **line 7** dynamically reaches out to the DataRobot platform to download the Scoring Code and make it available in your classpath. To avoid waiting for the network transfer, the scoring code can be downloaded ahead of time and imported as a new library in the Databricks cluster.\n\nIn order to perform the scoring transformation on the Spark dataframe, you must convert it to a Java dataframe and then back to a PySpark dataframe after scoring. You also capture the overall time it took to score in order to report that metric back to DataRobot MLOps in a later step.\n\n```python\njava_import(spark._jvm, \"com.datarobot.prediction.Predictors\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Model\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Predictors\")\n\nstart_time = (\n    time.time()\n)  # Grab timestamps before and after scoring to provide MLOps with an estimated execution time.\n# This next method call will use the endpoint, API token, and Deployment ID that were defined in previous cells to fetch our Scoring Code.\ndr_model = (\n    spark._jvm.com.datarobot.prediction.spark30.Predictors.getPredictorFromDeployment(\n        endpoint, deployment.id, api_key\n    )\n)\n\noutput_df = DataFrame(\n    dr_model.transform(input_df._jdf), spark\n)  # Apply the scoring transformation\nscore_time = (\n    time.time() - start_time\n)  # Get the total runtime of the fetching and scoring process\n\ndisplay(output_df)\n```\n\n### Score with Scala\n\nThe following cell performs the same scoring action as the previous one, only using Scala instead of Python.\n\nThis cell is commented out by default since variable values aren't shared between language contexts.\n\n```python\n%scala\n/**\nimport com.datarobot.prediction.spark30.Predictors\n\nval apiKey = \"\" //Provide DataRobot API token here\nval endpoint = \"https://app.datarobot.com/\" //This is the URL that you use to access the DataRobot UI\nval deploymentId = \"\" //The ID oif the deployment you'd like to use for scoring\nval inputDf = sql(\"select * from loans\") //Substitute a table name here\n\nval javaModel = Predictors.getPredictorFromDeployment(endpoint,deploymentId,apiKey)\n\nval outputDf = javaModel.transform(inputDf)\ndisplay(outputDf)\n**/\n```\n\n### Score with the Prediction API\n\nThis cell demonstrates scoring using a Pandas dataframe and the native DataRobot prediction API. This scoring method is limited to payloads under 50MB, so is not ideal for large datasets. An advantage to using this method would be easier access to monitoring data, since it does not require setup of the agent-based external monitoring framework.\n\n```python\nhost = \"https://example.dynamic.orm.datarobot.com\"  # This should be the URL of your prediction server, which you can find in the Deployment Overview page of the UI\nheaders = {\n    \"Content-Type\": \"application/json; charset=utf-8\",\n    \"Accept\": \"text/csv\",\n    \"datarobot-key\": \"\",  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    \"Authorization\": \"Bearer %s\" % (api_key),\n}\n\nparams = {\n    \"passthroughColumnsSet\": \"all\"  # This line tells the API to reflect back the input data along with the predictions\n}\n\ndata = df.to_json(orient=\"records\")\nresponse = requests.post(\n    \"{:}/predApi/v1.0/deployments/{:}/predictions\".format(host, deployment.id),\n    data=data,\n    headers=headers,\n    params=params,\n)\n\napi_df = pd.read_csv(\n    StringIO(response.text)\n)  # Here you read the API's CSV output into a Pandas dataframe\ndisplay(api_df)\n```\n\n### Report monitoring data\n\nPass monitoring data to the appropriate message channel - a Kafka topic, in this case. From there our external monitoring agent will pick up this data and pass it back to the DataRobot platform for display in the MLOps dashboard.\n\nNote that **this cell is provided as an example only**, and will not be executable without completing the full setup of the MLOps monitoring agent. More information regarding MLOps Monitoring Agent setup can be found [in the DataRobot documentation](https://docs.datarobot.com/en/docs/mlops/deployment/mlops-agent/monitoring-agent/index.html). This code cell illustrates the client library invocation that will push monitoring data to a message queue. More services need to be setup external to this notebook to complete the transfer of monitoring data to DataRobot.\n\n```python\n# java_import(spark._jvm, \"com.datarobot.mlops_spark_utils.MLOpsSparkUtils\")\n# channelConfig = \"spooler_type=kafka;kafka_topic_name=monitoring-agent-topic\"\n\n# spark._jvm.com.datarobot.mlops_spark_utils.MLOpsSparkUtils.reportPredictions(\n#     output_df._jdf, # scoring data\n#     deployment.id, # DeploymentId\n#     selection.id, # ModelId\n#     channelConfig, # MLOps channel configuration\n#     float(score_time), # scoring time\n#     ['target_1_PREDICTION','target_0_PREDICTION'] # target columns\n# )\n```\n\n### Write Results\nYou can now write our results back to a table. In this case you'll create a new table since the original source table's schema doesn't include columns to hold the scores or prediction explanations. In this example you are converting the results from the DataRobot Prediction API back to a Spark dataframe to facilitate writing to a table.\n\n```python\napi_spark_df = spark.createDataFrame(api_df)\napi_spark_df.write.mode(\"overwrite\").saveAsTable(scoring_table)\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/Databricks_template/Databricks_End_To_End.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/Databricks_template/Databricks_End_To_End.ipynb",
        "size": 24827,
        "cell_count": 31,
        "code_cell_count": 14
      },
      "code_examples": [
        "from io import StringIO\nimport time\n\nimport datarobot as dr\nimport pandas as pd\nfrom py4j.java_gateway import java_import\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import col\nimport requests\n\napi_key = \"\"  # Get this from the Developer Tools page in the DataRobot UI\nendpoint = \"https://app.datarobot.com/\"  # This should be the URL you use to access the DataRobot UI\n\nclient = dr.Client(\n    token=api_key,\n    endpoint=endpoint,\n    user_agent_suffix=\"AIA-E2E-DBX-8\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\n# Set these to empty strings to create a new project and/or deployment\nproject_id = \"\"\ndeployment_id = \"\"",
        "dr.Client()\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')",
        "training_table = \"\"\nscoring_table = \"\"\ntarget = \"\"\n\nif training_table == \"\":\n    scoring_table = \"white_wine_scored\"\n    target = \"quality\"\n    input_df = (\n        spark.read.option(\"header\", True)\n        .option(\"delimiter\", \";\")\n        .csv(\"dbfs:/databricks-datasets/wine-quality/winequality-white.csv\")\n    )\n    input_df = input_df.select(\n        [col(column).alias(column.replace(\" \", \"_\")) for column in input_df.columns]\n    )\nelse:\n    input_df = sql(\"select * from %s\" % (training_table))\n\ndf = input_df.toPandas()\ndisplay(input_df)",
        "# Create a project wothout setting the target\nif project_id == \"\":\n    project = dr.Project.create(\n        project_name=\"New Test Project (Databricks)\", sourcedata=df\n    )\n    print(project.id)",
        "if project_id == \"\":\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=target,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight",
        "if project_id == \"\":\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current autopilot status:\n    # project.stage\n    # project.get_model_jobs()",
        "# Optionally, skip Autopilot and start here:\nif project_id != \"\":\n    project = dr.Project.get(project_id)\n\n# Pull the list of all models. You can iterate over these and examine them.\nproject.get_models()",
        "print(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()",
        "# When you are happy with your model you can automate deployment\nif deployment_id == \"\":\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)",
        "java_import(spark._jvm, \"com.datarobot.prediction.Predictors\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Model\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Predictors\")\n\nstart_time = (\n    time.time()\n)  # Grab timestamps before and after scoring to provide MLOps with an estimated execution time.\n# This next method call will use the endpoint, API token, and Deployment ID that were defined in previous cells to fetch our Scoring Code.\ndr_model = (\n    spark._jvm.com.datarobot.prediction.spark30.Predictors.getPredictorFromDeployment(\n        endpoint, deployment.id, api_key\n    )\n)\n\noutput_df = DataFrame(\n    dr_model.transform(input_df._jdf), spark\n)  # Apply the scoring transformation\nscore_time = (\n    time.time() - start_time\n)  # Get the total runtime of the fetching and scoring process\n\ndisplay(output_df)",
        "%scala\n/**\nimport com.datarobot.prediction.spark30.Predictors\n\nval apiKey = \"\" //Provide DataRobot API token here\nval endpoint = \"https://app.datarobot.com/\" //This is the URL that you use to access the DataRobot UI\nval deploymentId = \"\" //The ID oif the deployment you'd like to use for scoring\nval inputDf = sql(\"select * from loans\") //Substitute a table name here\n\nval javaModel = Predictors.getPredictorFromDeployment(endpoint,deploymentId,apiKey)\n\nval outputDf = javaModel.transform(inputDf)\ndisplay(outputDf)\n**/",
        "host = \"https://example.dynamic.orm.datarobot.com\"  # This should be the URL of your prediction server, which you can find in the Deployment Overview page of the UI\nheaders = {\n    \"Content-Type\": \"application/json; charset=utf-8\",\n    \"Accept\": \"text/csv\",\n    \"datarobot-key\": \"\",  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    \"Authorization\": \"Bearer %s\" % (api_key),\n}\n\nparams = {\n    \"passthroughColumnsSet\": \"all\"  # This line tells the API to reflect back the input data along with the predictions\n}\n\ndata = df.to_json(orient=\"records\")\nresponse = requests.post(\n    \"{:}/predApi/v1.0/deployments/{:}/predictions\".format(host, deployment.id),\n    data=data,\n    headers=headers,\n    params=params,\n)\n\napi_df = pd.read_csv(\n    StringIO(response.text)\n)  # Here you read the API's CSV output into a Pandas dataframe\ndisplay(api_df)",
        "# java_import(spark._jvm, \"com.datarobot.mlops_spark_utils.MLOpsSparkUtils\")\n# channelConfig = \"spooler_type=kafka;kafka_topic_name=monitoring-agent-topic\"\n\n# spark._jvm.com.datarobot.mlops_spark_utils.MLOpsSparkUtils.reportPredictions(\n#     output_df._jdf, # scoring data\n#     deployment.id, # DeploymentId\n#     selection.id, # ModelId\n#     channelConfig, # MLOps channel configuration\n#     float(score_time), # scoring time\n#     ['target_1_PREDICTION','target_0_PREDICTION'] # target columns\n# )",
        "api_spark_df = spark.createDataFrame(api_df)\napi_spark_df.write.mode(\"overwrite\").saveAsTable(scoring_table)"
      ],
      "api_methods": [
        "deployment.get",
        "project.wait_for_autopilot",
        "deployment.create_from_learning_model",
        "dr.project.create",
        "deployment.update_drift_tracking_settings",
        "datarobot.prediction.spark30",
        "dr.client._global_client",
        "project.stage",
        "dr.enums.autopilot_mode",
        "dr.deployment.get",
        "project.get_model_jobs",
        "dr.enums.recommended_model_type",
        "dr.predictionserver.list",
        "model.transform",
        "dr.deployment.create_from_learning_model",
        "datarobot.mlops_spark_utils.mlopssparkutils",
        "project.id",
        "datarobot.prediction.predictors",
        "dr.modelrecommendation.get_all",
        "project.get",
        "deployment.update_association_id_settings",
        "deployment.id",
        "project.create",
        "dr.modelrecommendation.get",
        "dr.project.get",
        "project.analyze_and_model",
        "project.get_models",
        "datarobot.rest.restclientobject"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-5768410762685547527",
      "title": "GCP DataRobot End To End",
      "content": "Author: Luke Shulman \n\nVersion Date: 12/22/2022 \n# Build a DataRobot ML model and deploy from Google Cloud Platform\n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/DR%20and%20GCP%20Better%20Together.svg\" width=200 />\n\nIn this notebook, you will build an ML model using a combination of GCP services and DataRobot. It covers an end-to-end workflow that includes sourcing the data through exploratory data analysis, model development, and deployment.\n\nDataRobot recommends running this notebook in Google Colaboratory or Vertex AI Workbench, which both provide hosted notebooks with automatic configuration of Google services. Everything else in this notebook should work in any Jupyter environment with properly configured authentication. \n\n### Import libraries\n\n```python\nimport os\n\nimport pandas as pd\n\n# The Google Cloud Notebook product has specific requirements\nIS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/google\")\n\n# Google Cloud Notebook requires dependencies to be installed with '--user'\nUSER_FLAG = \"\"\nif IS_GOOGLE_CLOUD_NOTEBOOK:\n    USER_FLAG = \"--user\"\n```\n\n```python\n!pip install {USER_FLAG} --upgrade  google-cloud-resource-manager google-cloud-bigquery google-cloud-storage datarobot pandas altair google-cloud-secret-manager google-auth\n```\n\n### Configure a Google Cloud project\n\n**The following steps are required, regardless of your notebook environment.**\n\n1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute and storage costs.\n\n2. [Ensure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n\n3. [Enable the Big Query API, Secrets Manager, and Cloud Storage APIs](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com,storage_component,secretmanager.googleapis.com).\n\n4. If you are running this notebook locally, then install the [Cloud SDK](https://cloud.google.com/sdk).\n\n5. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the correct project for all the commands in this notebook.\n\n**Note**: Jupyter runs lines prefixed with `!` as shell commands and it interpolates Python variables prefixed with `$` into these commands.\n\n```python\n# Set project constants for Google\ntry:\n    import google.colab\n\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    credentials, _ = google.auth.default()\n    # @title Enter GCP/BigQuery Project ID\n    PROJECT_ID = \"datarobot-vertex-pipelines\"  # @param{type:\"string\"}\nelif IS_GOOGLE_CLOUD_NOTEBOOK:  # Likely using vertex or dataproc\n    import google\n\n    credentials, project = google.auth.default()\n    PROJECT_ID = project\nelse:  # Project running locally\n    from google import auth\n\n    credentials, project = auth.default()\n    PROJECT_ID = project\n\nif IN_COLAB:\n    # @title Enter GCP/BigQuery Project Number\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # @param{type:\"string\"}\nelse:\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # The ID number for you project\n```\n\n## Import data\n\nThis example uses loan data from a public dataset. To facilitate this demonstration, you will first load the data into a BigQuery table that will be used as the DataSource for DataRobot modeling.\n\n```python\nfrom tempfile import TemporaryFile\n\nfrom google.cloud import bigquery\nimport pandas as pd\nimport requests\n\n# Construct a BigQuery client object\nclient = bigquery.Client(project=PROJECT_ID)\n\n# Create the dataset if needed\ndataset_name = \"dr_sample_data\"\n\nclient.create_dataset(dataset_name, exists_ok=True)\n\nfull_table_name = client.project + \".\" + dataset_name + \".\" + \"lending_club\"\n\nprint(f\"\"\"Data will be written to {full_table_name}\"\"\")\n\njob_config = bigquery.LoadJobConfig(\n    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n\nwith TemporaryFile() as tmpfile:\n    r = requests.get(\n        \"https://s3.amazonaws.com/datarobot_public_datasets/10K_Lending_Club_Loans.csv\"\n    )\n    tmpfile.write(r.content)\n    tmpfile.seek(0)\n    load_job = client.load_table_from_file(\n        tmpfile, full_table_name, job_config=job_config\n    )  # Make an API request\n\n\nload_job.result()\n\ndestination_table = client.get_table(full_table_name)\nprint(\"Loaded {} rows.\".format(destination_table.num_rows))\n```\n\n```python\nimport datarobot as dr\nfrom google.cloud import secretmanager\n\napi_secret = f\"projects/{PROJECT_NUMBER}/secrets/DR_API_KEY/versions/1\"\nendpoint = f\"projects/{PROJECT_NUMBER}/secrets/DR_ENDPOINT/versions/1\"\nsecrets = secretmanager.SecretManagerServiceClient(credentials=credentials)\n\nDR_API_KEY = secrets.access_secret_version(name=api_secret).payload.data.decode(\"UTF-8\")\nDR_ENDPOINT = secrets.access_secret_version(name=endpoint).payload.data.decode(\"UTF-8\")\n\n\nclient = dr.Client(\n    token=DR_API_KEY,\n    endpoint=DR_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-GCP-6\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n```\n\n### Register data in the AI Catalog\n\nTo register the data with DataRobot, you will need to authorize DataRobot to access BigQuery data. As this requires user authorization, it must be enabled via the GUI. To authorize DataRobot to access data in BigQuery, follow these steps: \n\n1. In the AI Catalog, select **Add New Data Connection** and choose BigQuery.\n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/dr_new_data_connection.jpg\" width=300 />\n\n<span style=\"font-size:7;font-weight:100;\"><i>Create a new Data connection in DataRobot</i></span>\n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/BigQueryEnjoy.jpg\" width=300 />\n\n<span style=\"font-size:7;font-weight:100;\"><i>Select the BigQuery connection</i></span>\n\n2. Name the connection \"BigQuery,\" select the driver, and then enter your GCP project ID (saved in this notebook) in the text field as shown: \n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/BigQuery.jpg\" width=300 />\n\n3. Once the connection is saved, select **Test Data Connection**. This prompts you to authorize the DataRobot connection to BigQuery using your GCP Account. \n\nMore information on this process can be found in the [DataRobot BigQuery Documentation](https://app.datarobot.com/docs/data/connect-data/data-sources/dc-bigquery.html).\n\nOnce this process is complete, you can use the DataRobot API to access BigQuery datasets. \n\nTo facilitate data access, DataRobot defines the following entities:\n    \n- *Data store:* The system with the data in this case BigQuery. You created this in the previous step. \n- *Data source:* The query or table with the data. In this case, `dr_sample_data.lending_club`. \n- *Dataset:* A registered dataset for ML projects.\n\nThe following snippet creates all three of these assets.\n\n```python\n# Access the newly created DataStore that was named \"BigQuery\"\nfrom IPython.display import display, HTML\n\nDATA_STORE_NAME = \"DataRobot BigQuery Vertex\"\ndata_store = [ds for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][\n    0\n]\ncredential = [cred for cred in dr.Credential.list() if cred.name == \"bigquery-oauth\"][0]\n# now we will register the table as a data soruce.\n\n\nparams = dr.DataSourceParameters(\n    table=full_table_name, data_store_id=data_store.id  # from creating the table above\n)\n\ndata_source = dr.DataSource.create(\n    data_source_type=\"jdbc\", canonical_name=\"Test BigQuery\", params=params\n)\n\ndata_set = dr.Dataset.create_from_data_source(\n    data_source_id=data_source.id, credential_id=credential.credential_id\n)\n\nHTML(\n    f\"\"\"<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{data_set.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Dataset in DataRobot</a>\n</div>\"\"\"\n)\n```\n\nWith the dataset logged in the AI Catalog, you can quickly see key statistics about all of the features. \n\n```python\nfeatures_from_dr = data_set.get_all_features()\n\npd.DataFrame(\n    [\n        {\n            \"Feature Name\": f.name,\n            \"Feature Type\": f.feature_type,\n            \"Unique Count\": f.unique_count,\n            \"NA Count\": f.na_count,\n            \"Mean\": f.mean,\n            \"Median\": f.median,\n        }\n        for f in features_from_dr\n    ]\n)\n```\n\n## Initiate Autopilot\n\nWith the dataset logged in the AI Catalog, you can go ahead and kick off a project to predict `is_bad`, an indicator that the loan was not paid.\n\n```python\nproject = dr.Project.create_from_dataset(\n    dataset_id=data_set.id,\n)\n\n\ntry:\n    project.analyze_and_model(target=\"is_bad\")\nexcept dr.errors.AsyncTimeoutError:\n    print(\"Don't worry if it times out, the process is async and will continue to run\")\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{project.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Project in DataRobot</a>\n</div>\"\"\"\n)\n```\n\n## Evaluate the model \n\nAs DataRobot runs Autopilot, you can access the models on the Leaderboard using the `get_models` method. By default, this function returns models sorted by their performance so it is easy to find the top performing model. You can also call the `get_top_model` helper.   \n\n```python\ntop_model = project.get_top_model()\n\ndisplay(\n    HTML(\n        f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{top_model.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">{top_model.model_type}</a>\n</div>\"\"\"\n    )\n)\n\n\npd.DataFrame(top_model.metrics)\n```\n\n### Build an ROC curve\n\nBeyond the Leaderboard, you can access any analysis DataRobot does out-of-the-box for every model. In the following cell, reproduce the ROC curve by calling the `get_roc_curve` function from the top model.\n\n```python\nimport altair as alt\n\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)\n\n\nbase_line = pd.DataFrame({\"x\": [0, 1], \"y\": [0, 1]})\n\ncurve = (\n    alt.Chart(roc, title=\"ROC Curve For DataRobot Top Model\")\n    .mark_line()\n    .encode(x=\"false_positive_rate:Q\", y=\"true_positive_rate:Q\")\n)\n\nref_line = (\n    alt.Chart(base_line)\n    .mark_line(color=\"black\", strokeDash=[8, 4])\n    .encode(x=\"x:Q\", y=\"y:Q\")\n)\n\ncurve + ref_line\n```\n\n### Feature Impact \n\nTo demonstrate model explainability, you can trigger and get the feature impact values of any model with the `get_or_request_feature_impact` function.\n\n```python\n#### Retrieve Feature Impact ####\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False).head(\n    10\n)  # fist ten features\n\nalt.Chart(\n    FI_df, title=\"Feature Impact Chart for Top DataRobot Model\"\n).mark_bar().encode(x=\"impactNormalized:Q\", y=alt.X(\"featureName:N\", sort=\"-x\"))\n```\n\n## Deploy a model\n\nOnce selected, your top model can be easily deployed. \n\n```python\nprediction_server = dr.PredictionServer.list()[\n    0\n]  # Deploy to the first prediction server\n\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    description=\"Test Google End to End Deployment\",\n    prediction_threshold=0.5,\n    label=\"Test Google End to End\",\n    default_prediction_server_id=prediction_server.id,\n)\n\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{deployment.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Deployment in DataRobot</a>\n</div>\"\"\"\n)\n```\n\n### Run batch predictions\n\nThere are two ways of making batch predictions with the deployment. The first is to use the User OAuth JDBC connection you created in previous steps. The data will be saved to DataRobot and it can be accessed directly. \n\n```python\nfrom tempfile import TemporaryFile\n\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"query\": f\"\"\"SELECT * from {full_table_name};\"\"\",\n    \"data_store_id\": data_store.id,\n    \"credential_id\": credential.credential_id,\n}\n\n\njob = dr.BatchPredictionJob.score(deployment.id, intake_settings=intake_settings)\n\n\nwith TemporaryFile() as tmpfile:\n    job_csv = job.get_result_when_complete()\n    tmpfile.write(job_csv)\n    tmpfile.seek(0)\n    result = pd.read_csv(tmpfile)\n\nresult\n```\n\nYou can also use a service account to write data back to GCP directly. A service account is preferred here because it allows these jobs to be scheduled to happen automatically server to server. \n\n```python\nimport json\nfrom pathlib import Path\n\njson_credential = json.loads(\n    Path(\"PATH TO YOUR JSON SERVICE CREDENTIAL\").read_text()\n)  # You can obtain your service credentials in a number of ways\n\n# google_cloud_credential = dr.Credential.create_gcp(name='GCP Key Credential Test', gcp_key=json_credential, description=\"For GCP Batch Access\")\n\njob = dr.BatchPredictionJob.score(\n    deployment.id,  # this is the deployment id\n    intake_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club_predictions\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n)\n\njob.get_result_when_complete()\n```\n\n```python\nquery = f\"\"\"\n    SELECT count(*) as total_rows, avg(cast(is_bad_PREDICTION as numeric)) as avg_prediction from dr_sample_data.lending_club_predictions\n\"\"\"\nquery_job = client.query(query)  # Make an API request\n\nprint(\"The query data:\")\nfor row in query_job:\n    # Row values can be accessed by field name or index\n    print(f\"Result: {row['total_rows']} rows with avg of {row['avg_prediction']}\")\n```\n\n```python\n# CLEAN UP\n# Uncomment and run this cell to remove everything you added during this session\n\ndata_set.delete(data_set.id)\ndata_source.delete()\n# deployment.delete()\n# project.delete()\n# google_cloud_credential.delete()\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/GCP_template/GCP DataRobot End To End.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/GCP_template/GCP DataRobot End To End.ipynb",
        "size": 137659,
        "cell_count": 30,
        "code_cell_count": 16
      },
      "code_examples": [
        "import os\n\nimport pandas as pd\n\n# The Google Cloud Notebook product has specific requirements\nIS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/google\")\n\n# Google Cloud Notebook requires dependencies to be installed with '--user'\nUSER_FLAG = \"\"\nif IS_GOOGLE_CLOUD_NOTEBOOK:\n    USER_FLAG = \"--user\"",
        "!pip install {USER_FLAG} --upgrade  google-cloud-resource-manager google-cloud-bigquery google-cloud-storage datarobot pandas altair google-cloud-secret-manager google-auth",
        "# Set project constants for Google\ntry:\n    import google.colab\n\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    credentials, _ = google.auth.default()\n    # @title Enter GCP/BigQuery Project ID\n    PROJECT_ID = \"datarobot-vertex-pipelines\"  # @param{type:\"string\"}\nelif IS_GOOGLE_CLOUD_NOTEBOOK:  # Likely using vertex or dataproc\n    import google\n\n    credentials, project = google.auth.default()\n    PROJECT_ID = project\nelse:  # Project running locally\n    from google import auth\n\n    credentials, project = auth.default()\n    PROJECT_ID = project\n\nif IN_COLAB:\n    # @title Enter GCP/BigQuery Project Number\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # @param{type:\"string\"}\nelse:\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # The ID number for you project",
        "from tempfile import TemporaryFile\n\nfrom google.cloud import bigquery\nimport pandas as pd\nimport requests\n\n# Construct a BigQuery client object\nclient = bigquery.Client(project=PROJECT_ID)\n\n# Create the dataset if needed\ndataset_name = \"dr_sample_data\"\n\nclient.create_dataset(dataset_name, exists_ok=True)\n\nfull_table_name = client.project + \".\" + dataset_name + \".\" + \"lending_club\"\n\nprint(f\"\"\"Data will be written to {full_table_name}\"\"\")\n\njob_config = bigquery.LoadJobConfig(\n    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n\nwith TemporaryFile() as tmpfile:\n    r = requests.get(\n        \"https://s3.amazonaws.com/datarobot_public_datasets/10K_Lending_Club_Loans.csv\"\n    )\n    tmpfile.write(r.content)\n    tmpfile.seek(0)\n    load_job = client.load_table_from_file(\n        tmpfile, full_table_name, job_config=job_config\n    )  # Make an API request\n\n\nload_job.result()\n\ndestination_table = client.get_table(full_table_name)\nprint(\"Loaded {} rows.\".format(destination_table.num_rows))",
        "import datarobot as dr\nfrom google.cloud import secretmanager\n\napi_secret = f\"projects/{PROJECT_NUMBER}/secrets/DR_API_KEY/versions/1\"\nendpoint = f\"projects/{PROJECT_NUMBER}/secrets/DR_ENDPOINT/versions/1\"\nsecrets = secretmanager.SecretManagerServiceClient(credentials=credentials)\n\nDR_API_KEY = secrets.access_secret_version(name=api_secret).payload.data.decode(\"UTF-8\")\nDR_ENDPOINT = secrets.access_secret_version(name=endpoint).payload.data.decode(\"UTF-8\")\n\n\nclient = dr.Client(\n    token=DR_API_KEY,\n    endpoint=DR_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-GCP-6\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client",
        "# Access the newly created DataStore that was named \"BigQuery\"\nfrom IPython.display import display, HTML\n\nDATA_STORE_NAME = \"DataRobot BigQuery Vertex\"\ndata_store = [ds for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][\n    0\n]\ncredential = [cred for cred in dr.Credential.list() if cred.name == \"bigquery-oauth\"][0]\n# now we will register the table as a data soruce.\n\n\nparams = dr.DataSourceParameters(\n    table=full_table_name, data_store_id=data_store.id  # from creating the table above\n)\n\ndata_source = dr.DataSource.create(\n    data_source_type=\"jdbc\", canonical_name=\"Test BigQuery\", params=params\n)\n\ndata_set = dr.Dataset.create_from_data_source(\n    data_source_id=data_source.id, credential_id=credential.credential_id\n)\n\nHTML(\n    f\"\"\"<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{data_set.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Dataset in DataRobot</a>\n</div>\"\"\"\n)",
        "features_from_dr = data_set.get_all_features()\n\npd.DataFrame(\n    [\n        {\n            \"Feature Name\": f.name,\n            \"Feature Type\": f.feature_type,\n            \"Unique Count\": f.unique_count,\n            \"NA Count\": f.na_count,\n            \"Mean\": f.mean,\n            \"Median\": f.median,\n        }\n        for f in features_from_dr\n    ]\n)",
        "project = dr.Project.create_from_dataset(\n    dataset_id=data_set.id,\n)\n\n\ntry:\n    project.analyze_and_model(target=\"is_bad\")\nexcept dr.errors.AsyncTimeoutError:\n    print(\"Don't worry if it times out, the process is async and will continue to run\")\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{project.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Project in DataRobot</a>\n</div>\"\"\"\n)",
        "top_model = project.get_top_model()\n\ndisplay(\n    HTML(\n        f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{top_model.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">{top_model.model_type}</a>\n</div>\"\"\"\n    )\n)\n\n\npd.DataFrame(top_model.metrics)",
        "import altair as alt\n\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)\n\n\nbase_line = pd.DataFrame({\"x\": [0, 1], \"y\": [0, 1]})\n\ncurve = (\n    alt.Chart(roc, title=\"ROC Curve For DataRobot Top Model\")\n    .mark_line()\n    .encode(x=\"false_positive_rate:Q\", y=\"true_positive_rate:Q\")\n)\n\nref_line = (\n    alt.Chart(base_line)\n    .mark_line(color=\"black\", strokeDash=[8, 4])\n    .encode(x=\"x:Q\", y=\"y:Q\")\n)\n\ncurve + ref_line",
        "#### Retrieve Feature Impact ####\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False).head(\n    10\n)  # fist ten features\n\nalt.Chart(\n    FI_df, title=\"Feature Impact Chart for Top DataRobot Model\"\n).mark_bar().encode(x=\"impactNormalized:Q\", y=alt.X(\"featureName:N\", sort=\"-x\"))",
        "prediction_server = dr.PredictionServer.list()[\n    0\n]  # Deploy to the first prediction server\n\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    description=\"Test Google End to End Deployment\",\n    prediction_threshold=0.5,\n    label=\"Test Google End to End\",\n    default_prediction_server_id=prediction_server.id,\n)\n\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{deployment.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Deployment in DataRobot</a>\n</div>\"\"\"\n)",
        "from tempfile import TemporaryFile\n\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"query\": f\"\"\"SELECT * from {full_table_name};\"\"\",\n    \"data_store_id\": data_store.id,\n    \"credential_id\": credential.credential_id,\n}\n\n\njob = dr.BatchPredictionJob.score(deployment.id, intake_settings=intake_settings)\n\n\nwith TemporaryFile() as tmpfile:\n    job_csv = job.get_result_when_complete()\n    tmpfile.write(job_csv)\n    tmpfile.seek(0)\n    result = pd.read_csv(tmpfile)\n\nresult",
        "import json\nfrom pathlib import Path\n\njson_credential = json.loads(\n    Path(\"PATH TO YOUR JSON SERVICE CREDENTIAL\").read_text()\n)  # You can obtain your service credentials in a number of ways\n\n# google_cloud_credential = dr.Credential.create_gcp(name='GCP Key Credential Test', gcp_key=json_credential, description=\"For GCP Batch Access\")\n\njob = dr.BatchPredictionJob.score(\n    deployment.id,  # this is the deployment id\n    intake_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club_predictions\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n)\n\njob.get_result_when_complete()",
        "query = f\"\"\"\n    SELECT count(*) as total_rows, avg(cast(is_bad_PREDICTION as numeric)) as avg_prediction from dr_sample_data.lending_club_predictions\n\"\"\"\nquery_job = client.query(query)  # Make an API request\n\nprint(\"The query data:\")\nfor row in query_job:\n    # Row values can be accessed by field name or index\n    print(f\"Result: {row['total_rows']} rows with avg of {row['avg_prediction']}\")",
        "# CLEAN UP\n# Uncomment and run this cell to remove everything you added during this session\n\ndata_set.delete(data_set.id)\ndata_source.delete()\n# deployment.delete()\n# project.delete()\n# google_cloud_credential.delete()"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "model.metrics",
        "dr.datasource.create",
        "deployment.create_from_learning_model",
        "model.get_roc_curve",
        "deployment.update_drift_tracking_settings",
        "dr.client._global_client",
        "project.get_uri",
        "dr.credential.list",
        "model.id",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "dr.dataset.create_from_data_source",
        "deployment.delete",
        "deployment.id",
        "dr.datastore.list",
        "project.get_top_model",
        "deployment.get_uri",
        "project.delete",
        "model.get_uri",
        "dr.credential.create_gcp",
        "dr.errors.asynctimeouterror",
        "dr.batchpredictionjob.score",
        "model.model_type",
        "project.analyze_and_model",
        "project.create_from_dataset",
        "datarobot.rest.restclientobject",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-8108630507779539202",
      "title": "SAP_End_to_End",
      "content": "# End-to-End modeling workflow with SAP\n\nAuthor: Dennis Whitney\n\nVersion Date: 27/April/2023\n\n[Reference DataRobot's API documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n![datarobotandsaphana.png](attachment:datarobotandsaphana.png)\n\n# Overview\n\nThis notebook demonstrates an end-to-end workflow using DataRobot with SAP as the remote data source.\n\nThe data set used for this notebook is the [Auto MPG eample in the Quickstart example](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html) and an example method for loading this data into the SAP Hana database is given in the [Sample SAP HANA data load appendix](#appendix-sample-sap-hana-data-load) at the end of this document.\n\nThe workflow consists of the following steps:\n\n- Use the SAP HANA JDBC driver\n- [Create DataRobot credentials](https://docs.datarobot.com/en/docs/data/connect-data/stored-creds.html#stored-data-credentials)\n- Read a SAP HANA schema.table into DataRobot's AI Catalog\n- Start a project with the dataset\n- Deploy the recomended model\n- [Score via batch with various writeback options](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/batch_predictions.html?highlight=dr.BatchPredictionJob.score#jdbc-output)\n- Real-time prediction requests\n- Show service statistics\n- Clean up the created artifacts\n\n### Reference documentation\n\n- [DataRobot Python API documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/)\n- [DataRobot API quickstart guide](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n\n### Prerequisites\n\n- If your DataRobot version is <= 9.0, use Python SDK version 2.28.1\n- If your DataRobot version is later than 9.0, use Python SDK version 3.0 or later\n- You must provide a [DataRobot API key](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management) for the DataRobot cluster in use\n- [The SAP HANA JDBC Driver must be installed](https://repo1.maven.org/maven2/com/sap/cloud/db/jdbc/ngdbc/2.16.11/)\n- Use a working SAP HANA installation\n  - [This example uses Red Hat Enterprise Linux 9.0](https://cloud.redhat.com/blog/containerizing-sap-hana-express-on-red-hat-enterprise-linux)\n- [Valid DataRobot credentials for the data source](https://docs.datarobot.com/en/docs/data/connect-data/stored-creds.html#stored-data-credentials) \n- Networking must allow for port 22 for SSH and 39041 for the SAP HANA server\n\n# Notes\n\nBelow are the assorted notes found while using this notebook that may be useful\n\n**Note 1:** The jdbc connector, credentials and the data connection do not allow for finding with an id, so this notebook will use the name instead.\n\n**Note 2:** A future release of this notebook will demonstrate prediction write back.\n\n# Install Libraries\n\nIf you are using VS Code, you can install the required DataRobot packages with the cell below. Otherwise, use the cell that follows it.\n\n```python\n!{sys.executable} -m pip install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !{sys.executable} -m pip install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!{sys.executable} -m pip install datarobot\n```\n\n```python\n!pip3 install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !pip3 install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!pip3 install datarobot\n```\n\n# Import libraries and set parameters\n\nThe following code block will import the required python packages and provides a single place to set the parameters required to make the notebook work properly.\n\n**Please note:** These parameters are aligned to use the [Sample SAP HANA data load appendix](#appendix-sample-sap-hana-data-load) at the end of this document and you will need to update to suit your needs.\n\n```python\nimport csv\nimport json\nimport pprint\nfrom urllib.parse import urlparse\n\nimport datarobot as dr\nfrom datarobot import AUTOPILOT_MODE\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.models import Deployment\nimport requests\n\n# Provide the URL protocol, address (IP or FQDN)\n# Example: https://app.datarobot.com or https://datarobot.example.com or http://10.1.2.3\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com\"\n\n# # Provide an API key from a user with permission from this cluster\nDATAROBOT_API_TOKEN = \"<A valid DataRobot API key goes here>\"\n\n# Required by app.datarobot.com\nDATAROBOT_KEY = \"<A valid DataRobot App Key goes here>\"\n\n# Define the SAP Hana deployment\n# The settings below assumes the usage of the example data load defined at the end of this AI Accelerator\n# # If you are using a custom database, you must update as required\nSAP_JDBC_URL = \"jdbc:sap://<the SAP Hana IP goes here>:39041?databaseName=HXE\"\n\n# # When using https://www.sap.com/products/erp/s4hana.html, your jdbc url will look like:\n# SAP_JDBC_URL = \"jdbc:sap://<SAP Hana cloud ID goes here>.hanacloud.ondemand.com:443\"\n\n### JDBC Connectivity settings\nJDBC_USERNAME = \"JDBC_DR_WORKER\"\nJDBC_PASSWORD = \"His_Password_1\"\n\nJDBC_WRITE_SCHEMA = \"SAP_DEMO\"\nJDBC_WRITE_TABLE = \"scored_data_from_notebook\"\n\n### JDBC Driver Settings\nJDBC_CREDENTIAL_NAME = \"SAP_HANA_CREDENTIAL\"\nSAP_JDBC_DRIVER_NAME = \"2.15.10 recommended\"\nSAP_DATASTORE_NAME = \"SAP_HANA_DataStore\"\n\n### SQL Data Extraction statements\nTRAIN_DATASOURCE_NAME = \"SAP_HANA_DataSource\"\nTRAIN_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG;\"\n\nPREDICT_DATASOURCE_NAME = \"SAP_HANA_DataSource4Prediction\"\nPREDICT_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG LIMIT 10;\"\n\n### DataRobot project settings\nPROJECT_NAME = \"AutomobileMpG-burn-notice\"\nPROJECT_WORKERS = -1\nTARGET = \"MPG\"\nMETRIC = \"RMSE\"\n\n# Turn on DEBUG statements\nDEBUG = False  # True #\n\n# Set Autopilot mode\nOMODE = AUTOPILOT_MODE.QUICK\n\n# These values are mostly constant\n# Create the shared DataRobot client\nmy_client = dr.Client(\n    token=\"%s\" % (DATAROBOT_API_TOKEN),\n    endpoint=\"%s/api/v2\" % (DATAROBOT_ENDPOINT),\n    ssl_verify=True if (urlparse(DATAROBOT_ENDPOINT)).scheme == \"https\" else False,\n    trace_context=\"AIA-E2E-SAPWORKFLOW-125\",  # Optional. Helps DataRobot improve this workflow\n)\n\n# Find the dedicated prediction engine URL\nPREDICTION_SERVER = dr.PredictionServer.list()[0]\n\n# Verbose settings statement\nprint(\"# -----------------------------------------\")\nprint(\"DataRobot client version: %s\" % dr.__version__)\nprint(\"# -----------------------------------------\")\nprint(\"DATAROBOT_ENDPOINT: %s | %s\" % (DATAROBOT_ENDPOINT, PREDICTION_SERVER))\nprint(\"DATAROBOT_API_TOKEN: %s\" % (len(DATAROBOT_API_TOKEN)))\nprint(\"# Credentials settings -------------------\")\nprint(\"JDBC_CREDENTIAL_NAME: %s\" % (JDBC_CREDENTIAL_NAME))\nprint(\"JDBC_USERNAME: %s\" % (JDBC_USERNAME))\nprint(\"JDBC_PASSWORD: %s\" % (len(JDBC_PASSWORD)))\nprint(\"JDBC_WRITE_SCHEMA: %s\" % (JDBC_WRITE_SCHEMA))\nprint(\"JDBC_WRITE_TABLE: %s\" % (JDBC_WRITE_TABLE))\nprint(\"# DataStore settings ---------------------\")\nprint(\"SAP_JDBC_DRIVER_NAME: %s\" % (SAP_JDBC_DRIVER_NAME))\nprint(\"SAP_DATASTORE_NAME: %s\" % (SAP_DATASTORE_NAME))\nprint(\"SAP_JDBC_URL: %s\" % (len(SAP_JDBC_URL)))\nprint(\"TRAIN_DATASOURCE_NAME: %s\" % (TRAIN_DATASOURCE_NAME))\nprint(\"TRAIN_EXTRACTION_SQL: %s\" % (TRAIN_EXTRACTION_SQL))\nprint(\"PREDICT_DATASOURCE_NAME: %s\" % (PREDICT_DATASOURCE_NAME))\nprint(\"PREDICT_EXTRACTION_SQL: %s\" % (PREDICT_EXTRACTION_SQL))\nprint(\"# Project settings ------------------------\")\nprint(\"PROJECT_NAME: %s\" % (PROJECT_NAME))\nprint(\"TARGET: %s | METRIC: %s\" % (TARGET, METRIC))\n```\n\n# Create the Basic Credentials\n\nThe API will now build a Credentials set that will be used for Authentication when data is extracted, using the JDBC username and password\n\nFor more information on credentials, reference the [Python SDK documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/credentials.html?highlight=Credential#basic-credentials).\n\n**Please Note:** This notebook shares a set of credentials across the train and predict data sources.\n\n```python\nCREDENTIAL = dr.Credential.create_basic(\n    name=JDBC_CREDENTIAL_NAME,\n    user=JDBC_USERNAME,\n    password=JDBC_PASSWORD,\n)\n\nprint(\"Created CREDENTIAL: %s\" % (CREDENTIAL))\n```\n\n# Create a DataStore\n\nThe API code block below will find the existing SAP Hana JDBC driver ([See the Prerequisites above](#prerequisites)) and build a DataStore (or Data Connection) based on it, passing the jdbc url used to conenct to the source.\n\nFor more information on data connections, reference the [Python SDK documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/database_connectivity.html#creating-datastores).\n\nTo find a suitable JDBC driver, please refer to the [SAP Hana Maven repo](https://mvnrepository.com/artifact/com.sap.cloud.db.jdbc/ngdbc)\n\n```python\n# Set the driver name in the SAP Hana JDBC driver name temaplate\njdbc_driver_name = \"SAP HANA (%s)\" % SAP_JDBC_DRIVER_NAME\n\nJDBC_DRIVER = [\n    drs for drs in dr.DataDriver.list() if drs.canonical_name == jdbc_driver_name\n][-1]\nprint(\"Using JDBC_DRIVER: %s\" % (JDBC_DRIVER))\n\nDATASTORE = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=SAP_DATASTORE_NAME,\n    driver_id=JDBC_DRIVER.id,\n    jdbc_url=SAP_JDBC_URL,\n)\nprint(\"Created DATASTORE: %s\" % (DATASTORE))\n```\n\n# Create the DataSources\n\nThis section of the notebook will create the Training and Prediction DataSources used in the AI Catalog.\n\n**Please Note:** The Training DataSource is used during the Model Creation and Training phase and the Prediction DataSource is used to make predictions.\n\nFor more information on data sources, reference the [Python SDK documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/database_connectivity.html#creating-datasources).\n\n```python\nTRAIN_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=TRAIN_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=TRAIN_EXTRACTION_SQL,\n    ),\n)\nprint(\"Created TRAIN_DATASOURCE id: %s | %s\" % (TRAIN_DATASOURCE.id, TRAIN_DATASOURCE))\n\nPREDICT_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=PREDICT_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=PREDICT_EXTRACTION_SQL,\n    ),\n)\nprint(\n    \"Created PREDICT_DATASOURCE id: %s | %s\"\n    % (PREDICT_DATASOURCE.id, PREDICT_DATASOURCE)\n)\n```\n\n# Create the Datasets\n\nThis is where the notebook will push the 2 DataSources to the AI Catalog as a DataSet, with:\n1. The Train Extraction SQL statement\n2. The Prediction Extraction SQL statement\n\nFor more information on creating a dataset, reference the [Python SDK referenece documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html?highlight=create_dataset#datarobot.DataSource.create_dataset) and [its user guide](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/dataset.html?highlight=find%20dataset). \n\n```python\n# Create for the Training DataSet\nTRAIN_DATASET = TRAIN_DATASOURCE.create_dataset(\n    do_snapshot=False, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created TRAIN_DATASET id: %s | %s\" % (TRAIN_DATASET.id, TRAIN_DATASET))\n\n# Create for the Prediction Source\n# Please note the \"do_snapshot=True\" which is required for prediction datasets\nPREDICT_DATASET = PREDICT_DATASOURCE.create_dataset(\n    do_snapshot=True, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PREDICT_DATASET id: %s | %s\" % (PREDICT_DATASET.id, PREDICT_DATASET))\n\n# Handy debug statement to open a browser tab pointing to the Training DataSet in the AI Catalog\nif str(dr.__version__) > str(3) and DEBUG:\n    TRAIN_DATASET.open_in_browser()\n```\n\n# Create a project\n\nThe notebook will now create a Project based off the Training DataSet in the AI Catalog. This is where the API sets the name, credentails to use, the target and the other required parameters.\n\nThe code set below will show the existing worker and job state to show the current usage, the Project status as it processes the models and data and finally the exit worker state.\n\nFor more information on DataRobot Projects, please [Reference documentation for the Python SDK](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/project.html)\n\n```python\n# Find the current worker and job status, which is a good measure of system usage\nWORKERS_BEFORE = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers before: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_BEFORE[\"total\"],\n        WORKERS_BEFORE[\"inUse\"],\n        WORKERS_BEFORE[\"usersRunning\"],\n        WORKERS_BEFORE[\"jobsWaiting\"],\n        WORKERS_BEFORE[\"usersWaiting\"],\n    )\n)\n\nPROJECT = TRAIN_DATASET.create_project(\n    project_name=PROJECT_NAME, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PROJECT: %s\" % (PROJECT))\n\nPROJECT.analyze_and_model(\n    target=TARGET, metric=METRIC, mode=OMODE, worker_count=PROJECT_WORKERS\n)\n\nPROJECT.wait_for_autopilot()\n\nprint(\"PROJECT %s info: %s\" % (PROJECT.id, PROJECT))\n\n# An easy way to confirm the worker autoscaling is working as expected\nWORKERS_AFTER = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers after: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_AFTER[\"total\"],\n        WORKERS_AFTER[\"inUse\"],\n        WORKERS_AFTER[\"usersRunning\"],\n        WORKERS_AFTER[\"jobsWaiting\"],\n        WORKERS_AFTER[\"usersWaiting\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    PROJECT.open_in_browser()\n```\n\n# Get the model recomended for deployment\n\nThe code set below will get what DataRobot has determined to be the recomended model for deployment and display the models validation and crossValidation metrics.\n\n[Reference documentation for model recommendation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/model_recommendation.html?highlight=dr.ModelRecommendation.get#get-recommended-model)\n\n```python\nTOP_MODEL = dr.ModelRecommendation.get(\n    PROJECT.id, dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT\n).get_model()\n\nprint(\"Using Recommended Model id: %s | %s\" % (TOP_MODEL.id, TOP_MODEL))\nprint(\n    \"validation: %s | crossValidation: %s\"\n    % (\n        TOP_MODEL.metrics[PROJECT.metric][\"validation\"],\n        TOP_MODEL.metrics[PROJECT.metric][\"crossValidation\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    TOP_MODEL.open_model_browser()\n```\n\n# Deploy the top model\n\nThis code block will push the given model out to the DataRobot Prediction Server such that it can now serve requests with predictions. For more information, please [Reference the documentation for deployments](https://datarobot-public-api-client.readthedocs-hosted.com/en/v3.0.2/reference/mlops/deployment.html#create-a-deployment)\n\n```python\nDEPLOYMENT = dr.Deployment.create_from_learning_model(\n    TOP_MODEL.id,\n    label=\"%s RECOMMENDED_MODEL\" % PROJECT.project_name,\n    description=\"API %s deployment of: %s\"\n    % (PROJECT.project_name, TOP_MODEL.model_type),\n    default_prediction_server_id=PREDICTION_SERVER.id,\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    DEPLOYMENT.open_in_browser()\n\nprint(\"Created DEPLOYMENT: %s | %s\" % (DEPLOYMENT.id, DEPLOYMENT))\n```\n\n# Predictions\n\nNow that the heavy lifting of Training and Deploying the model is complete, we can start making predictions with that model via the Dedicated Prediction Server.\n\nThis note book will demonstate the 2 types of Predictions: Batch and Real-Time\n\n## Make Batch Predictions with SQL Statement\n\nThis example shows how to pull data via a fresh JDBC select statement and saves the results to a local file.\n\n[Reference documentation for batch predictions](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.1/reference/predictions/batch_predictions.html)\n\n```python\nJDBC_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,  #   \"select * from new_york_taxi.input limit 1000\",\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"localFile\",\n        \"path\": \"jdbc-batch-predictions.csv\",\n    },\n)\n\nJDBC_BATCH_PREDICTION_JOB.wait_for_completion()\n\nwith open(\"jdbc-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)\n```\n\n## Batch Predictions using the AI Catalog\n\nThe following cell reads from the latest version of the PREDICT_DATASOURCE_NAME in the AI Catalog and outputs to a local file. \n\n**Please Note:** Becuase this method uses an AI Catalog entry to score, it is expected to be faster than the method above that makes a fresh resqust to the SAP Hana database.\n\nFor further information, please see the [Reference documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.1/reference/predictions/batch_predictions.html#local-file-output)\n\n```python\nAICATALOG_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"dataset\",\n        \"dataset\": PREDICT_DATASET,\n        \"dataset_version_id\": PREDICT_DATASET.version_id,\n    },\n    output_settings={\"type\": \"localFile\", \"path\": \"aicatalog-batch-predictions.csv\"},\n)\nAICATALOG_BATCH_PREDICTION_JOB.wait_for_completion()\nAICATALOG_BATCH_PREDICTION_JOB.get_status()\n\nwith open(\"aicatalog-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)\n```\n\n# Make Batch Predictions\n\nThis section will read from the latest version in the AI Catalog and outputs to the SAP Hana database using the JDBC connector\n\n**Please Note:** this is a work in progress and will be ready soon after this page says its available: [Data sources supported for batch predictions](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/index.html#data-sources-supported-for-batch-predictions)\n\nFor further information, please see the [Reference documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.1/reference/predictions/batch_predictions.html#jdbc-output)\n\n```python\nAICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"jdbc\",\n        \"statement_type\": \"insert\",\n        \"schema\": JDBC_WRITE_SCHEMA,\n        \"table\": JDBC_WRITE_TABLE,\n        \"data_store_id\": DATASTORE.id,\n        \"credential_id\": CREDENTIAL.credential_id,\n        \"create_table_if_not_exists\": True,\n    },\n)\nAICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK.wait_for_completion()\n```\n\n# Make real-time predictions\n\nThe notebook will now make a small set of real time predictions where speed is king\n\n[Reference documentation](https://docs.datarobot.com/en/docs/api/reference/predapi/dr-predapi.html#datarobot-prediction-api)\n\n```python\n# JSON records for example automobiles for which to predict mpg\nautos = [\n    {\n        \"CYLINDERS\": 4,\n        \"DISPLACEMENT\": 119.0,\n        \"HORSEPOWER\": 82.00,\n        \"WEIGHT\": 2720.0,\n        \"ACCELERATION\": 19.4,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n    {\n        \"CYLINDERS\": 8,\n        \"DISPLACEMENT\": 120.0,\n        \"HORSEPOWER\": 79.00,\n        \"WEIGHT\": 2625.0,\n        \"ACCELERATION\": 18.6,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n]\n\n# Create REST request for prediction API\nprediction_server = DEPLOYMENT.default_prediction_server\n\nprint(\"prediction_server: %s\" % prediction_server)\n\nprediction_headers = {\n    \"Authorization\": \"Bearer {}\".format(DATAROBOT_API_TOKEN),\n    \"User-Agent\": \"AIA-E2E-SAPWORKFLOW-125\",  # Optional but helps DataRobot improve this workflow\n    \"DataRobot-Key\": DATAROBOT_KEY,\n    \"Content-Type\": \"application/json\",\n}\n\nREALTINE_PREDICTIONS = requests.post(\n    \"%s/predApi/v1.0/deployments/%s/predictions?passthroughColumns=CYLINDERS&passthroughColumns=DISPLACEMENT&passthroughColumns=HORSEPOWER&passthroughColumns=WEIGHT&passthroughColumns=ACCELERATION&passthroughColumns=MODELYEAR&passthroughColumns=ORIGIN\"\n    % (prediction_server[\"url\"], DEPLOYMENT.id),\n    data=json.dumps(autos),\n    headers=prediction_headers,\n)\n\npprint.pprint(REALTINE_PREDICTIONS.json())\n```\n\n# Show Deployment Prediction Information\n\n[Reference documentation for service stats](https://datarobot-public-api-client.readthedocs-hosted.com/en/v3.0.2/reference/mlops/deployment.html#service-stats)\n\n```python\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\nservice_stats = DEPLOYMENT.get_service_stats()\nprint(\"Service Statistics:\")\nprint(service_stats)\n\nfor item in service_stats.metrics:\n    print(\"%s => %s\" % (item, service_stats.metrics[item]))\n```\n\n## Cleanup\n\nThis section below will remove everything that was created above.\n\n```python\nitems_deleted = 0\n\n# Delete the elemets created above\nDR_ELEMENTS = [\n    TRAIN_DATASET,\n    PREDICT_DATASET,\n    DEPLOYMENT,\n    PROJECT,\n    TRAIN_DATASOURCE,\n    PREDICT_DATASOURCE,\n    DATASTORE,\n    CREDENTIAL,\n]\n\nfor item in DR_ELEMENTS:\n    if \"Credential\" in str(type(item)):\n        my_id = item.credential_id\n    else:\n        my_id = item.id\n\n    try:\n        print(\"Deleting: %s\" % (item.name))\n        item.delete()\n        items_deleted += 1\n\n        # my_id = item.credential_id if credential_id in item else item.id\n        print(\"Deleted %s: %s id: %s | %s\" % (items_deleted, item.name, my_id, item))\n\n    except Exception as ex:\n        print(\"id not found: %s (%s)\" % (my_id, type(item)))\n        pass\n    except NameError as n:\n        print(\"name %s not found\" % (n))\n        pass\n\nprint(\"Cleaned up %s items\" % (items_deleted))\n```\n\n# Appendix: Sample SAP Hana data load\n\nThis is the process used to get the SAP Hana database into a place where it is usable via the DataRobot JDBC connector.\n\n[This sample uses a varient of the data file referenced API Quickstart example](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n\nThe directions below are a packaged version of the [the RedHat example](https://cloud.redhat.com/blog/containerizing-sap-hana-express-on-red-hat-enterprise-linux).\n\n## System size\n\n- OS: RHEL 9.1\n- Instance Type: R6i.large (2cpu/16GB RAM)\n- Storage:\n  - root disk = 10GB\n  - second disk = 40GB for filesystem “/containerstorage”\n\n## Example command for USER_DATA\n\nCopy paste the script below to the USER_DATA text box given during system creation.\n\n```bash\n#!/bin/bash\n\n# Set volume name\ndisk=nvme1n1\necho \"y\" | mkfs -t xfs /dev/${disk}\nmkdir -p /containerstorage\nmount -t xfs /dev/${disk} /containerstorage\n# Add entry to the fstab\necho \"/dev/${disk} /containerstorage xfs defaults,nofail 0 0\" | tee -a /etc/fstab\n\nsetenforce 0\nsed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=permissive/' /etc/sysconfig/selinux\n\necho \"export TMPDIR='/containerstorage/tmp'\" | tee -a /root/.bashrc\n\necho -e \"fs.file-max=20000000\\nfs.aio-max-nr=262144\\nvm.memory_failure_early_kill=1\\nvm.max_map_count=135217728\\nnet.ipv4.ip_local_port_range=60000 65535\" | tee -a /etc/sysctl.conf\n\nmkdir -p /data/hxe\nchown 12000:79 /data/hxe\necho -e '{\\n\"master_password\" : \"HXEHana1\"}' | tee -a /data/hxe/hxepasswd.json\n\nyum install tmux container-tools -y\nsed -i 's|/var/lib/containers/storage|/containerstorage|' /etc/containers/storage.conf\n```\n\n## Finish Docker Image Pull\n\nThis portion will run pulled SAP Hana container in a tmux'ed process.\n\n**Please note:** this notebook uses `docker.io` for the container location and to update as needed and the user will need to supply a valid docker username and password\n\n```bash\n# Connect\nssh -i <Your Key> ec2-user@<yourhost>\n\n# become root\nsudo su - \n\n# Set your docker username and password\nD_USER=''\nD_PASSWORD=''\n\npodman login --username ${D_USER} --password ${D_PASSWORD} docker.io/store/saplabs\n\npodman pull docker.io/saplabs/hanaexpress:2.00.061.00.20220519.1\n\ntmux\n\npodman run -p 39013:39013 -p 39017:39017 -p 39041-39045:39041-39045 -p 1128-1129:1128-1129 -p 59013-59014:59013-59014 -v /data/hxe:/hana/mounts --ulimit nofile=1048576:1048576 --sysctl kernel.shmmax=1073741824 --sysctl net.ipv4.ip_local_port_range='40000 60999' --sysctl kernel.shmmni=4096 --sysctl kernel.shmall=8388608 --name hxecont docker.io/saplabs/hanaexpress:2.00.061.00.20220519.1 --passwords-url file:///hana/mounts/hxepasswd.json --agree-to-sap-license\n```\n\n## Load the Sample Data\n\nBelow find the \n\nOpen a second SSH window, create the data sources, the user, grant permissions, and test.\n\n```bash\n# Connect\nssh -i <Your Key> ec2-user@<yourhost>\n\n# become root\nsudo su - \n\n# Download and prep the source file\n\ncurl -o /tmp/auto-mpg.csv https://raw.githubusercontent.com/datarobot-community/quickstart-guide/master/data/auto-mpg.csv\nsed -i 's/[a-z]/\\U&/g' /tmp/auto-mpg.csv\nsed -i '1s/ //' /tmp/auto-mpg.csv\n\npodman cp /tmp/auto-mpg.csv hxecont:/usr/sap/HXE/HDB90/work\npodman exec -it hxecont /usr/sap/HXE/HDB90/exe/hdbsql -i 90 -d HXE -u system -p HXEHana1\n\n# Make the schma, table and Load the data\nALTER SYSTEM ALTER CONFIGURATION ('indexserver.ini', 'system') set ('import_export', 'enable_csv_import_path_filter') = 'false' with reconfigure;\nCREATE SCHEMA DATAINPUT\n\nCREATE TABLE DATAINPUT.AUTOMPG(MPG NUMERIC, CYLINDERS INT, DISPLACEMENT NUMERIC, HORSEPOWER NUMERIC, WEIGHT NUMERIC, ACCELERATION NUMERIC, MODELYEAR INT, ORIGIN INT );\n\nIMPORT FROM CSV FILE '/usr/sap/HXE/HDB90/work/auto-mpg.csv' INTO DATAINPUT.AUTOMPG WITH COLUMN LIST IN FIRST ROW FIELD DELIMITED BY ',';\nSELECT COUNT(*) FROM DATAINPUT.AUTOMPG;\n\nCREATE USER JDBC_DR_WORKER PASSWORD His_Password_1 NO FORCE_FIRST_PASSWORD_CHANGE\nGRANT SELECT, INSERT, UPDATE, DELETE, EXECUTE ON SCHEMA DATAINPUT TO JDBC_DR_WORKER\nGRANT SELECT, INSERT, UPDATE, DELETE ON DATAINPUT.AUTOMPG TO JDBC_DR_WORKER\n\n# Log in to the client as the JDBC_DR_WORKER user to test\npodman exec -it hxecont /usr/sap/HXE/HDB90/exe/hdbsql -i 90 -d HXE -u JDBC_DR_WORKER -p His_Password_1\nSELECT * FROM DATAINPUT.AUTOMPG LIMIT 10\n```\n\n---\n## Copyright 2023 DataRobot Inc. All Rights Reserved.\n\n**This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied**\n\n---",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/SAP_template/SAP_End_to_End.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "ai-accelerators",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/SAP_template/SAP_End_to_End.ipynb",
        "size": 56455,
        "cell_count": 33,
        "code_cell_count": 16
      },
      "code_examples": [
        "!{sys.executable} -m pip install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !{sys.executable} -m pip install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!{sys.executable} -m pip install datarobot",
        "!pip3 install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !pip3 install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!pip3 install datarobot",
        "import csv\nimport json\nimport pprint\nfrom urllib.parse import urlparse\n\nimport datarobot as dr\nfrom datarobot import AUTOPILOT_MODE\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.models import Deployment\nimport requests\n\n# Provide the URL protocol, address (IP or FQDN)\n# Example: https://app.datarobot.com or https://datarobot.example.com or http://10.1.2.3\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com\"\n\n# # Provide an API key from a user with permission from this cluster\nDATAROBOT_API_TOKEN = \"<A valid DataRobot API key goes here>\"\n\n# Required by app.datarobot.com\nDATAROBOT_KEY = \"<A valid DataRobot App Key goes here>\"\n\n# Define the SAP Hana deployment\n# The settings below assumes the usage of the example data load defined at the end of this AI Accelerator\n# # If you are using a custom database, you must update as required\nSAP_JDBC_URL = \"jdbc:sap://<the SAP Hana IP goes here>:39041?databaseName=HXE\"\n\n# # When using https://www.sap.com/products/erp/s4hana.html, your jdbc url will look like:\n# SAP_JDBC_URL = \"jdbc:sap://<SAP Hana cloud ID goes here>.hanacloud.ondemand.com:443\"\n\n### JDBC Connectivity settings\nJDBC_USERNAME = \"JDBC_DR_WORKER\"\nJDBC_PASSWORD = \"His_Password_1\"\n\nJDBC_WRITE_SCHEMA = \"SAP_DEMO\"\nJDBC_WRITE_TABLE = \"scored_data_from_notebook\"\n\n### JDBC Driver Settings\nJDBC_CREDENTIAL_NAME = \"SAP_HANA_CREDENTIAL\"\nSAP_JDBC_DRIVER_NAME = \"2.15.10 recommended\"\nSAP_DATASTORE_NAME = \"SAP_HANA_DataStore\"\n\n### SQL Data Extraction statements\nTRAIN_DATASOURCE_NAME = \"SAP_HANA_DataSource\"\nTRAIN_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG;\"\n\nPREDICT_DATASOURCE_NAME = \"SAP_HANA_DataSource4Prediction\"\nPREDICT_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG LIMIT 10;\"\n\n### DataRobot project settings\nPROJECT_NAME = \"AutomobileMpG-burn-notice\"\nPROJECT_WORKERS = -1\nTARGET = \"MPG\"\nMETRIC = \"RMSE\"\n\n# Turn on DEBUG statements\nDEBUG = False  # True #\n\n# Set Autopilot mode\nOMODE = AUTOPILOT_MODE.QUICK\n\n# These values are mostly constant\n# Create the shared DataRobot client\nmy_client = dr.Client(\n    token=\"%s\" % (DATAROBOT_API_TOKEN),\n    endpoint=\"%s/api/v2\" % (DATAROBOT_ENDPOINT),\n    ssl_verify=True if (urlparse(DATAROBOT_ENDPOINT)).scheme == \"https\" else False,\n    trace_context=\"AIA-E2E-SAPWORKFLOW-125\",  # Optional. Helps DataRobot improve this workflow\n)\n\n# Find the dedicated prediction engine URL\nPREDICTION_SERVER = dr.PredictionServer.list()[0]\n\n# Verbose settings statement\nprint(\"# -----------------------------------------\")\nprint(\"DataRobot client version: %s\" % dr.__version__)\nprint(\"# -----------------------------------------\")\nprint(\"DATAROBOT_ENDPOINT: %s | %s\" % (DATAROBOT_ENDPOINT, PREDICTION_SERVER))\nprint(\"DATAROBOT_API_TOKEN: %s\" % (len(DATAROBOT_API_TOKEN)))\nprint(\"# Credentials settings -------------------\")\nprint(\"JDBC_CREDENTIAL_NAME: %s\" % (JDBC_CREDENTIAL_NAME))\nprint(\"JDBC_USERNAME: %s\" % (JDBC_USERNAME))\nprint(\"JDBC_PASSWORD: %s\" % (len(JDBC_PASSWORD)))\nprint(\"JDBC_WRITE_SCHEMA: %s\" % (JDBC_WRITE_SCHEMA))\nprint(\"JDBC_WRITE_TABLE: %s\" % (JDBC_WRITE_TABLE))\nprint(\"# DataStore settings ---------------------\")\nprint(\"SAP_JDBC_DRIVER_NAME: %s\" % (SAP_JDBC_DRIVER_NAME))\nprint(\"SAP_DATASTORE_NAME: %s\" % (SAP_DATASTORE_NAME))\nprint(\"SAP_JDBC_URL: %s\" % (len(SAP_JDBC_URL)))\nprint(\"TRAIN_DATASOURCE_NAME: %s\" % (TRAIN_DATASOURCE_NAME))\nprint(\"TRAIN_EXTRACTION_SQL: %s\" % (TRAIN_EXTRACTION_SQL))\nprint(\"PREDICT_DATASOURCE_NAME: %s\" % (PREDICT_DATASOURCE_NAME))\nprint(\"PREDICT_EXTRACTION_SQL: %s\" % (PREDICT_EXTRACTION_SQL))\nprint(\"# Project settings ------------------------\")\nprint(\"PROJECT_NAME: %s\" % (PROJECT_NAME))\nprint(\"TARGET: %s | METRIC: %s\" % (TARGET, METRIC))",
        "CREDENTIAL = dr.Credential.create_basic(\n    name=JDBC_CREDENTIAL_NAME,\n    user=JDBC_USERNAME,\n    password=JDBC_PASSWORD,\n)\n\nprint(\"Created CREDENTIAL: %s\" % (CREDENTIAL))",
        "# Set the driver name in the SAP Hana JDBC driver name temaplate\njdbc_driver_name = \"SAP HANA (%s)\" % SAP_JDBC_DRIVER_NAME\n\nJDBC_DRIVER = [\n    drs for drs in dr.DataDriver.list() if drs.canonical_name == jdbc_driver_name\n][-1]\nprint(\"Using JDBC_DRIVER: %s\" % (JDBC_DRIVER))\n\nDATASTORE = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=SAP_DATASTORE_NAME,\n    driver_id=JDBC_DRIVER.id,\n    jdbc_url=SAP_JDBC_URL,\n)\nprint(\"Created DATASTORE: %s\" % (DATASTORE))",
        "TRAIN_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=TRAIN_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=TRAIN_EXTRACTION_SQL,\n    ),\n)\nprint(\"Created TRAIN_DATASOURCE id: %s | %s\" % (TRAIN_DATASOURCE.id, TRAIN_DATASOURCE))\n\nPREDICT_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=PREDICT_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=PREDICT_EXTRACTION_SQL,\n    ),\n)\nprint(\n    \"Created PREDICT_DATASOURCE id: %s | %s\"\n    % (PREDICT_DATASOURCE.id, PREDICT_DATASOURCE)\n)",
        "# Create for the Training DataSet\nTRAIN_DATASET = TRAIN_DATASOURCE.create_dataset(\n    do_snapshot=False, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created TRAIN_DATASET id: %s | %s\" % (TRAIN_DATASET.id, TRAIN_DATASET))\n\n# Create for the Prediction Source\n# Please note the \"do_snapshot=True\" which is required for prediction datasets\nPREDICT_DATASET = PREDICT_DATASOURCE.create_dataset(\n    do_snapshot=True, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PREDICT_DATASET id: %s | %s\" % (PREDICT_DATASET.id, PREDICT_DATASET))\n\n# Handy debug statement to open a browser tab pointing to the Training DataSet in the AI Catalog\nif str(dr.__version__) > str(3) and DEBUG:\n    TRAIN_DATASET.open_in_browser()",
        "# Find the current worker and job status, which is a good measure of system usage\nWORKERS_BEFORE = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers before: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_BEFORE[\"total\"],\n        WORKERS_BEFORE[\"inUse\"],\n        WORKERS_BEFORE[\"usersRunning\"],\n        WORKERS_BEFORE[\"jobsWaiting\"],\n        WORKERS_BEFORE[\"usersWaiting\"],\n    )\n)\n\nPROJECT = TRAIN_DATASET.create_project(\n    project_name=PROJECT_NAME, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PROJECT: %s\" % (PROJECT))\n\nPROJECT.analyze_and_model(\n    target=TARGET, metric=METRIC, mode=OMODE, worker_count=PROJECT_WORKERS\n)\n\nPROJECT.wait_for_autopilot()\n\nprint(\"PROJECT %s info: %s\" % (PROJECT.id, PROJECT))\n\n# An easy way to confirm the worker autoscaling is working as expected\nWORKERS_AFTER = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers after: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_AFTER[\"total\"],\n        WORKERS_AFTER[\"inUse\"],\n        WORKERS_AFTER[\"usersRunning\"],\n        WORKERS_AFTER[\"jobsWaiting\"],\n        WORKERS_AFTER[\"usersWaiting\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    PROJECT.open_in_browser()",
        "TOP_MODEL = dr.ModelRecommendation.get(\n    PROJECT.id, dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT\n).get_model()\n\nprint(\"Using Recommended Model id: %s | %s\" % (TOP_MODEL.id, TOP_MODEL))\nprint(\n    \"validation: %s | crossValidation: %s\"\n    % (\n        TOP_MODEL.metrics[PROJECT.metric][\"validation\"],\n        TOP_MODEL.metrics[PROJECT.metric][\"crossValidation\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    TOP_MODEL.open_model_browser()",
        "DEPLOYMENT = dr.Deployment.create_from_learning_model(\n    TOP_MODEL.id,\n    label=\"%s RECOMMENDED_MODEL\" % PROJECT.project_name,\n    description=\"API %s deployment of: %s\"\n    % (PROJECT.project_name, TOP_MODEL.model_type),\n    default_prediction_server_id=PREDICTION_SERVER.id,\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    DEPLOYMENT.open_in_browser()\n\nprint(\"Created DEPLOYMENT: %s | %s\" % (DEPLOYMENT.id, DEPLOYMENT))",
        "JDBC_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,  #   \"select * from new_york_taxi.input limit 1000\",\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"localFile\",\n        \"path\": \"jdbc-batch-predictions.csv\",\n    },\n)\n\nJDBC_BATCH_PREDICTION_JOB.wait_for_completion()\n\nwith open(\"jdbc-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)",
        "AICATALOG_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"dataset\",\n        \"dataset\": PREDICT_DATASET,\n        \"dataset_version_id\": PREDICT_DATASET.version_id,\n    },\n    output_settings={\"type\": \"localFile\", \"path\": \"aicatalog-batch-predictions.csv\"},\n)\nAICATALOG_BATCH_PREDICTION_JOB.wait_for_completion()\nAICATALOG_BATCH_PREDICTION_JOB.get_status()\n\nwith open(\"aicatalog-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)",
        "AICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"jdbc\",\n        \"statement_type\": \"insert\",\n        \"schema\": JDBC_WRITE_SCHEMA,\n        \"table\": JDBC_WRITE_TABLE,\n        \"data_store_id\": DATASTORE.id,\n        \"credential_id\": CREDENTIAL.credential_id,\n        \"create_table_if_not_exists\": True,\n    },\n)\nAICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK.wait_for_completion()",
        "# JSON records for example automobiles for which to predict mpg\nautos = [\n    {\n        \"CYLINDERS\": 4,\n        \"DISPLACEMENT\": 119.0,\n        \"HORSEPOWER\": 82.00,\n        \"WEIGHT\": 2720.0,\n        \"ACCELERATION\": 19.4,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n    {\n        \"CYLINDERS\": 8,\n        \"DISPLACEMENT\": 120.0,\n        \"HORSEPOWER\": 79.00,\n        \"WEIGHT\": 2625.0,\n        \"ACCELERATION\": 18.6,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n]\n\n# Create REST request for prediction API\nprediction_server = DEPLOYMENT.default_prediction_server\n\nprint(\"prediction_server: %s\" % prediction_server)\n\nprediction_headers = {\n    \"Authorization\": \"Bearer {}\".format(DATAROBOT_API_TOKEN),\n    \"User-Agent\": \"AIA-E2E-SAPWORKFLOW-125\",  # Optional but helps DataRobot improve this workflow\n    \"DataRobot-Key\": DATAROBOT_KEY,\n    \"Content-Type\": \"application/json\",\n}\n\nREALTINE_PREDICTIONS = requests.post(\n    \"%s/predApi/v1.0/deployments/%s/predictions?passthroughColumns=CYLINDERS&passthroughColumns=DISPLACEMENT&passthroughColumns=HORSEPOWER&passthroughColumns=WEIGHT&passthroughColumns=ACCELERATION&passthroughColumns=MODELYEAR&passthroughColumns=ORIGIN\"\n    % (prediction_server[\"url\"], DEPLOYMENT.id),\n    data=json.dumps(autos),\n    headers=prediction_headers,\n)\n\npprint.pprint(REALTINE_PREDICTIONS.json())",
        "from datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\nservice_stats = DEPLOYMENT.get_service_stats()\nprint(\"Service Statistics:\")\nprint(service_stats)\n\nfor item in service_stats.metrics:\n    print(\"%s => %s\" % (item, service_stats.metrics[item]))",
        "items_deleted = 0\n\n# Delete the elemets created above\nDR_ELEMENTS = [\n    TRAIN_DATASET,\n    PREDICT_DATASET,\n    DEPLOYMENT,\n    PROJECT,\n    TRAIN_DATASOURCE,\n    PREDICT_DATASOURCE,\n    DATASTORE,\n    CREDENTIAL,\n]\n\nfor item in DR_ELEMENTS:\n    if \"Credential\" in str(type(item)):\n        my_id = item.credential_id\n    else:\n        my_id = item.id\n\n    try:\n        print(\"Deleting: %s\" % (item.name))\n        item.delete()\n        items_deleted += 1\n\n        # my_id = item.credential_id if credential_id in item else item.id\n        print(\"Deleted %s: %s id: %s | %s\" % (items_deleted, item.name, my_id, item))\n\n    except Exception as ex:\n        print(\"id not found: %s (%s)\" % (my_id, type(item)))\n        pass\n    except NameError as n:\n        print(\"name %s not found\" % (n))\n        pass\n\nprint(\"Cleaned up %s items\" % (items_deleted))"
      ],
      "api_methods": [
        "project.wait_for_autopilot",
        "model.metrics",
        "dr.datasource.create",
        "dr.datastore.create",
        "datarobot.example.com",
        "deployment.create_from_learning_model",
        "datarobot.datasource.create_dataset",
        "dr.enums.recommended_model_type",
        "model.id",
        "dr.predictionserver.list",
        "project.html",
        "project.project_name",
        "dr.deployment.create_from_learning_model",
        "deployment.get_service_stats",
        "project.id",
        "model.open_model_browser",
        "datarobot.helpers.partitioning_methods",
        "deployment.default_prediction_server",
        "deployment.open_in_browser",
        "deployment.id",
        "dr.datadriver.list",
        "deployment.html",
        "dr.credential.create_basic",
        "dr.modelrecommendation.get",
        "dr.batchpredictionjob.score",
        "project.open_in_browser",
        "model.model_type",
        "project.analyze_and_model",
        "project.metric"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_7427299798524897450",
      "title": "Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud",
      "content": "# Integrate DataRobot and Snowpark by maximizing the data cloud\n\nAuthor: Atalia Horenshtien\n\nVersion Date: April 6 2023\n\n## Overview\n\nThis notebook showcases the native integration between DataRobot and Snowflake's data cloud, leveraging DataRobot notebooks and Snowflake Snowpark (with Python and Java).\n\n### DataRobot\nThe notebook leverages DataRobot ML experimentation. DataRobot hosts notebooks, AutoML, model selection, insights and explainability, and ML production as a single shop for deployment and model monitoring.\n\n### Snowflake\nYou will leverage Snowflake for data storage and Snowpark for deployment, feature engineering, and model scoring. They facilitate data source access and utilize the Spark's backbone to perform distributed scoring to support large-scale use cases.\n\nThe dataset for this project is a fraud detection use case and the notebook was created from a DataRobot notebook. DataRobot notebooks are Jupyter compatible and supporting preloaded packages for Python and R. DataRobot notebooks can also be used for code-snippets, versioning, and simple environment management for rapid AI experimentation to increase your productivity.\n\n### Summary\n\nThis notebook covers the following steps:\n\n1. Load data to Snowflake from an S3 file\n2. Acquire a training dataset from a Snowflake table using Snowpark for Python\n3. Feature engineering: analyze data and create new features using Snowpark\n4. Build a new DataRobot project\n5. Analyze and evaluate model performance and explainability using DataRobot AutoML\n6. Deploy the recommended model to Snowflake using DataRobot MLOps\n7. Score the model via Snowpark for Java\n8. Monitor the model with MLOps\n\n## Setup\n\nBefore executing the cells in the notebook, ensure the following:\n\n* Make sure the environment is Python 3.8\n* Configure the following environmental variables (with your own Snowflake credentials):\n    'snowflake_user' - Snowflake user name\n    'snowflake_password' - Snowflake password\n    'snowflake_account' - Snowflake account name\n\nMost of this workflow is executed by the notebook, but you will create a deployment and monitor it via the DataRobot GUI. Supporting documentation for these steps are included the notebook.\n\n```python\n# If needed, install the following:\n!pip install Flask-SQLAlchemy\n!pip install --upgrade snowflake-sqlalchemy\n```\n\n### Import libraries\n\n```python\nimport datetime as datetime\nfrom datetime import datetime\nimport json\nimport os\nfrom pathlib import Path\n\nfrom datarobot.enums import CHART_DATA_SOURCE, DATA_DRIFT_METRIC\nfrom datarobot.models.deployment import Deployment, FeatureDrift\nfrom datarobot.models.prediction_server import PredictionServer\nfrom datarobot.models.project import Project\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import pd_writer\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import call_udf, col, udf\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.dialects import registry\n\n# from datarobot.models import FeatureDrift\n\n\nregistry.register(\"snowflake\", \"snowflake.sqlalchemy\", \"dialect\")\nimport urllib.parse\n\nfrom snowflake.sqlalchemy import URL\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n```\n\n### Bind data source credentials\n\n```python\nsnowflake_password = os.environ[\"snowflake_password\"]\nsnowflake_user = os.environ[\"snowflake_user\"]\nsnowflake_account = os.environ[\"snowflake_account\"]\n```\n\n```python\n# No need to change these bindings as they are for the demo workflow\n\ndatabase_name = \"SANDBOX\"\nschema_name = \"FINANCE\"\nwarehouse_name = \"DEMO_WH\"\n```\n\n### Import data\n\nUse the following cell to load training data for the sample workflow: importing the \"Fraud Detection\" dataset from S3 to Snowflake.\n\n```python\n# load training data - 'Fraud Detection' from S3 to Snowflake\nsafe_password_string = urllib.parse.quote_plus(snowflake_password)\n\nconn_string = f\"snowflake://{snowflake_user}:{safe_password_string}@{snowflake_account}/{database_name}\"\n\nengine = create_engine(conn_string)\n\n# %%\n\nengine.execute(\n    \"\"\"\n    CREATE SCHEMA IF NOT EXISTS FINANCE;\n    \n    \n    \"\"\"\n)\n\nrenamer = {\"date\": \"DATE_COLUMNS\"}\npd.read_csv(\n    \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/FRAUD_DETECTION_TRAINING.csv\"\n).rename(columns=renamer).to_sql(\n    name=\"fraud_detection_training\",\n    schema=schema_name,\n    con=engine,\n    if_exists=\"replace\",\n    method=pd_writer,\n    index=False,\n)\n\n\n# %\n```\n\n```python\nCONNECTION_PARAMETERS = {\n    \"account\": snowflake_account,\n    \"user\": snowflake_user,\n    \"password\": snowflake_password,\n    \"database\": database_name,\n    \"schema\": schema_name,\n    \"warehouse\": warehouse_name,\n}\n```\n\n### Data Preparation \n\nThis workflow uses Snowpark for feature engineering. Snowpark is a developer framework where you can work in a familiar syntax such as Python. Snowpark pushes down processing to Snowflake to run consistently in a highly secure and elastic engine.\n\n```python\n# Create the ‘session’ object that represents Snowpark and a dataframe that references the data in Snowflake\n\nsession = Session.builder.configs(CONNECTION_PARAMETERS).create()\n```\n\n```python\ndf_train = session.table(\"FRAUD_DETECTION_TRAINING\")\n```\n\n```python\ndf_train.show()\n```\n\n```python\n# Snowpark - Create an instance of UserDefinedFunction using the @udf decorator\n\n\n@udf\n# Define the function arguments: WHOLE_WEIGHT, HEIGHT\ndef is_long_session(session_length_in_mins: float) -> bool:\n    if session_length_in_mins > 20:\n        return 1\n    else:\n        return 0\n```\n\n```python\n# Snowpark call UDFs on a dataframe, with calculation in Snowflake\nudf_df_train = df_train.select(\n    col(\"*\"), is_long_session(col(\"SESSION_LENGTH_IN_MINS\")).alias(\"LONG_SESSION\")\n)\n```\n\n```python\nudf_df_train.write.mode(\"overwrite\").save_as_table(\"FRAUD_DETECTION_TRAINING\")\n```\n\n```python\n# Snowpark supports Pandas\ndf_train_pd = df_train.toPandas()\n```\n\n```python\nsns.countplot(data=df_train_pd, x=\"ISFRAUD\")\n```\n\n## Modeling\n\nIn this section you'll leverage DataRobot’s AutoML capabilities, including explainability and compliance documentation to speed up the model’s results and regulatory compliance.\n\n```python\n# Optional - If you've already run Autopilot before, get the project by replacing the ID\n# project = Project.get('63853765cfc79e4555fa2537')\n```\n\n```python\n# Define the project name and current date\nnow = datetime.now().strftime(\"%Y-%m-%dT%H:%M\")\nproject = Project.create(df_train_pd, project_name=\"Fraud_detection_{}\".format(now))\n```\n\n### Start Autopilot\n\n```python\n# Set the target variable and start Autopilot\nproject.analyze_and_model(\n    target=\"ISFRAUD\",  # setting project target\n    mode=\"quick\",  # setting up project run mode, leaving in auto\n    worker_count=-1,  # assigning worker count, -1 will use all available\n    max_wait=24\n    * 60\n    * 60,  # time series projects can take a little more time to get through EDA2 than normal AutoML projects\n)\n```\n\n```python\n# This (optional) will block execution of the notebook until the full autopilot process has completed. This can take several minutes or hours, depending on the autopilot mode selected, the size of the dataset, and the type of problem we're trying to solve.\n\nproject.wait_for_autopilot()\n```\n\n```python\n# This output a list of all the models trained in the project, sorted by the selected validation metric.\n\nproject.get_models()[:10]\n```\n\n### Get the most accurate model\n\nDataRobot provides a recommendation for an accurate and well-performing model at the end of training process.\n\n```python\n# Get the most accurate model based on the project's metric\nmost_accurate_model = project.get_top_model()\n```\n\n```python\n# Evaluate the model using a lift chart\nplot = most_accurate_model.get_lift_chart(\"crossValidation\")\ndf = pd.DataFrame.from_dict(plot.bins)\n\n# Create data\nx = df.index\ny = df[\"actual\"]\ny2 = df[\"predicted\"]\n\n# Plot lines\nplt.plot(x, y, label=\"actuals\")\nplt.plot(x, y2, label=\"predicted\")\nplt.legend()\nplt.title(\"lift chart\")\nplt.show()\n```\n\n```python\n# Evaluate the model using ROC Curve\n\n# Get the ROC curve\nroc = most_accurate_model.get_roc_curve(source=CHART_DATA_SOURCE.VALIDATION)\n\n# Save the result into a Pandas dataframe\nroc_df = pd.DataFrame(roc.roc_points)\nroc_df.head()\n\n# Set chart colors\ndr_roc_green = \"#03c75f\"\nwhite = \"#ffffff\"\ndr_purple = \"#65147D\"\ndr_dense_green = \"#018f4f\"\ndr_dark_blue = \"#08233F\"\n\n# Create chart\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=dr_dark_blue)\nplt.scatter(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot([0, 1], [0, 1], color=white, alpha=0.25)\n\n# Format chart\nplt.title(\"ROC curve\")\nplt.xlabel(\"False Positive Rate (Fallout)\")\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate (Sensitivity)\")\nplt.ylim([0, 1])\n```\n\n```python\n# Get insights on which feature driving model's outcome\n\n# Get Feature Impact\nfeature_impact = most_accurate_model.get_or_request_feature_impact()\n\n# Save Feature Impact in a Pandas dataframe\nfi_df = pd.DataFrame(feature_impact)\n\n# Plot the top five most impactful features\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.barplot(x=\"featureName\", y=\"impactNormalized\", data=fi_df[0:5], color=\"g\")\n```\n\n## Model Deploymnet\n\nTo deploy to Snowflake, you need to do it through the DataRobot GUI. Reference the documentation to [create a snowflake prediction environment and deployment](https://docs.datarobot.com/en/docs/mlops/mlops-preview/pp-snowflake-sc-deploy-replace.html).\n\n```python\n# Get the deployed model and change the ID to your deployment's\ndeployment = Deployment.get(\"6413a07ea0a9ee22d203464b\")\ndeployment\n```\n\n```python\n# Create the scoring data\n# This workflow uses an example for subset data\nquery = (\n    \" create or replace table FRAUD_DETECTION_SCORING as \"\n    + \"  select * from FRAUD_DETECTION_TRAINING limit 10 \"\n)\n\nsession.sql(query).collect()\n```\n\nIn order to score the data in Snowflake, leverage Snowpark for Java.\n\nGo to the created deployment's **Predictions** tab. Copy the signature of the function included there and replace it in the next section. \n\nExample:\nquery = \" SELECT *, scoring_code_udf_schema.replace_function_name_here(OBJECT_CONSTRUCT_KEEP_NULL(*)) \" +\\\n        \" FROM FRAUD_DETECTION_SCORING \"\n\n```python\n# Score new data\n\nquery = (\n    \" CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as\"\n    + \" SELECT *, datarobot_deployment_642adf008b22ed8d5de483ba(OBJECT_CONSTRUCT_KEEP_NULL(*)) as model_score FROM FRAUD_DETECTION_SCORING;\"\n)\n\n\nsession.sql(query).collect()\n```\n\n```python\n# Store results and modify column names\n\nquery = ' CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as select *, model_score:\"0\"::float false_fraud, model_score:\"1\"::float true_fraud  from FRAUD_DETECTION_SCORING; '\n\nsession.sql(query).collect()\n```\n\n```python\n# Convert the session object to a dataframe\n\ndf_scored = session.table(\"FRAUD_DETECTION_SCORING\")\ndf_scored_pd = df_scored.toPandas()\ndf_scored_pd\n```\n\nYou can see the predictions in the last two columns.\n\n## Model monitoring\n\nTo make sure business decisions are aligned with external and internal factors, monitor the model performance and understand if the model needs to be replaced or retrained. To learn how to define a monitoring job, [reference the documentation](https://docs.datarobot.com/en/docs/mlops/mlops-preview/pp-monitoring-jobs.html).\n\nOnce the monitoring job is executed, you can see data drift and accuracy in the UI or retrieve it with the API.\n\n```python\n# Get target drift\n\ntarget_drift = deployment.get_target_drift(\n    start_time=datetime(2023, 4, 1, hour=15), end_time=datetime(2023, 4, 4, hour=15)\n)\ntarget_drift.drift_score\n```\n\n## Conclusion and next steps\n\nDataRobot and Snowflake together offer an end-to-end, enterprise-grade AI experience and expertise to enterprises by reducing complexity and productionizing ML models at scale, unlocking business value.\n\n[Visit the DataRobot website](https://www.datarobot.com/partners/technology-partners/snowflake/) to learn more.",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/Snowflake_snowpark_template/Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "time-series",
        "ai-accelerators",
        "openai",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/Snowflake_snowpark_template/Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud.ipynb",
        "size": 176228,
        "cell_count": 48,
        "code_cell_count": 29
      },
      "code_examples": [
        "# If needed, install the following:\n!pip install Flask-SQLAlchemy\n!pip install --upgrade snowflake-sqlalchemy",
        "import datetime as datetime\nfrom datetime import datetime\nimport json\nimport os\nfrom pathlib import Path\n\nfrom datarobot.enums import CHART_DATA_SOURCE, DATA_DRIFT_METRIC\nfrom datarobot.models.deployment import Deployment, FeatureDrift\nfrom datarobot.models.prediction_server import PredictionServer\nfrom datarobot.models.project import Project\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import pd_writer\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import call_udf, col, udf\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.dialects import registry\n\n# from datarobot.models import FeatureDrift\n\n\nregistry.register(\"snowflake\", \"snowflake.sqlalchemy\", \"dialect\")\nimport urllib.parse\n\nfrom snowflake.sqlalchemy import URL\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker",
        "snowflake_password = os.environ[\"snowflake_password\"]\nsnowflake_user = os.environ[\"snowflake_user\"]\nsnowflake_account = os.environ[\"snowflake_account\"]",
        "# No need to change these bindings as they are for the demo workflow\n\ndatabase_name = \"SANDBOX\"\nschema_name = \"FINANCE\"\nwarehouse_name = \"DEMO_WH\"",
        "# load training data - 'Fraud Detection' from S3 to Snowflake\nsafe_password_string = urllib.parse.quote_plus(snowflake_password)\n\nconn_string = f\"snowflake://{snowflake_user}:{safe_password_string}@{snowflake_account}/{database_name}\"\n\nengine = create_engine(conn_string)\n\n# %%\n\nengine.execute(\n    \"\"\"\n    CREATE SCHEMA IF NOT EXISTS FINANCE;\n    \n    \n    \"\"\"\n)\n\nrenamer = {\"date\": \"DATE_COLUMNS\"}\npd.read_csv(\n    \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/FRAUD_DETECTION_TRAINING.csv\"\n).rename(columns=renamer).to_sql(\n    name=\"fraud_detection_training\",\n    schema=schema_name,\n    con=engine,\n    if_exists=\"replace\",\n    method=pd_writer,\n    index=False,\n)\n\n\n# %",
        "CONNECTION_PARAMETERS = {\n    \"account\": snowflake_account,\n    \"user\": snowflake_user,\n    \"password\": snowflake_password,\n    \"database\": database_name,\n    \"schema\": schema_name,\n    \"warehouse\": warehouse_name,\n}",
        "# Create the ‘session’ object that represents Snowpark and a dataframe that references the data in Snowflake\n\nsession = Session.builder.configs(CONNECTION_PARAMETERS).create()",
        "df_train = session.table(\"FRAUD_DETECTION_TRAINING\")",
        "df_train.show()",
        "# Snowpark - Create an instance of UserDefinedFunction using the @udf decorator\n\n\n@udf\n# Define the function arguments: WHOLE_WEIGHT, HEIGHT\ndef is_long_session(session_length_in_mins: float) -> bool:\n    if session_length_in_mins > 20:\n        return 1\n    else:\n        return 0",
        "# Snowpark call UDFs on a dataframe, with calculation in Snowflake\nudf_df_train = df_train.select(\n    col(\"*\"), is_long_session(col(\"SESSION_LENGTH_IN_MINS\")).alias(\"LONG_SESSION\")\n)",
        "udf_df_train.write.mode(\"overwrite\").save_as_table(\"FRAUD_DETECTION_TRAINING\")",
        "# Snowpark supports Pandas\ndf_train_pd = df_train.toPandas()",
        "sns.countplot(data=df_train_pd, x=\"ISFRAUD\")",
        "# Optional - If you've already run Autopilot before, get the project by replacing the ID\n# project = Project.get('63853765cfc79e4555fa2537')",
        "# Define the project name and current date\nnow = datetime.now().strftime(\"%Y-%m-%dT%H:%M\")\nproject = Project.create(df_train_pd, project_name=\"Fraud_detection_{}\".format(now))",
        "# Set the target variable and start Autopilot\nproject.analyze_and_model(\n    target=\"ISFRAUD\",  # setting project target\n    mode=\"quick\",  # setting up project run mode, leaving in auto\n    worker_count=-1,  # assigning worker count, -1 will use all available\n    max_wait=24\n    * 60\n    * 60,  # time series projects can take a little more time to get through EDA2 than normal AutoML projects\n)",
        "# This (optional) will block execution of the notebook until the full autopilot process has completed. This can take several minutes or hours, depending on the autopilot mode selected, the size of the dataset, and the type of problem we're trying to solve.\n\nproject.wait_for_autopilot()",
        "# This output a list of all the models trained in the project, sorted by the selected validation metric.\n\nproject.get_models()[:10]",
        "# Get the most accurate model based on the project's metric\nmost_accurate_model = project.get_top_model()",
        "# Evaluate the model using a lift chart\nplot = most_accurate_model.get_lift_chart(\"crossValidation\")\ndf = pd.DataFrame.from_dict(plot.bins)\n\n# Create data\nx = df.index\ny = df[\"actual\"]\ny2 = df[\"predicted\"]\n\n# Plot lines\nplt.plot(x, y, label=\"actuals\")\nplt.plot(x, y2, label=\"predicted\")\nplt.legend()\nplt.title(\"lift chart\")\nplt.show()",
        "# Evaluate the model using ROC Curve\n\n# Get the ROC curve\nroc = most_accurate_model.get_roc_curve(source=CHART_DATA_SOURCE.VALIDATION)\n\n# Save the result into a Pandas dataframe\nroc_df = pd.DataFrame(roc.roc_points)\nroc_df.head()\n\n# Set chart colors\ndr_roc_green = \"#03c75f\"\nwhite = \"#ffffff\"\ndr_purple = \"#65147D\"\ndr_dense_green = \"#018f4f\"\ndr_dark_blue = \"#08233F\"\n\n# Create chart\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=dr_dark_blue)\nplt.scatter(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot([0, 1], [0, 1], color=white, alpha=0.25)\n\n# Format chart\nplt.title(\"ROC curve\")\nplt.xlabel(\"False Positive Rate (Fallout)\")\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate (Sensitivity)\")\nplt.ylim([0, 1])",
        "# Get insights on which feature driving model's outcome\n\n# Get Feature Impact\nfeature_impact = most_accurate_model.get_or_request_feature_impact()\n\n# Save Feature Impact in a Pandas dataframe\nfi_df = pd.DataFrame(feature_impact)\n\n# Plot the top five most impactful features\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.barplot(x=\"featureName\", y=\"impactNormalized\", data=fi_df[0:5], color=\"g\")",
        "# Get the deployed model and change the ID to your deployment's\ndeployment = Deployment.get(\"6413a07ea0a9ee22d203464b\")\ndeployment",
        "# Create the scoring data\n# This workflow uses an example for subset data\nquery = (\n    \" create or replace table FRAUD_DETECTION_SCORING as \"\n    + \"  select * from FRAUD_DETECTION_TRAINING limit 10 \"\n)\n\nsession.sql(query).collect()",
        "# Score new data\n\nquery = (\n    \" CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as\"\n    + \" SELECT *, datarobot_deployment_642adf008b22ed8d5de483ba(OBJECT_CONSTRUCT_KEEP_NULL(*)) as model_score FROM FRAUD_DETECTION_SCORING;\"\n)\n\n\nsession.sql(query).collect()",
        "# Store results and modify column names\n\nquery = ' CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as select *, model_score:\"0\"::float false_fraud, model_score:\"1\"::float true_fraud  from FRAUD_DETECTION_SCORING; '\n\nsession.sql(query).collect()",
        "# Convert the session object to a dataframe\n\ndf_scored = session.table(\"FRAUD_DETECTION_SCORING\")\ndf_scored_pd = df_scored.toPandas()\ndf_scored_pd",
        "# Get target drift\n\ntarget_drift = deployment.get_target_drift(\n    start_time=datetime(2023, 4, 1, hour=15), end_time=datetime(2023, 4, 4, hour=15)\n)\ntarget_drift.drift_score"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "project.create",
        "project.get_top_model",
        "project.wait_for_autopilot",
        "deployment.get",
        "model.get_roc_curve",
        "deployment.get_target_drift",
        "model.get_lift_chart",
        "datarobot.models.project",
        "project.get",
        "datarobot.models.deployment",
        "datarobot.models.prediction_server",
        "project.analyze_and_model",
        "project.get_models"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_7593584042684844248",
      "title": "Snowflake - End-to-end Ecommerce Churn",
      "content": "# End-to-end modeling and production workflow with DataRobot and Snowflake\n\nAuthors: Austin Chou, Arjun Arora\n\nVersion Date: 01/11/2023\n\n[Reference DataRobot's API documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n![DR_and_Snowflake_logo.png](attachment:DR_and_Snowflake_logo.png)\n\n## Notebook overview\n\nThis notebook focuses on working with **Snowflake** as a data source and walks through how to use the Python API client to:\n\n1. [Connect to DataRobot](#Setup)\n2. [Import data from Snowflake into the AI catalog](#Import)\n3. [Create a project and run Autopilot](#Project)\n4. [Select and evaluate the top recommended model from a project)](#Eval)\n5. [Deploy a model in a Docker container with a REST API endpoint](#Deploy)\n6. [Orchestrate scheduled predictions with an MLOps job definition](#Schedule)\n\n### Libraries used in this notebook\n\n* The [**DataRobot Python client**](https://pypi.org/project/datarobot/).\n* [**Snowflake Python Connector**](https://docs.snowflake.com/en/user-guide/python-connector.html) if pulling the demo dataset to follow along with this notebook.\n* **Matplotlib** for model evaluation.\n* **Pandas** for model evaluation.\n\n#### Optional:  Import public demo data\n\nFor this walkthrough, you can use a publicly available dataset (`DR_Demo_E-commerce_churn_train.csv`) from DataRobot's S3 bucket to create a table in your Snowflake instance. This will let you run the cells in this notebook and follow along.\n\nYou will need to update the fields below with your Snowflake information. You will also need the following files found in the same repo as this notebook:\n\n* utils.py\n* datasets.yaml\n\nOnce you are done with this walkthrough, remember to delete the data from your Snowflake instance.\n\n**If you have already established a Snowflake data connection in DataRobot, make sure the following information matches your credentials and data connection details.**\n\n```python\nfrom utils import prepare_demo_tables_in_db\n```\n\n```python\n# Fill out the credentials for your Snowflake instance. You will need write access to a database\ndb_user = \"your_username\"  # Username to access Snowflake database\ndb_password = \"your_password\"  # Password\naccount = \"eg:datarobotap_partner.ap-southeast-2\"  # Snowflake account identifier, can be found in the db_url\ndb = \"YOUR_DB_NAME\"  # Database to Write_To\nwarehouse = \"YOUR_WAREHOUSE\"  # Warehouse\nschema = \"YOUR_SCHEMA\"  # Schema\n\ndb_url = \"jdbc:snowflake://{account}.snowflakecomputing.com/?warehouse={warehouse}&db={db}\".format(\n    account=account, db=db, warehouse=warehouse\n)\n```\n\n```python\n# Use the util function to pull the data from DataRobot's public S3 and import into your Snowflake instance\nreponse = prepare_demo_tables_in_db(\n    db_user=db_user,\n    db_password=db_password,\n    account=account,\n    db=db,\n    warehouse=warehouse,\n    schema=schema,\n)\n```\n\n## Setup <a id='Setup'></a>\n\n### Import libraries\n\n```python\nimport datarobot as dr\n\n# The following are libraries used in this notebook during model evaluation\nimport matplotlib.pyplot as plt\nimport pandas as pd\n```\n\n### Connect to DataRobot\n\n**To connect to DataRobot,** you need to provide your **API Token** and the **endpoint**. For more information, please refer to the following documentation:\n\n* [**Create/Manage API keys via Developer tools in the GUI**](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management)\n* [**Different options to connect to DataRobot from the API client**](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)\n\n```python\n# API Token\ntoken = open(\n    \"..\\\\..\\\\API token.txt\"\n).read()  # Load the API token from a .txt file for ease of repeatable access\nDATAROBOT_API_TOKEN = (\n    token  # You can also find the API token under the Developer Tools in the UI\n)\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\n# To connect from Jupyter notebook:\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-SNF-17\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n```\n\n## Import data from Snowflake to DataRobot AI Catalog  <a id='Import'></a>\n\nYou can upload data to Datarobot in a variety of ways:\n\n* Data Sources (e.g., through a Snowflake data connection)\n* Local Files\n* Links\n\nIn this notebook, you will upload a dataset from a **Snowflake** data source. To do so:\n\n1. Retrieve the Snowflake data connection and credentials.\n2. Create a new data source (i.e., setting up the query to a Snowflake data table) if needed.\n3. Import the dataset into the AI Catalog.\n\n### Retrieve credentials\n\nYou will need to provide your Snowflake credentials to DataRobot to ingest the data from Snowflake to the AI Catalog.\n\nYou can easily [create and configure your credentials in the DataRobot UI](https://docs.datarobot.com/en/docs/data/connect-data/stored-creds.html). DataRobot will store all your credentials in the Credentials Management tab under your profile on the platform.\n\nYou can also create a new set of credentials by using the `dr.Credential.create_basic` function:\n\n```python\n# If you need to create the credentials, run this cell\n# If you already have Snowflake credentials in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_user and db_password from the Setup section\n\ncred = dr.Credential.create_basic(\n    name=\"[AIA] Your_Snowflake_Credentials\",  # Rename\n    user=db_user,\n    password=db_password,\n)\n\ncredential_id = cred.credential_id\n```\n\nIf you have already created credentials on DataRobot, you can retrieve them via the API. This notebook uses the credentials named \"Snowflake Sandbox Demo Connection\" - **replace it with the name you've given your Snowflake credentials in DataRobot's Credentials Management.**\n\n```python\n# # If you already have Snowflake credentials in DataRobot, you can retrieve and reuse the same credentials\n# # Uncomment and run this cell instead of creating new credentials\n\n# CREDS_NAME = 'Snowflake Sandbox Demo Connection' # Replace with your credentials name given during Snowflake credentials creation\n\n# credential_id = [cr.credential_id for cr in dr.Credential.list() if cr.name == CREDS_NAME][0]\n\n# # You can verify that you have the right credential by showing the ID and name\n# dr.Credential.get(credential_id)\n```\n\n### Retrieve a Snowflake data connection\n\nYou will also need to create a data connection from DataRobot to Snowflake. This will allow DataRobot to pull data directly from your Snowflake database.\n\nYou can easily [create a new data connection in the DataRobot UI](https://docs.datarobot.com/en/docs/data/connect-data/data-conn.html). You can find all your established data connections in the **Data Connections** tab under your profile on the platform.\n\n\nYou can also create a new data connection by finding the Snowflake driver and using the `dr.DataStore.create` function via the API.\n\n```python\n# Find a Snowflake driver ID by name. This can be skipped if you have the ID - code shown here for completeness\nfor d in dr.DataDriver.list():\n    if d.canonical_name in \"Snowflake (3.13.9 - recommended)\":\n        print((d.id, d.canonical_name))\n```\n\n```python\n# Create a datastore\n# If you already have the Snowflake data connection to the right database in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_url from the \"Setup\" section\n\ndata_store = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=\"[AIA] Your New Snowflake Data Connection\",  # Rename\n    driver_id=\"626bae0a98b54f9ba70b4122\",  # Snowflake driver id\n    jdbc_url=db_url,\n)\n\n# Test new data connection; access with your Snowflake credentials\ndata_store.test(username=db_user, password=db_password)\n\ndata_store_id = data_store.id\n```\n\nIf you have already created the data connection on DataRobot, you can retrieve it via API. This notebook uses a data connection named \"Snowflake - TEST_DB\" - **please replace with the name you've given your appropriate Snowflake data connection in DataRobot's Data Connections tab.**\n\n```python\n# # If you already have a Snowflake data connection to your Snowflake database in DataRobot, you can\n# # retrieve and reuse the same connection. Uncomment and run this cell instead of creating a new connection.\n\n# # Once the connection is established via the UI, you can retrieve your data connection by name\n# DATA_STORE_NAME = 'Snowflake - TEST_DB' # Replace with your Data Store name given during Snowflake data connection creation\n\n# data_store_id = [ds.id for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][0]\n\n# # You can verify that you have the right DataStore by name\n# dr.DataStore.get(data_store_id)\n```\n\nNext, create a data source specifying the query that you will use to pull data from the Snowflake data connection and into the DataRobot AI Catalog. For more information, please refer to the [**DataRobot documentation on data sources**](https://docs.datarobot.com/en/docs/data/connect-data/data-conn.html#add-data-sources).\n\nIf you already set up an existing data source, you can search for it by name.\n\n```python\n# Pick the data source\n# If you had already established a data source, just pick the existing data source\n# Otherwise, create a new data source\n\nDATA_SOURCE_NAME = \"[AIA] Snowflake ECommerce Churn\"  # Rename\nquery_train = (\n    \"SELECT * FROM \" + schema + '.\"DR_Demo_E-commerce_churn_train\";'\n)  # Edit this query to pull the appropriate table\n\ndata_sources = [\n    ds for ds in dr.DataSource.list() if ds.canonical_name == DATA_SOURCE_NAME\n]\n\nif len(data_sources) > 0:\n    data_source_train = data_sources[0].id\n    print(\"Existing data source ID:\", data_source_train)\nelse:\n    query_train = query_train\n    ds_params = dr.DataSourceParameters(data_store_id=data_store_id, query=query_train)\n    data_source_train = dr.DataSource.create(\n        data_source_type=\"jdbc\", canonical_name=DATA_SOURCE_NAME, params=ds_params\n    ).id\n    print(\"New data source created (ID):\", data_source_train)\n```\n\n### Import dataset to AI Catalog\n\nTo **upload a dataset to AI Catalog**, use the `dr.create_from_` family of functions. In this case, we use `create_from_data_source`.\n\n```python\nnew_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=data_source_train, do_snapshot=True, credential_id=credential_id\n)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Ecommerce Customer Churn Data\")\n```\n\nDatasets in the AI Catalog are assigned a **dataset ID** which you can use to reference/get the dataset via the API.\n\n```python\n# Quick link to the AI Catalog dataset you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\"AI Catalog URL: \" + \"https://app.datarobot.com/ai-catalog/\" + new_dataset.id)\nprint(\"Dataset ID: \" + new_dataset.id)\n```\n\n## Create a project and run Autopilot <a id='Project'></a>\n\nYou can **create DataRobot projects** directly from:\n\n* A dataset in AI Catalog (using the dataset's ID in DataRobot)\n* A pandas dataframe (you do not need to write back to a data source or disk)\n* Directly from data sources\n\nOnce a project is created, you can pass in DataRobot options and start modeling. At a minimum, you need to provide the prediction **Target**. With just two function calls (`create_from_` and `analyze_and_model`), you can go from data to optimizing 10 to 100 models with data science best practices.\n\nYou can actively watch the project in action in the DataRobot UI once Autopilot is triggered - the API and UI are parallel gateways to working on the same project which allows for **cross-functional collaboration.**\n\nNote that each created project is associated with a unique **project ID**. You can use the project ID to retrieve the project of interest via the API.\n\n### Create a project\n\nCreate a DataRobot project by uploading the dataset.\n\n```python\nproject = dr.Project.create_from_dataset(\n    dataset_id=new_dataset.id, project_name=\"[AIA] Ecommerce Churn Project\"\n)\n```\n\n```python\n# Quick link to the DataRobot project you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Project URL: \"\n    + \"https://app.datarobot.com/projects/\"\n    + project.id\n    + \"/eda\"\n)\nprint(\"Project ID: \" + project.id)\n```\n\n### Initiate Autopilot\n\nTo start the Autopilot process, call the `analyze_and_model` function. Provide the prediction target as part of the function call and use the default \"Quick\" Autopilot mode.\n\n```python\nproject.analyze_and_model(\n    target=\"Is_dormant\",\n    worker_count=-1,  # Setting worker count to -1 will use your maximum available workers\n)\n```\n\n```python\n# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)\n```\n\nWhile out of scope for this notebook, you can set advanced options to further configure Autopilot with the `set_advanced_options` function before triggering `analyze_and_model`. For visibility, you can view the advanced options used by the current project (which would be the default options):\n\n```python\n# View advanced options used by Autopilot for the project\nproject = dr.Project.get(project.id)  # Update the project object after Autopilot\n\nprint(\"Advanced Settings used:\")\nproject.list_advanced_options()\n```\n\n## Select and evaluate the top recommended model <a id='Eval'></a>\n\nAfter Autopilot completes, you can evaluate the results. Evaluation can include compiling the Leaderboard as a dataframe, measuring performances across different partitions with different metrics, visualizing the ROC curve, analyzing Feature Impact and Feature Effects to understand each models' behaviors, and more. This can be done for every single model created by DataRobot.\n\nAs a simple example in this notebook, **select the recommended model created by Autopilot and evaluate**:\n\n* LogLoss performance\n* AUC performance\n* ROC curve\n* Feature Impact of Top 10 features\n\nNote: The recommended model is the best performing model that DataRobot identifies during Autopilot and then effectively retrains the blueprint on 100% of the data. For more information, refer to the [Model recommendation process documentation](https://docs.datarobot.com/en/docs/modeling/reference/model-detail/model-rec-process.html#model-recommendation-process).\n\n```python\n# Select the model recommended by DataRobot AutoML\ntop_model = project.recommended_model()\n```\n\n```python\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)\n```\n\n### Model performance by metric\n\n```python\nprint(\n    \"Top Model LogLoss performance: \"\n    + str(top_model.metrics[\"LogLoss\"][\"crossValidation\"])\n)\nprint(\"Top Model AUC performance: \" + str(top_model.metrics[\"AUC\"][\"crossValidation\"]))\n```\n\n### Create an ROC Curve\n\nBeyond the Leaderboard, you can access any analysis DataRobot does out-of-the-box for every single model. The example below reproduces the ROC curve by calling the `get_roc_curve` function from the top model.\n\n```python\n# Retrieve ROC Curve\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)\n```\n\n```python\n# Plot the ROC Curve\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.plot(roc.false_positive_rate, roc.true_positive_rate, color=\"b\")\nplt.plot([0, 1], [0, 1], color=\"r\", linestyle=\"dashed\", alpha=0.25)\nplt.title(\"ROC curve\", size=16, fontweight=\"bold\")\nplt.xlabel(\"False Positive Rate\", size=14)\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate\", size=14)\nplt.ylim([0, 1])\nplt.show()\n```\n\n### Feature Impact\n\nAs an example of model explainability, we calculate the Feature Impact values of the model using the `get_or_request_feature_impact` function.\n\n```python\n# Retrieve Feature Impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n```\n\n```python\n# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()\n```\n\n## Deploy a model with monitoring in MLOps <a id='Deploy'></a>\n\nWith a single function call or click on the UI, DataRobot can quickly deploy models into production while fully reproducing the entire modeling pipeline including the necessary data preprocessing steps utilized by the blueprints and any advanced feature engineering that are part of the project (such as in [Feature Discovery projects](https://www.datarobot.com/platform/feature-discovery/)). Once deployed, you can call DataRobot's REST or Python API to make batch and real-time predictions. You can also configure and schedule recurring batch prediction jobs that write back into a database.\n\nOnce a model is deployed, you can access MLOps monitoring capabilities such as:\n\n- Service health\n- Data drift\n- Prediction accuracy\n- Model retraining\n\nTo **deploy a model from the Leaderboard**, call the `create_from_learning_model` function and provide the **ID of the model** you want to deploy and the **ID of the prediction server** you want to deploy into.\n\nFor additional information, please see documentation for:\n\n1. [**MLOps monitoring**](https://docs.datarobot.com/en/docs/mlops/mlops-overview.html) \n2. [**Available prediction methods**](https://docs.datarobot.com/en/docs/predictions/index.html)\n3. [**Other deployment workflows with DataRobot**](https://docs.datarobot.com/en/docs/mlops/deployment/deploy-workflows/index.html)\n\n```python\n# Prepare new deployment\ndeployment_model_id = top_model.id\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    deployment_model_id,\n    label=\"[AIA] Customer Churn Deployment\",\n    description=\"[AIA] Customer Churn Prediction\",\n    default_prediction_server_id=prediction_server.id,\n)\n```\n\nEvery deployment in DataRobot is assigned a **deployment ID** which you can use to reference/retrieve the deployment via the API.\n\n```python\n# Quick link to the deployment you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Deployment URL: \"\n    + \"https://app.datarobot.com/deployments/\"\n    + deployment.id\n    + \"/overview\"\n)\nprint(\"Deployment ID: \" + deployment.id)\n```\n\n### Configure model monitoring\n\nIn this example, you can set up the deployment to monitor data drift and accuracy using the following functions:\n\n* Data drift: `update_drift_tracking_settings`\n* Accuracy: `update_association_id_settings`\n\n```python\n# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n```\n\n```python\n# Turn on Accuracy tracking by providing an association ID\ndeployment.update_association_id_settings(\n    column_names=[\n        \"customerID\"\n    ],  # Association ID to associate prediction results to actuals for accuracy monitoring\n    required_in_prediction_requests=True,\n)\n```\n\n## Orchestrate scheduled prediction jobs <a id='Schedule'></a>\n\nYou can **schedule a recurring batch prediction job** that looks up data from a data source (e.g.,  **Snowflake**), score predictions, and write back the results to a data table in a data source (e.g., **Snowflake**).\n\nSimilar to [**working in the UI**](https://docs.datarobot.com/en/docs/predictions/batch/batch-dep/batch-pred-jobs.html), you need to provide settings to set up a prediction job. In a [**code-first approach**](https://datarobot-public-api-client.readthedocs-hosted.com/en/v3.0.2/reference/predictions/batch_predictions.html#batch-prediction-job-definitions-1), this is done by passing dictionaries containing settings for:\n\n* Intake Settings (input data table or data source)\n* Output Settings (output data table or data source)\n* Schedule\n\nYou also need to provide the intake and output with IDs for:\n\n* Data Store (i.e., data connection as set up in DataRobot)\n* Credentials for the data store (as set up in DataRobot)\n\n```python\n# Select the data store and credentials needed\n# This notebook uses the same Snowflake data store and credentials as the ones used\n# at the start of this notebook\nwriteback_data_store_id = data_store_id\nwriteback_cred_id = credential_id\n\n# Set the Snowflake catalog (database) and schema for intake and output\n# This notebook uses the same Snowflake database and schema as the ones\n# at the start of this notebook\nintake_catalog = db\nintake_schema = schema\noutput_catalog = db\noutput_schema = schema\n```\n\n```python\n# Set up settings dictionaries - edit this to your data and prediction pipeline\n\n# Intake settings\n# This example uses the training dataset as if it was also used for scoring\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"DR_Demo_E-commerce_churn_train\",  # Data table in data source\n    \"schema\": intake_schema,\n    \"catalog\": intake_catalog,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Output settings\noutput_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"E-commerce_churn_prediction_demo_output\",  # Rename; Data table to write out to\n    \"schema\": output_schema,\n    \"catalog\": output_catalog,\n    \"statement_type\": \"insert\",\n    \"create_table_if_not_exists\": True,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Schedule settings\nschedule = {\n    \"minute\": [59],\n    \"hour\": [7],\n    \"month\": [\"*\"],\n    \"dayOfWeek\": [\"*\"],\n    \"dayOfMonth\": [1],\n}\n```\n\n```python\n# Set up the job dictionary\n# Includes intake and output settings within the job dictionary\njob = {\n    \"deployment_id\": deployment.id,\n    \"num_concurrent\": 4,\n    \"intake_settings\": intake_settings,\n    \"output_settings\": output_settings,\n    \"passthroughColumnsSet\": \"all\",\n}\n\n# Create the Batch Prediction job definition\njob_definition = dr.BatchPredictionJobDefinition.create(\n    enabled=True,\n    batch_prediction_job=job,\n    name=\"[AIA] ECommerce Churn - Monthly Prediction Job JDBC\",  # Make sure you use a unique name from other existing JobDefinitions in your organization\n    schedule=schedule,\n)\n```\n\nThe job definition should now be listed in the deployment's **Job Definitions** tab.\n\nAs a test, you can trigger the job manually to see DataRobot MLOps in action. **The next snippet will manually trigger the Job Definition to run.**\n\n```python\n# Trigger the job manually\njob_definition.run_once()\n```\n\nWhen the job completes successfully, you will find the output written to the corresponding Snowflake table. The MLOps deployment will also update accordingly and you should see service health and data drift metrics update.\n\n## Clean up\n\nTo remove everything added in the DataRobot platform as part of this notebook, run the following cell. If you want to delete any tables created in Snowflake as a result of running this notebook, you will need to manually do so in Snowflake.\n\n```python\n# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# job_definition.delete()\n# deployment.delete()\n# project.delete()\n# new_dataset.delete(new_dataset.id)\n# dr.DataSource.get(data_source_train).delete()\n# data_store.delete()\n# cred.delete()\n```\n\n## Additional Resources\n\n* Check the [**DataRobot Community AI Accelerator page**](https://community.datarobot.com/t5/ai-accelerators/bd-p/ai_accelerators) for similar walkthroughs using DataRobot with other data sources (e.g. AWS, GCP, Azure, etc).\n* Check the [**DataRobot API user guide for other code examples**](https://docs.datarobot.com/en/docs/api/guide/python/index.html) covering various topics such as model factories, feature impact rank ensembling, and more.",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/Snowflake_template/Snowflake - End-to-end Ecommerce Churn.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "ai-accelerators",
        "openai",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/Snowflake_template/Snowflake - End-to-end Ecommerce Churn.ipynb",
        "size": 127058,
        "cell_count": 65,
        "code_cell_count": 34
      },
      "code_examples": [
        "from utils import prepare_demo_tables_in_db",
        "# Fill out the credentials for your Snowflake instance. You will need write access to a database\ndb_user = \"your_username\"  # Username to access Snowflake database\ndb_password = \"your_password\"  # Password\naccount = \"eg:datarobotap_partner.ap-southeast-2\"  # Snowflake account identifier, can be found in the db_url\ndb = \"YOUR_DB_NAME\"  # Database to Write_To\nwarehouse = \"YOUR_WAREHOUSE\"  # Warehouse\nschema = \"YOUR_SCHEMA\"  # Schema\n\ndb_url = \"jdbc:snowflake://{account}.snowflakecomputing.com/?warehouse={warehouse}&db={db}\".format(\n    account=account, db=db, warehouse=warehouse\n)",
        "# Use the util function to pull the data from DataRobot's public S3 and import into your Snowflake instance\nreponse = prepare_demo_tables_in_db(\n    db_user=db_user,\n    db_password=db_password,\n    account=account,\n    db=db,\n    warehouse=warehouse,\n    schema=schema,\n)",
        "import datarobot as dr\n\n# The following are libraries used in this notebook during model evaluation\nimport matplotlib.pyplot as plt\nimport pandas as pd",
        "# API Token\ntoken = open(\n    \"..\\\\..\\\\API token.txt\"\n).read()  # Load the API token from a .txt file for ease of repeatable access\nDATAROBOT_API_TOKEN = (\n    token  # You can also find the API token under the Developer Tools in the UI\n)\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\n# To connect from Jupyter notebook:\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-SNF-17\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client",
        "# If you need to create the credentials, run this cell\n# If you already have Snowflake credentials in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_user and db_password from the Setup section\n\ncred = dr.Credential.create_basic(\n    name=\"[AIA] Your_Snowflake_Credentials\",  # Rename\n    user=db_user,\n    password=db_password,\n)\n\ncredential_id = cred.credential_id",
        "# # If you already have Snowflake credentials in DataRobot, you can retrieve and reuse the same credentials\n# # Uncomment and run this cell instead of creating new credentials\n\n# CREDS_NAME = 'Snowflake Sandbox Demo Connection' # Replace with your credentials name given during Snowflake credentials creation\n\n# credential_id = [cr.credential_id for cr in dr.Credential.list() if cr.name == CREDS_NAME][0]\n\n# # You can verify that you have the right credential by showing the ID and name\n# dr.Credential.get(credential_id)",
        "# Find a Snowflake driver ID by name. This can be skipped if you have the ID - code shown here for completeness\nfor d in dr.DataDriver.list():\n    if d.canonical_name in \"Snowflake (3.13.9 - recommended)\":\n        print((d.id, d.canonical_name))",
        "# Create a datastore\n# If you already have the Snowflake data connection to the right database in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_url from the \"Setup\" section\n\ndata_store = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=\"[AIA] Your New Snowflake Data Connection\",  # Rename\n    driver_id=\"626bae0a98b54f9ba70b4122\",  # Snowflake driver id\n    jdbc_url=db_url,\n)\n\n# Test new data connection; access with your Snowflake credentials\ndata_store.test(username=db_user, password=db_password)\n\ndata_store_id = data_store.id",
        "# # If you already have a Snowflake data connection to your Snowflake database in DataRobot, you can\n# # retrieve and reuse the same connection. Uncomment and run this cell instead of creating a new connection.\n\n# # Once the connection is established via the UI, you can retrieve your data connection by name\n# DATA_STORE_NAME = 'Snowflake - TEST_DB' # Replace with your Data Store name given during Snowflake data connection creation\n\n# data_store_id = [ds.id for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][0]\n\n# # You can verify that you have the right DataStore by name\n# dr.DataStore.get(data_store_id)",
        "# Pick the data source\n# If you had already established a data source, just pick the existing data source\n# Otherwise, create a new data source\n\nDATA_SOURCE_NAME = \"[AIA] Snowflake ECommerce Churn\"  # Rename\nquery_train = (\n    \"SELECT * FROM \" + schema + '.\"DR_Demo_E-commerce_churn_train\";'\n)  # Edit this query to pull the appropriate table\n\ndata_sources = [\n    ds for ds in dr.DataSource.list() if ds.canonical_name == DATA_SOURCE_NAME\n]\n\nif len(data_sources) > 0:\n    data_source_train = data_sources[0].id\n    print(\"Existing data source ID:\", data_source_train)\nelse:\n    query_train = query_train\n    ds_params = dr.DataSourceParameters(data_store_id=data_store_id, query=query_train)\n    data_source_train = dr.DataSource.create(\n        data_source_type=\"jdbc\", canonical_name=DATA_SOURCE_NAME, params=ds_params\n    ).id\n    print(\"New data source created (ID):\", data_source_train)",
        "new_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=data_source_train, do_snapshot=True, credential_id=credential_id\n)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Ecommerce Customer Churn Data\")",
        "# Quick link to the AI Catalog dataset you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\"AI Catalog URL: \" + \"https://app.datarobot.com/ai-catalog/\" + new_dataset.id)\nprint(\"Dataset ID: \" + new_dataset.id)",
        "project = dr.Project.create_from_dataset(\n    dataset_id=new_dataset.id, project_name=\"[AIA] Ecommerce Churn Project\"\n)",
        "# Quick link to the DataRobot project you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Project URL: \"\n    + \"https://app.datarobot.com/projects/\"\n    + project.id\n    + \"/eda\"\n)\nprint(\"Project ID: \" + project.id)",
        "project.analyze_and_model(\n    target=\"Is_dormant\",\n    worker_count=-1,  # Setting worker count to -1 will use your maximum available workers\n)",
        "# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)",
        "# View advanced options used by Autopilot for the project\nproject = dr.Project.get(project.id)  # Update the project object after Autopilot\n\nprint(\"Advanced Settings used:\")\nproject.list_advanced_options()",
        "# Select the model recommended by DataRobot AutoML\ntop_model = project.recommended_model()",
        "# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)",
        "print(\n    \"Top Model LogLoss performance: \"\n    + str(top_model.metrics[\"LogLoss\"][\"crossValidation\"])\n)\nprint(\"Top Model AUC performance: \" + str(top_model.metrics[\"AUC\"][\"crossValidation\"]))",
        "# Retrieve ROC Curve\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)",
        "# Plot the ROC Curve\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.plot(roc.false_positive_rate, roc.true_positive_rate, color=\"b\")\nplt.plot([0, 1], [0, 1], color=\"r\", linestyle=\"dashed\", alpha=0.25)\nplt.title(\"ROC curve\", size=16, fontweight=\"bold\")\nplt.xlabel(\"False Positive Rate\", size=14)\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate\", size=14)\nplt.ylim([0, 1])\nplt.show()",
        "# Retrieve Feature Impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)",
        "# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()",
        "# Prepare new deployment\ndeployment_model_id = top_model.id\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    deployment_model_id,\n    label=\"[AIA] Customer Churn Deployment\",\n    description=\"[AIA] Customer Churn Prediction\",\n    default_prediction_server_id=prediction_server.id,\n)",
        "# Quick link to the deployment you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Deployment URL: \"\n    + \"https://app.datarobot.com/deployments/\"\n    + deployment.id\n    + \"/overview\"\n)\nprint(\"Deployment ID: \" + deployment.id)",
        "# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)",
        "# Turn on Accuracy tracking by providing an association ID\ndeployment.update_association_id_settings(\n    column_names=[\n        \"customerID\"\n    ],  # Association ID to associate prediction results to actuals for accuracy monitoring\n    required_in_prediction_requests=True,\n)",
        "# Select the data store and credentials needed\n# This notebook uses the same Snowflake data store and credentials as the ones used\n# at the start of this notebook\nwriteback_data_store_id = data_store_id\nwriteback_cred_id = credential_id\n\n# Set the Snowflake catalog (database) and schema for intake and output\n# This notebook uses the same Snowflake database and schema as the ones\n# at the start of this notebook\nintake_catalog = db\nintake_schema = schema\noutput_catalog = db\noutput_schema = schema",
        "# Set up settings dictionaries - edit this to your data and prediction pipeline\n\n# Intake settings\n# This example uses the training dataset as if it was also used for scoring\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"DR_Demo_E-commerce_churn_train\",  # Data table in data source\n    \"schema\": intake_schema,\n    \"catalog\": intake_catalog,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Output settings\noutput_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"E-commerce_churn_prediction_demo_output\",  # Rename; Data table to write out to\n    \"schema\": output_schema,\n    \"catalog\": output_catalog,\n    \"statement_type\": \"insert\",\n    \"create_table_if_not_exists\": True,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Schedule settings\nschedule = {\n    \"minute\": [59],\n    \"hour\": [7],\n    \"month\": [\"*\"],\n    \"dayOfWeek\": [\"*\"],\n    \"dayOfMonth\": [1],\n}",
        "# Set up the job dictionary\n# Includes intake and output settings within the job dictionary\njob = {\n    \"deployment_id\": deployment.id,\n    \"num_concurrent\": 4,\n    \"intake_settings\": intake_settings,\n    \"output_settings\": output_settings,\n    \"passthroughColumnsSet\": \"all\",\n}\n\n# Create the Batch Prediction job definition\njob_definition = dr.BatchPredictionJobDefinition.create(\n    enabled=True,\n    batch_prediction_job=job,\n    name=\"[AIA] ECommerce Churn - Monthly Prediction Job JDBC\",  # Make sure you use a unique name from other existing JobDefinitions in your organization\n    schedule=schedule,\n)",
        "# Trigger the job manually\njob_definition.run_once()",
        "# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# job_definition.delete()\n# deployment.delete()\n# project.delete()\n# new_dataset.delete(new_dataset.id)\n# dr.DataSource.get(data_source_train).delete()\n# data_store.delete()\n# cred.delete()"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "dr.credential.get",
        "dr.datasource.list",
        "project.wait_for_autopilot",
        "model.metrics",
        "dr.datasource.create",
        "dr.datastore.create",
        "deployment.create_from_learning_model",
        "datarobot.rest.restclientobject",
        "model.get_roc_curve",
        "deployment.update_drift_tracking_settings",
        "dr.client._global_client",
        "project.list_advanced_options",
        "dr.project.create_from_dataset",
        "dr.credential.list",
        "model.id",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "project.get",
        "deployment.update_association_id_settings",
        "dr.dataset.create_from_data_source",
        "deployment.delete",
        "deployment.id",
        "dr.datadriver.list",
        "dr.datastore.list",
        "dr.batchpredictionjobdefinition.create",
        "project.delete",
        "model.get_uri",
        "dr.credential.create_basic",
        "project.recommended_model",
        "dr.project.get",
        "model.model_type",
        "project.analyze_and_model",
        "project.create_from_dataset",
        "dr.datasource.get",
        "dr.datastore.get"
      ],
      "complexity_score": 1.0,
      "use_case_category": "general"
    },
    {
      "id": "github_notebook_-2263436976975892963",
      "title": "create_llm_deployment",
      "content": "```python\n!pip install \"langchain==0.1.0\" \\\n             \"faiss-cpu==1.7.4\" \\\n             \"sentence-transformers==2.2.2\" \\\n             \"unstructured==0.8.4\" \\\n             \"openai==0.27.8\" \\\n             \"datarobotx\" \\\n             \"pydantic\"\n```\n\n```python\n# Decompress the documents\n!tar -xf ./storage/dr_docs.tar -C ./storage/\n```\n\n```python\nimport re\n\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import MarkdownTextSplitter\n\nSOURCE_DOCUMENTS_DIR = \"storage/datarobot_docs/\"\nSOURCE_DOCUMENTS_FILTER = \"**/*.md\"\n\nloader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\nsplitter = MarkdownTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=1000,\n)\n\nprint(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\ndata = loader.load()\nprint(f\"Splitting {len(data)} documents\")\ndocs = splitter.split_documents(data)\nfor doc in docs:\n    doc.metadata[\"source\"] = re.sub(\n        r\"storage/datarobot_docs/en/(.+)\\.md\",\n        r\"https://docs.datarobot.com/en/docs/\\1.html\",\n        doc.metadata[\"source\"],\n    )\nprint(f\"Created {len(docs)} documents\")\n```\n\n```python\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain.vectorstores.faiss import FAISS\nimport torch\n\nif not torch.cuda.is_available():\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\nelse:\n    EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n\n# Will download the model the first time it runs\nembedding_function = SentenceTransformerEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    cache_folder=\"storage/deploy/sentencetransformers\",\n)\ntry:\n    # Load existing db from disk if previously built\n    db = FAISS.load_local(\"storage/deploy/faiss-db\", embedding_function)\nexcept:\n    texts = [doc.page_content for doc in docs]\n    metadatas = [doc.metadata for doc in docs]\n    # Build and save the FAISS db to persistent notebook storage; this can take some time w/o GPUs\n    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)\n    db.save_local(\"storage/deploy/faiss-db\")\n\nprint(f\"FAISS VectorDB has {db.index.ntotal} documents\")\n```\n\n```python\nimport os\n\nOPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\nOPENAI_ORGANIZATION = os.environ[\"OPENAI_ORGANIZATION\"]\nOPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nOPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\nOPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\nOPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n\n\ndef load_model(input_dir):\n    \"\"\"Custom model hook for loading our knowledge base.\"\"\"\n    import os\n\n    from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n    from langchain.vectorstores.faiss import FAISS\n\n    os.environ[\"OPENAI_API_TYPE\"] = OPENAI_API_TYPE\n    os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n    embedding_function = SentenceTransformerEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        cache_folder=input_dir + \"/\" + \"storage/deploy/sentencetransformers\",\n    )\n    db = FAISS.load_local(\n        input_dir + \"/\" + \"storage/deploy/faiss-db\", embedding_function\n    )\n    return OPENAI_DEPLOYMENT_NAME, db\n\n\ndef score_unstructured(model, data, query, **kwargs) -> str:\n    \"\"\"Custom model hook for making completions with our knowledge base.\"\"\"\n\n    import json\n\n    from langchain.chains import ConversationalRetrievalChain\n    from langchain.chat_models import AzureChatOpenAI\n    from langchain.vectorstores.base import VectorStoreRetriever\n\n    try:\n        deployment_name, db = model\n        data_dict = json.loads(data)\n        llm = AzureChatOpenAI(\n            deployment_name=OPENAI_DEPLOYMENT_NAME,\n            openai_api_type=OPENAI_API_TYPE,\n            openai_api_base=OPENAI_API_BASE,\n            openai_api_version=OPENAI_API_VERSION,\n            openai_api_key=OPENAI_API_KEY,\n            openai_organization=OPENAI_ORGANIZATION,\n            model_name=OPENAI_DEPLOYMENT_NAME,\n            temperature=0,\n            verbose=True,\n            max_retries=0,\n            request_timeout=20,\n        )\n        retriever = VectorStoreRetriever(vectorstore=db)\n        chain = ConversationalRetrievalChain.from_llm(\n            llm, retriever=retriever, return_source_documents=True\n        )\n        if \"chat_history\" in data_dict:\n            chat_history = [\n                (\n                    human,\n                    ai,\n                )\n                for human, ai in data_dict[\"chat_history\"]\n            ]\n        else:\n            chat_history = []\n        rv = chain(\n            inputs={\n                \"question\": data_dict[\"question\"],\n                \"chat_history\": chat_history,\n            },\n        )\n        rv[\"references\"] = [\n            doc.metadata[\"source\"] for doc in rv.pop(\"source_documents\")\n        ]\n    except Exception as e:\n        rv = {\"error\": f\"{e.__class__.__name__}: {str(e)}\"}\n    return json.dumps(rv)\n```\n\n```python\nimport datarobotx as drx\n\ndeployment = drx.deploy(\n    \"storage/deploy/\",\n    name=\"Teams Bot\",\n    hooks={\"score_unstructured\": score_unstructured, \"load_model\": load_model},\n    extra_requirements=[\n        \"langchain==0.1.0\",\n        #                    \"faiss-cpu==1.7.4\",\n        #                    \"sentence-transformers==2.2.2\",\n        #                    \"unstructured==0.8.4\",\n        #                    \"openai==0.27.8\",\n        #                    \"datarobotx\",\n        #                    \"pydantic\"\n    ],\n    # Re-use existing environment if you want to change the hook code,\n    # and not requirements\n    environment_id=\"64c964448dd3f0c07f47d040\",\n)\n# enable storing prediction data, necessary for Data Export for monitoring purposes\ndeployment.dr_deployment.update_predictions_data_collection_settings(enabled=True)\n```\n\n```python\ndeployment.predict_unstructured({\"question\": \"Does datarobot support Azure AD?\"})\n```",
      "content_type": "workflow",
      "source_type": "github_notebook",
      "source_file": "ecosystem_integration_templates/teams_datarobot/create_llm_deployment.ipynb",
      "tags": [
        "integration",
        "deployment",
        "jupyter-notebook",
        "ecosystem",
        "predictions",
        "ai-accelerators",
        "openai",
        "datarobot",
        "templates",
        "tutorial"
      ],
      "metadata": {
        "repo_name": "ai-accelerators",
        "file_path": "ecosystem_integration_templates/teams_datarobot/create_llm_deployment.ipynb",
        "size": 48908,
        "cell_count": 8,
        "code_cell_count": 7
      },
      "code_examples": [
        "!pip install \"langchain==0.1.0\" \\\n             \"faiss-cpu==1.7.4\" \\\n             \"sentence-transformers==2.2.2\" \\\n             \"unstructured==0.8.4\" \\\n             \"openai==0.27.8\" \\\n             \"datarobotx\" \\\n             \"pydantic\"",
        "# Decompress the documents\n!tar -xf ./storage/dr_docs.tar -C ./storage/",
        "import re\n\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import MarkdownTextSplitter\n\nSOURCE_DOCUMENTS_DIR = \"storage/datarobot_docs/\"\nSOURCE_DOCUMENTS_FILTER = \"**/*.md\"\n\nloader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\nsplitter = MarkdownTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=1000,\n)\n\nprint(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\ndata = loader.load()\nprint(f\"Splitting {len(data)} documents\")\ndocs = splitter.split_documents(data)\nfor doc in docs:\n    doc.metadata[\"source\"] = re.sub(\n        r\"storage/datarobot_docs/en/(.+)\\.md\",\n        r\"https://docs.datarobot.com/en/docs/\\1.html\",\n        doc.metadata[\"source\"],\n    )\nprint(f\"Created {len(docs)} documents\")",
        "from langchain.docstore.document import Document\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain.vectorstores.faiss import FAISS\nimport torch\n\nif not torch.cuda.is_available():\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\nelse:\n    EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n\n# Will download the model the first time it runs\nembedding_function = SentenceTransformerEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    cache_folder=\"storage/deploy/sentencetransformers\",\n)\ntry:\n    # Load existing db from disk if previously built\n    db = FAISS.load_local(\"storage/deploy/faiss-db\", embedding_function)\nexcept:\n    texts = [doc.page_content for doc in docs]\n    metadatas = [doc.metadata for doc in docs]\n    # Build and save the FAISS db to persistent notebook storage; this can take some time w/o GPUs\n    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)\n    db.save_local(\"storage/deploy/faiss-db\")\n\nprint(f\"FAISS VectorDB has {db.index.ntotal} documents\")",
        "import os\n\nOPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\nOPENAI_ORGANIZATION = os.environ[\"OPENAI_ORGANIZATION\"]\nOPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nOPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\nOPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\nOPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n\n\ndef load_model(input_dir):\n    \"\"\"Custom model hook for loading our knowledge base.\"\"\"\n    import os\n\n    from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n    from langchain.vectorstores.faiss import FAISS\n\n    os.environ[\"OPENAI_API_TYPE\"] = OPENAI_API_TYPE\n    os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n    embedding_function = SentenceTransformerEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        cache_folder=input_dir + \"/\" + \"storage/deploy/sentencetransformers\",\n    )\n    db = FAISS.load_local(\n        input_dir + \"/\" + \"storage/deploy/faiss-db\", embedding_function\n    )\n    return OPENAI_DEPLOYMENT_NAME, db\n\n\ndef score_unstructured(model, data, query, **kwargs) -> str:\n    \"\"\"Custom model hook for making completions with our knowledge base.\"\"\"\n\n    import json\n\n    from langchain.chains import ConversationalRetrievalChain\n    from langchain.chat_models import AzureChatOpenAI\n    from langchain.vectorstores.base import VectorStoreRetriever\n\n    try:\n        deployment_name, db = model\n        data_dict = json.loads(data)\n        llm = AzureChatOpenAI(\n            deployment_name=OPENAI_DEPLOYMENT_NAME,\n            openai_api_type=OPENAI_API_TYPE,\n            openai_api_base=OPENAI_API_BASE,\n            openai_api_version=OPENAI_API_VERSION,\n            openai_api_key=OPENAI_API_KEY,\n            openai_organization=OPENAI_ORGANIZATION,\n            model_name=OPENAI_DEPLOYMENT_NAME,\n            temperature=0,\n            verbose=True,\n            max_retries=0,\n            request_timeout=20,\n        )\n        retriever = VectorStoreRetriever(vectorstore=db)\n        chain = ConversationalRetrievalChain.from_llm(\n            llm, retriever=retriever, return_source_documents=True\n        )\n        if \"chat_history\" in data_dict:\n            chat_history = [\n                (\n                    human,\n                    ai,\n                )\n                for human, ai in data_dict[\"chat_history\"]\n            ]\n        else:\n            chat_history = []\n        rv = chain(\n            inputs={\n                \"question\": data_dict[\"question\"],\n                \"chat_history\": chat_history,\n            },\n        )\n        rv[\"references\"] = [\n            doc.metadata[\"source\"] for doc in rv.pop(\"source_documents\")\n        ]\n    except Exception as e:\n        rv = {\"error\": f\"{e.__class__.__name__}: {str(e)}\"}\n    return json.dumps(rv)",
        "import datarobotx as drx\n\ndeployment = drx.deploy(\n    \"storage/deploy/\",\n    name=\"Teams Bot\",\n    hooks={\"score_unstructured\": score_unstructured, \"load_model\": load_model},\n    extra_requirements=[\n        \"langchain==0.1.0\",\n        #                    \"faiss-cpu==1.7.4\",\n        #                    \"sentence-transformers==2.2.2\",\n        #                    \"unstructured==0.8.4\",\n        #                    \"openai==0.27.8\",\n        #                    \"datarobotx\",\n        #                    \"pydantic\"\n    ],\n    # Re-use existing environment if you want to change the hook code,\n    # and not requirements\n    environment_id=\"64c964448dd3f0c07f47d040\",\n)\n# enable storing prediction data, necessary for Data Export for monitoring purposes\ndeployment.dr_deployment.update_predictions_data_collection_settings(enabled=True)",
        "deployment.predict_unstructured({\"question\": \"Does datarobot support Azure AD?\"})"
      ],
      "api_methods": [
        "deployment.predict_unstructured",
        "deployment.dr_deployment"
      ],
      "complexity_score": 1.0,
      "use_case_category": "deployment"
    },
    {
      "id": "ai_accelerator_9002795185719928829",
      "title": "AWS_Athena_template: AWS_Athena_End_to_End.ipynb",
      "content": "<center><H1>DataRobot AutoML end-to-end with Amazon Athena</H1></center>\n\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<td><img src=\"https://www.datarobot.com/wp-content/uploads/2021/08/DataRobot-logo-color.svg\" height=200px width=200px>\n</td>\n<td><font size=10> + </font> </td>\n<td> <img src=\"https://vectorwiki.com/images/1BalA__aws-athena.svg\" height=100px width=100px> </td>\n\nAuthor: Biju Krishnan\n\n[API reference documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n<font>\nThis example notebook outlines the following tasks: <p>\n<ol>\n<li> Read in an Amazon Athena table and upload it to DataRobot's AI Catalog </li>\n<li> Create a project with the dataset</li>\n<li> Deploy the top performing model to a DataRobot prediction server </li>\n<li> Make batch predictions with a test dataset </li>\n</ol>\n<p>\n</font>\n\n## Setup\n\n### Import libraries\n\n```python\nimport datarobot as dr\n```\n\n### Bind variables\n\n```python\n# These variables can aso be fetched from a secret store or config files\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-16\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret\n```\n\n### Connect to DataRobot\n\nYou can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n```python\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT, ssl_verify=False)\n```\n\n```python\n# This line reads the driver object needed for creating a datastore\nathena_driver = [\n    drv for drv in dr.DataDriver.list() if drv.canonical_name == \"AWS Athena (v5)\"\n][-1]\n```\n\n## Import data\n\n### Create a data connection\n\nUse the cell below to define the parameters required to make a connector.\n\n```python\nathena_s3_bucket = \"e2eaccelerator09122022\"  # Specifythe name of the bucket followed by any prefix, later you format it as an S3 URI\n\njdbc_url = \"jdbc:awsathena://athena.eu-west-1.amazonaws.com;AwsRegion=eu-west-1;S3OutputLocation=s3://{}/\".format(\n    athena_s3_bucket\n)\n\nquery = 'SELECT * FROM \"new_york_taxi\".\"input\" limit 10000;'\n```\n\n```python\n# Create a data connection (AKA a datastore)\n\nDR_DATASTORE_NAME = \"ATHENA Data Connection\"  # This name can be altered\n\n# Checking if datastore already exists\nfor dstore in dr.DataStore.list():\n    if dstore.canonical_name == DR_DATASTORE_NAME:\n        datastore_flag = False\n        datastore = dstore\n        break\n    else:\n        datastore_flag = True\n\nif datastore_flag:\n    datastore = dr.DataStore.create(\n        data_store_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Connection\",  # This name can be replaced\n        driver_id=athena_driver.id,\n        jdbc_url=jdbc_url,\n    )\n```\n\n```python\n# Create a data connection based on a query\n# The Athena JDBC driver only supports query-based ingestion\n\nparams = dr.DataSourceParameters(data_store_id=datastore.id, query=query)\n\nDR_DATASOURCE_NAME = \"ATHENA Data Source\"  # This name can be altered\n\nfor dsource in dr.DataSource.list():\n    if dsource.canonical_name == DR_DATASOURCE_NAME:\n        datasource_flag = False\n        datasource = dsource\n        break\n    else:\n        datasource_flag = True\n\nif datasource_flag:\n    datasource = dr.DataSource.create(\n        data_source_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Source\",  # This name can be altered\n        params=params,\n    )\n```\n\n```python\n# This code snippet creates a snapshot of the Athena table and stores it in the AI Catalog\n\ndatarobot_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=datasource.id, username=AWS_KEY, password=AWS_SECRET\n)\n```\n\n## Create a project and initiate Autopilot\n\n```python\n# This cell will take several minutes to complete execution\n# An AutoML project named \"E2E Demo Amazon Athena\" is created with \"tip_amount\" as the target column\n# Quick mode is designated, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon Athena\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"tip_amount\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)\n```\n\n### Get the top-performing model\n\nOnce the AutoML project is complete, select the top-performing model on the Leaderboard based on the chosen metric for deployment.\n\n```python\ndef sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)\n```\n\n### Deploy a model\n\nNote that steps in the following sections require DataRobot MLOps licensed features. Contact your DataRobot account representatives if you are missing some licensed MLOps features.\n\n```python\n# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon Athena Test\",\n    description=\"Model trained on New York Taxi trips dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id\n```\n\n### Make predictions\n\nDataRobot's batch predictions API is capable of directly reading and writing to Amazon S3 storage. \n\n```python\n# To run a batch prediction job you need to store the Amazon Athena Credentials in the DataRobot credentials manager\n\nDR_CREDENTIAL_NAME = \"Amazon Athena Credentials\"  # Choose a name\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        athena_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if they do not exist\nif cred_flag:\n    credential = dr.Credential.create_basic(\n        name=DR_CREDENTIAL_NAME,  # The username and password is the AWS KEY and SECRET respectively\n        user=AWS_KEY,\n        password=AWS_SECRET,\n    )\n    athena_credential_id = credential.credential_id\n\nprint(athena_credential_id)\n```\n\n```python\nDR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        s3_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if it does not exist\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    s3_credential_id = credential.credential_id\n\nprint(s3_credential_id)\n```\n\n### Batch predictions snippet\n\nThe snippet below provides sample code to demonstratehow to make batch predictions to and from Amazon S3.\n\n```python\n# This example scores the training data but there needs to be an Athena table with test data.\n\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": \"select * from new_york_taxi.input limit 1000\",  # This has to be a query, since the JDBC driver does not seem to understand table schema structure\n        \"data_store_id\": datastore.id,  # The ID of the datastore you want\n        \"credential_id\": athena_credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\",  # Note this has to be a filename and not just a bucket name\n        \"credential_id\": s3_credential_id,\n    },\n)\njob.wait_for_completion()\njob.get_status()\n```\n\n<font family=verdana>\n<p>\nThe output of the batch predictions is available under the path s3://e2eaccelerator09122022/predictions/output/\n<pre><code><font color=grey size=1>\naws s3 ls s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\n2022-12-12 17:47:49      22725 new_york_taxi_predictions.csv\n</font></code></pre>\n</font>\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_Athena_template/AWS_Athena_End_to_End.ipynb",
      "tags": [
        "integration",
        "athena",
        "ai-accelerators",
        "aws",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_Athena_template",
        "file_type": "notebook",
        "size": 14525
      },
      "code_examples": [
        "import datarobot as dr",
        "# These variables can aso be fetched from a secret store or config files\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-16\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret",
        "dr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT, ssl_verify=False)",
        "# This line reads the driver object needed for creating a datastore\nathena_driver = [\n    drv for drv in dr.DataDriver.list() if drv.canonical_name == \"AWS Athena (v5)\"\n][-1]",
        "athena_s3_bucket = \"e2eaccelerator09122022\"  # Specifythe name of the bucket followed by any prefix, later you format it as an S3 URI\n\njdbc_url = \"jdbc:awsathena://athena.eu-west-1.amazonaws.com;AwsRegion=eu-west-1;S3OutputLocation=s3://{}/\".format(\n    athena_s3_bucket\n)\n\nquery = 'SELECT * FROM \"new_york_taxi\".\"input\" limit 10000;'",
        "# Create a data connection (AKA a datastore)\n\nDR_DATASTORE_NAME = \"ATHENA Data Connection\"  # This name can be altered\n\n# Checking if datastore already exists\nfor dstore in dr.DataStore.list():\n    if dstore.canonical_name == DR_DATASTORE_NAME:\n        datastore_flag = False\n        datastore = dstore\n        break\n    else:\n        datastore_flag = True\n\nif datastore_flag:\n    datastore = dr.DataStore.create(\n        data_store_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Connection\",  # This name can be replaced\n        driver_id=athena_driver.id,\n        jdbc_url=jdbc_url,\n    )",
        "# Create a data connection based on a query\n# The Athena JDBC driver only supports query-based ingestion\n\nparams = dr.DataSourceParameters(data_store_id=datastore.id, query=query)\n\nDR_DATASOURCE_NAME = \"ATHENA Data Source\"  # This name can be altered\n\nfor dsource in dr.DataSource.list():\n    if dsource.canonical_name == DR_DATASOURCE_NAME:\n        datasource_flag = False\n        datasource = dsource\n        break\n    else:\n        datasource_flag = True\n\nif datasource_flag:\n    datasource = dr.DataSource.create(\n        data_source_type=\"jdbc\",\n        canonical_name=\"ATHENA Data Source\",  # This name can be altered\n        params=params,\n    )",
        "# This code snippet creates a snapshot of the Athena table and stores it in the AI Catalog\n\ndatarobot_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=datasource.id, username=AWS_KEY, password=AWS_SECRET\n)",
        "# This cell will take several minutes to complete execution\n# An AutoML project named \"E2E Demo Amazon Athena\" is created with \"tip_amount\" as the target column\n# Quick mode is designated, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon Athena\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"tip_amount\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)",
        "def sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)",
        "# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon Athena Test\",\n    description=\"Model trained on New York Taxi trips dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id",
        "# To run a batch prediction job you need to store the Amazon Athena Credentials in the DataRobot credentials manager\n\nDR_CREDENTIAL_NAME = \"Amazon Athena Credentials\"  # Choose a name\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        athena_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if they do not exist\nif cred_flag:\n    credential = dr.Credential.create_basic(\n        name=DR_CREDENTIAL_NAME,  # The username and password is the AWS KEY and SECRET respectively\n        user=AWS_KEY,\n        password=AWS_SECRET,\n    )\n    athena_credential_id = credential.credential_id\n\nprint(athena_credential_id)",
        "DR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        s3_credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\n# Create credentials in DataRobot credential store if it does not exist\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    s3_credential_id = credential.credential_id\n\nprint(s3_credential_id)",
        "# This example scores the training data but there needs to be an Athena table with test data.\n\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": \"select * from new_york_taxi.input limit 1000\",  # This has to be a query, since the JDBC driver does not seem to understand table schema structure\n        \"data_store_id\": datastore.id,  # The ID of the datastore you want\n        \"credential_id\": athena_credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/new_york_taxi_predictions.csv\",  # Note this has to be a filename and not just a bucket name\n        \"credential_id\": s3_credential_id,\n    },\n)\njob.wait_for_completion()\njob.get_status()"
      ],
      "api_methods": [
        "dr.datasource.list",
        "project.wait_for_autopilot",
        "model.metrics",
        "dr.datasource.create",
        "dr.datastore.create",
        "deployment.create_from_learning_model",
        "dr.client._global_client",
        "dr.autopilot_mode.quick",
        "dr.credential.list",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "dr.credential.create_s3",
        "project.get",
        "dr.dataset.create_from_data_source",
        "deployment.id",
        "dr.datadriver.list",
        "dr.datastore.list",
        "dr.credential.create_basic",
        "dr.batchpredictionjob.score",
        "dr.project.get",
        "project.analyze_and_model",
        "project.get_models",
        "project.metric",
        "project.create_from_dataset",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-8257183951654074456",
      "title": "zero_shot_LMM_error_analysis_NLP: Zero Shot Text Classification for Error Analysis.ipynb",
      "content": "# Zero-shot text classification for error analysis\n\n**Author:** Glen Koundry  \n**Date:** June 6, 2023  \n\n## Overview\n\nThis notebook outlines how to use zero-shot text classification with large language models (LLMs), focusing on its application in error analysis of supervised text classification models.\n\nThis notebook's primary goal is to equip you with practical knowledge of using LLMs for zero-shot text classification and its application in performing error analysis of supervised text classification models. This understanding is becoming an increasingly valuable skill in the era of big data and dynamically evolving categories.\n\n### Core concepts\n\n**Text Classification** is the process of attributing predefined categories (or labels) to text. For example, identifying emails as spam or not spam is a classic text classification task.\n\n**Zero-shot learning** represents an approach in machine learning where a model can tackle a task it hasn't explicitly seen during its training phase. This implies that the text classifier can discern and attribute labels that it hasn't been specifically trained on, a critical ability in real-world situations where labeled data is scarce or when new categories surface post-training.\n\n**Error analysis** involves examining and understanding the mistakes made by a supervised learning model. In the context of text classification, this entails identifying the cases where a model has misclassified the text and understanding why those misclassifications occurred. Error analysis can provide insights into how to improve the model, whether by gathering more or different data, modifying the model architecture, or adjusting the training process.\n\nZero-shot classification exploits the power of **large language models (LLMs)** such as GPT-3, GPT-4, and others by OpenAI. These models, trained on extensive and diverse internet text, can produce human-like text, answer queries, translate languages, and more. A collateral benefit of this wide-ranging training is their ability to comprehend context and make intelligent predictions, a quality you can utilize for zero-shot classification and error analysis.\n\n### Approaches for zero-shot text classification\n\nIn this notebook, you will delve into three distinctive approaches for zero-shot text classification using large language models:\n\n* Embeddings-based classification: Extract high-dimensional vector representations (embeddings) of both text and labels, then measure their similarity.\n* Natural Language Inference-based (NLI) Classification: This method capitalizes on the model's capability to ascertain whether a given statement is true or false within a specific context.\n* Direct classification: This method involves framing the task as a question to a conversational model and interpreting its generated response.\n\nEach of these techniques comes with its unique strengths and challenges, and are explored throughout the notebook.\n\n## Setup\n\n### Import libraries\n\n```python\nimport datetime\nimport json\n\nfrom IPython.display import display, HTML, Markdown\nimport datarobot as dr\nfrom datasets import load_dataset\nimport numpy as np\nimport openai\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\nimport pandas as pd\nfrom transformers import pipeline\n```\n\n### Credentials\n\n**IMPORTANT**: Before running this cell, you need to provide your personal DataRobot API key and your OpenAI API key. Read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n\nTo find your OpenAI API key, log in to your OpenAI account, click on your name in the upper right corner and select \"View API Keys\".\n\nIn the cell, replace the existing string within the quotes (`\"your_openapi_api_key\"`) with your personal OpenAI API Key. Be sure to keep your key within the quotes.\n        \nDon't forget to keep your API keys secure and avoid sharing them publicly.\n\n```python\n# Place your DataRobot API key and URL here\n# DATAROBOT_API_KEY = \"<insert DataRobot API key here>\"\nDATAROBOT_API_ENDPOINT = \"https://app.datarobot.com/api/v2/\"\n\n# Place you OpenAI API key here\nOPENAI_API_KEY = \"<insert OpenAI API key here>\"\nopenai.api_key = OPENAI_API_KEY\n```\n\n### Import data\n\nFor experiments with text classification. use the `financial_phrasebank` dataset from Huggingface. This dataset comprises sentences from financial news, each labeled with a sentiment: negative, neutral, or positive, as determined by human annotators.\n\n```python\n# Dataset to load from Huggingface dataset hub\nDATASET_INFO = {\n    \"path\": \"financial_phrasebank\",\n    \"name\": \"sentences_50agree\",\n    \"split\": \"train\",\n}\nTEXT_FIELD_NAME = \"sentence\"\nLABEL_FIELD_NAME = \"label\"\n\ndataset = load_dataset(**DATASET_INFO)\n\n# Financial phrasebank labels\n# 0 = negative, 1 = neutral, 2 = positive\nlabels = [\n    \"negative\",\n    \"neutral\",\n    \"positive\",\n]\n\ndataset.select(range(5)).to_pandas()\n```\n\n### Create a training and testing split\n\nPrepare the data for your project. Divide the dataset into two subsets: one for training a supervised model and one for testing its performance. After the model is trained, the test subset will be used to evaluate the model's accuracy and to conduct error analysis, exploring where and why the model makes mistakes.\n\n```python\nTEST_SIZE = 1000\n\n# Create a training/testing split with TEST_SIZE test_split\ndataset = dataset.train_test_split(test_size=TEST_SIZE)\n\n# Make DataFrames from the dataset\n# `train_df` is only used for DataRobot supervised model training\ntrain_df = dataset[\"train\"].to_pandas()\ntest_df = dataset[\"test\"].to_pandas()\n```\n\n## Modeling\n\n### Create a project for multi-class supervised learning\n\nThis code creates and trains a supervised learning model using DataRobot's AutoML functionality. It first establishes a connection to the DataRobot application. It then initiates a project and starts the AutoML process, which automatically selects the best model and retrains it on the full training dataset. Once the best model is identified and trained, the code generates predictions for the test dataset. These predictions are then converted into a more readable format, facilitating subsequent error analysis. This model will serve as the primary tool for understanding the strengths and weaknesses of supervised learning in text classification.\n\n```python\n# Connect to DataRobot\ndr.Client(\n    token=DATAROBOT_API_KEY,\n    endpoint=DATAROBOT_API_ENDPOINT,\n)\n\n# Create a project and start Autopilot\nproject = dr.Project.start(\n    train_df,\n    target=LABEL_FIELD_NAME,\n    target_type=\"Multiclass\",\n    project_name=f\"{DATASET_INFO['path']}_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n)\nproject.wait_for_autopilot()\n\n# Get the best-performing model (highest scoring model retrained on 100% of training data)\nmodels = project.get_models()\nfinal_model = [model for model in models if model.sample_pct == 100][0]\nprint(f\"Best model: {final_model.model_type}\\n\")\n\n# Get predictions for the testing dataset\npredict_job = final_model.request_predictions(dataframe=test_df)\nprediction_df = predict_job.get_result_when_complete()\n\n# Change the label to `int` to make indexing the `labels` list easier\nprediction_df[\"prediction\"] = prediction_df[\"prediction\"].astype(int)\n\n# Show predictions\nprediction_df = prediction_df.rename(\n    columns={f\"class_{i}\": label for i, label in enumerate(labels)}\n)\nprediction_df\n```\n\n### Supervised learning model results\n\nThis code evaluates the accuracy of the supervised learning model and collects details about misclassified examples. It first determines the model's accuracy by comparing predictions against actual labels. Then, it creates a list of details about instances where the model's predictions were incorrect. Each entry includes the actual and predicted labels and the text that was misclassified. For demo purposes, this process stops after collecting details for a maximum of ten misclassified instances. This information is crucial for understanding and diagnosing the types of errors our model makes.\n\n```python\n# Show model accuracy\ncorrect_predictions = (\n    prediction_df[\"prediction\"].astype(int) == test_df[LABEL_FIELD_NAME]\n)\nprint(\n    f\"Model accuracy: {correct_predictions.sum()} correct out of {correct_predictions.shape[0]}\"\n)\n\n# Limit the number of mistakes to classify since this is just a demo\nMAX_ERRORS = 20\n\n# Create list of misclassification details\nresult_df = pd.concat((prediction_df, test_df), axis=1)\nerror_details = []\nfor _, result_row in result_df.iterrows():\n    if result_row[LABEL_FIELD_NAME] != result_row[\"prediction\"]:\n        error_details.append(\n            {\n                \"actual_label\": labels[result_row[LABEL_FIELD_NAME]],\n                \"predicted_label\": labels[result_row[\"prediction\"]],\n                \"text\": result_row[TEXT_FIELD_NAME],\n            }\n        )\n        if len(error_details) == MAX_ERRORS:\n            break\n```\n\n## Error analysis of misclassified text\n\n### Define error classes\n\nThe first step is to decide on what types of misclassification you are interested in. This piece of code declares a list, `error_class_templates`, containing potential classes of errors that may occur during text sentiment classification. These error classes are expressed in the form of descriptive strings:\n\n1. \"The sentiment of this statement is ambiguous.\" This error class indicates that the sentiment expressed in the given statement cannot be clearly determined. It could be due to the use of both positive and negative language, sarcasm, or complex language structure.\n\n2. \"The sentiment is unclear and requires more context\" - This error class implies that the given statement lacks enough context to make an accurate sentiment classification. This can occur if the statement is too short, vague, or depends heavily on preceding or following information not included in the text.\n\n3. \"The sentiment of this statement is not {}\" - This error class represents situations where the classifier incorrectly assigns a sentiment. The `{}` is a placeholder that will be replaced with the actual sentiment label predicted by the classifier, allowing this message to dynamically reflect the specific misclassification that occurred.\n\n4. \"The sentiment of this statement is {}\" - This means that the example and label are valid and that\nthe supervised classifier made a mistake.\n\nThese classes represent common types of errors that the classifier might make, and they serve as a starting point for error analysis. During this analysis, misclassified examples will be categorized according to these templates.\n\n```python\n# List of possible classification errors. Replace \"{}\" with the actual label.\nerror_class_templates = [\n    \"The sentiment of this statement is ambiguous.\",\n    \"The sentiment is unclear and requires more context\",\n    \"The sentiment of this statement is not {}\",\n    \"The sentiment of this statement is {}\",\n]\n```\n\n### Helper function for visualizing results\n\nThis function takes a DataFrame containing the results of the error analysis and displays the results using markdown formatting.\n\n```python\ndef show_error_class_results(result_df):\n    for _, result_row in result_df.iterrows():\n        display(Markdown(f\"## Sentence\\n{result_row['sentence']}\"))\n        display(Markdown(f\"**Actual label:** {result_row['actual_label']}\"))\n        display(Markdown(f\"**Predicted label:** {result_row['predicted_label']}\"))\n        probabilities = result_row[error_class_templates].tolist()\n        error_classes = [\n            class_template.format(result_row[\"actual_label\"])\n            for class_template in error_class_templates\n        ]\n        display(\n            pd.Series(probabilities, index=error_classes).to_frame(name=\"probability\")\n        )\n```\n\n## Zero-shot classification\n\nWith a text classification model and list of errors, you can now venture into the exciting domain of zero-shot learning. Zero-shot learning allows you to apply the model to text categories it has never seen before during training, broadening the applicability and versatility of the classifier. In the next sections, explore three distinct methods for implementing zero-shot text classification. These methods are:\n\n* Embeddings-based classification. \n* Natural Language Inference-based (NLI) classification\n* Direct classification \n\nEach technique offers a unique approach to the problem and is based on different principles, with its own benefits and considerations. The aim is to equip you with a variety of tools and perspectives on how to leverage large language models for text classification in a zero-shot setting. Let's delve into each of these techniques and understand how they enhance text classification capabilities.\n\n### Embeddings-based classification\n\nEmbeddings-based classification is a technique for zero-shot text classification that leverages the semantic representations of text encoded as high-dimensional vectors, also known as embeddings. Large language models (LLMs), like GPT-3, GPT-4, or BERT, are excellent at generating these embeddings as they have been trained on a diverse range of internet text, learning the contextual and semantic nuances of the language in the process.\n\nHere's a simplified breakdown of how the process works:\n\n1. **Generate embeddings**: For each text input and each possible label, generate embeddings. This is done by passing the text and labels through the LLM. The LLM then produces a high-dimensional vector for each, encapsulating their semantic information.\n\n2. **Calculate cosine similarity**: Calculate the cosine similarity between the text's embedding and each of the label embeddings. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The higher the cosine similarity, the smaller the angle and greater the match between vectors.\n\n3. **Classify**: The label whose embedding has the highest cosine similarity to the text's embedding is chosen as the predicted label for that text.\n\nThe power of this method lies in its ability to capture semantic similarity between text and labels, allowing it to categorize text into labels that it wasn't explicitly trained on. This method is especially useful when you don't have a lot of labeled data for training, a common scenario in real-world applications.\n\n#### Implement embeddings-based zero-shot classification\n\nThe cell below performs zero-shot text classification using the embeddings-based method. It starts by importing the necessary functions from OpenAI's `embeddings_utils` module. The `get_embedding` function is used to generate embeddings for each label and sentence in the test dataset using OpenAI's `text-embedding-ada-002` engine.\n\nNext, the cosine similarity between the embeddings of each sentence and each label is computed. These similarities are then scaled and converted into pseudo-probabilities representing the likelihood of each label being the correct one for each sentence.\n\nFinally, the predictions and pseudo-probabilities are stored in a DataFrame, which is then passed to the `show_error_class_results` function to display the results of the classification.\n\n\n```python\n# This method doesn't estimate probabilities but you can approximate them by using a scaled softmax\nCS_SCALE = 32\n\nembedding_results = []\nfor error_detail in error_details:\n    # Get embeddings for the misclassified examples\n    sentence_embedding = get_embedding(\n        error_detail[\"text\"], engine=\"text-embedding-ada-002\"\n    )\n\n    # Add the label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Get embeddings for each error class\n    label_embeddings = [\n        get_embedding(error_class, engine=\"text-embedding-ada-002\")\n        for error_class in error_classes\n    ]\n\n    # Compute the similarity of the sentence with each label\n    similarities = [\n        cosine_similarity(sentence_embedding, label_embedding)\n        for label_embedding in label_embeddings\n    ]\n\n    # Make pseudo-probabilities from similarity scores\n    probs = np.exp(CS_SCALE * np.array(similarities))\n    probs = probs / probs.sum()\n\n    embedding_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probs)),\n        )\n    )\n\nembedding_results_df = pd.DataFrame(embedding_results)\nshow_error_class_results(embedding_results_df)\n```\n\n### Use an Natural Language Inference (NLI) model\n\nZero-shot classification using a Natural Language Inference (NLI) model is a method where the model is not trained directly to predict the desired classes. Instead, the task is framed as an inference problem. For example, if you want to classify a sentence into one of several categories, you would pair the sentence with a category in the form of a hypothesis (e.g., \"The sentiment of this sentence is positive\"). The NLI model then predicts whether this hypothesis is true or false. You repeat this for each category, essentially asking the model to infer the most likely category. This is a powerful technique that allows you to classify text into categories that the model was never explicitly trained on, hence the term \"zero-shot\".\n\n#### Implement Natural Language Inference (NLI) zero-shot classification\n\nThis code performs error analysis on misclassified instances using a Natural Language Inference (NLI) model. It creates an NLI-based classifier using the HuggingFace transformers library. For each misclassified example, it generates custom error classes using the actual label and error class templates. The NLI model is then used to determine the probabilities of each error class given the misclassified text. Finally, the code displays the sentence, actual label, predicted label, and a table of probabilities for each error class. This analysis helps in understanding why a particular misclassification occurred.\n\n```python\n# Create the classifier using HuggingFace's zero-shot-classification pipeline\n# NOTE: Change `device=0` to `device=-1` if you do not have a GPU\nnli_classifier = pipeline(\n    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0\n)\n\nnli_results = []\nfor error_detail in error_details:\n    # Add actual label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Suppress warnings from the pipeline\n    nli_classifier.call_count = 0\n\n    # Get error class probabilities\n    predictions = nli_classifier(error_detail[\"text\"], error_classes)\n\n    # Get probabilities and change the order to match error class order\n    probabilites = pd.Series(\n        predictions[\"scores\"],\n        index=[\n            error_classes.index(error_class) for error_class in predictions[\"labels\"]\n        ],\n    ).sort_index()\n\n    nli_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilites)),\n        )\n    )\n\nnli_results_df = pd.DataFrame(nli_results)\nshow_error_class_results(nli_results_df)\n```\n\n### Conversational model prompting\n\nZero-shot classification using a conversational model is a method in which you frame the classification task as a question and answer conversation. Instead of providing an explicit label, the input text is presented to the model in the form of a question, and the model generates a response. \n\nFor example, if you want to classify a piece of text as either 'positive', 'negative', or 'neutral', you can ask the model a question like \"What is the sentiment of this text?\" The model will then generate a response based on its understanding of the text and the question, predicting one of the classes 'positive', 'negative', or 'neutral'. \n\nThis approach leverages the ability of conversational models to understand and generate human-like text. As these models have been trained on diverse and vast amounts of data, they can often generate surprisingly accurate predictions for classes they were not explicitly trained on, hence the term \"zero-shot\".\n\n#### Implement conversational model prompting\n\nThis code analyzes misclassification errors using a conversational AI model. For each error, it provides the AI model with a set of instructions to assess why the error might have occurred based on four possible reasons. The AI model is then asked to assign a probability to each reason for the given misclassified statement. The model's response is processed and displayed as a DataFrame, providing insights into the potential causes of misclassification. By incorporating human-like understanding and context awareness, this method provides an additional layer of depth to your error analysis.\n\n```python\n# Instructions for the conversational model\n# Note that the instructions ask for JSON formatted output so you can have Python process the results\nsystem_prompt_template = (\n    \"ChatGPT, I would like you to analyze a series of misclassified statements from a financial \"\n    \"news sentiment classification model and estimate the probabilites of reasons for the misclassification. \"\n    \"Please review each statement, and based on your understanding, determine whether the misclassification \"\n    \"likely occurred because:\\n\"\n    \"    1. There was not enough context to accurately determine the sentiment.\\n\"\n    \"    2. The statement itself is ambiguous, making it hard to assign a clear sentiment.\\n\"\n    \"    3. The sentiment label ({label}) provided in the dataset is incorrect.\\n\"\n    \"    4. The sentiment label ({label}) provided in the dataset is correct and the model is wrong.\\n\"\n    \"Remember, your task is to analyze the statement and give your best guess for the probability \"\n    \"of each misclassification reason from the above four numbered options.\\n\"\n    \"Please respond in JSON format with only the reason numbers and probability estimates. \"\n    \"Use the numbers (1, 2, 3 and 4) for the reasons.\\n\"\n    \"IMPORTANT: Don't include any explanations or anything that cannot be parsed as JSON.\"\n)\n\nchat_results = []\nfor error_detail in error_details:\n    # Create message with the instructions and the text you want to classify\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt_template.format(\n                label=error_detail[\"actual_label\"]\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f'Statement: \"{error_detail[\"text\"]}\"\\n'\n                f'Correct Sentiment: {error_detail[\"actual_label\"]}\\n'\n                f'Predicted Sentiment: {error_detail[\"predicted_label\"]}'\n            ),\n        },\n    ]\n\n    # Send your request to a conversational model (GPT3.5)\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0\n    )\n\n    # Turn ChatGPT response into a DataFrame\n    result = completion.choices[0].message.content.strip()\n    result_json = json.loads(result)\n\n    # Get probabilities and make sure order matches error class order\n    probabilities = pd.Series(\n        result_json.values(), index=result_json.keys()\n    ).sort_index()\n\n    chat_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilities)),\n        )\n    )\n\n# Convert results to DataFrame and set missing probabilities to zero\nchat_results_df = pd.DataFrame(chat_results).fillna(0)\n\nshow_error_class_results(chat_results_df)\n```\n\n## Compare results from error analysis methods\n\n### Create a table of results for the three methods\n\nThis following cell synthesizes the results of the three zero-shot learning methods – embedding-based, NLI-based, and chat-based – to analyze misclassified examples from the supervised model. It assigns simplified labels to the most probable error class for each method, resulting in a clear and concise summary. This information is then used to create a comparison table, presenting a side-by-side view of each sentence and the respective error analyses from each zero-shot method. The table provides an intuitive way to understand and compare the insights gained from different techniques.\n\n```python\n# Short names to use as column headers\nshort_error_classes = [\n    \"Ambiguous\",\n    \"No Context\",\n    \"Mislabeled\",\n    \"Model Misclassification\",\n]\n\n# For each method, get the highest probability and assign a short label\nembedding_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(\n        embedding_results_df[error_class_templates].values, axis=1\n    )\n]\nnli_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(nli_results_df[error_class_templates].values, axis=1)\n]\nchat_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(chat_results_df[error_class_templates].values, axis=1)\n]\n\n# Show summary of results in a table\nhtml = \"<table><tr><th>Sentence</th><th>Embedding</th><th>NLI</th><th>Chat</th></tr>\"\nfor sentence, embedding_error_class, nli_error_class, chat_error_class in zip(\n    embedding_results_df[\"sentence\"],\n    embedding_results_short,\n    nli_results_short,\n    chat_results_short,\n):\n    html += (\n        f\"<tr><td>{sentence}</td><td>{embedding_error_class}</td><td>{nli_error_class}</td>\"\n        f\"<td>{chat_error_class}</td></tr>\"\n    )\ndisplay(HTML(html + \"</table>\"))\n```\n\n### Compare Zero-Shot Techniques\n\nThe table presents a comparative analysis of the outcomes from three different methods of zero-shot classification— Embeddings-based, Natural Language Inference (NLI), and Conversational model prompting (Chat)—applied on a set of sentences. The goal is to identify the primary cause of misclassification for each sentence as determined by the prior trained model. The potential error classes include \"Ambiguous\", \"No Context\", \"Mislabeled\", and \"Model Misclassification\".\n\nOne key observation from the results is the variability in error classification among the three methods. Each method often interprets and analyzes the same sentence differently, leading to diverse classifications of the source of the error. This indicates that the choice of method can significantly influence the outcomes and subsequent interpretation of error analysis.\n\nHowever, there are instances where all three methods agree on the error classification for a sentence. These instances can provide robust evidence for a particular type of misclassification, suggesting that these might be areas where the original supervised model struggled most.\n\nConversely, sentences that elicit widely varied error classifications across the three methods might indicate particularly challenging instances for automatic text classification. These cases could provide valuable insights into potential areas for improvement in model training or data annotation.\n\n## Conclusion\n\nThis notebook demonstrates the use of zero-shot text classification for error analysis in a supervised learning model. After training the model and identifying its errors, you employed various zero-shot techniques, such as embeddings-based, NLI-based, and conversational AI models, to analyze these misclassifications.\n\nThrough this approach, you gained valuable insights into the reasons behind the model's mistakes, helping to understand its limitations and potential areas of improvement. This showcases how zero-shot classification can enrich the error analysis process, ultimately leading to more robust and accurate models.",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "generative_ai/zero_shot_LMM_error_analysis_NLP/Zero Shot Text Classification for Error Analysis.ipynb",
      "tags": [
        "ai-accelerators",
        "genai",
        "zero-shot",
        "generative-ai",
        "classification",
        "nlp",
        "openai",
        "predictions",
        "datarobot",
        "llm",
        "ai-accelerator",
        "generative_ai"
      ],
      "metadata": {
        "section": "generative_ai",
        "subsection": "zero_shot_LMM_error_analysis_NLP",
        "file_type": "notebook",
        "size": 212479
      },
      "code_examples": [
        "import datetime\nimport json\n\nfrom IPython.display import display, HTML, Markdown\nimport datarobot as dr\nfrom datasets import load_dataset\nimport numpy as np\nimport openai\nfrom openai.embeddings_utils import cosine_similarity, get_embedding\nimport pandas as pd\nfrom transformers import pipeline",
        "# Place your DataRobot API key and URL here\n# DATAROBOT_API_KEY = \"<insert DataRobot API key here>\"\nDATAROBOT_API_ENDPOINT = \"https://app.datarobot.com/api/v2/\"\n\n# Place you OpenAI API key here\nOPENAI_API_KEY = \"<insert OpenAI API key here>\"\nopenai.api_key = OPENAI_API_KEY",
        "# Dataset to load from Huggingface dataset hub\nDATASET_INFO = {\n    \"path\": \"financial_phrasebank\",\n    \"name\": \"sentences_50agree\",\n    \"split\": \"train\",\n}\nTEXT_FIELD_NAME = \"sentence\"\nLABEL_FIELD_NAME = \"label\"\n\ndataset = load_dataset(**DATASET_INFO)\n\n# Financial phrasebank labels\n# 0 = negative, 1 = neutral, 2 = positive\nlabels = [\n    \"negative\",\n    \"neutral\",\n    \"positive\",\n]\n\ndataset.select(range(5)).to_pandas()",
        "TEST_SIZE = 1000\n\n# Create a training/testing split with TEST_SIZE test_split\ndataset = dataset.train_test_split(test_size=TEST_SIZE)\n\n# Make DataFrames from the dataset\n# `train_df` is only used for DataRobot supervised model training\ntrain_df = dataset[\"train\"].to_pandas()\ntest_df = dataset[\"test\"].to_pandas()",
        "# Connect to DataRobot\ndr.Client(\n    token=DATAROBOT_API_KEY,\n    endpoint=DATAROBOT_API_ENDPOINT,\n)\n\n# Create a project and start Autopilot\nproject = dr.Project.start(\n    train_df,\n    target=LABEL_FIELD_NAME,\n    target_type=\"Multiclass\",\n    project_name=f\"{DATASET_INFO['path']}_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n)\nproject.wait_for_autopilot()\n\n# Get the best-performing model (highest scoring model retrained on 100% of training data)\nmodels = project.get_models()\nfinal_model = [model for model in models if model.sample_pct == 100][0]\nprint(f\"Best model: {final_model.model_type}\\n\")\n\n# Get predictions for the testing dataset\npredict_job = final_model.request_predictions(dataframe=test_df)\nprediction_df = predict_job.get_result_when_complete()\n\n# Change the label to `int` to make indexing the `labels` list easier\nprediction_df[\"prediction\"] = prediction_df[\"prediction\"].astype(int)\n\n# Show predictions\nprediction_df = prediction_df.rename(\n    columns={f\"class_{i}\": label for i, label in enumerate(labels)}\n)\nprediction_df",
        "# Show model accuracy\ncorrect_predictions = (\n    prediction_df[\"prediction\"].astype(int) == test_df[LABEL_FIELD_NAME]\n)\nprint(\n    f\"Model accuracy: {correct_predictions.sum()} correct out of {correct_predictions.shape[0]}\"\n)\n\n# Limit the number of mistakes to classify since this is just a demo\nMAX_ERRORS = 20\n\n# Create list of misclassification details\nresult_df = pd.concat((prediction_df, test_df), axis=1)\nerror_details = []\nfor _, result_row in result_df.iterrows():\n    if result_row[LABEL_FIELD_NAME] != result_row[\"prediction\"]:\n        error_details.append(\n            {\n                \"actual_label\": labels[result_row[LABEL_FIELD_NAME]],\n                \"predicted_label\": labels[result_row[\"prediction\"]],\n                \"text\": result_row[TEXT_FIELD_NAME],\n            }\n        )\n        if len(error_details) == MAX_ERRORS:\n            break",
        "# List of possible classification errors. Replace \"{}\" with the actual label.\nerror_class_templates = [\n    \"The sentiment of this statement is ambiguous.\",\n    \"The sentiment is unclear and requires more context\",\n    \"The sentiment of this statement is not {}\",\n    \"The sentiment of this statement is {}\",\n]",
        "def show_error_class_results(result_df):\n    for _, result_row in result_df.iterrows():\n        display(Markdown(f\"## Sentence\\n{result_row['sentence']}\"))\n        display(Markdown(f\"**Actual label:** {result_row['actual_label']}\"))\n        display(Markdown(f\"**Predicted label:** {result_row['predicted_label']}\"))\n        probabilities = result_row[error_class_templates].tolist()\n        error_classes = [\n            class_template.format(result_row[\"actual_label\"])\n            for class_template in error_class_templates\n        ]\n        display(\n            pd.Series(probabilities, index=error_classes).to_frame(name=\"probability\")\n        )",
        "# This method doesn't estimate probabilities but you can approximate them by using a scaled softmax\nCS_SCALE = 32\n\nembedding_results = []\nfor error_detail in error_details:\n    # Get embeddings for the misclassified examples\n    sentence_embedding = get_embedding(\n        error_detail[\"text\"], engine=\"text-embedding-ada-002\"\n    )\n\n    # Add the label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Get embeddings for each error class\n    label_embeddings = [\n        get_embedding(error_class, engine=\"text-embedding-ada-002\")\n        for error_class in error_classes\n    ]\n\n    # Compute the similarity of the sentence with each label\n    similarities = [\n        cosine_similarity(sentence_embedding, label_embedding)\n        for label_embedding in label_embeddings\n    ]\n\n    # Make pseudo-probabilities from similarity scores\n    probs = np.exp(CS_SCALE * np.array(similarities))\n    probs = probs / probs.sum()\n\n    embedding_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probs)),\n        )\n    )\n\nembedding_results_df = pd.DataFrame(embedding_results)\nshow_error_class_results(embedding_results_df)",
        "# Create the classifier using HuggingFace's zero-shot-classification pipeline\n# NOTE: Change `device=0` to `device=-1` if you do not have a GPU\nnli_classifier = pipeline(\n    \"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0\n)\n\nnli_results = []\nfor error_detail in error_details:\n    # Add actual label to \"{}\" fields in error class templates\n    error_classes = [\n        error_class_template.format(error_detail[\"actual_label\"])\n        for error_class_template in error_class_templates\n    ]\n\n    # Suppress warnings from the pipeline\n    nli_classifier.call_count = 0\n\n    # Get error class probabilities\n    predictions = nli_classifier(error_detail[\"text\"], error_classes)\n\n    # Get probabilities and change the order to match error class order\n    probabilites = pd.Series(\n        predictions[\"scores\"],\n        index=[\n            error_classes.index(error_class) for error_class in predictions[\"labels\"]\n        ],\n    ).sort_index()\n\n    nli_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilites)),\n        )\n    )\n\nnli_results_df = pd.DataFrame(nli_results)\nshow_error_class_results(nli_results_df)",
        "# Instructions for the conversational model\n# Note that the instructions ask for JSON formatted output so you can have Python process the results\nsystem_prompt_template = (\n    \"ChatGPT, I would like you to analyze a series of misclassified statements from a financial \"\n    \"news sentiment classification model and estimate the probabilites of reasons for the misclassification. \"\n    \"Please review each statement, and based on your understanding, determine whether the misclassification \"\n    \"likely occurred because:\\n\"\n    \"    1. There was not enough context to accurately determine the sentiment.\\n\"\n    \"    2. The statement itself is ambiguous, making it hard to assign a clear sentiment.\\n\"\n    \"    3. The sentiment label ({label}) provided in the dataset is incorrect.\\n\"\n    \"    4. The sentiment label ({label}) provided in the dataset is correct and the model is wrong.\\n\"\n    \"Remember, your task is to analyze the statement and give your best guess for the probability \"\n    \"of each misclassification reason from the above four numbered options.\\n\"\n    \"Please respond in JSON format with only the reason numbers and probability estimates. \"\n    \"Use the numbers (1, 2, 3 and 4) for the reasons.\\n\"\n    \"IMPORTANT: Don't include any explanations or anything that cannot be parsed as JSON.\"\n)\n\nchat_results = []\nfor error_detail in error_details:\n    # Create message with the instructions and the text you want to classify\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt_template.format(\n                label=error_detail[\"actual_label\"]\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f'Statement: \"{error_detail[\"text\"]}\"\\n'\n                f'Correct Sentiment: {error_detail[\"actual_label\"]}\\n'\n                f'Predicted Sentiment: {error_detail[\"predicted_label\"]}'\n            ),\n        },\n    ]\n\n    # Send your request to a conversational model (GPT3.5)\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0\n    )\n\n    # Turn ChatGPT response into a DataFrame\n    result = completion.choices[0].message.content.strip()\n    result_json = json.loads(result)\n\n    # Get probabilities and make sure order matches error class order\n    probabilities = pd.Series(\n        result_json.values(), index=result_json.keys()\n    ).sort_index()\n\n    chat_results.append(\n        dict(\n            sentence=error_detail[\"text\"],\n            actual_label=error_detail[\"actual_label\"],\n            predicted_label=error_detail[\"predicted_label\"],\n            **dict(zip(error_class_templates, probabilities)),\n        )\n    )\n\n# Convert results to DataFrame and set missing probabilities to zero\nchat_results_df = pd.DataFrame(chat_results).fillna(0)\n\nshow_error_class_results(chat_results_df)",
        "# Short names to use as column headers\nshort_error_classes = [\n    \"Ambiguous\",\n    \"No Context\",\n    \"Mislabeled\",\n    \"Model Misclassification\",\n]\n\n# For each method, get the highest probability and assign a short label\nembedding_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(\n        embedding_results_df[error_class_templates].values, axis=1\n    )\n]\nnli_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(nli_results_df[error_class_templates].values, axis=1)\n]\nchat_results_short = [\n    short_error_classes[error_class]\n    for error_class in np.argmax(chat_results_df[error_class_templates].values, axis=1)\n]\n\n# Show summary of results in a table\nhtml = \"<table><tr><th>Sentence</th><th>Embedding</th><th>NLI</th><th>Chat</th></tr>\"\nfor sentence, embedding_error_class, nli_error_class, chat_error_class in zip(\n    embedding_results_df[\"sentence\"],\n    embedding_results_short,\n    nli_results_short,\n    chat_results_short,\n):\n    html += (\n        f\"<tr><td>{sentence}</td><td>{embedding_error_class}</td><td>{nli_error_class}</td>\"\n        f\"<td>{chat_error_class}</td></tr>\"\n    )\ndisplay(HTML(html + \"</table>\"))"
      ],
      "api_methods": [
        "project.wait_for_autopilot",
        "model.sample_pct",
        "model.model_type",
        "project.start",
        "model.request_predictions",
        "project.get_models",
        "dr.project.start"
      ],
      "complexity_score": 1.0,
      "use_case_category": "genai"
    },
    {
      "id": "ai_accelerator_-8057759570243150651",
      "title": "FP&A: FP&A.ipynb",
      "content": "# Financial Planning and Analysis Workflow\n\n## Summary\nThis notebook illustrates an end-to-end FP&A workflow in DataRobot. Time series forecasting in DataRobot has a huge suite of tools and approaches to handle highly complex multiseries problems. \n\nDataRobot will be used for the model training, selection, deployment, and creating forecasts. While this example will leverage a snapshot file as a datasource this workflow applies to any data source, e.g. Redshift, S3, Big Query, Synapse, etc. \n\n\nThis notebook will demonstrate how to use the Python API client to:\n\n1.  Connect to DataRobot\n2.  Import and preparation of data for time series modeling\n3.  Create a time series forecasting project and run Autopilot\n4.  Retrieve and evaluate model performance and insights\n5.  Making forward looking forecasts\n6.  Evaluating forecasts vs. historical trends\n7.  Deploy a model\n\n## Setup - Import libraries\n\n```python\nfrom datetime import datetime as dt\nfrom platform import python_version\n\nimport datarobot as dr\nfrom datarobot.models.data_engine_query_generator import (\n    QueryGeneratorDataset,\n    QueryGeneratorSettings,\n)\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nprint(\"Python version:\", python_version())\nprint(\"Client version:\", dr.__version__)\n```\n\n## Connect to DataRobot\nTo connect to DataRobot, you need to provide your API token and the endpoint. For more information, please refer to the following documentation:\n\n - [Create and manage API keys via developer tools in the GUI](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management)\n - [Different options to connect to DataRobot from the API client](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n   \nYour API token can be found in the DataRobot UI in the Developer tools section, accessed from the profile menu in the top right corner. Copy the API token and paste in the cell below.\n\n```python\n# Instantiate the DataRobot connection\n\n# Get the token from the Developer Tools page in the DataRobot UI\nDATAROBOT_API_TOKEN = \"\"\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # This should be the URL you use to access the DataRobot UI\n\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n```\n\n## Import Data\n\nWe will be importing 4 years of raw transactional sales data and then wrangling that data, for each product segment, into a time series format for modeling\n\n\n```python\n# Read in csv file to dataframe\ndf = pd.read_csv(\"storage/sales.csv\")\n\n# Convert 'Order Date' columns to datetime format\ndf[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n\n# Display first few rows of data\ndf.head()\n```\n\n## Data Preparation\nWe now need to transform our transactional data into a time series dataset with evenly spaced intervals. We will leverage DataRobot's [Data Prep for Time Series](https://docs.datarobot.com/en/docs/modeling/time/ts-modeling-data/ts-data-prep.html#set-manual-options) to transform the dataset and aggregate each product segment to the monthly level.\n<br/>\n<br/>\nGenerally speaking, DataRobot's Data Prep for Time Series will aggregates the dataset to the selected time step, and, if missing rows are detected will impute the target value. Data Prep for Time Series allows you to choose aggregation methods for numeric, categorical, and text values. You can also use it to explore modeling at different time scales. The resulting dataset is then published to DataRobot's AI Catalog.\n\n```python\n# Upload the dataset to the AI Catalog\ndataset = dr.Dataset.upload(df)\n\n# Rename the entry in AI Catalog\ndataset.modify(name=\"Transactional_Sales_Data\", categories=dataset.categories)\n\n# Create a time series data prep query generator from the dataset we just uploaded to AI Catalog\nquery_generator_dataset = QueryGeneratorDataset(\n    alias=\"Transactional_Sales_Data\",\n    dataset_id=dataset.id,\n    dataset_version_id=dataset.version_id,\n)\n\n# Set the parameters for our time series Data Prep\nquery_generator_settings = QueryGeneratorSettings(\n    datetime_partition_column=\"Order Date\",  # Date/time feature used as the basis for partitioning\n    time_unit=\"MONTH\",  # Time unit (seconds, days, months, etc.) that comprise the time step\n    time_step=1,  # Number of (time) units that comprise the time step.\n    default_numeric_aggregation_method=\"sum\",  # Aggregate the target using either mean & most recent or sum & zero\n    default_categorical_aggregation_method=\"last\",  # Aggregate categorical features using the most frequent value or the last value within the aggregation time step.\n    target=\"Sales\",  # Numeric column in the dataset to predict.\n    multiseries_id_columns=[\n        \"Segment\"\n    ],  # Column containing the series identifier, which allows DataRobot to process the dataset as a separate time series.\n    default_text_aggregation_method=\"meanLength\",  # Choose ignore to skip handling of text features or aggregate by: 'concat', 'last', 'meanLength', 'mostFrequent', 'totalLength'\n    start_from_series_min_datetime=True,  # Basis for the series start date, either the earliest date for each series (per series) or the earliest date found for any series (global).\n    end_to_series_max_datetime=True,  # Basis for the series end date, either the last entry date for each series (per series) or the latest date found for any series (global).\n)\nquery_generator = dr.DataEngineQueryGenerator.create(\n    generator_type=\"TimeSeries\",\n    datasets=[query_generator_dataset],\n    generator_settings=query_generator_settings,\n)\n\n# Prep the training dataset\ntraining_dataset = query_generator.create_dataset()\n\n# Rename the entry in AI Catalog\ntraining_dataset.modify(\n    name=\"ts_monthly_training\", categories=training_dataset.categories\n)\n```\n\n## Exploratory Data Analysis\n\nLet's visualize the monthly sales for each segment in order to quickly identify anomalies, evaluate patterns and seasonality, and identify anything that may warrant further expoloration.\n\n```python\n# Load the dataset into a pandas dataframe\ntraining_df = training_dataset.get_as_dataframe()\n\n# Convert 'Order Date' to datetime format and sort\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\n\n# Adding Total Sales as an additional segment\ntotal_sales = training_df.groupby(\"Order Date\").agg({\"Sales\": \"sum\"}).reset_index()\ntotal_sales[\"Segment\"] = \"Total\"\ntraining_df = pd.concat([training_df, total_sales], ignore_index=True)\n\n# Visualize our data:\nfig = go.Figure()\n\n# Line chart for monthly sales by segment\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment A\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment B\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment C\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Total\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Total\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Total\",\n    )\n)\n\nfig.update_layout(\n    title=\"Monthly Sales by Segment and Total\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    template=\"plotly_dark\",\n)\nfig.show()\n```\n\n![ExploratoryDataAnalysis.png](attachment:ExploratoryDataAnalysis.png)\n\n## Create a time series forecasting project and run Autopilot\n\nWe can create a project using our dataset in the AI Catalog:\n\n```python\n# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"Monthly_Sales_Forecast\", dataset_id=training_dataset.id\n)\n```\n\n```python\n# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)\n```\n\nConfigure time-series modeling settings\nTime-series projects have a number of parameters we can adjust. This includes:\n\n- Multi-series (i.e. Series ID column)\n- Backtest partitioning\n- Feature Derivation Window\n- Forecast Window\n- Known-in-advance (KA) Variables\n- Do not derive (DND) Variables\n- Calendars\n\n```python\n# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-6, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 12)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = []\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Order Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Order Date\"]),\n)  # Last date in calendar\n```\n\nWe pass all our settings to a [DatetimePartitioningSpecification](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/datetime_partition.html?highlight=datetimepartitioningspecification#setting-up-a-datetime-partitioned-project) object which will then be passed to our Autopilot process.\n\n```python\n# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Order Date\",  # Date column\n    multiseries_id_columns=[\"Segment\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)\n```\n\n## Start modeling with autopilot\nTo start the Autopilot process, call the analyze_and_model function. Provide the prediction target and our DatetimePartitioningSpecification as part of the function call. We have several modes to spin up Autopilot - in this demo, we will use the default \"Quick\" mode.\n\n```python\n# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)\n```\n\n```python\n# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)\n```\n\n## Retrieve and evaluate model performances and insights\nAfter Autopilot completes, you can easily evaluate your model results. Evaluation can include compiling the Leaderboard as a dataframe, measuring performances across different backtest partitions with different metrics, visualizing the accuracy across series, analyzing Feature Impact and Feature Effects to understand each models' behaviors, and more. This can be done for every single model created by DataRobot.\n\nAs a simple example in this notebook, <b>we identify the best model created by Autopilot and evaluate</b>:\n\n- RMSE performance\n- MASE performance\n- Accuracy for Time for various Forecast Distance and Series combinations\n- Feature Impact of Top 10 features\n- Compare Accuracy across Series\n\n```python\n# Unlock the holdout set within the project\nproject.unlock_holdout()\n```\n\n```python\n# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\ndf_list = []  # This will store each individual DataFrame to concatenate at the end\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    df_list.append(model_performances)  # Append the DataFrame to the list\n\n# Concatenate all DataFrames in the list\nscores = pd.concat(df_list, ignore_index=True)\n\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores\n```\n\n```python\n# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)\n```\n\n```python\nprint(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)\n```\n\n## Get Accuracy Over Time\nDataRobot provides two helpful views of our forecasts out-of-the-box:\n\n- [Accuracy Over Time](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/aot.html) fixes the forecast distance and visualizes the corresponding forecast for each forecasted date.\n- [Forecast vs Actual](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/fore-act.html) sets a specific forecast point and visualizes the corresponding forecasts for the entire forecast window.\nWe can pull the results out for either analysis. As a demonstration, we will generate the Accuracy Over Time plots for forecast distances of 1 day and 7 day.\n\n```python\n# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=1, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=1\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n)\n\n# Display the plot\n\n# Update layout\nfig.update_layout(template=\"plotly_dark\")\nfig.show()\n```\n\n![AccuracyOverTime_All.png](attachment:AccuracyOverTime_All.png)\n\n```python\n# Get Accuracy over Time for FD=6, For just Segment A\nacc_plot_FD6_Consumer = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=6, series_id=\"Segment A\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD6_Consumer.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=6 (Segment A)\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()\n```\n\n![AccuracyOverTimeSegmentA.png](attachment:AccuracyOverTimeSegmentA.png)\n\n## Retrieve Feature Impact\nAs an example of model explainability, calculate the Feature Impact values of the model using the get_or_request_feature_impact function.\n\n```python\n# get model\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][2])\n\n# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:5]\n\n# Plotly Graph\nfig = go.Figure()\n\n# Add bar trace\nfig.add_trace(\n    go.Bar(y=FI_df[\"featureName\"], x=FI_df[\"impactNormalized\"], orientation=\"h\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Feature Impact\",\n    xaxis=dict(title=\"Normalized Feature Impact\", range=[0, 1.1]),\n    yaxis=dict(title=\"Feature\", autorange=\"reversed\"),\n    margin=dict(\n        l=200\n    ),  # this is to ensure the y-labels (feature names) are not cut off\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()\n```\n\n![FeatureImpact.png](attachment:FeatureImpact.png)\n\n## Analyze Accuracy for each Series\nThe [Series Insight](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/series-insights-multi.html) tool provides the ability to compute the accuracy for each indivudal series. This is especially powerful to help us identify which series the model is doing particularly better or worse in forecasting.\n\nIn this demonstration, we see that the model has particularly high RMSE for the Savannah and Louisville store forecasts. We may consider refining our model by splitting those two series into a separate model as a future modeling experiment.\n\n```python\n# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing\n```\n\n```python\n# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight\n```\n\n```python\n# Create a scatter plot with Plotly\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x=model_series_insight[\"Series\"],\n        y=model_series_insight[\"backtestingScore\"],\n        mode=\"markers\",\n    )\n)\n\n# Update the layout\nfig.update_layout(\n    title=\"Performance by Segment\",\n    xaxis=dict(title=\"Series\", tickangle=45),\n    yaxis=dict(title=\"RMSE\"),\n)\n\n# Display the plot\nfig.update_layout(template=\"plotly_dark\")\nfig.show()\n```\n\n![PerformanceBySegment.png](attachment:PerformanceBySegment.png)\n\n## Make new predictions with a test dataset\nWe can make new predictions directly on the leaderboard by uploading new test datasets to the project. We can then score the test dataset with any model on the leaderboard and retrieve the results.\n\nIn this notebook, we will load the data into the notebook and upload to the project. As with the training dataset, you can also use the JDBC connector to ingest the test dataset into the AI catalog and then directly use the dataset ID in the request_predictions call. The JDBC path provides more efficient upload of large datasets to DataRobot.\n\nFor time-series predictions, DataRobot expects new prediction datasets to have rows for each new date we want to forecast. These rows should have values for the known-in-advance features and NA everywhere else. For example, since the model is forecasting 1-7 days out from each forecast point, we will have 7 new rows (corresponding from June 15 to June 21, 2014).\n\n```python\n# Upload data to modeling project\ndf = training_dataset.get_as_dataframe()\ntest_dataset = project.upload_dataset(df)\n\n# Get frozen model\nfrozen_model = all_models[0]\n\n# Request Predictions\npred_job = frozen_model.request_predictions(\n    dataset_id=test_dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=85,\n)\n\npreds = pred_job.get_result_when_complete()\n\npreds.head(5)\n```\n\n```python\n# Step 1: Rename columns in preds to match training_df\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Step 2: Drop columns from preds that are not in training_df\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]\n```\n\n## Join Forecasts to Actuals\n\n```python\n# Function to evaluate date time values\n\n\ndef convert_or_localize_to_utc(series):\n    if series.dt.tz is not None:  # If it's already timezone-aware\n        return series.dt.tz_convert(\"UTC\")\n    return series.dt.tz_localize(\"UTC\")  # If it's naive, then localize it\n\n\n# Convert or localize 'Order Date' in training_df to UTC\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\ntraining_df[\"Order Date\"] = convert_or_localize_to_utc(training_df[\"Order Date\"])\n\n# Rename columns in prediction data\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Drop columns from prediction data that is not in training data\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]\n\n# Convert or localize 'Order Date' in preds_subset to UTC\npreds_subset = modified_preds.copy()  # To avoid SettingWithCopyWarning\npreds_subset[\"Order Date\"] = pd.to_datetime(preds_subset[\"Order Date\"])\npreds_subset[\"Order Date\"] = convert_or_localize_to_utc(preds_subset[\"Order Date\"])\n\n# Concatenate the DataFrames\ncombined_df = pd.concat([training_df, preds_subset], ignore_index=True)\n\n# Sort by Segment and Order Date\ncombined_df.sort_values(by=[\"Segment\", \"Order Date\"], inplace=True)\n```\n\n## Visualize Forecasts With a Line Chart\n\n```python\n# Initialize the figure\nfig = go.Figure()\n\n# Define a color mapping for segments (you can extend or modify this as needed)\ncolor_map = {\"Segment A\": \"blue\", \"Segment B\": \"green\", \"Segment C\": \"orange\"}\n\n# For each segment, plot actual sales and forecasted sales\nfor segment, color in color_map.items():\n    segment_df = combined_df[combined_df[\"Segment\"] == segment]\n    actuals = segment_df[:-12]  # Adjust based on your actual data\n    forecast = segment_df[-12:]\n\n    fig.add_trace(\n        go.Scatter(\n            x=actuals[\"Order Date\"],\n            y=actuals[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Actuals\",\n            line=dict(color=color),  # Use the color from the color_map\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=forecast[\"Order Date\"],\n            y=forecast[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Forecast\",\n            line=dict(\n                dash=\"dot\", color=color\n            ),  # Use the color from the color_map and make the line dotted\n        )\n    )\n\nfig.update_layout(\n    title=\"Actual + Forecasted Sales by Segment\",\n    xaxis_title=\"Order Date\",\n    yaxis_title=\"Sales\",\n    legend_title=\"Segments\",\n    template=\"plotly_dark\",\n)\n\nfig.show()\n```\n\n![ActualsAndForecasts.png](attachment:ActualsAndForecasts.png)\n\n## Evaluate Forecasted Growth Metrics\n\nFP&A teams typically use a variety of growth metrics to gain a comprehensive understanding of sales performance. Different metrics can provide unique insights into how a financial metrics is trending and what underlying factors might be driving those trends. \n\n1. **Month Over Month Growth (MoM Growth)**: evaluates the growth of sales (or any other metric) from one month to the next. MoM Growth can provide a short-term perspective on trends to quickly identify any sudden changes or spikes which might be due to seasonality, promotions, or other short-term factors. Useful for identifying short-term operational challenges or successes.\n\n2. **Monthly Year Over Year Growth (YoY Growth)**: evaluates a specific month to the sales in the same month the previous year. This view accounts for seasonality, as it compares the same month across years and can helps identify longer-term trends and determine if a particular month's sales are genuinely growing or declining over the years.\n\n3. **Year to Date Year over Year Growth (YTD YoY Growth)**: evaluates the cumulative sum from the beginning of the year up to a specific month as compared to the same time period in the previous year. This provides an aggregated view of performance over a year, making it easier to see if the company is on track to meet annual targets. This can also mitigate any month-to-month volatility, providing a more consistent perspective on how the year is progressing.\n\n4. **Rolling 12 Month Year Over Year Growth (R12M YoY Growth)**: evaluates the most recent 12-month period to sales in the 12 months before that. This offers a continuously updated yearly perspective, irrespective of where you are in the calendar year and smoothens out short-term fluctuations and seasonality, as it always encompasses a full year of data.\n\n### Calculate the forecasted year over year growth\n\n```python\ndef calculate_growth(segment_data):\n    forecast_sum = segment_data.tail(12)[\"Sales\"].sum()\n    prior_year_sum = segment_data.iloc[-24:-12][\"Sales\"].sum()\n    two_years_ago_sum = segment_data.iloc[-36:-24][\"Sales\"].sum()\n\n    forecasted_growth = (forecast_sum - prior_year_sum) / prior_year_sum * 100\n    prior_year_growth = (prior_year_sum - two_years_ago_sum) / two_years_ago_sum * 100\n\n    return forecast_sum, prior_year_sum, forecasted_growth, prior_year_growth\n\n\nsegments = combined_df[\"Segment\"].unique()\ngrowth_data = [\n    (*calculate_growth(combined_df[combined_df[\"Segment\"] == segment]), segment)\n    for segment in segments\n]\n\ngrowth_df = pd.DataFrame(\n    growth_data,\n    columns=[\n        \"Forecasted Total\",\n        \"Prior Year Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n        \"Segment\",\n    ],\n)\n\n# Calculate aggregate level\naggregate_prior_year = growth_df[\"Prior Year Total\"].sum()\naggregate_forecast = growth_df[\"Forecasted Total\"].sum()\naggregate_growth = (\n    (aggregate_forecast - aggregate_prior_year) / aggregate_prior_year * 100\n)\n\nadjusted_prior_year_growth = (\n    (\n        aggregate_prior_year\n        - growth_df[\"Prior Year Total\"].sum()\n        + growth_df.iloc[-1][\"Prior Year Total\"]\n    )\n    / (growth_df[\"Prior Year Total\"].sum() - growth_df.iloc[-1][\"Prior Year Total\"])\n    * 100\n)\n\naggregate_row = pd.DataFrame(\n    [\n        {\n            \"Segment\": \"Total\",\n            \"Prior Year Total\": aggregate_prior_year,\n            \"Forecasted Total\": aggregate_forecast,\n            \"Forecasted Growth\": aggregate_growth,\n            \"Prior Year Growth\": adjusted_prior_year_growth,\n        }\n    ]\n)\n\ngrowth_df = pd.concat([growth_df, aggregate_row], ignore_index=True)\n```\n\n## Visualize the year over year growth in a bar chart\n\n```python\ndef plot_sales_data_with_table(growth_df):\n    # Filter out the 'Total' row for the bar chart\n    chart_df = growth_df[growth_df[\"Segment\"] != \"Total\"]\n\n    # Create subplots: one row for bar chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.15,\n        subplot_titles=(\n            \"Prior Year vs Forecasted Sales Totals by Segment\",\n            \"Data Table\",\n        ),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Add bar chart to the first row of subplot\n    fig.add_trace(\n        go.Bar(\n            name=\"Prior Year Total\",\n            x=chart_df[\"Segment\"],\n            y=chart_df[\"Prior Year Total\"],\n        ),\n        row=1,\n        col=1,\n    )\n    forecasted_total_bar = go.Bar(\n        name=\"Forecasted Total\", x=chart_df[\"Segment\"], y=chart_df[\"Forecasted Total\"]\n    )\n    fig.add_trace(forecasted_total_bar, row=1, col=1)\n\n    # Overlay forecast growth on top of the Forecasted Total bars\n    for i, segment in enumerate(chart_df[\"Segment\"]):\n        fig.add_annotation(\n            x=segment,\n            y=chart_df.loc[chart_df[\"Segment\"] == segment, \"Forecasted Total\"].values[\n                0\n            ],\n            text=f\"{chart_df.loc[chart_df['Segment'] == segment, 'Forecasted Growth'].values[0]:.2f}%\",\n            showarrow=False,\n            font=dict(color=\"white\"),\n            row=1,\n            col=1,\n        )\n\n    # Change the bar mode to 'group'\n    fig.update_layout(barmode=\"group\")\n\n    # Specify the order of columns for the table\n    ordered_columns = [\n        \"Segment\",\n        \"Prior Year Total\",\n        \"Forecasted Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n    ]\n\n    # Format the numeric values with commas and two decimal points\n    formatted_data = []\n    for col in ordered_columns:\n        if growth_df[col].dtype in [float, \"float64\"]:\n            # Format float columns\n            formatted_data.append(growth_df[col].map(\"{:,.2f}\".format).tolist())\n        elif growth_df[col].dtype in [int, \"int64\"]:\n            # Format integer columns\n            formatted_data.append(growth_df[col].map(\"{:,}\".format).tolist())\n        else:\n            # Keep non-numeric columns as is\n            formatted_data.append(growth_df[col].tolist())\n\n    table_header = ordered_columns\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=formatted_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\")\n\n    fig.show()\n\n\n# Example usage with your growth_df dataframe\nplot_sales_data_with_table(growth_df)\n```\n\n![PriorYearVsForecast.png](attachment:PriorYearVsForecast.png)\n\n## Calculate additional metrics to evaluate forecasted growth\n\n```python\n# Create a copy of training_df with only the necessary columns\ndf_copy = combined_df[[\"Segment\", \"Order Date\", \"Sales\"]].copy()\n\n# Convert \"Order Date\" column of df_copy to datetime and set as index\ndf_copy[\"Order Date\"] = pd.to_datetime(df_copy[\"Order Date\"])\ndf_copy.set_index(\"Order Date\", inplace=True)\n\n\ndef calculate_growth_metrics(segment_data):\n    # Using .loc to set values to avoid SettingWithCopyWarning\n    segment_data = segment_data.copy()\n    segment_data.loc[:, \"Year\"] = segment_data.index.year\n    segment_data.loc[:, \"Month\"] = segment_data.index.month\n\n    # Group by year and month to get monthly sales and specify numeric_only=True\n    monthly_sales = segment_data.groupby([\"Year\", \"Month\"]).sum(numeric_only=True)\n\n    # Calculate growth metrics\n    monthly_sales[\"MoM Growth\"] = monthly_sales[\"Sales\"].pct_change() * 100\n    monthly_sales[\"Monthly YoY Growth\"] = (\n        monthly_sales[\"Sales\"].pct_change(periods=12) * 100\n    )\n    monthly_sales[\"Rolling 12 Mth YoY Growth\"] = (\n        monthly_sales[\"Sales\"].rolling(window=12).sum().pct_change(periods=12) * 100\n    )\n\n    # New YTD Growth logic\n    monthly_sales[\"YTD\"] = monthly_sales[\"Sales\"].groupby(level=0).cumsum()\n    previous_ytd = monthly_sales[\"YTD\"].shift(12)\n    monthly_sales[\"YTD YoY Growth\"] = (\n        (monthly_sales[\"YTD\"] - previous_ytd) / previous_ytd\n    ) * 100\n\n    # Convert 'Year' and 'Month' back to actual date (the first day of each month)\n    monthly_sales.reset_index(inplace=True)\n    monthly_sales[\"Order Date\"] = pd.to_datetime(\n        monthly_sales[[\"Year\", \"Month\"]].assign(DAY=1)\n    )\n    monthly_sales.drop([\"Year\", \"Month\"], axis=1, inplace=True)\n\n    return monthly_sales\n\n\n# Filter data based on segments and calculate metrics\nsegmenta_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment A\"])\nsegmentb_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment B\"])\nsegmentc_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment C\"])\ntotal_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Total\"])\n```\n\n## Visualize the additional metrics with line charts\n\n```python\n# Function to plot a specific growth metric for all segments and show table below\n\n\ndef plot_metric_across_segments_with_table(metric_name, datasets, yaxis_range=None):\n    # Create subplots: one row for line chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.1,\n        subplot_titles=(f\"Comparison of {metric_name} Across Segments\", \"Data Table\"),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Lists to store filtered data for the table\n    table_dates = None\n    table_metric_values = []\n\n    # Add line plots to the first row of subplot\n    for segment_name, segment_df in datasets.items():\n        # Filter DataFrame to remove null values in the metric column and keep data from January 2014 onwards\n        segment_df = segment_df[\n            pd.notnull(segment_df[metric_name])\n            & (segment_df[\"Order Date\"] >= \"2014-01-01\")\n        ]\n\n        # Store filtered data for table\n        if table_dates is None:\n            table_dates = segment_df[\"Order Date\"].tolist()\n\n        # Format metric values to two decimal points\n        formatted_metric_values = [\n            round(val, 2) for val in segment_df[metric_name].tolist()\n        ]\n        table_metric_values.append(formatted_metric_values)\n\n        fig.add_trace(\n            go.Scatter(\n                x=segment_df[\"Order Date\"],\n                y=segment_df[metric_name],\n                mode=\"lines\",\n                name=segment_name,\n            ),\n            row=1,\n            col=1,\n        )\n\n    # Create a table with the filtered data\n    table_data = [table_dates] + table_metric_values\n    table_header = [\"Order Date\"] + list(datasets.keys())\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=table_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\", yaxis_range=yaxis_range)\n\n    fig.show()\n\n\n# Datasets for all segments\ndatasets = {\n    \"Segment A\": segmenta_df,\n    \"Segment B\": segmentb_df,\n    \"Segment C\": segmentc_df,\n    \"Total\": total_df,\n}\n\n# Create the plots for each specific metric:\nplot_metric_across_segments_with_table(\"MoM Growth\", datasets, yaxis_range=[-200, 500])\nplot_metric_across_segments_with_table(\n    \"Monthly YoY Growth\", datasets, yaxis_range=[-200, 600]\n)\nplot_metric_across_segments_with_table(\n    \"YTD YoY Growth\", datasets, yaxis_range=[-200, 300]\n)\nplot_metric_across_segments_with_table(\"Rolling 12 Mth YoY Growth\", datasets)\n```\n\n![ComparisonOfMOM.png](attachment:ComparisonOfMOM.png)\n\n![MonthlyYoY.png](attachment:MonthlyYoY.png)\n\n![YTDYoY.png](attachment:YTDYoY.png)\n\n![Rolling12YoY.png](attachment:Rolling12YoY.png)\n\n## Deploy Model to DataRobot ML Production for Monitoring and Governance\n\n```python\n# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=frozen_model.id,\n    label=\"FP&A - Segment Forecasting Model\",\n    description=\"FP&A - Segment Forecasting Model\",\n    default_prediction_server_id=prediction_server_id,\n)\n```\n\n## Request Predictions from Deployment\n\n```python\n# Score the dataset using the given deployment ID\njob, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, df\n)  # Deployment ID and Scoring dataset\n\n# Print a message to indicate that scoring has started\nprint(\"Started scoring...\", job)\n\n# Wait for the job to complete\njob.wait_for_completion()\n```\n\n## Clean Up\n\n```python\n# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\ndeployment.delete()\n# project.delete()\n```\n\n# Next Steps and Additional Resources\n\n## Improve Accuracy\n\nThere are a number of approaches that we could apply to improve model accuracy. We can try:\n\n\n-  Running DataRobot's entire blueprint repository\n-  Evaluating different feature derivation windows across projects\n-  Evaluating different training lengths\n-  Evaluating different blenders / ensemble models\n-  Adding in additional data or other data sources (e.g. macroeconomic data)\n\n## Other Use Cases\n- [Scenario analysis](https://docs.datarobot.com/en/docs/app-builder/ts-app.html#what-if-widget) - evaluate what happens to our sales forecasts under certain conditions by leveraging [known in advance variables](https://docs.datarobot.com/en/docs/modeling/build-models/adv-opt/time-series-adv-opt.html#set-known-in-advance-ka) in DataRobot\n- Long term strategic planning where we can forecast over longer time horizons than annual planning requires.\n- Risk management - model potential risks that have financial impacts, such as: churn, write-downs, etc.\n\n## Additional Resources\n- [The DataRobot AI Accelerator Library](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library) has similar accelerators for other [Ecosystem Integrations](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library/label-name/ecosystem%20integration%20templates) to use DataRobot with other tools (e.g. AWS, GCP, Azure, etc) as well as accelerators for more advanced time-series applications.\n- [The DataRobot API user guide](https://docs.datarobot.com/en/docs/api/guide/python/index.html) provides code examples covering topics such as model factories, classification problems, feature impact rank ensembling, and more.\n\nTo learn more about advanced workflows for handling complex and large scale time series problems:\n\n- [Time series clustering](https://docs.datarobot.com/en/docs/modeling/time/ts-clustering.html#time-series-clustering)\n- [Segmented modeling](https://docs.datarobot.com/en/docs/modeling/time/ts-segmented.html)",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "use_cases_and_horizontal_approaches/FP&A/FP&A.ipynb",
      "tags": [
        "analysis",
        "business-applications",
        "ai-accelerators",
        "use-cases",
        "finance",
        "openai",
        "predictions",
        "datarobot",
        "deployment",
        "time-series",
        "planning",
        "ai-accelerator",
        "use_cases_and_horizontal_approaches"
      ],
      "metadata": {
        "section": "use_cases_and_horizontal_approaches",
        "subsection": "FP&A",
        "file_type": "notebook",
        "size": 1557853
      },
      "code_examples": [
        "from datetime import datetime as dt\nfrom platform import python_version\n\nimport datarobot as dr\nfrom datarobot.models.data_engine_query_generator import (\n    QueryGeneratorDataset,\n    QueryGeneratorSettings,\n)\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nprint(\"Python version:\", python_version())\nprint(\"Client version:\", dr.__version__)",
        "# Instantiate the DataRobot connection\n\n# Get the token from the Developer Tools page in the DataRobot UI\nDATAROBOT_API_TOKEN = \"\"\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # This should be the URL you use to access the DataRobot UI\n\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)",
        "# Read in csv file to dataframe\ndf = pd.read_csv(\"storage/sales.csv\")\n\n# Convert 'Order Date' columns to datetime format\ndf[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n\n# Display first few rows of data\ndf.head()",
        "# Upload the dataset to the AI Catalog\ndataset = dr.Dataset.upload(df)\n\n# Rename the entry in AI Catalog\ndataset.modify(name=\"Transactional_Sales_Data\", categories=dataset.categories)\n\n# Create a time series data prep query generator from the dataset we just uploaded to AI Catalog\nquery_generator_dataset = QueryGeneratorDataset(\n    alias=\"Transactional_Sales_Data\",\n    dataset_id=dataset.id,\n    dataset_version_id=dataset.version_id,\n)\n\n# Set the parameters for our time series Data Prep\nquery_generator_settings = QueryGeneratorSettings(\n    datetime_partition_column=\"Order Date\",  # Date/time feature used as the basis for partitioning\n    time_unit=\"MONTH\",  # Time unit (seconds, days, months, etc.) that comprise the time step\n    time_step=1,  # Number of (time) units that comprise the time step.\n    default_numeric_aggregation_method=\"sum\",  # Aggregate the target using either mean & most recent or sum & zero\n    default_categorical_aggregation_method=\"last\",  # Aggregate categorical features using the most frequent value or the last value within the aggregation time step.\n    target=\"Sales\",  # Numeric column in the dataset to predict.\n    multiseries_id_columns=[\n        \"Segment\"\n    ],  # Column containing the series identifier, which allows DataRobot to process the dataset as a separate time series.\n    default_text_aggregation_method=\"meanLength\",  # Choose ignore to skip handling of text features or aggregate by: 'concat', 'last', 'meanLength', 'mostFrequent', 'totalLength'\n    start_from_series_min_datetime=True,  # Basis for the series start date, either the earliest date for each series (per series) or the earliest date found for any series (global).\n    end_to_series_max_datetime=True,  # Basis for the series end date, either the last entry date for each series (per series) or the latest date found for any series (global).\n)\nquery_generator = dr.DataEngineQueryGenerator.create(\n    generator_type=\"TimeSeries\",\n    datasets=[query_generator_dataset],\n    generator_settings=query_generator_settings,\n)\n\n# Prep the training dataset\ntraining_dataset = query_generator.create_dataset()\n\n# Rename the entry in AI Catalog\ntraining_dataset.modify(\n    name=\"ts_monthly_training\", categories=training_dataset.categories\n)",
        "# Load the dataset into a pandas dataframe\ntraining_df = training_dataset.get_as_dataframe()\n\n# Convert 'Order Date' to datetime format and sort\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\n\n# Adding Total Sales as an additional segment\ntotal_sales = training_df.groupby(\"Order Date\").agg({\"Sales\": \"sum\"}).reset_index()\ntotal_sales[\"Segment\"] = \"Total\"\ntraining_df = pd.concat([training_df, total_sales], ignore_index=True)\n\n# Visualize our data:\nfig = go.Figure()\n\n# Line chart for monthly sales by segment\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment A\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment A\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment B\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment B\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Segment C\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Segment C\",\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=training_df[training_df[\"Segment\"] == \"Total\"][\"Order Date\"],\n        y=training_df[training_df[\"Segment\"] == \"Total\"][\"Sales\"],\n        mode=\"lines\",\n        name=\"Total\",\n    )\n)\n\nfig.update_layout(\n    title=\"Monthly Sales by Segment and Total\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    template=\"plotly_dark\",\n)\nfig.show()",
        "# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"Monthly_Sales_Forecast\", dataset_id=training_dataset.id\n)",
        "# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)",
        "# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-6, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 12)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = []\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Order Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Order Date\"]),\n)  # Last date in calendar",
        "# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Order Date\",  # Date column\n    multiseries_id_columns=[\"Segment\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)",
        "# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)",
        "# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)",
        "# Unlock the holdout set within the project\nproject.unlock_holdout()",
        "# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\ndf_list = []  # This will store each individual DataFrame to concatenate at the end\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    df_list.append(model_performances)  # Append the DataFrame to the list\n\n# Concatenate all DataFrames in the list\nscores = pd.concat(df_list, ignore_index=True)\n\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores",
        "# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)",
        "print(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)",
        "# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=1, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=1\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n)\n\n# Display the plot\n\n# Update layout\nfig.update_layout(template=\"plotly_dark\")\nfig.show()",
        "# Get Accuracy over Time for FD=6, For just Segment A\nacc_plot_FD6_Consumer = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=6, series_id=\"Segment A\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD6_Consumer.bins)\n\n# Plotly Graph\nfig = go.Figure()\n\n# Adding traces for \"predicted\" and \"actual\"\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"predicted\"], mode=\"lines\", name=\"Predicted\")\n)\nfig.add_trace(\n    go.Scatter(x=df[\"start_date\"], y=df[\"actual\"], mode=\"lines\", name=\"Actual\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Accuracy Over Time for FD=6 (Segment A)\",\n    xaxis_title=\"Start Date\",\n    yaxis_title=\"Value\",\n    legend_title=\"Legend\",\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()",
        "# get model\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][2])\n\n# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:5]\n\n# Plotly Graph\nfig = go.Figure()\n\n# Add bar trace\nfig.add_trace(\n    go.Bar(y=FI_df[\"featureName\"], x=FI_df[\"impactNormalized\"], orientation=\"h\")\n)\n\n# Update layout for better visualization\nfig.update_layout(\n    title=\"Feature Impact\",\n    xaxis=dict(title=\"Normalized Feature Impact\", range=[0, 1.1]),\n    yaxis=dict(title=\"Feature\", autorange=\"reversed\"),\n    margin=dict(\n        l=200\n    ),  # this is to ensure the y-labels (feature names) are not cut off\n    template=\"plotly_dark\",\n)\n\n# Display the plot\nfig.show()",
        "# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing",
        "# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight",
        "# Create a scatter plot with Plotly\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x=model_series_insight[\"Series\"],\n        y=model_series_insight[\"backtestingScore\"],\n        mode=\"markers\",\n    )\n)\n\n# Update the layout\nfig.update_layout(\n    title=\"Performance by Segment\",\n    xaxis=dict(title=\"Series\", tickangle=45),\n    yaxis=dict(title=\"RMSE\"),\n)\n\n# Display the plot\nfig.update_layout(template=\"plotly_dark\")\nfig.show()",
        "# Upload data to modeling project\ndf = training_dataset.get_as_dataframe()\ntest_dataset = project.upload_dataset(df)\n\n# Get frozen model\nfrozen_model = all_models[0]\n\n# Request Predictions\npred_job = frozen_model.request_predictions(\n    dataset_id=test_dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=85,\n)\n\npreds = pred_job.get_result_when_complete()\n\npreds.head(5)",
        "# Step 1: Rename columns in preds to match training_df\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Step 2: Drop columns from preds that are not in training_df\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]",
        "# Function to evaluate date time values\n\n\ndef convert_or_localize_to_utc(series):\n    if series.dt.tz is not None:  # If it's already timezone-aware\n        return series.dt.tz_convert(\"UTC\")\n    return series.dt.tz_localize(\"UTC\")  # If it's naive, then localize it\n\n\n# Convert or localize 'Order Date' in training_df to UTC\ntraining_df[\"Order Date\"] = pd.to_datetime(training_df[\"Order Date\"])\ntraining_df[\"Order Date\"] = convert_or_localize_to_utc(training_df[\"Order Date\"])\n\n# Rename columns in prediction data\nrenamed_preds = preds.rename(\n    columns={\"prediction\": \"Sales\", \"timestamp\": \"Order Date\", \"series_id\": \"Segment\"}\n)\n\n# Drop columns from prediction data that is not in training data\ncolumns_to_keep = [\n    \"Segment\",\n    \"Order Date\",\n    \"Sales\",\n]  # Columns from preds that correspond to training_df\nmodified_preds = renamed_preds[columns_to_keep]\n\n# Convert or localize 'Order Date' in preds_subset to UTC\npreds_subset = modified_preds.copy()  # To avoid SettingWithCopyWarning\npreds_subset[\"Order Date\"] = pd.to_datetime(preds_subset[\"Order Date\"])\npreds_subset[\"Order Date\"] = convert_or_localize_to_utc(preds_subset[\"Order Date\"])\n\n# Concatenate the DataFrames\ncombined_df = pd.concat([training_df, preds_subset], ignore_index=True)\n\n# Sort by Segment and Order Date\ncombined_df.sort_values(by=[\"Segment\", \"Order Date\"], inplace=True)",
        "# Initialize the figure\nfig = go.Figure()\n\n# Define a color mapping for segments (you can extend or modify this as needed)\ncolor_map = {\"Segment A\": \"blue\", \"Segment B\": \"green\", \"Segment C\": \"orange\"}\n\n# For each segment, plot actual sales and forecasted sales\nfor segment, color in color_map.items():\n    segment_df = combined_df[combined_df[\"Segment\"] == segment]\n    actuals = segment_df[:-12]  # Adjust based on your actual data\n    forecast = segment_df[-12:]\n\n    fig.add_trace(\n        go.Scatter(\n            x=actuals[\"Order Date\"],\n            y=actuals[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Actuals\",\n            line=dict(color=color),  # Use the color from the color_map\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=forecast[\"Order Date\"],\n            y=forecast[\"Sales\"],\n            mode=\"lines\",\n            name=f\"{segment} Forecast\",\n            line=dict(\n                dash=\"dot\", color=color\n            ),  # Use the color from the color_map and make the line dotted\n        )\n    )\n\nfig.update_layout(\n    title=\"Actual + Forecasted Sales by Segment\",\n    xaxis_title=\"Order Date\",\n    yaxis_title=\"Sales\",\n    legend_title=\"Segments\",\n    template=\"plotly_dark\",\n)\n\nfig.show()",
        "def calculate_growth(segment_data):\n    forecast_sum = segment_data.tail(12)[\"Sales\"].sum()\n    prior_year_sum = segment_data.iloc[-24:-12][\"Sales\"].sum()\n    two_years_ago_sum = segment_data.iloc[-36:-24][\"Sales\"].sum()\n\n    forecasted_growth = (forecast_sum - prior_year_sum) / prior_year_sum * 100\n    prior_year_growth = (prior_year_sum - two_years_ago_sum) / two_years_ago_sum * 100\n\n    return forecast_sum, prior_year_sum, forecasted_growth, prior_year_growth\n\n\nsegments = combined_df[\"Segment\"].unique()\ngrowth_data = [\n    (*calculate_growth(combined_df[combined_df[\"Segment\"] == segment]), segment)\n    for segment in segments\n]\n\ngrowth_df = pd.DataFrame(\n    growth_data,\n    columns=[\n        \"Forecasted Total\",\n        \"Prior Year Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n        \"Segment\",\n    ],\n)\n\n# Calculate aggregate level\naggregate_prior_year = growth_df[\"Prior Year Total\"].sum()\naggregate_forecast = growth_df[\"Forecasted Total\"].sum()\naggregate_growth = (\n    (aggregate_forecast - aggregate_prior_year) / aggregate_prior_year * 100\n)\n\nadjusted_prior_year_growth = (\n    (\n        aggregate_prior_year\n        - growth_df[\"Prior Year Total\"].sum()\n        + growth_df.iloc[-1][\"Prior Year Total\"]\n    )\n    / (growth_df[\"Prior Year Total\"].sum() - growth_df.iloc[-1][\"Prior Year Total\"])\n    * 100\n)\n\naggregate_row = pd.DataFrame(\n    [\n        {\n            \"Segment\": \"Total\",\n            \"Prior Year Total\": aggregate_prior_year,\n            \"Forecasted Total\": aggregate_forecast,\n            \"Forecasted Growth\": aggregate_growth,\n            \"Prior Year Growth\": adjusted_prior_year_growth,\n        }\n    ]\n)\n\ngrowth_df = pd.concat([growth_df, aggregate_row], ignore_index=True)",
        "def plot_sales_data_with_table(growth_df):\n    # Filter out the 'Total' row for the bar chart\n    chart_df = growth_df[growth_df[\"Segment\"] != \"Total\"]\n\n    # Create subplots: one row for bar chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.15,\n        subplot_titles=(\n            \"Prior Year vs Forecasted Sales Totals by Segment\",\n            \"Data Table\",\n        ),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Add bar chart to the first row of subplot\n    fig.add_trace(\n        go.Bar(\n            name=\"Prior Year Total\",\n            x=chart_df[\"Segment\"],\n            y=chart_df[\"Prior Year Total\"],\n        ),\n        row=1,\n        col=1,\n    )\n    forecasted_total_bar = go.Bar(\n        name=\"Forecasted Total\", x=chart_df[\"Segment\"], y=chart_df[\"Forecasted Total\"]\n    )\n    fig.add_trace(forecasted_total_bar, row=1, col=1)\n\n    # Overlay forecast growth on top of the Forecasted Total bars\n    for i, segment in enumerate(chart_df[\"Segment\"]):\n        fig.add_annotation(\n            x=segment,\n            y=chart_df.loc[chart_df[\"Segment\"] == segment, \"Forecasted Total\"].values[\n                0\n            ],\n            text=f\"{chart_df.loc[chart_df['Segment'] == segment, 'Forecasted Growth'].values[0]:.2f}%\",\n            showarrow=False,\n            font=dict(color=\"white\"),\n            row=1,\n            col=1,\n        )\n\n    # Change the bar mode to 'group'\n    fig.update_layout(barmode=\"group\")\n\n    # Specify the order of columns for the table\n    ordered_columns = [\n        \"Segment\",\n        \"Prior Year Total\",\n        \"Forecasted Total\",\n        \"Forecasted Growth\",\n        \"Prior Year Growth\",\n    ]\n\n    # Format the numeric values with commas and two decimal points\n    formatted_data = []\n    for col in ordered_columns:\n        if growth_df[col].dtype in [float, \"float64\"]:\n            # Format float columns\n            formatted_data.append(growth_df[col].map(\"{:,.2f}\".format).tolist())\n        elif growth_df[col].dtype in [int, \"int64\"]:\n            # Format integer columns\n            formatted_data.append(growth_df[col].map(\"{:,}\".format).tolist())\n        else:\n            # Keep non-numeric columns as is\n            formatted_data.append(growth_df[col].tolist())\n\n    table_header = ordered_columns\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=formatted_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\")\n\n    fig.show()\n\n\n# Example usage with your growth_df dataframe\nplot_sales_data_with_table(growth_df)",
        "# Create a copy of training_df with only the necessary columns\ndf_copy = combined_df[[\"Segment\", \"Order Date\", \"Sales\"]].copy()\n\n# Convert \"Order Date\" column of df_copy to datetime and set as index\ndf_copy[\"Order Date\"] = pd.to_datetime(df_copy[\"Order Date\"])\ndf_copy.set_index(\"Order Date\", inplace=True)\n\n\ndef calculate_growth_metrics(segment_data):\n    # Using .loc to set values to avoid SettingWithCopyWarning\n    segment_data = segment_data.copy()\n    segment_data.loc[:, \"Year\"] = segment_data.index.year\n    segment_data.loc[:, \"Month\"] = segment_data.index.month\n\n    # Group by year and month to get monthly sales and specify numeric_only=True\n    monthly_sales = segment_data.groupby([\"Year\", \"Month\"]).sum(numeric_only=True)\n\n    # Calculate growth metrics\n    monthly_sales[\"MoM Growth\"] = monthly_sales[\"Sales\"].pct_change() * 100\n    monthly_sales[\"Monthly YoY Growth\"] = (\n        monthly_sales[\"Sales\"].pct_change(periods=12) * 100\n    )\n    monthly_sales[\"Rolling 12 Mth YoY Growth\"] = (\n        monthly_sales[\"Sales\"].rolling(window=12).sum().pct_change(periods=12) * 100\n    )\n\n    # New YTD Growth logic\n    monthly_sales[\"YTD\"] = monthly_sales[\"Sales\"].groupby(level=0).cumsum()\n    previous_ytd = monthly_sales[\"YTD\"].shift(12)\n    monthly_sales[\"YTD YoY Growth\"] = (\n        (monthly_sales[\"YTD\"] - previous_ytd) / previous_ytd\n    ) * 100\n\n    # Convert 'Year' and 'Month' back to actual date (the first day of each month)\n    monthly_sales.reset_index(inplace=True)\n    monthly_sales[\"Order Date\"] = pd.to_datetime(\n        monthly_sales[[\"Year\", \"Month\"]].assign(DAY=1)\n    )\n    monthly_sales.drop([\"Year\", \"Month\"], axis=1, inplace=True)\n\n    return monthly_sales\n\n\n# Filter data based on segments and calculate metrics\nsegmenta_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment A\"])\nsegmentb_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment B\"])\nsegmentc_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Segment C\"])\ntotal_df = calculate_growth_metrics(df_copy[df_copy[\"Segment\"] == \"Total\"])",
        "# Function to plot a specific growth metric for all segments and show table below\n\n\ndef plot_metric_across_segments_with_table(metric_name, datasets, yaxis_range=None):\n    # Create subplots: one row for line chart, one row for table\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.1,\n        subplot_titles=(f\"Comparison of {metric_name} Across Segments\", \"Data Table\"),\n        row_heights=[0.7, 0.3],\n        specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}]],\n    )\n\n    # Lists to store filtered data for the table\n    table_dates = None\n    table_metric_values = []\n\n    # Add line plots to the first row of subplot\n    for segment_name, segment_df in datasets.items():\n        # Filter DataFrame to remove null values in the metric column and keep data from January 2014 onwards\n        segment_df = segment_df[\n            pd.notnull(segment_df[metric_name])\n            & (segment_df[\"Order Date\"] >= \"2014-01-01\")\n        ]\n\n        # Store filtered data for table\n        if table_dates is None:\n            table_dates = segment_df[\"Order Date\"].tolist()\n\n        # Format metric values to two decimal points\n        formatted_metric_values = [\n            round(val, 2) for val in segment_df[metric_name].tolist()\n        ]\n        table_metric_values.append(formatted_metric_values)\n\n        fig.add_trace(\n            go.Scatter(\n                x=segment_df[\"Order Date\"],\n                y=segment_df[metric_name],\n                mode=\"lines\",\n                name=segment_name,\n            ),\n            row=1,\n            col=1,\n        )\n\n    # Create a table with the filtered data\n    table_data = [table_dates] + table_metric_values\n    table_header = [\"Order Date\"] + list(datasets.keys())\n\n    fig.add_trace(\n        go.Table(header=dict(values=table_header), cells=dict(values=table_data)),\n        row=2,\n        col=1,\n    )\n\n    # Update layout\n    fig.update_layout(template=\"plotly_dark\", yaxis_range=yaxis_range)\n\n    fig.show()\n\n\n# Datasets for all segments\ndatasets = {\n    \"Segment A\": segmenta_df,\n    \"Segment B\": segmentb_df,\n    \"Segment C\": segmentc_df,\n    \"Total\": total_df,\n}\n\n# Create the plots for each specific metric:\nplot_metric_across_segments_with_table(\"MoM Growth\", datasets, yaxis_range=[-200, 500])\nplot_metric_across_segments_with_table(\n    \"Monthly YoY Growth\", datasets, yaxis_range=[-200, 600]\n)\nplot_metric_across_segments_with_table(\n    \"YTD YoY Growth\", datasets, yaxis_range=[-200, 300]\n)\nplot_metric_across_segments_with_table(\"Rolling 12 Mth YoY Growth\", datasets)",
        "# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=frozen_model.id,\n    label=\"FP&A - Segment Forecasting Model\",\n    description=\"FP&A - Segment Forecasting Model\",\n    default_prediction_server_id=prediction_server_id,\n)",
        "# Score the dataset using the given deployment ID\njob, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, df\n)  # Deployment ID and Scoring dataset\n\n# Print a message to indicate that scoring has started\nprint(\"Started scoring...\", job)\n\n# Wait for the job to complete\njob.wait_for_completion()",
        "# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\ndeployment.delete()\n# project.delete()"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "project.wait_for_autopilot",
        "model.metrics",
        "project.get_datetime_models",
        "model.get_series_accuracy_as_dataframe",
        "deployment.create_from_learning_model",
        "dr.dataenginequerygenerator.create",
        "dr.dataset.upload",
        "model.request_predictions",
        "project.get_uri",
        "model.id",
        "dr.predictionserver.list",
        "project.project_name",
        "datarobot.models.data_engine_query_generator",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "dr.batchpredictionjob.score_pandas",
        "model.compute_series_accuracy",
        "project.upload_dataset",
        "project.unlock_holdout",
        "deployment.delete",
        "deployment.id",
        "project.delete",
        "model.get_uri",
        "model.get",
        "dr.calendarfile.create_calendar_from_country_code",
        "dr.model.get",
        "model.model_type",
        "project.analyze_and_model",
        "project.metric",
        "model.get_accuracy_over_time_plot",
        "project.create_from_dataset",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "business_use_case"
    },
    {
      "id": "ai_accelerator_5347305789099697250",
      "title": "AWS_Athena_template: AWS_Athena_End_to_End.config.yaml",
      "content": "---\ndescription: \"This example notebook outlines how to read in an Amazon Athena table and upload it to DataRobot's\\\n  \\ AI Catalog.\\nThen, using this dataset, we'll create a project, deploy the top performing model to\\\n  \\ a DataRobot prediction server, and make batch predictions with a test dataset.\"\nfile_name: Amazon_S3_End_to_End.ipynb\nlanguages:\n  - python\nauthors:\n  - Biju Krishnan\n  - João Gomes\nauthors_email:\n  - biju.krishnan@datarobot.com\n  - joao@datarobot.com\nsmoke_test:\n  run_smoke_test: false\nmaintainers:\n  - João Gomes\nmaintainers_email:\n  - joao@datarobot.com\ntags: []\ntitle: DataRobot AutoML end-to-end with Amazon Athena\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_Athena_template/AWS_Athena_End_to_End.config.yaml",
      "tags": [
        "integration",
        "athena",
        "ai-accelerators",
        "aws",
        "predictions",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_Athena_template",
        "file_type": "yaml",
        "size": 647
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.44999999999999996,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_6721331318240433161",
      "title": "AWS_S3_template: Amazon_S3_End_to_End.config.yaml",
      "content": "---\ndescription: \"This example notebook outlines how to read in PARQUET files from an Amazon S3 bucket into\\\n  \\ a pandas dataframe using AWS Wrangler Python library and upload it to DataRobot's AI Catalog.\\nThen,\\\n  \\ using this dataset, we'll create a project, deploy the top performing model to a DataRobot prediction\\\n  \\ server, and make batch predictions with a test dataset.\"\nfile_name: Amazon_S3_End_to_End.ipynb\nlanguages:\n  - python\nauthors:\n  - Biju Krishnan\n  - João Gomes\nauthors_email:\n  - biju.krishnan@datarobot.com\n  - joao@datarobot.com\nsmoke_test:\n  run_smoke_test: false\nmaintainers:\n  - João Gomes\nmaintainers_email:\n  - joao@datarobot.com\ntags: []\ntitle: DataRobot AutoML end-to-end with Amazon Athena\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_S3_template/Amazon_S3_End_to_End.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "predictions",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_S3_template",
        "file_type": "yaml",
        "size": 726
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.44999999999999996,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-4662061732685148295",
      "title": "AWS_S3_template: Amazon_S3_End_to_End.ipynb",
      "content": "<center><H1>End to End DataRobot AutoML workflow with Amazon S3</H1></center>\n\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<td><img src=\"https://www.datarobot.com/wp-content/uploads/2021/08/DataRobot-logo-color.svg\" height=200px width=200px>\n</td>\n<td><font size=10> + </font> </td>\n<td> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Amazon-S3-Logo.svg/1712px-Amazon-S3-Logo.svg.png\" height=100px width=100px> </td>\n\nAuthor: Biju Krishnan\n\n[API reference documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n\n<font>\nThis example notebook outlines the following tasks: <p>\n<ol>\n<li> Read PARQUET files from an Amazon S3 bucket into a pandas dataframe using AWS Wrangler Python library </li>\n<li> Upload a dataset in a dataframe to DataRobot's AI Catalog </li>\n<li> Initiate a DataRobot AutoML project with the dataset</li>\n<li> Deploy the top performing model to a DataRobot prediction server. </li>\n<li> Make batch predictions with a test dataset. </li>\n</ol>\n<p>\nThe files stored in S3 used for training can be in any format supported by the AWS Wrangler Python library. For batch predictions, DataRobot supports Parquet and CSV.\n</font>\n\n## Setup\n\n### Import libraries\n\n```python\nfrom io import StringIO\n\nimport awswrangler as wr  # This notebooks uses AWS Wrangler because its easy to read multiple files from the S3 bucket\nimport boto3\nimport datarobot as dr\nimport pandas as pd\n```\n\n### Bind variables\n\n```python\n# Bind variables\n# These variables can aso be fetched from a secret store or config files\n\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-14\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret\n```\n\n### Connect to DataRobot\n\nYou can read more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n```python\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n```\n\n```python\n# Instantiate a BOTO3 connection for connection to AWS\n# This session will be used in the next cell to read files from S3\n\nmy_session = boto3.Session(\n    aws_access_key_id=AWS_KEY,\n    aws_secret_access_key=AWS_SECRET,\n    # aws_session_token = <Optional>\n)\n```\n\n## Import data\n\n<font>\n<p>\nFor illustration purposes, the training dataset containing patient visits to a hospital is stored in an S3 bucket named e2eaccelerator09122022 under the path <code>s3://e2eaccelerator09122022/training/input/</code> .\n<pre><code><font color=grey size=1>\naws s3 ls s3://e2eaccelerator09122022/training/input/\n2022-12-09 09:55:47          0\n2022-12-09 09:56:15     267017 10k_diabetes.parquet\n</font></code></pre>\n<p>\nThe input folder contains only one file in this scenario, however the code will also work in case of multiple files.\n</font>\n\n```python\n# Read parquet files from an S3 bucket into a pandas dataframe using AWS Wrangler\n\ns3_training_input = \"s3://e2eaccelerator09122022/training/input/\"\ndf = wr.s3.read_parquet(path=s3_training_input, dataset=True, boto3_session=my_session)\n# Specifying dataset=True allows reading multiple files\ndf.head()\n```\n\n### Create a dataset\n\nCreate a dataset in the AI Catalog to use it for project creation.\n\n```python\ndatarobot_dataset = dr.Dataset.create_from_in_memory_data(\n    data_frame=df, fname=\"10K diabetes E2E accelerator\"\n)\ndatarobot_dataset.id\n```\n\n### Create a project and initiate Autopilot\n\n```python\n# This cell will take several minutes to complete execution\n# Creates an AutoML project named \"E2E Demo Amazon S3\" with \"readmitted\" as the target column\n# Quick mode is the designated training mode in this example, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon S3\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"readmitted\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)\n```\n\nOnce the AutoML project is complete, select the top-performing model on the Leaderboard based on the chosen metric for deployment.\n\n```python\ndef sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)\n```\n\n### Deploy a model\n\nNote that steps in the following sections require DataRobot MLOps licensed features. Contact your DataRobot account representatives if you are missing some licensed MLOps features.\n\n```python\n# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon S3 Test\",\n    description=\"Model trained on 10k diabetes dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id\n```\n\n### Make predictions\n\n<font family=verdana>\nDataRobot's batch predictions API is capable of directly reading and writing to Amazon S3 storage. \n<p>\n<i>Note: Parquet support for batch predictions is still in preview mode. Contact your DataRobot representative to enable the feature flags for trial.</i>\n</font>\n\n```python\n# To run a batch prediction job you need to store the AWS Credentials in the DataRobot credentials manager\n# The AWS key and secret should be unique\n# If they are already stored in the Credentials manager this code will throw an error\n\nDR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    credential_id = credential.credential_id\n\nprint(credential_id)\n```\n\n### Batch predictions snippet\n\nThe snippet below provides sample code to demonstratehow to make batch predictions to and from Amazon S3\n\n```python\ndr.BatchPredictionJob._s3_settings = dr.BatchPredictionJob._s3_settings.allow_extra(\"*\")\n\n# Use the manipulated batch job class to score:\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"csv\",  # Can also be Parquet\n        \"url\": \"s3://e2eaccelerator09122022/predictions/input/10k_diabetes_test.csv\",  ## This can be a path or a file depending on the format chosen\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"parquet\",  # Can also be CSV\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/10k_diabetes_test.parquet\",  ## This should point to a file not a path\n    },\n)\n\njob.wait_for_completion()\njob.get_status()\n```\n\n<font family=verdana>\n<p>\nThe output of the batch predictions is thus available under the path s3://e2eaccelerator09122022/predictions/output/\n<pre><code><font color=grey size=1>\naws s3 ls s3://e2eaccelerator09122022/predictions/output/\n2022-12-09 11:35:32          0\n2022-12-09 14:09:28      21244 10k_diabetes_test.parquet\n</font></code></pre>\n</font>\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_S3_template/Amazon_S3_End_to_End.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_S3_template",
        "file_type": "notebook",
        "size": 13146
      },
      "code_examples": [
        "from io import StringIO\n\nimport awswrangler as wr  # This notebooks uses AWS Wrangler because its easy to read multiple files from the S3 bucket\nimport boto3\nimport datarobot as dr\nimport pandas as pd",
        "# Bind variables\n# These variables can aso be fetched from a secret store or config files\n\nDATAROBOT_ENDPOINT = \"https://app.eu.datarobot.com/api/v2\"\n# The URL may vary depending on your hosting preference, the above example is for DataRobot EU Managed AI Cloud\n\nDATAROBOT_API_TOKEN = \"<INSERT YOUR DataRobot API Token>\"\n# The API Token can be found by click the avatar icon and then </> Developer Tools\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AWS-14\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\nAWS_KEY = \"<INSERT YOUR AWS ACCESS KEY>\"  # Enter your AWS Key ID\nAWS_SECRET = \"<INSERT YOUR AWS SECRETS>\"  # Enter your AWS Secret",
        "dr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)",
        "# Instantiate a BOTO3 connection for connection to AWS\n# This session will be used in the next cell to read files from S3\n\nmy_session = boto3.Session(\n    aws_access_key_id=AWS_KEY,\n    aws_secret_access_key=AWS_SECRET,\n    # aws_session_token = <Optional>\n)",
        "# Read parquet files from an S3 bucket into a pandas dataframe using AWS Wrangler\n\ns3_training_input = \"s3://e2eaccelerator09122022/training/input/\"\ndf = wr.s3.read_parquet(path=s3_training_input, dataset=True, boto3_session=my_session)\n# Specifying dataset=True allows reading multiple files\ndf.head()",
        "datarobot_dataset = dr.Dataset.create_from_in_memory_data(\n    data_frame=df, fname=\"10K diabetes E2E accelerator\"\n)\ndatarobot_dataset.id",
        "# This cell will take several minutes to complete execution\n# Creates an AutoML project named \"E2E Demo Amazon S3\" with \"readmitted\" as the target column\n# Quick mode is the designated training mode in this example, however other modes are also available\n\n\nEXISTING_PROJECT_ID = (\n    None  # If you've already created a project, replace None with the ID here\n)\n\nif EXISTING_PROJECT_ID is None:\n    # Create project and pass in data\n    project = dr.Project.create_from_dataset(\n        datarobot_dataset.id, project_name=\"E2E Demo Amazon S3\"\n    )\n\n    # Set the project target to the appropriate feature. Use the LogLoss metric to measure performance\n    project.analyze_and_model(\n        target=\"readmitted\", mode=dr.AUTOPILOT_MODE.QUICK, worker_count=\"-1\"\n    )\nelse:\n    # Fetch the existing project\n    project = dr.Project.get(EXISTING_PROJECT_ID)\n\nproject.wait_for_autopilot(check_interval=30)",
        "def sorted_by_metric(models, test_set, metric):\n    models_with_score = [\n        model for model in models if model.metrics[metric][test_set] is not None\n    ]\n\n    return sorted(models_with_score, key=lambda model: model.metrics[metric][test_set])\n\n\nmodels = project.get_models()\n\nmetric = project.metric\n\n# Get the top-performing model\nmodel_top = sorted_by_metric(models, \"crossValidation\", metric)[0]\n\nprint(\n    \"\"\"The top performing model is {model} using metric, {metric}\"\"\".format(\n        model=str(model_top), metric=metric\n    )\n)",
        "# Get the prediction server\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create a deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    model_top.id,\n    label=\"E2E Amazon S3 Test\",\n    description=\"Model trained on 10k diabetes dataset\",\n    default_prediction_server_id=prediction_server.id,\n)\ndeployment.id",
        "# To run a batch prediction job you need to store the AWS Credentials in the DataRobot credentials manager\n# The AWS key and secret should be unique\n# If they are already stored in the Credentials manager this code will throw an error\n\nDR_CREDENTIAL_NAME = \"AWS S3 Credentials\"  # Choose a name as per your convenience\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        cred_flag = False\n        credential_id = cred.credential_id\n        break\n    else:\n        cred_flag = True\n\nif cred_flag:\n    credential = dr.Credential.create_s3(\n        name=DR_CREDENTIAL_NAME,\n        aws_access_key_id=AWS_KEY,\n        aws_secret_access_key=AWS_SECRET,\n        # aws_session_token= <Optional>\n    )\n    credential_id = credential.credential_id\n\nprint(credential_id)",
        "dr.BatchPredictionJob._s3_settings = dr.BatchPredictionJob._s3_settings.allow_extra(\"*\")\n\n# Use the manipulated batch job class to score:\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"csv\",  # Can also be Parquet\n        \"url\": \"s3://e2eaccelerator09122022/predictions/input/10k_diabetes_test.csv\",  ## This can be a path or a file depending on the format chosen\n    },\n    output_settings={\n        \"type\": \"s3\",\n        \"credential_id\": credential_id,\n        \"format\": \"parquet\",  # Can also be CSV\n        \"url\": \"s3://e2eaccelerator09122022/predictions/output/10k_diabetes_test.parquet\",  ## This should point to a file not a path\n    },\n)\n\njob.wait_for_completion()\njob.get_status()"
      ],
      "api_methods": [
        "project.wait_for_autopilot",
        "model.metrics",
        "deployment.create_from_learning_model",
        "dr.client._global_client",
        "dr.autopilot_mode.quick",
        "dr.credential.list",
        "dr.predictionserver.list",
        "dr.batchpredictionjob._s3_settings",
        "dr.deployment.create_from_learning_model",
        "dr.credential.create_s3",
        "project.get",
        "deployment.id",
        "dr.dataset.create_from_in_memory_data",
        "dr.batchpredictionjob.score",
        "dr.project.get",
        "project.analyze_and_model",
        "project.metric",
        "project.create_from_dataset",
        "dr.project.create_from_dataset",
        "project.get_models"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-3972181717812864277",
      "title": "AWS_monitor_sagemaker_model_in_DataRobot: AWS_SageMaker_DataRobot_MLOps.config.yaml",
      "content": "---\ndescription: This notebook walks you through the steps to train and host a SageMaker model that can be\n  monitored in the DataRobot platform.\nfile_name: AWS_SageMaker_DataRobot_MLOps.ipynb\nlanguages:\n  - python\nmaintainers:\n  - Oleksandr Saienko\n  - Mao Shun (AWS)\nmaintainers_email:\n  - oleksandr.saienko@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: Integrate Amazon SageMaker with DataRobot MLOps\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_monitor_sagemaker_model_in_DataRobot/AWS_SageMaker_DataRobot_MLOps.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_monitor_sagemaker_model_in_DataRobot",
        "file_type": "yaml",
        "size": 424
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.4,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_2987579447386754782",
      "title": "AWS_monitor_sagemaker_model_in_DataRobot: AWS_SageMaker_DataRobot_MLOps.ipynb",
      "content": "# Integrate Amazon SageMaker with DataRobot MLOps\n\nAuthors: Oleksandr Saienko, DataRobot\nMao Shun, AWS\n\nVersion 1.3 (04/03/2022)\n\nWith Amazon SageMaker, you can package your own algorithms that can than be trained and deployed in the SageMaker environment. [DataRobot MLOps](https://docs.datarobot.com/en/docs/mlops/index.html) monitoring provides service health, data drift, accuracy monitoring, reports, and alerts about machine learning performance. This notebook is modified based on a [SageMaker example notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb) to show integration capabilities of DataRobot MLOps. \n\nTo integrate with SageMaker, you must first build and register a SageMaker container.\n\nThe README demonstrates how to build a custom SageMaker container in your local environment by including custom Python libraries for both training and inference. The README also includes DataRobot-related libraries useful for model monitoring.\n\nAdditionally, modify the Dockerfile as you need and follow the command instructions.\n\nOnce you have your container packaged, you can use it to train models and use the model for hosting.\n\nDataRobot recommends running the cells below in a SageMaker notebook instance for simplicity. If you want to run it locally, some settings need be added.\n\n## Setup\n\nSpecify a bucket to use and the role used when working with SageMaker.\n\n```python\n# S3 prefix\nprefix = \"DEMO-scikit-byo-iris-v3\"\n\nimport json\nimport os\nimport re\n\n# Define IAM role\nimport boto3\nimport numpy as np\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)\n```\n\n### Create a session\n\nThe session remembers your connection parameters to SageMaker. Use it to perform all of the SageMaker operations.\n\n```python\nfrom time import gmtime, strftime\n\nimport sagemaker as sage\n\nsess = sage.Session()\n```\n\n### Import data\n\nThis example workflow uses the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) and is included in the notebook folder.\n\nUse the tools provided by the SageMaker Python SDK to upload the data to a default bucket. \n\n```python\nWORK_DIRECTORY = \"data\"\n\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n```\n\n## Create an estimator and fit the model\n\nIn order to use SageMaker to fit your algorithm, create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n\n* The __container name__. This is constructed in the shell commands above.\n* The __role__. Defined above.\n* The __instance count__ is the number of machines to use for training.\n* The __instance type__ is the type of machine to use for training.\n* The __output path__ determines where the model artifact is written.\n* The __session__ is the SageMaker session object that you defined above.\n\nUse fit() on the estimator to train against the data uploaded above.\n\n```python\naccount = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\nregion = sess.boto_session.region_name\nimage = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-datarobot-decision-trees:latest\".format(\n    account, region\n)\n\nprint(image)\n\nprint(\"data_location\")\nprint(data_location)\n\ntree = sage.estimator.Estimator(\n    image,\n    role,\n    1,\n    \"ml.c4.2xlarge\",\n    output_path=\"s3://{}/output\".format(sess.default_bucket()),\n    sagemaker_session=sess,\n)\n\ntree.fit(data_location)\n```\n\n## Configure DataRobot MLOps\n\nBefore proceeding with the workflow, install a pip package in the current kernel.\n\n```python\nimport sys\n\n# installing DataRobot MLOps client\n!{sys.executable} -m pip install datarobot-mlops\n\n# installing mlops-cli tool\n!{sys.executable} -m pip install datarobot-mlops-connected-client\n```\n\n### Connect to DataRobot\n\nTo use the DataRobot API, you first need to [create an API key](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#create-a-datarobot-api-key).\n\nThen, add `MLOPS_SERVICE_URL` and `MLOPS_API_TOKEN` as environment variables.\n\n```python\n%env MLOPS_SERVICE_URL=https://app.datarobot.com\n# PUT Your DataRobot API Key here:\n%env MLOPS_API_TOKEN=PUT_YOUR_API_TOKEN\n```\n\n### Upload a training dataset to DataRobot AI Catalog\n\nIn the UI, you can [import a dataset via the AI catalog](https://app.datarobot.com/docs/data/ai-catalog/catalog.html#add-data).\n\nAlternatively, you can use `mlops-cli` from the command line as demonstrated in the cells below.\n\n```python\n%%capture cap --no-stderr\n# ^^^^ Just to catch mlops-cli commands output to process it programmatically, comment it for cell output\n# Load the training dataset using mlops-cli,\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n!mlops-cli dataset upload --input \"data/iris_with_header.csv\" --timeout 600 --json --quiet\n```\n\n```python\n# Output of this command will contain uploaded Dataset ID in 'id' field that needs to be used on the next steps:\nprint(cap.stdout)\nif \"ERROR\" not in cap.stdout:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Training dataset uploaded successfully, TRAINING_DATASET_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"TRAINING_DATASET_ID\"] = stdout_json[\"id\"]\nelse:\n    # Print output of mlops-cli in case of error:\n    print(\"Training dataset uploading failed:\")\n    print(cap.stdout)\n```\n\n## Create a model package\n\nIn the UI, you can view existing model packages or add a new one by navigating to [**Model Registry > Model Packages**](https://app.datarobot.com/docs/mlops/deployment/registry/reg-create.html#create-model-packages).\n\nAlternatively, you can use `mlops-cli` as shown in the following cells:\n\n```python\nMODEL_PACKAGE_NAME = \"SageMaker_MLOps_Demo_v2\"\n\n# Set model type\nprediction_type = \"Multiclass\"\n# Set traget column\nmodel_target = \"variety\"\n# Set traget classes\nclass_names = [\"setosa\", \"versicolor\", \"virginica\"]\n\nmodel_config = {\n    \"name\": MODEL_PACKAGE_NAME,\n    \"modelDescription\": {\n        \"modelName\": \"Iris classification model\",\n        \"description\": \"Classification on iris dataset\",\n    },\n    \"target\": {\n        \"type\": prediction_type,\n        \"name\": model_target,\n        \"classNames\": class_names,\n    },\n}\n\n# write model configuration json to a file:\nwith open(\"demo_model.json\", \"w\") as model_json_file:\n    model_json_file.write(json.dumps(model_config, indent=4))\n```\n\n```python\n%%capture cap --no-stderr\n# Create model package\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n# Using Dataset ID from previouse step as a training-dataset-id argument:\n!mlops-cli model create --json-config \"demo_model.json\" --training-dataset-id $TRAINING_DATASET_ID  --json --quiet\n# Output of this command will contain json with created model package ID that needs to be used on the next steps:\n```\n\n```python\nprint(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # catch Model Package ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model package created successfully, MODEL_PACKAGE_ID=\" + stdout_json[\"id\"])\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"MODEL_PACKAGE_ID\"] = stdout_json[\"id\"]\n    # set Model Package ID corresponding variable:\n    model_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model package creation failed:\")\n    print(cap.stdout)\n```\n\n## Create a DataRobot prediction environment\n\nModels that run on your own infrastructure (outside of DataRobot) may be run in different environments and can have differing deployment permissions and approval processes. \nTo deploy models on external infrastructure, you need create a custom external prediction environment using the UI or the DataRobot API and copying the prediction environment ID.\n\nFor more information, reference the documentation for [creating external prediction environments](https://app.datarobot.com/docs/mlops/deployment/ext-model-prep/pred-env.html).\n\nTo create a prediction environment from `mlops-cli` you can use the following cells:\n\n```python\n# Create a  configuration\ndemo_pe_config = {\n    \"name\": \"MLOps SageMaker Demo v2\",\n    \"description\": \"AWS Sagemaker DataRobot MLOps Demo\",\n    \"platform\": \"aws\",\n    \"supportedModelFormats\": [\"externalModel\"],\n}\n\n# Write the configuration json to a file\nwith open(\"demo_pe.json\", \"w\") as demo_pe_file:\n    demo_pe_file.write(json.dumps(demo_pe_config, indent=4))\n```\n\n```python\n%%capture cap --no-stderr\n# Used to catch mlops-cli commands output\n# Run this only once, or at least clean up after so you don't end up with a lot of deployments\n!mlops-cli prediction-environment create --json-config \"demo_pe.json\"  --json --quiet\n# The output of this command will contain a prediction environment ID that is required in the following cells\n```\n\n```python\n# The output of this command contains a prediction environment ID that is required in the following cells\nprint(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch prediction environment corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Prediction environment created successfully, PREDICTION_ENVIRONMENT_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Set the PREDICTION_ENVIRONMENT_ID environment variable to use it in the following cells\n    os.environ[\"PREDICTION_ENVIRONMENT_ID\"] = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Prediction environment creation failed:\")\n    print(cap.stdout)\n```\n\n```python\n%%capture cap --no-stderr\n# In the UI, you can create a deployment from a model package under Model Registry -> {model package} -> Deployments.\n# Set --deployment-label with name that you choose\n# --model-package-id from previous step\n# --prediction-environment-id from previous step\n!mlops-cli model deploy --model-package-id $MODEL_PACKAGE_ID --prediction-environment-id $PREDICTION_ENVIRONMENT_ID --deployment-label \"SageMaker_MLOps_Demo\"  --json --quiet\n```\n\n```python\n# The output of this command contains a deployment ID that is required in the following cells\nprint(cap.stdout)  # Used to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch deployment ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model deployment created successfully, DEPLOYMENT_ID=\" + stdout_json[\"id\"])\n    # Set the deployment_id variable to use it in the next steps:\n    deployment_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model deployment creation failed:\")\n    print(cap.stdout)\n```\n\n## Create an SQS queue as a spooler channel\n\nThe MLOps library communicates to the MLOps agent using a spooler. This workflow uses AWS SQS as a spooler channel, more details how to create SQS queue:\nYou can read more about how to [create an SQS queue using the cloud console UI](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/step-create-queue.html) or [by using the AWS CLI](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sqs/create-queue.html).\n\n```python\n#!aws sqs create-queue --queue-name datarobot-mlops-demo-v2\n# MLOps spooler channel SQS queue\n# Put your SQS queue URL here:\nMLOPS_SQS_QUEUE = \"https://sqs.us-east-1.amazonaws.com/12345678/aws-mlops-blogpost-demo\"\n```\n\n## Host your model\n\nYou can use a trained model to get real time predictions using an HTTP endpoint.\n\n### Deploy the model\n\nDeploying a model to SageMaker hosting just requires a `deploy` call with the fitted model. This call requires an instance count, instance type, and optional serializer and deserializer functions. These functions are used when the resulting predictor is created on the endpoint.\n\n```python\nimport json\n\nfrom sagemaker.serializers import CSVSerializer\n\n# Pass all required environment variables to the SageMaker deployment\nenv_vars = {\n    \"MLOPS_DEPLOYMENT_ID\": deployment_id,\n    \"MLOPS_MODEL_ID\": model_id,\n    \"MLOPS_SQS_QUEUE\": MLOPS_SQS_QUEUE,\n    \"prediction_type\": prediction_type,\n    \"CLASS_NAMES\": json.dumps(class_names),\n}\n\nprint(env_vars)\n\npredictor = tree.deploy(1, \"ml.m4.xlarge\", serializer=CSVSerializer(), env=env_vars)\n```\n\n### Get prediction data\n\nIn order to make predictions, extract the data used for training to make predictions against it. This is is strictly for demo purposes as it is bad statistical practice. However it is a good demonstration of how the mechanism works.\n\n```python\nshape = pd.read_csv(\"data/iris_with_header.csv\", header=None)\nshape.sample(3)\n```\n\n```python\n# Drop the label column in the training set\nshape.drop(shape.columns[[0]], axis=1, inplace=True)\nshape.sample(3)\n```\n\n```python\nimport itertools\n\na = [50 * i for i in range(3)]\nb = [40 + i for i in range(10)]\nindices = [i + j for i, j in itertools.product(a, b)]\n\ntest_data = shape.iloc[indices[:-1]]\n\ntest_data\n```\n\nMaking prediction is as easy as calling `predict` with the predictor you get back from `deploy` and the data you want to make predictions with. The serializers convert the data for you.\n\n```python\nimport io\n\nprint(test_data)\nout = io.StringIO()\npd.DataFrame(test_data).to_csv(out, header=True, index=False)\nprint(predictor.predict(out.getvalue()).decode(\"utf-8\"))\n```\n\n### Cleanup\n\nOptional. When you're done with the endpoint, you can clean it up.\n\n```python\nsess.delete_endpoint(predictor.endpoint)\n```",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_monitor_sagemaker_model_in_DataRobot/AWS_SageMaker_DataRobot_MLOps.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_monitor_sagemaker_model_in_DataRobot",
        "file_type": "notebook",
        "size": 42513
      },
      "code_examples": [
        "# S3 prefix\nprefix = \"DEMO-scikit-byo-iris-v3\"\n\nimport json\nimport os\nimport re\n\n# Define IAM role\nimport boto3\nimport numpy as np\nimport pandas as pd\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)",
        "from time import gmtime, strftime\n\nimport sagemaker as sage\n\nsess = sage.Session()",
        "WORK_DIRECTORY = \"data\"\n\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)",
        "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\nregion = sess.boto_session.region_name\nimage = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-datarobot-decision-trees:latest\".format(\n    account, region\n)\n\nprint(image)\n\nprint(\"data_location\")\nprint(data_location)\n\ntree = sage.estimator.Estimator(\n    image,\n    role,\n    1,\n    \"ml.c4.2xlarge\",\n    output_path=\"s3://{}/output\".format(sess.default_bucket()),\n    sagemaker_session=sess,\n)\n\ntree.fit(data_location)",
        "import sys\n\n# installing DataRobot MLOps client\n!{sys.executable} -m pip install datarobot-mlops\n\n# installing mlops-cli tool\n!{sys.executable} -m pip install datarobot-mlops-connected-client",
        "%env MLOPS_SERVICE_URL=https://app.datarobot.com\n# PUT Your DataRobot API Key here:\n%env MLOPS_API_TOKEN=PUT_YOUR_API_TOKEN",
        "%%capture cap --no-stderr\n# ^^^^ Just to catch mlops-cli commands output to process it programmatically, comment it for cell output\n# Load the training dataset using mlops-cli,\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n!mlops-cli dataset upload --input \"data/iris_with_header.csv\" --timeout 600 --json --quiet",
        "# Output of this command will contain uploaded Dataset ID in 'id' field that needs to be used on the next steps:\nprint(cap.stdout)\nif \"ERROR\" not in cap.stdout:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Training dataset uploaded successfully, TRAINING_DATASET_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"TRAINING_DATASET_ID\"] = stdout_json[\"id\"]\nelse:\n    # Print output of mlops-cli in case of error:\n    print(\"Training dataset uploading failed:\")\n    print(cap.stdout)",
        "MODEL_PACKAGE_NAME = \"SageMaker_MLOps_Demo_v2\"\n\n# Set model type\nprediction_type = \"Multiclass\"\n# Set traget column\nmodel_target = \"variety\"\n# Set traget classes\nclass_names = [\"setosa\", \"versicolor\", \"virginica\"]\n\nmodel_config = {\n    \"name\": MODEL_PACKAGE_NAME,\n    \"modelDescription\": {\n        \"modelName\": \"Iris classification model\",\n        \"description\": \"Classification on iris dataset\",\n    },\n    \"target\": {\n        \"type\": prediction_type,\n        \"name\": model_target,\n        \"classNames\": class_names,\n    },\n}\n\n# write model configuration json to a file:\nwith open(\"demo_model.json\", \"w\") as model_json_file:\n    model_json_file.write(json.dumps(model_config, indent=4))",
        "%%capture cap --no-stderr\n# Create model package\n# we are using --json --quiet options here to catch command output as a json to process it programmatically\n# if you need text output you can use --terse option\n# Using Dataset ID from previouse step as a training-dataset-id argument:\n!mlops-cli model create --json-config \"demo_model.json\" --training-dataset-id $TRAINING_DATASET_ID  --json --quiet\n# Output of this command will contain json with created model package ID that needs to be used on the next steps:",
        "print(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # catch Model Package ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model package created successfully, MODEL_PACKAGE_ID=\" + stdout_json[\"id\"])\n    # Setting TRAINING_DATASET_ID env variable to use it in the next steps:\n    os.environ[\"MODEL_PACKAGE_ID\"] = stdout_json[\"id\"]\n    # set Model Package ID corresponding variable:\n    model_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model package creation failed:\")\n    print(cap.stdout)",
        "# Create a  configuration\ndemo_pe_config = {\n    \"name\": \"MLOps SageMaker Demo v2\",\n    \"description\": \"AWS Sagemaker DataRobot MLOps Demo\",\n    \"platform\": \"aws\",\n    \"supportedModelFormats\": [\"externalModel\"],\n}\n\n# Write the configuration json to a file\nwith open(\"demo_pe.json\", \"w\") as demo_pe_file:\n    demo_pe_file.write(json.dumps(demo_pe_config, indent=4))",
        "%%capture cap --no-stderr\n# Used to catch mlops-cli commands output\n# Run this only once, or at least clean up after so you don't end up with a lot of deployments\n!mlops-cli prediction-environment create --json-config \"demo_pe.json\"  --json --quiet\n# The output of this command will contain a prediction environment ID that is required in the following cells",
        "# The output of this command contains a prediction environment ID that is required in the following cells\nprint(cap.stdout)  # Just to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch prediction environment corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\n        \"Prediction environment created successfully, PREDICTION_ENVIRONMENT_ID=\"\n        + stdout_json[\"id\"]\n    )\n    # Set the PREDICTION_ENVIRONMENT_ID environment variable to use it in the following cells\n    os.environ[\"PREDICTION_ENVIRONMENT_ID\"] = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Prediction environment creation failed:\")\n    print(cap.stdout)",
        "%%capture cap --no-stderr\n# In the UI, you can create a deployment from a model package under Model Registry -> {model package} -> Deployments.\n# Set --deployment-label with name that you choose\n# --model-package-id from previous step\n# --prediction-environment-id from previous step\n!mlops-cli model deploy --model-package-id $MODEL_PACKAGE_ID --prediction-environment-id $PREDICTION_ENVIRONMENT_ID --deployment-label \"SageMaker_MLOps_Demo\"  --json --quiet",
        "# The output of this command contains a deployment ID that is required in the following cells\nprint(cap.stdout)  # Used to check mlops-cli command output\nif \"ERROR\" not in cap.stdout:\n    # Used to catch deployment ID corresponding variable:\n    stdout_json = json.loads(cap.stdout)\n    print(\"Model deployment created successfully, DEPLOYMENT_ID=\" + stdout_json[\"id\"])\n    # Set the deployment_id variable to use it in the next steps:\n    deployment_id = stdout_json[\"id\"]\nelse:\n    # Handle or output of mlops-cli in case of error:\n    print(\"Model deployment creation failed:\")\n    print(cap.stdout)",
        "#!aws sqs create-queue --queue-name datarobot-mlops-demo-v2\n# MLOps spooler channel SQS queue\n# Put your SQS queue URL here:\nMLOPS_SQS_QUEUE = \"https://sqs.us-east-1.amazonaws.com/12345678/aws-mlops-blogpost-demo\"",
        "import json\n\nfrom sagemaker.serializers import CSVSerializer\n\n# Pass all required environment variables to the SageMaker deployment\nenv_vars = {\n    \"MLOPS_DEPLOYMENT_ID\": deployment_id,\n    \"MLOPS_MODEL_ID\": model_id,\n    \"MLOPS_SQS_QUEUE\": MLOPS_SQS_QUEUE,\n    \"prediction_type\": prediction_type,\n    \"CLASS_NAMES\": json.dumps(class_names),\n}\n\nprint(env_vars)\n\npredictor = tree.deploy(1, \"ml.m4.xlarge\", serializer=CSVSerializer(), env=env_vars)",
        "shape = pd.read_csv(\"data/iris_with_header.csv\", header=None)\nshape.sample(3)",
        "# Drop the label column in the training set\nshape.drop(shape.columns[[0]], axis=1, inplace=True)\nshape.sample(3)",
        "import itertools\n\na = [50 * i for i in range(3)]\nb = [40 + i for i in range(10)]\nindices = [i + j for i, j in itertools.product(a, b)]\n\ntest_data = shape.iloc[indices[:-1]]\n\ntest_data",
        "import io\n\nprint(test_data)\nout = io.StringIO()\npd.DataFrame(test_data).to_csv(out, header=True, index=False)\nprint(predictor.predict(out.getvalue()).decode(\"utf-8\"))",
        "sess.delete_endpoint(predictor.endpoint)"
      ],
      "api_methods": [
        "model.json"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-3200326625158040459",
      "title": "AWS_monitor_sagemaker_model_in_DataRobot: readme.md",
      "content": "## Introduction \n\nThis notebook walks you through the steps to train and host a SageMaker model that can be monitored in the DataRobot platform.\n\n## Prerequisites\n\n1. Terraform - Since this repo uses terraform to provision the AWS infrastructure, you to need to install terraform in your local environment. Read more about [how to install terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli).\n2. An AWS account.\n3. A DataRobot account.\n4. AWS CLI and Docker installed in your local environment.\n\n## Build a DataRobot compatible SageMaker container to host and train models\n\nIn your local enviroment, go the directory `container`. Open the `Dockerfile` and inspect the Python libraries you need to install. Some of the popular python data science librairies are already installed, as well as libriaies that enable model monitoring in the DataRobot platform.\n\nOnce the Dockerfile is ready, run the following commands to build and push it to your AWS ECR repo.\n\n    - aws ecr create-repository --repository-name \"sagemaker-datarobot-decision-trees\"\n\n    - aws ecr get-login-password --region us-east-1|docker login --username AWS --password-stdin 012345678901.dkr.ecr.us-east-1.amazonaws.com/sagemaker-datarobot-decision-trees   (replace the region and account id with your own one)\n\n    - docker build . -t sagemaker-datarobot-decision-trees\n\n    - docker tag sagemaker-datarobot-decision-trees 123456789.dkr.ecr.us-east-1.amazonaws.com/sagemaker-datarobot-decision-trees:latest\n\n    - docker push 012345678901.dkr.ecr.us-east-1.amazonaws.com/sagemaker-datarobot-decision-trees:latest (replace this with your own ECR repo url)\n\n\nAfter some time, you should be able to see the image in your AWS ECR repo.\n\n## Model building and hosting\n\nOnce the ECR image has been built, you can run the notebook `AWS_SageMaker_DataRobot_MLOps.ipynb` in your SageMaker notebook instance. \n\nJust upload the notebook together with the \"data\" directory to your SageMaker notebook instance and follow the instructions to run it.",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_monitor_sagemaker_model_in_DataRobot/readme.md",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_monitor_sagemaker_model_in_DataRobot",
        "file_type": "markdown",
        "size": 2044
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.85,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-5222760830343087105",
      "title": "AWS_sagemaker_deployment: README.md",
      "content": "# End to end ML workflow with a deployment in AWS SageMaker\n\n<img src=\"images/DR%20and%20AWS%20Better%20Together.svg\" width=\"250\"/>\n  \nIn this AI Accelerator notebook, you will build an AI/ML model within DataRobot which will then be deployed and hosted within AWS SageMaker. This will include uploading data for model training, model development, and exporting from DataRobot into SageMaker.  \n\nAdditionally, this notebook contains all the commands needed to prepare your AWS environment to successfully host a DataRobot model Scoring Code JAR file.  \n  \nThese steps can also be done manually. You can view more documentation on how to do this [from the DataRobot documentation site](https://docs.datarobot.com/en/docs/platform/integrations/aws/sc-sagemaker.html#use-scoring-code-with-aws-sagemaker).\n\n## Prerequisites\n\nIn order to successfully build up an environment in AWS to host a DataRobot model, you will need to have the following software installed on your system:\n- Docker\n- Access to Dockerhub\n- DataRobot Python SDK (this notebook was tested with version 3.0.2)\n- AWS Boto3 library\n\nAdditionally, you will need to have access to DataRobot with a generated API token and access to AWS with seperate generated access tokens.\n\n## Scope\n\nThe notebook accomplishes the following tasks and builds the following items:\n\nFor AWS:\n\n- ECR Repository\n- S3 Bucket\n- IAM Role for SageMaker\n- SageMaker inference model\n- SageMaker endpoint configuration\n- SageMaker endpoint (for real time predictions)\n- SageMaker batch transform job (for batch predictions)\n  \nFor DataRobot:\n- Create a project\n- Build DataRobot models\n- Create a Scoring Code JAR file from a DataRobot Model\n  \n## Directory outline\n\n- *dr_model_sagemaker.ipynb*  \nThis is the python notebook that contains all the code needed to run the steps in this AI Accelerator.  \n  \nTo make it easier to get up and running, the following folders include sample data that is used by the notebook for model training and for making predictions against.\n- *scoring_data*  \nThis folder contains two datasets for making predictions against the model that has been deployed to AWS SageMaker. the 1 row dataset is for testing real time predictions while the 10k row dataset is for testing batch predictions.\n- *training_data*  \nThis folder contains public sample data from Lending Club that will be used to train a simple binary classification model within DataRobot.\n\n\nAuthor: Alex Yeager \\\nVersion Date: 12/22/2022\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_sagemaker_deployment/README.md",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_sagemaker_deployment",
        "file_type": "markdown",
        "size": 2466
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_7523520104024005714",
      "title": "AWS_sagemaker_deployment: dr_model_sagemaker.config.yaml",
      "content": "---\ndescription: In this AI Accelerator notebook, you will build an AI/ML model within DataRobot which will\n  then be deployed and hosted within AWS SageMaker. This will include uploading data for model training,\n  model development, and exporting from DataRobot into SageMaker.\nfile_name: dr_model_sagemaker.ipynb\nlanguages:\n  - python\nmaintainers:\n  - Alex Yeager\nmaintainers_email:\n  - alex.yeager@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: End to end ML workflow with a deployment in AWS SageMaker\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_sagemaker_deployment/dr_model_sagemaker.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_sagemaker_deployment",
        "file_type": "yaml",
        "size": 525
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.44999999999999996,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_658341266384200599",
      "title": "AWS_sagemaker_deployment: dr_model_sagemaker.ipynb",
      "content": "## Setup\n\n### Import libraries\n\n```python\nfrom datetime import datetime\nimport json\nimport logging\nimport os.path\nimport sys\nimport tarfile\nimport time\n\nimport boto3\nfrom botocore.exceptions import ClientError\nimport datarobot as dr\nimport pandas as pd\nimport requests\n```\n\n### Set some logging paramaters\n\n```python\n# Configure formatting for logging\nlog = logging.getLogger()\nlog.setLevel(logging.INFO)\n\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"[%(asctime)s][%(name)s][%(levelname)s]: %(message)s\")\nhandler.setFormatter(formatter)\nlog.addHandler(handler)\n```\n\n### Variable configuration\nIn this cell, define all of the variables and access tokens.\n  \n#### DataRobot\n`dr_host`: The DataRobot cluster you are connecting to. Please ensure that the `/api/v2` part of the URL is still in the string.  \n`api_key`: The API key of the DataRobot user used to interact with the platform.\n  \n#### AWS\n`aws_region`: The AWS region that everything will be deployed to.  \n`aws_access_key_id`: AWS Access Key for authentication.  \n`aws_secret_access_key`: AWS Secret Access Key for authentication.  \n`aws_session_token`: AWS Session Token for authentication.  \n  \n`s3_bucket`: The name of the S3 bucket that will be created for uploading your Scoring Code JAR file into.  \n`aws_ecr_repo`: The name of the ECR repo into which you upload your runtime docker image.  \n`sagemaker_execution_role_name`: The name of the IAM Role that will be created to allow SageMaker to interact with S3 and other SageMaker services.  \n\n```python\n# DataRobot Variables\n\n# DataRobot Host\ndr_host = \"https://app.datarobot.com/api/v2\"\n\n# DataRobot API Key\napi_key = \"<API_TOKEN>\"\n\n\n# AWS Variables\ns3_bucket = \"<YOUR_S3_BUCKET_NAME>\"\naws_ecr_repo = \"<YOUR_ECR_REPO_NAME>\"\nsagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-Demo\"\naws_region = \"us-east-1\"\n\naws_access_key_id = \"\"\naws_secret_access_key = \"\"\naws_session_token = \"\"\n```\n\n### Connect to DataRobot\n\n```python\nclient = dr.Client(\n    token=api_key,\n    endpoint=dr_host,\n    user_agent_suffix=\"AIA-E2E-AWS-7\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')\n```\n\nRead more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n## Modeling\n\nThis section of the notebook focueses on the steps for creating and exporting an ML model developed within DataRobot.\n\n### Create a project and initiate Autopilot\n\nIn the following snippet you will upload your training data to DataRobot. This example uses a dataset of Lending Club loans to predict if a loan will default or not.\n\nThis example sets an advanced option for the project to include only models that are compatible with Scoring Code Export. Java Scoring Code can be downloaded as a binary file or compiled, and contains all of the data transformations, feature engineering and final model parameters from the DataRobot Model . Since the data and feature engineering pipeline are completely contained in the portable JAR file, predictions can be made outside of DataRobot, as long as the scoring data is in the same format as the training data. More information can be [found in the DataRobot documentation](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/index.html#model-support).\n\nNext, you will initiate Autopilot to build models.\n\nIf you already have a model that you want to deploy, then this part can be skipped, but you must manually define the project and model ID below to continue using the notebook.\n\n```python\n# Create a project, kick off Autopilot, and wait for completion\ndf = pd.read_csv(\"training_data/10K_Lending_Club_Loans.csv\")\n\nadvanced_options = dr.AdvancedOptions(\n    blend_best_models=False, scoring_code_only=True, prepare_model_for_deployment=True\n)\n\nproject = dr.Project.create(\n    sourcedata=df,\n    project_name=\"DR_Demo_Sagemaker_{}\".format(\n        datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    ),\n)\nproject.analyze_and_model(\n    target=\"is_bad\", worker_count=-1, advanced_options=advanced_options\n)\nproject.wait_for_autopilot(verbosity=1)\n```\n\n### Get our Project ID and the ID of the top model in the leaderboard for export.\n\n```python\n# Get your Project ID and Model ID of the top rated model on the leaderboard\nproject_id = project.id\ntop_model = project.get_top_model()\nmodel_id = top_model.id\n\nlog.info(\"Project ID: {} | Model ID: {}\".format(project_id, model_id))\n```\n\n## Export a DataRobot model\n\nUse the following cells to download the model as a Scoring Code JAR file (in a local directory called model) and then compress that file into a .tar.gz archive to upload to S3.\n\n```python\n# Below is a helper function that downloads your JAR file to your local system from a target server\n# The output returns the model path name if the file has been downloaded and returns None if not\n\n\ndef get_scoring_code(session, host, project_id, model_id):\n    apiEndpoint = format(\n        \"{}/projects/{}/models/{}/scoringCode/\".format(host, project_id, model_id)\n    )\n\n    try:\n        r = session.get(apiEndpoint)\n        r.raise_for_status()\n        return r\n    except requests.exceptions.HTTPError as err:\n        log.error(err)\n        return None\n```\n\n```python\nheaders = {}\nheaders[\"Authorization\"] = \"Bearer {}\".format(api_key)\n\nsession = requests.Session()\nsession.headers.update(headers)\n\nlog.info(\"Getting scoring code jar file from DataRobot location: \" + dr_host)\n# Get scoring code jar\noutput = get_scoring_code(session, dr_host, project_id, model_id)\nif output is None:\n    log.error(\"download failed\")\nelse:\n    # Model name is grabbed from Content-Disposition header, which provides a dynamically generated suggested name for the model (usually model_id.jar)\n    modeldir = \"model/\"\n\n    # cCeate local model directory if it doesn't exist already\n    if not os.path.exists(modeldir):\n        os.mkdir(modeldir)\n\n    fd = output.headers.get(\"Content-Disposition\")\n    modelname = fd.split(\";\")[1].strip().split(\"=\")[1]\n    modelpath = modeldir + modelname\n\n    with open(modelpath, \"wb\") as f:\n        f.write(output.content)\n\n    log.info(\"Scoring Code jar downloaded to {}\".format(modelpath))\n\n    # Compress the jar file into a tar.gz as required by SageMaker\n    log.info(\"Compressing jar file into tar.gz\")\n    tgz_name = modelname + \".tar.gz\"\n    tgz_path = modeldir + tgz_name\n\n    with tarfile.open(tgz_path, \"w:gz\") as tar:\n        tar.add(modelpath, arcname=os.path.basename(modelpath))\n\n    log.info(\"COMPLETE!\")\n```\n\n## Import to AWS\n\nThis section of the notebook focuses on the steps required to prepare AWS for hosting a DataRobot model within SageMaker. It includes examples for how to make predictions against the model for both real-time and batch use cases.\n\n### Download docker runtime image\n\nThis step will pull down the scoring-inference-code-sagemaker docker image that will be used to run the Scoring Code JAR file in SageMaker.\n\n```python\n%%bash\n# Pull down the scoring-inference-code-sagemaker image that will be pushed to AWS ECR for hosting our Scoring Code models in Sagemaker\ndocker pull datarobot/scoring-inference-code-sagemaker:latest\n```\n\n### Create an AWS session connection\n\n```python\n# Create an AWS Boto3 Session\nsession = boto3.Session(\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key,\n    aws_session_token=aws_session_token,\n    region_name=aws_region,\n)\n```\n\nNext, create an AWS ECR repo to hold the `scoring-inference-code-sagemaker` docker image.\n\n```python\n# Create the AWS ECR repo\nlog.info(\"Creating ECR Repo to hold our base image for running scoring code jar file.\")\necr_client = session.client(\"ecr\")\n\necr_response = ecr_client.create_repository(repositoryName=aws_ecr_repo)\n\nlog.info(\"ECR Name: {}\".format(ecr_response.get(\"repository\").get(\"repositoryName\")))\nlog.info(\"ECR ARN: {}\".format(ecr_response.get(\"repository\").get(\"repositoryArn\")))\nlog.info(\"ECR URI: {}\".format(ecr_response.get(\"repository\").get(\"repositoryUri\")))\n\necr_repo_uri = ecr_response.get(\"repository\").get(\"repositoryUri\")\necr_registry_id = ecr_response.get(\"repository\").get(\"registryId\")\nregistry_url = ecr_registry_id + \".dkr.ecr.\" + aws_region + \".amazonaws.com\"\n\nlog.info(\"ECR Repo created!\")\n```\n\nNow you can push `scoring-inference-code-sagemaker` to the ECR repo.\n\n```python\n%%bash -s \"$ecr_repo_uri\" \"$registry_url\" \"$aws_access_key_id\" \"$aws_secret_access_key\" \"$aws_session_token\" \"$aws_region\"\n# Push datarobot/scoring-inference-code-sagemaker:latest to ECR Repo\n\nexport AWS_ACCESS_KEY_ID=$3\nexport AWS_SECRET_ACCESS_KEY=$4\nexport AWS_SESSION_TOKEN=$5\n\ndocker login -u AWS -p $(aws ecr get-login-password --region $6) $2\ndocker tag datarobot/scoring-inference-code-sagemaker:latest $1:latest\ndocker push $1:latest\n```\n\n### Create an S3 Bucket\n\nThis S3 bucket stores your DataRobot model.\n\n```python\n# Create S3 Bucket\nlog.info(\"Creating S3 Bucket {}\".format(s3_bucket))\n\ns3 = session.resource(\"s3\")\ntry:\n    s3.create_bucket(Bucket=s3_bucket)\n    log.info(\"S3 Bucket Creation Complete!\")\nexcept ClientError as e:\n    log.error(e)\n```\n\nNext, upload the Scoring Code JAR file to S3.\n\n```python\n# Upload scoring code jar tarball to AWS S3\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(tgz_name, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_model = \"sagemaker/models/\" + tgz_name\ntry:\n    s3.meta.client.upload_file(tgz_path, s3_bucket, s3_obj_name_model)\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)\n```\n\n### Upload sample data to S3\n\nIn this cell, you upload a sample dataset to make batch predictions in SageMaker. This dataset is specifically designed for the model that was created earlier in this notebook.\n\n```python\n# Upload Batch Scoring Data to S3\nbatch_path = \"scoring_data/10K_Lending_Club_Loans_scoring.csv\"\n\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(batch_path, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_csv = \"sagemaker/\" + batch_path\ntry:\n    s3.meta.client.upload_file(batch_path, s3_bucket, s3_obj_name_csv)\n    batch_input_file = \"s3://\" + s3_bucket + \"/\" + s3_obj_name_csv\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)\n```\n\nThis cell will create an IAM role for SageMaker that will grant access to run things within SageMaker itself, and to allow for access to the S3 bucket contianing the uploaded Scoring Code model file.\n\n```python\n# Create IAM Role for Sagemaker to use\nlog.info(\"Creating Execution IAM Role for Sagemaker to use\")\niam = session.client(\"iam\")\niamr = session.resource(\"iam\")\n\nrole_policy = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Action\": [\"s3:ListBucket\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket],\n            },\n            {\n                \"Action\": [\"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket + \"/*\"],\n            },\n        ],\n    }\n)\n\nlog.info(\"Creating Exeuction S3 Access Policy\")\npolicy = iam.create_policy(\n    PolicyName=sagemaker_execution_role_name + \"-policy\", PolicyDocument=role_policy\n)\n\npolicy_arn = policy.get(\"Policy\").get(\"Arn\")\n\nassume_role_policy_document = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n                \"Action\": \"sts:AssumeRole\",\n            }\n        ],\n    }\n)\n\nlog.info(\"Creating actual role\")\nrole = iam.create_role(\n    RoleName=sagemaker_execution_role_name,\n    AssumeRolePolicyDocument=assume_role_policy_document,\n)\n\n# Attach our execution Policy\nlog.info(\"Attaching Execution Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name, PolicyArn=policy_arn\n)\n\n# Attach the AmazonSageMakerFullAccess Policy\nlog.info(\"Attaching AmazonSageMakerFullAccess Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name,\n    PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n)\n\nrole_resource = iamr.Role(sagemaker_execution_role_name)\nlog.info(\"IAM Role Info:\")\nlog.info(\"IAM Role Name: {}\".format(role_resource.name))\nlog.info(\"IAM Role ARN: {}\".format(role_resource.arn))\nlog.info(\"IAM Role Policies:\")\nfor p in role_resource.attached_policies.all():\n    log.info(p)\n\nlog.info(\"COMPLETE!\")\n```\n\n### Create a SageMaker model\n\n```python\n# Create model in sagemaker\nlog.info(\"Creating Sagemaker Model\")\nsm_client = session.client(\"sagemaker\")\naws_model_name = modelname.split(\".\")[0]\n\nresponse = sm_client.create_model(\n    ModelName=aws_model_name,\n    PrimaryContainer={\n        \"Image\": ecr_repo_uri + \":latest\",\n        \"ImageConfig\": {\"RepositoryAccessMode\": \"Platform\"},\n        \"Mode\": \"SingleModel\",\n        \"ModelDataUrl\": \"s3://\" + s3_bucket + \"/\" + s3_obj_name_model,\n    },\n    ExecutionRoleArn=role_resource.arn,\n)\n\nif response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Created!\")\n    log.info(\"model name: {}\".format(aws_model_name))\n    log.info(\"model arn: {}\".format(response.get(\"ModelArn\")))\n    log.info(\"COMPLETE!\")\n```\n\n### SageMaker endpoint configuration\n\nThis is used as part of the assembly of a SageMaker Endpoint that is required for real time API prediction requests.\n\n```python\n# Create Sagemaker Endpoint Configuration\nlog.info(\"Creating Sagemaker Model Endpoint Configuration\")\naws_endpoint_config_name = aws_model_name + \"-ec\"\n\nec_response = sm_client.create_endpoint_config(\n    EndpointConfigName=aws_endpoint_config_name,\n    ProductionVariants=[\n        {\n            \"VariantName\": \"variant-1\",\n            \"ModelName\": aws_model_name,\n            \"InitialInstanceCount\": 1,\n            \"InstanceType\": \"ml.m4.xlarge\",\n        }\n    ],\n)\n\nif ec_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Endpoint Configuration Created!\")\n    log.info(\"endpoint configuration name: {}\".format(aws_endpoint_config_name))\n    log.info(\n        \"endpoint configuration arn: {}\".format(ec_response.get(\"EndpointConfigArn\"))\n    )\n    log.info(\"COMPLETE!\")\n```\n\nUse the cell below to create a Sagemaker endpoint.\n\n```python\n# Create Sagemaker Endpoint\nlog.info(\"Creating Sagemaker Model Endpoint... This process can take a few minutes\")\naws_endpoint_name = aws_model_name + \"-ep\"\n\nep_response = sm_client.create_endpoint(\n    EndpointName=aws_endpoint_name,\n    EndpointConfigName=aws_endpoint_config_name,\n)\n\nif ep_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when sending endpoint creation request to Sagemaker\")\n    log.error(ep_response)\nelse:\n    i = 0\n    while i < 30:\n        status_r = sm_client.describe_endpoint(EndpointName=aws_endpoint_name)\n        status = status_r.get(\"EndpointStatus\")\n        log.info(\"Endpoint Creation Status: {}\".format(status_r.get(\"EndpointStatus\")))\n\n        if status == \"InService\":\n            break\n        else:\n            time.sleep(20)\n            i = i + 1\n\n    if status == \"InService\":\n        log.info(\"Sagemaker Model Endpoint Created!\")\n        log.info(\"Endpoint Name: {}\".format(status_r.get(\"EndpointName\")))\n        log.info(\"Endpoint ARN: {}\".format(status_r.get(\"EndpointArn\")))\n        invocation_url = \"https://runtime.sagemaker.{}.amazonaws.com/endpoints/{}/invocations\".format(\n            aws_region, status_r.get(\"EndpointName\")\n        )\n        log.info(\"Endpoint API URL: {}\".format(invocation_url))\n        log.info(\"COMPLETE!\")\n    else:\n        log.error(\"Sagemaker did not return an 'InService' status in time!\")\n        log.error(\"Last status received: {}\".format(status))\n        log.error(status_r)\n```\n\n## Predicion examples\nThe following cells will show how to make predictions against the deployed model using both batch and real-time methods.\n  \n### Create SageMaker Batch Transform Job\n\nUse this cell to programatically create a batch transform job in SageMaker that can be used for batch predictions.  This job reads in a CSV that you previously uploaded to an S3 bucket. The output of the job will then be written to another folder (`scoring_output`) that will exist in the S3 bucket that you previously created.\n\n```python\n# Create a batch transform job for batch predictions\nlog.info(\"Creating Sagemaker Batch Transform Job\")\nbtj_client = session.client(\"sagemaker\")\n\njob_name = (\n    aws_model_name + \"-batch-transform-job-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n)\n\nbatch_output_folder = \"s3://\" + s3_bucket + \"/scoring_output/\"\nresponse = btj_client.create_transform_job(\n    TransformJobName=job_name,\n    ModelName=aws_model_name,\n    TransformInput={\n        \"DataSource\": {\n            \"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input_file}\n        },\n        \"ContentType\": \"string\",\n        \"CompressionType\": \"None\",\n        \"SplitType\": \"None\",\n    },\n    TransformOutput={\n        \"S3OutputPath\": batch_output_folder,\n        \"Accept\": \"string\",\n        \"AssembleWith\": \"None\",\n    },\n    TransformResources={\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n)\n\n# Response\nlog.info(\"Running Sagemaker Batch Transform Job {}\".format(job_name))\ni = 0\nwhile i < 30:\n    status_r = btj_client.describe_transform_job(TransformJobName=job_name)\n    status = status_r.get(\"TransformJobStatus\")\n    log.info(\"Batch Job Status: {}\".format(status_r.get(\"TransformJobStatus\")))\n\n    if status not in [\"InProgress\", \"Stopping\"]:\n        break\n    else:\n        time.sleep(20)\n        i = i + 1\n```\n\n### View batch transform Job results\n\nIn this cell you download the results file from the batch transform job that you just ran in SageMaker and output the contents of the dataframe to show what was scored.  \n\nIn this case, you are scoring a binary classification model, so your output will be two columns that contain the scores of our positive and negative classes, which will translate into whether a potential loan will default or not.\n\n```python\ns3 = session.client(\"s3\")\noutput_dir = \"scoring_output\"\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nfor key in s3.list_objects(Bucket=s3_bucket)[\"Contents\"]:\n    if \".out\" in key[\"Key\"]:\n        s3.download_file(s3_bucket, key[\"Key\"], \"scoring_output/output.csv\")\n\ndf = pd.read_csv(output_dir + \"/output.csv\")\ndf\n```\n\n### Real-time predictions with SageMaker\n\nThis cell shows how to interact with the SageMaker endpoint that you previously created for your model to use with real time prediction workloads.  \n\nYou will be using the AWS boto3 client and making a call to the SageMaker endpoint to score a row of data from a CSV file and then print out the result.\n\n```python\ns_client = session.client(\"sagemaker-runtime\")\n\nbuffer = open(\"scoring_data/1_row_Lending_Club_Loans_scoring.csv\")\npayload = buffer.read()\n\nresponse = s_client.invoke_endpoint(\n    EndpointName=aws_endpoint_name, ContentType=\"text/csv\", Body=payload\n)\n\ndata = response.get(\"Body\").read()\nlog.info(\"Scoring output:\\n{}\".format(data.decode(\"utf-8\")))\n```",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/AWS_sagemaker_deployment/dr_model_sagemaker.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "aws",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "AWS_sagemaker_deployment",
        "file_type": "notebook",
        "size": 28903
      },
      "code_examples": [
        "from datetime import datetime\nimport json\nimport logging\nimport os.path\nimport sys\nimport tarfile\nimport time\n\nimport boto3\nfrom botocore.exceptions import ClientError\nimport datarobot as dr\nimport pandas as pd\nimport requests",
        "# Configure formatting for logging\nlog = logging.getLogger()\nlog.setLevel(logging.INFO)\n\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"[%(asctime)s][%(name)s][%(levelname)s]: %(message)s\")\nhandler.setFormatter(formatter)\nlog.addHandler(handler)",
        "# DataRobot Variables\n\n# DataRobot Host\ndr_host = \"https://app.datarobot.com/api/v2\"\n\n# DataRobot API Key\napi_key = \"<API_TOKEN>\"\n\n\n# AWS Variables\ns3_bucket = \"<YOUR_S3_BUCKET_NAME>\"\naws_ecr_repo = \"<YOUR_ECR_REPO_NAME>\"\nsagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-Demo\"\naws_region = \"us-east-1\"\n\naws_access_key_id = \"\"\naws_secret_access_key = \"\"\naws_session_token = \"\"",
        "client = dr.Client(\n    token=api_key,\n    endpoint=dr_host,\n    user_agent_suffix=\"AIA-E2E-AWS-7\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')",
        "# Create a project, kick off Autopilot, and wait for completion\ndf = pd.read_csv(\"training_data/10K_Lending_Club_Loans.csv\")\n\nadvanced_options = dr.AdvancedOptions(\n    blend_best_models=False, scoring_code_only=True, prepare_model_for_deployment=True\n)\n\nproject = dr.Project.create(\n    sourcedata=df,\n    project_name=\"DR_Demo_Sagemaker_{}\".format(\n        datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    ),\n)\nproject.analyze_and_model(\n    target=\"is_bad\", worker_count=-1, advanced_options=advanced_options\n)\nproject.wait_for_autopilot(verbosity=1)",
        "# Get your Project ID and Model ID of the top rated model on the leaderboard\nproject_id = project.id\ntop_model = project.get_top_model()\nmodel_id = top_model.id\n\nlog.info(\"Project ID: {} | Model ID: {}\".format(project_id, model_id))",
        "# Below is a helper function that downloads your JAR file to your local system from a target server\n# The output returns the model path name if the file has been downloaded and returns None if not\n\n\ndef get_scoring_code(session, host, project_id, model_id):\n    apiEndpoint = format(\n        \"{}/projects/{}/models/{}/scoringCode/\".format(host, project_id, model_id)\n    )\n\n    try:\n        r = session.get(apiEndpoint)\n        r.raise_for_status()\n        return r\n    except requests.exceptions.HTTPError as err:\n        log.error(err)\n        return None",
        "headers = {}\nheaders[\"Authorization\"] = \"Bearer {}\".format(api_key)\n\nsession = requests.Session()\nsession.headers.update(headers)\n\nlog.info(\"Getting scoring code jar file from DataRobot location: \" + dr_host)\n# Get scoring code jar\noutput = get_scoring_code(session, dr_host, project_id, model_id)\nif output is None:\n    log.error(\"download failed\")\nelse:\n    # Model name is grabbed from Content-Disposition header, which provides a dynamically generated suggested name for the model (usually model_id.jar)\n    modeldir = \"model/\"\n\n    # cCeate local model directory if it doesn't exist already\n    if not os.path.exists(modeldir):\n        os.mkdir(modeldir)\n\n    fd = output.headers.get(\"Content-Disposition\")\n    modelname = fd.split(\";\")[1].strip().split(\"=\")[1]\n    modelpath = modeldir + modelname\n\n    with open(modelpath, \"wb\") as f:\n        f.write(output.content)\n\n    log.info(\"Scoring Code jar downloaded to {}\".format(modelpath))\n\n    # Compress the jar file into a tar.gz as required by SageMaker\n    log.info(\"Compressing jar file into tar.gz\")\n    tgz_name = modelname + \".tar.gz\"\n    tgz_path = modeldir + tgz_name\n\n    with tarfile.open(tgz_path, \"w:gz\") as tar:\n        tar.add(modelpath, arcname=os.path.basename(modelpath))\n\n    log.info(\"COMPLETE!\")",
        "%%bash\n# Pull down the scoring-inference-code-sagemaker image that will be pushed to AWS ECR for hosting our Scoring Code models in Sagemaker\ndocker pull datarobot/scoring-inference-code-sagemaker:latest",
        "# Create an AWS Boto3 Session\nsession = boto3.Session(\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key,\n    aws_session_token=aws_session_token,\n    region_name=aws_region,\n)",
        "# Create the AWS ECR repo\nlog.info(\"Creating ECR Repo to hold our base image for running scoring code jar file.\")\necr_client = session.client(\"ecr\")\n\necr_response = ecr_client.create_repository(repositoryName=aws_ecr_repo)\n\nlog.info(\"ECR Name: {}\".format(ecr_response.get(\"repository\").get(\"repositoryName\")))\nlog.info(\"ECR ARN: {}\".format(ecr_response.get(\"repository\").get(\"repositoryArn\")))\nlog.info(\"ECR URI: {}\".format(ecr_response.get(\"repository\").get(\"repositoryUri\")))\n\necr_repo_uri = ecr_response.get(\"repository\").get(\"repositoryUri\")\necr_registry_id = ecr_response.get(\"repository\").get(\"registryId\")\nregistry_url = ecr_registry_id + \".dkr.ecr.\" + aws_region + \".amazonaws.com\"\n\nlog.info(\"ECR Repo created!\")",
        "%%bash -s \"$ecr_repo_uri\" \"$registry_url\" \"$aws_access_key_id\" \"$aws_secret_access_key\" \"$aws_session_token\" \"$aws_region\"\n# Push datarobot/scoring-inference-code-sagemaker:latest to ECR Repo\n\nexport AWS_ACCESS_KEY_ID=$3\nexport AWS_SECRET_ACCESS_KEY=$4\nexport AWS_SESSION_TOKEN=$5\n\ndocker login -u AWS -p $(aws ecr get-login-password --region $6) $2\ndocker tag datarobot/scoring-inference-code-sagemaker:latest $1:latest\ndocker push $1:latest",
        "# Create S3 Bucket\nlog.info(\"Creating S3 Bucket {}\".format(s3_bucket))\n\ns3 = session.resource(\"s3\")\ntry:\n    s3.create_bucket(Bucket=s3_bucket)\n    log.info(\"S3 Bucket Creation Complete!\")\nexcept ClientError as e:\n    log.error(e)",
        "# Upload scoring code jar tarball to AWS S3\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(tgz_name, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_model = \"sagemaker/models/\" + tgz_name\ntry:\n    s3.meta.client.upload_file(tgz_path, s3_bucket, s3_obj_name_model)\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)",
        "# Upload Batch Scoring Data to S3\nbatch_path = \"scoring_data/10K_Lending_Club_Loans_scoring.csv\"\n\nlog.info(\"Uploading {} to S3 Bucket: {}\".format(batch_path, s3_bucket))\ns3 = session.resource(\"s3\")\n\ns3_obj_name_csv = \"sagemaker/\" + batch_path\ntry:\n    s3.meta.client.upload_file(batch_path, s3_bucket, s3_obj_name_csv)\n    batch_input_file = \"s3://\" + s3_bucket + \"/\" + s3_obj_name_csv\n    log.info(\"S3 Upload Complete!\")\nexcept ClientError as e:\n    log.error(e)",
        "# Create IAM Role for Sagemaker to use\nlog.info(\"Creating Execution IAM Role for Sagemaker to use\")\niam = session.client(\"iam\")\niamr = session.resource(\"iam\")\n\nrole_policy = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Action\": [\"s3:ListBucket\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket],\n            },\n            {\n                \"Action\": [\"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\"],\n                \"Effect\": \"Allow\",\n                \"Resource\": [\"arn:aws:s3:::\" + s3_bucket + \"/*\"],\n            },\n        ],\n    }\n)\n\nlog.info(\"Creating Exeuction S3 Access Policy\")\npolicy = iam.create_policy(\n    PolicyName=sagemaker_execution_role_name + \"-policy\", PolicyDocument=role_policy\n)\n\npolicy_arn = policy.get(\"Policy\").get(\"Arn\")\n\nassume_role_policy_document = json.dumps(\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n                \"Action\": \"sts:AssumeRole\",\n            }\n        ],\n    }\n)\n\nlog.info(\"Creating actual role\")\nrole = iam.create_role(\n    RoleName=sagemaker_execution_role_name,\n    AssumeRolePolicyDocument=assume_role_policy_document,\n)\n\n# Attach our execution Policy\nlog.info(\"Attaching Execution Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name, PolicyArn=policy_arn\n)\n\n# Attach the AmazonSageMakerFullAccess Policy\nlog.info(\"Attaching AmazonSageMakerFullAccess Policy to Role\")\nresponse = iam.attach_role_policy(\n    RoleName=sagemaker_execution_role_name,\n    PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n)\n\nrole_resource = iamr.Role(sagemaker_execution_role_name)\nlog.info(\"IAM Role Info:\")\nlog.info(\"IAM Role Name: {}\".format(role_resource.name))\nlog.info(\"IAM Role ARN: {}\".format(role_resource.arn))\nlog.info(\"IAM Role Policies:\")\nfor p in role_resource.attached_policies.all():\n    log.info(p)\n\nlog.info(\"COMPLETE!\")",
        "# Create model in sagemaker\nlog.info(\"Creating Sagemaker Model\")\nsm_client = session.client(\"sagemaker\")\naws_model_name = modelname.split(\".\")[0]\n\nresponse = sm_client.create_model(\n    ModelName=aws_model_name,\n    PrimaryContainer={\n        \"Image\": ecr_repo_uri + \":latest\",\n        \"ImageConfig\": {\"RepositoryAccessMode\": \"Platform\"},\n        \"Mode\": \"SingleModel\",\n        \"ModelDataUrl\": \"s3://\" + s3_bucket + \"/\" + s3_obj_name_model,\n    },\n    ExecutionRoleArn=role_resource.arn,\n)\n\nif response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Created!\")\n    log.info(\"model name: {}\".format(aws_model_name))\n    log.info(\"model arn: {}\".format(response.get(\"ModelArn\")))\n    log.info(\"COMPLETE!\")",
        "# Create Sagemaker Endpoint Configuration\nlog.info(\"Creating Sagemaker Model Endpoint Configuration\")\naws_endpoint_config_name = aws_model_name + \"-ec\"\n\nec_response = sm_client.create_endpoint_config(\n    EndpointConfigName=aws_endpoint_config_name,\n    ProductionVariants=[\n        {\n            \"VariantName\": \"variant-1\",\n            \"ModelName\": aws_model_name,\n            \"InitialInstanceCount\": 1,\n            \"InstanceType\": \"ml.m4.xlarge\",\n        }\n    ],\n)\n\nif ec_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when creating model in Sagemaker\")\nelse:\n    log.info(\"Sagemaker Model Endpoint Configuration Created!\")\n    log.info(\"endpoint configuration name: {}\".format(aws_endpoint_config_name))\n    log.info(\n        \"endpoint configuration arn: {}\".format(ec_response.get(\"EndpointConfigArn\"))\n    )\n    log.info(\"COMPLETE!\")",
        "# Create Sagemaker Endpoint\nlog.info(\"Creating Sagemaker Model Endpoint... This process can take a few minutes\")\naws_endpoint_name = aws_model_name + \"-ep\"\n\nep_response = sm_client.create_endpoint(\n    EndpointName=aws_endpoint_name,\n    EndpointConfigName=aws_endpoint_config_name,\n)\n\nif ep_response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") != 200:\n    log.error(\"Error when sending endpoint creation request to Sagemaker\")\n    log.error(ep_response)\nelse:\n    i = 0\n    while i < 30:\n        status_r = sm_client.describe_endpoint(EndpointName=aws_endpoint_name)\n        status = status_r.get(\"EndpointStatus\")\n        log.info(\"Endpoint Creation Status: {}\".format(status_r.get(\"EndpointStatus\")))\n\n        if status == \"InService\":\n            break\n        else:\n            time.sleep(20)\n            i = i + 1\n\n    if status == \"InService\":\n        log.info(\"Sagemaker Model Endpoint Created!\")\n        log.info(\"Endpoint Name: {}\".format(status_r.get(\"EndpointName\")))\n        log.info(\"Endpoint ARN: {}\".format(status_r.get(\"EndpointArn\")))\n        invocation_url = \"https://runtime.sagemaker.{}.amazonaws.com/endpoints/{}/invocations\".format(\n            aws_region, status_r.get(\"EndpointName\")\n        )\n        log.info(\"Endpoint API URL: {}\".format(invocation_url))\n        log.info(\"COMPLETE!\")\n    else:\n        log.error(\"Sagemaker did not return an 'InService' status in time!\")\n        log.error(\"Last status received: {}\".format(status))\n        log.error(status_r)",
        "# Create a batch transform job for batch predictions\nlog.info(\"Creating Sagemaker Batch Transform Job\")\nbtj_client = session.client(\"sagemaker\")\n\njob_name = (\n    aws_model_name + \"-batch-transform-job-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n)\n\nbatch_output_folder = \"s3://\" + s3_bucket + \"/scoring_output/\"\nresponse = btj_client.create_transform_job(\n    TransformJobName=job_name,\n    ModelName=aws_model_name,\n    TransformInput={\n        \"DataSource\": {\n            \"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input_file}\n        },\n        \"ContentType\": \"string\",\n        \"CompressionType\": \"None\",\n        \"SplitType\": \"None\",\n    },\n    TransformOutput={\n        \"S3OutputPath\": batch_output_folder,\n        \"Accept\": \"string\",\n        \"AssembleWith\": \"None\",\n    },\n    TransformResources={\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n)\n\n# Response\nlog.info(\"Running Sagemaker Batch Transform Job {}\".format(job_name))\ni = 0\nwhile i < 30:\n    status_r = btj_client.describe_transform_job(TransformJobName=job_name)\n    status = status_r.get(\"TransformJobStatus\")\n    log.info(\"Batch Job Status: {}\".format(status_r.get(\"TransformJobStatus\")))\n\n    if status not in [\"InProgress\", \"Stopping\"]:\n        break\n    else:\n        time.sleep(20)\n        i = i + 1",
        "s3 = session.client(\"s3\")\noutput_dir = \"scoring_output\"\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nfor key in s3.list_objects(Bucket=s3_bucket)[\"Contents\"]:\n    if \".out\" in key[\"Key\"]:\n        s3.download_file(s3_bucket, key[\"Key\"], \"scoring_output/output.csv\")\n\ndf = pd.read_csv(output_dir + \"/output.csv\")\ndf",
        "s_client = session.client(\"sagemaker-runtime\")\n\nbuffer = open(\"scoring_data/1_row_Lending_Club_Loans_scoring.csv\")\npayload = buffer.read()\n\nresponse = s_client.invoke_endpoint(\n    EndpointName=aws_endpoint_name, ContentType=\"text/csv\", Body=payload\n)\n\ndata = response.get(\"Body\").read()\nlog.info(\"Scoring output:\\n{}\".format(data.decode(\"utf-8\")))"
      ],
      "api_methods": [
        "model.id",
        "project.create",
        "project.get_top_model",
        "project.wait_for_autopilot",
        "project.id",
        "dr.project.create",
        "project.analyze_and_model",
        "dr.client._global_client"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-5073482770308643103",
      "title": "Azure_template: Azure_End_to_End.config.yaml",
      "content": "---\ndescription: \"This notebook illustrates an end-to-end data science workflow using DataRobot.\\nThe workflow\\\n  \\ ingests a dataset hosted in an Azure blob container, trains a series of models using DataRobot's AutoML\\\n  \\ capabilities, deploys a recommended model, and sets up a batch prediction job that writes predictions\\\n  \\ back to the original container.\"\nfile_name: Azure_End_to_End.ipynb\nlanguages:\n  - python\nmaintainers:\n  - Brent Hinks\nmaintainers_email:\n  - brent.hinks@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: End to end modeling workflow with Azure\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Azure_template/Azure_End_to_End.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Azure_template",
        "file_type": "yaml",
        "size": 591
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.6,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-588466627783698926",
      "title": "Azure_template: Azure_End_to_End.ipynb",
      "content": "# End to end modeling workflow with Azure\n\nAuthor: Brent Hinks (2023-01-27)\n\n## Overview\n\nThis notebook illustrates an end-to-end data science workflow using DataRobot. The workflow ingests a dataset hosted in an Azure blob container, trains a series of models using DataRobot's AutoML capabilities, deploys a recommended model, and sets up a batch prediction job that writes predictions back to the original container.\n\nIn this notebook you'll cover the following steps:\n\n- Acquiring a training dataset from an Azure storage container\n- Building a new DataRobot project\n- Deploying a recommended model\n- Scoring via batch prediction API\n- Writing results back to a source Azure container\n\n## Setup\n\nPrior to execution, ensure that the following dependencies are available in your notebook environment:\n\n- **datarobot**, provided via PyPi (Python library used to communicate with the DataRobot platform)\n- **azure.storage.blob**, provided via PyPi (Python library used to access Azure storage services)\n- **pandas**, provided via PyPi (common data science library)\n- **Azure CLI**, used to authenticate to Azure. You can reference [installation instructions](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) for more information.\n\n### Import libraries\n\nThe first cell of the notebook imports necessary packages, and sets up the connection to the DataRobot platform. There are also optional values that can be provided to use an existing project and deployment - if they are omitted then a new autopilot session will be kicked off and a new deployment will be created using DataRobot's recommended model.\n\n```python\nfrom io import StringIO\n\nfrom azure.storage.blob import BlobServiceClient\nimport datarobot as dr\nimport pandas as pd\n```\n\n### Connect to DataRobot\n\n```python\n# Set DataRobot connection info here\nDATAROBOT_API_TOKEN = \"\"\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AZURE-78\",  # Optional but helps DataRobot improve this workflow\n)\n```\n\n### Bind credentials\n\n```python\n# Set Azure connection blob info here\nAZURE_STORAGE_ACCOUNT = \"\"\nAZURE_STORAGE_CONTAINER = \"\"\n\n# Find this value by following the \"Access keys\" link from your storage account in the Azure console\nAZURE_STORAGE_ACCESS_KEY = \"\"\n\n# Provide dataset filenames and the modeling target feature\nAZURE_INPUT_FILE = \"input.csv\"\nAZURE_OUTPUT_FILE = \"scored.csv\"\nAZURE_INPUT_TARGET = \"target\"\n\n# Set name for Azure credentials in DataRobot\nDR_CREDENTIAL_NAME = \"Azure_{}\".format(AZURE_STORAGE_ACCOUNT)\n\nproject_id = None\ndeployment_id = None\n```\n\nBefore running the next cell, which creates the storage service client, you should run `az login` from your terminal to establish an authenticated session to Azure.\n\n```python\naccount_url = \"https://{}.blob.core.windows.net\".format(AZURE_STORAGE_ACCOUNT)\nblob_service_client = BlobServiceClient(account_url)\n```\n\n### Import data\n\nLoad the dataset stored in your Azure container into a pandas dataframe.\n\n```python\ncontainer_client = blob_service_client.get_container_client(\n    container=AZURE_STORAGE_CONTAINER\n)\ndownloaded_blob = container_client.download_blob(AZURE_INPUT_FILE)\n\ndf = pd.read_csv(StringIO(downloaded_blob.content_as_text()))\n```\n\nEnsure that proper Azure credentials are stored in DataRobot. This credential can be used in the future to automate data reads and writes in scoring jobs. Check for an existing credential matching the name we provided above. If none is found, then create a new one.\n\n```python\n# Use this code to look up the ID of the credential object created.\ncredential = None\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        credential = cred\n\nif credential == None:\n    credential = dr.Credential.create_azure(\n        name=DR_CREDENTIAL_NAME,\n        azure_connection_string=\"DefaultEndpointsProtocol=https;AccountName={};AccountKey={};\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_ACCESS_KEY\n        ),\n    )\n\ncredential\n```\n\n## Modeling\n\n### Create a project\n\nCreate a new project in DataRobot and upload the data stored in your dataframe. After that you will set the target and start the AutoML process.\n\nIf a `project_id` was supplied above, skip these steps.\n\n```python\n# Create a project without setting the target\nif project_id == None:\n    project = dr.Project.create(project_name=\"New Test Project (Azure)\", sourcedata=df)\n    print(project.id)\n```\n\n### Initate Autopilot\n\n```python\nif project_id == None:\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=AZURE_INPUT_TARGET,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight\n```\n\n```python\nif project_id == None:\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current Autopilot status:\n    # project.stage\n    # project.get_model_jobs()\n```\n\n## Select and deploy a model\n\nReview DataRobot's model recommendations and select one for deployment. If `deployment_id` was supplied above, skip this step.\n\n```python\nprint(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()\n```\n\nWhen you are happy with your model you can automate deployment.\n\n```python\nif deployment_id == None:\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment (Azure)\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)\n```\n\n## Make batch predictions\n\nCreate a batch prediction job that will read in your training dataset, produce scores with optional explanations, and write the results back to the original container. If any errors occur along the way, get details from `job.get_status()` to assist in troubleshooting.\n\n```python\njob = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_INPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_OUTPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    # Uncomment the next line to include prediction explanations.\n    # max_explanations=3,\n    passthrough_columns_set=\"all\",\n)\njob.wait_for_completion()\njob.get_status()\n```",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Azure_template/Azure_End_to_End.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Azure_template",
        "file_type": "notebook",
        "size": 12013
      },
      "code_examples": [
        "from io import StringIO\n\nfrom azure.storage.blob import BlobServiceClient\nimport datarobot as dr\nimport pandas as pd",
        "# Set DataRobot connection info here\nDATAROBOT_API_TOKEN = \"\"\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-AZURE-78\",  # Optional but helps DataRobot improve this workflow\n)",
        "# Set Azure connection blob info here\nAZURE_STORAGE_ACCOUNT = \"\"\nAZURE_STORAGE_CONTAINER = \"\"\n\n# Find this value by following the \"Access keys\" link from your storage account in the Azure console\nAZURE_STORAGE_ACCESS_KEY = \"\"\n\n# Provide dataset filenames and the modeling target feature\nAZURE_INPUT_FILE = \"input.csv\"\nAZURE_OUTPUT_FILE = \"scored.csv\"\nAZURE_INPUT_TARGET = \"target\"\n\n# Set name for Azure credentials in DataRobot\nDR_CREDENTIAL_NAME = \"Azure_{}\".format(AZURE_STORAGE_ACCOUNT)\n\nproject_id = None\ndeployment_id = None",
        "account_url = \"https://{}.blob.core.windows.net\".format(AZURE_STORAGE_ACCOUNT)\nblob_service_client = BlobServiceClient(account_url)",
        "container_client = blob_service_client.get_container_client(\n    container=AZURE_STORAGE_CONTAINER\n)\ndownloaded_blob = container_client.download_blob(AZURE_INPUT_FILE)\n\ndf = pd.read_csv(StringIO(downloaded_blob.content_as_text()))",
        "# Use this code to look up the ID of the credential object created.\ncredential = None\nfor cred in dr.Credential.list():\n    if cred.name == DR_CREDENTIAL_NAME:\n        credential = cred\n\nif credential == None:\n    credential = dr.Credential.create_azure(\n        name=DR_CREDENTIAL_NAME,\n        azure_connection_string=\"DefaultEndpointsProtocol=https;AccountName={};AccountKey={};\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_ACCESS_KEY\n        ),\n    )\n\ncredential",
        "# Create a project without setting the target\nif project_id == None:\n    project = dr.Project.create(project_name=\"New Test Project (Azure)\", sourcedata=df)\n    print(project.id)",
        "if project_id == None:\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=AZURE_INPUT_TARGET,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight",
        "if project_id == None:\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current Autopilot status:\n    # project.stage\n    # project.get_model_jobs()",
        "print(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()",
        "if deployment_id == None:\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment (Azure)\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)",
        "job = dr.BatchPredictionJob.score(\n    deployment=deployment.id,\n    intake_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_INPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"azure\",\n        \"url\": \"https://{}.blob.core.windows.net/{}/{}\".format(\n            AZURE_STORAGE_ACCOUNT, AZURE_STORAGE_CONTAINER, AZURE_OUTPUT_FILE\n        ),\n        \"credential_id\": credential.credential_id,\n    },\n    # Uncomment the next line to include prediction explanations.\n    # max_explanations=3,\n    passthrough_columns_set=\"all\",\n)\njob.wait_for_completion()\njob.get_status()"
      ],
      "api_methods": [
        "deployment.get",
        "project.wait_for_autopilot",
        "deployment.create_from_learning_model",
        "dr.project.create",
        "deployment.update_drift_tracking_settings",
        "project.stage",
        "dr.enums.autopilot_mode",
        "dr.deployment.get",
        "project.get_model_jobs",
        "dr.enums.recommended_model_type",
        "dr.credential.list",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "dr.modelrecommendation.get_all",
        "deployment.update_association_id_settings",
        "deployment.id",
        "project.create",
        "dr.modelrecommendation.get",
        "dr.credential.create_azure",
        "dr.batchpredictionjob.score",
        "project.analyze_and_model"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_8815823537286100560",
      "title": "Custom Model End-to-End With Compliance Docs: Custom Model End-to-End With Compliance Docs.config.yaml",
      "content": "---\ntitle: Custom Model End-to-End With Compliance Docs\nfile_name: Custom Model End-to-End With Compliance Docs.ipynb\ndescription: |-\n  This accelerator outlines how to create, deploy, and monitor a custom inference model with DataRobot's Python client. You can use the Custom Model Workshop to upload a model artifact to create, test, and deploy custom inference models to DataRobot’s centralized deployment hub.\nlanguages:\n  - python\nmaintainers:\n  - Luke Shulman\nmaintainers_email:\n  - luke.shulman@datarobot.com\nsmoke_test:\n  run_smoke_test: false\n  user_permissions:\n    - ENABLE_PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS\ntags: []\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Custom Model End-to-End With Compliance Docs/Custom Model End-to-End With Compliance Docs.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Custom Model End-to-End With Compliance Docs",
        "file_type": "yaml",
        "size": 640
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.55,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_7394199518252556440",
      "title": "Custom Model End-to-End With Compliance Docs: Custom Model End-to-End With Compliance Docs.ipynb",
      "content": "",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Custom Model End-to-End With Compliance Docs/Custom Model End-to-End With Compliance Docs.ipynb",
      "tags": [
        "templates",
        "integration",
        "ai-accelerators",
        "ecosystem",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Custom Model End-to-End With Compliance Docs",
        "file_type": "notebook",
        "size": 1697363
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-7489533336302726481",
      "title": "DataRobot-GraphQL: README.md",
      "content": "# DataRobot and GraphQL\n\nThis notebook is intended for a GraphQL developer who wants to integrate with DataRobot.\n\nIn this is example implementation, a GraphQL server is connecting to the DataRobot OpenAPI specification using  [GraphQL Mesh](https://the-guild.dev/graphql/mesh), the currently maintained option.\n\n## Reference\n\nBelow are a some urls that where found useful during development\n\n- https://the-guild.dev/graphql/mesh/docs/getting-started/installation\n- https://app.datarobot.com/api/v2/openapi.yaml\n\n## Requirements\n\nFor this process to work, the developer will need the folowing software available either via a command on the command line or as a url in the browser\n\n- A working DataRobot log in with a valid API Key\n- node\n- yarn\n\n### Prep the 3rd party software\n\n```bash\nnpm i @graphql-mesh/cli graphql\n\nyarn add concurrently # (optional: -W)\n```\n\n## Configure Mesh\n\nBelow is a sample of the `./DataRobot-GraphQL/packages/datarobot-source/.meshrc.yaml` which is used to configure the GraqhQL Mesh gateway\n\n```yaml\nsources:\n  - name: DataRobot\n    handler:\n      openapi:\n        source: https://app.datarobot.com/api/v2/openapi.yaml\n        operationHeaders:\n          Content-Type: application/json\n          Authorization: Bearer <your key goes here>\n\ntransforms:\n  - rename:\n      renames:\n        - from:\n            type: Query\n            field: AppController_(.*)\n          to:\n            type: Query\n            field: $1\n          useRegExpForFields: true\n```\n\n## Example GraphQL Mesh Server Start\n\n```bash\nyarn start-datarobot-source\nyarn run v1.22.21\n$ concurrently --kill-others \"yarn workspace mesh-datarobot-source start\"\n$ mesh dev\n[0] 💡 🕸️  Mesh - Server Starting GraphQL Mesh...\n[0] 💡 🕸️  Mesh - DataRobot Generating GraphQL schema from OpenAPI schema\n[0] 💡 🕸️  Mesh - DataRobot Processing annotations for the execution layer\n[0] 💡 🕸️  Mesh Generating index file in TypeScript\n[0] 💡 🕸️  Mesh - Server Serving GraphQL Mesh: http://0.0.0.0:4000\n[0] 💡 🕸️  Mesh Writing index.ts for ESM to the disk.\n[0] 💡 🕸️  Mesh Writing index.ts for CJS to the disk.\n```\n\n## Review the results\n\nYou can now start working with this integration by opening a browser and pointing to:\n\n`http://localhost:4000/`\n\n### Example Results\n\nBelow, please find example output for connecting to DataRobot via the OpenAPI spec via GraphQL UI @ http://0.0.0.0:4000\n\n![Example Credential Search Result with GraphQL ](img/example-graphql-result.png)",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/DataRobot-GraphQL/README.md",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "DataRobot-GraphQL",
        "file_type": "markdown",
        "size": 2503
      },
      "code_examples": [
        "## Configure Mesh\n\nBelow is a sample of the `./DataRobot-GraphQL/packages/datarobot-source/.meshrc.yaml` which is used to configure the GraqhQL Mesh gateway"
      ],
      "api_methods": [],
      "complexity_score": 0.456,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-2797619138746958662",
      "title": "DataRobot-GraphQL: package.json",
      "content": "{\n  \"name\": \"datarobotapi\",\n  \"version\": \"1.0.0\",\n  \"private\": true,\n  \"repository\": \"https://github.com/datarobot/denniswhitney-super-duper-repo.git\",\n  \"author\": \"Dennis Whitney <dennis.whitney@datarobot.com>\",\n  \"license\": \"MIT\",\n  \"description\": \"Connects the DataRobot OpenAPI spec to GraphQL\",\n  \"workspaces\": {\n    \"packages\": [\n      \"packages/*\"\n    ]\n  },\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\",\n    \"start-datarobot-source\": \"concurrently --kill-others \\\"yarn workspace mesh-datarobot-source start\\\"\"\n  },\n  \"dependencies\": {\n    \"@graphql-mesh/cli\": \"^0.88.5\",\n    \"concurrently\": \"^8.2.2\",\n    \"graphql\": \"^16.8.1\"\n  }\n}\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/DataRobot-GraphQL/package.json",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "DataRobot-GraphQL",
        "file_type": "json",
        "size": 668
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_2937192416160857292",
      "title": "Databricks & Datarobot - Large Scale Forecasting: Databricks & Datarobot - Large Scale Forecasting.config.yaml",
      "content": "---\ntitle: End-to-end time series workflow with DataRobot and Databricks\nfile_name: Databricks & Datarobot - Large Scale Forecasting.ipynb\ndescription: |-\n  This accelerator is developed for use with Databricks to help users leverage the power of DataRobot for time-series modeling within their Databricks ecosystem.\nlanguages:\n  - python\nmaintainers:\n  - Austin Chou\nmaintainers_email:\n  - austin.chou@datarobot.com\ntags: []\nsmoke_test:\n  run_smoke_test: false\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Databricks & Datarobot - Large Scale Forecasting/Databricks & Datarobot - Large Scale Forecasting.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "time-series",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Databricks & Datarobot - Large Scale Forecasting",
        "file_type": "yaml",
        "size": 462
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.15,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_927313794518775080",
      "title": "Databricks & Datarobot - Large Scale Forecasting: Databricks & Datarobot - Large Scale Forecasting.ipynb",
      "content": "# End-to-end demand forecasting workflow with DataRobot and Databricks \n\nAuthors: Austin Chou, Andrew Mathis\n\nReference: [DataRobot API documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n## Summary\n\nThis notebook outlines a use case forecasting future sales for multiple stores via multiseries modeling. Multiseries modeling allows you to model datasets that contain multiple time series based on a common set of input features. As in this example, multiseries forecasting is often useful for large chain businesses that want to more accurately manage their inventory and staffing by predicting the sales volume in the future. \n\nThis notebook focuses on working with Databricks and walks through how to use the Python API client to:\n\n1. Connect to DataRobot\n2. Import data from Databricks into the AI Catalog\n3. Create a time series forecasting project and run Autopilot\n4. Retrieve and evaluate model performances and insights\n5. Make new predictions with a test dataset\n6. Deploy a model with monitoring in MLOps\n7. Forecast predictions via the Prediction API\n\nFor this walkthrough, you can use the following publicly available dataset from the public DataRobot S3 bucket. Datasets for the excercise can be downloaded from here:\n\n* [Multiseries sales forecasting - Training data](https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_training.csv)\n* [Multiseries sales forecasting - Predictions data](https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_prediction.csv)\n\n## Setup\n\n### Import libraries\n\n```python\n# DataRobot Python library\nimport datetime as dt\n\nimport datarobot as dr\nimport matplotlib.pyplot as plt\n\n# The following are libraries used in this notebook for data and model evaluation\nimport pandas as pd\n```\n\n#### Optional: Import public training data for walkthrough (If data not already available in Databricks)\n\nFor this walkthrough, you can usea publicly available dataset ('DR_Demo_Sales_Multiseries_training.csv') from the public DataRobot S3 bucket to create a temporary table in Databricks. This will let you run the cells in this notebook and follow along.\n\n```python\n# Pull data from public DataRobot datasets\n# Training Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_training.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_training\")\n\n\n# Test Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_prediction.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_prediction\")\n```\n\n## Connect to DataRobot\n\nTo connect to DataRobot, you need to provide your API token and the endpoint. For more information, please refer to the following documentation:\n\n- [Create and manage API keys via developer tools in the GUI](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management)\n- [Different options to connect to DataRobot from the API client](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n\nYour API token can be found in the DataRobot UI in the **Developer tools** section, accessed from the profile menu in the top right corner. Copy the API token and paste in the cell below.\n\n```python\n# Connect to the DataRobot client\n# API Token\nDATAROBOT_API_TOKEN = \"INSERT YOUR DATAROBOT API TOKEN\"  # You can find the API token under the Developer Tools in the UI\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # If you have another endpoint (e.g. on-prem installs), change this accordingly\n\n# Connect to client\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)\n```\n\n## Import data from Databricks into the AI Catalog\n\nCurrently, DataRobot supports ingesting data via (1) pulling the data into a notebook, converting to a pandas dataframe, and then ingesting to DataRobot OR (2) directly ingesting data via JDBC connection to your Databricks data source.\n\nWe will demonstrate the first approach here. For large datasets, we recommend the second approach in order to bypass the need to convert the PySpark dataframe into a Pandas dataframe. ([Click here for documentation on setting up data connections and data sources for direct ingest.](https://docs.datarobot.com/en/docs/data/connect-data/data-conn.html))\n\n```python\n# Query and prepare data for ingest\ntraining_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_training \"\"\")\n\n# Convert to pandas df\ntraining_df = training_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# Prepare our dataframe accordingly:\ntraining_df[[\"Date\"]] = training_df[[\"Date\"]].apply(\n    pd.to_datetime\n)  # Convert Date to datetime\ntraining_df = training_df.sort_values(by=[\"Store\", \"Date\"])  # Order\n\ntraining_df.head(5)\n```\n\n```python\n# Let's also visualize our data:\ntraining_df.pivot(index=\"Date\", columns=\"Store\", values=\"Sales\").plot(figsize=(18, 8))\nplt.show()\n```\n\n```python\n# Upload data to DataRobot AI Catalog\nnew_dataset = dr.Dataset.upload(source=training_df)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Sales_Multiseries_training\")\n```\n\nDatasets in the AI Catalog are assigned a **dataset ID** which you can use to reference/get the dataset via the API.\n\n```python\n# Quick link to the AI Catalog dataset you just created\nprint(\"AI Catalog URL: \" + new_dataset.get_uri())\nprint(\"Dataset ID: \" + new_dataset.id)\n```\n\n## Create a time series forecasting project and run Autopilot\n\nYou can **create DataRobot projects** directly from:\n\n* A dataset in AI Catalog (using the dataset's ID in DataRobot)\n* A pandas dataframe (you do not need to write back to a data source or disk)\n* Directly from data sources\n\nOnce a project is created, you can tune and iterate on various modeling options (time series modeling, partition details, accuracy metrics, feature lists, etc.) and start modeling. For time series, these models can range from integrated models (Arima, RNNs), per forecast distance models (XGboost, elastic net), Trends and decomposition models, and more. In addition to this, the modeling process will also create and explore various time-series features - baseline features, rolling statistics, seasonal features based on date time, etc.\n\nYou can actively watch the project in action in the DataRobot UI after initiating Autopilot. The API and UI are parallel gateways to working on the same project which allows for cross-functional collaboration.\n\nEach created project is associated with a unique project ID. You can use the project ID to retrieve the project of interest via the API later on.\n\nReference the following resources for more information about time series modeling projects:\n* [Python API reference](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/time_series.html)\n* [Time Series modeling framework](https://docs.datarobot.com/en/docs/modeling/time/ts-reference/ts-framework.html)\n\n### Create a project\n\nWe create a project using our dataset in the AI Catalog:\n\n```python\n# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"[AIA] Multi_Store_Sales_Forecast\", dataset_id=new_dataset.id\n)\n```\n\nDataRobot Projects are assigned a **project ID** which you can use to reference/get the dataset via the API.\n\n```python\n# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)\n```\n\n### Configure time-series modeling settings\n\nTime-series projects have a number of parameters we can adjust. This includes:\n\n* Multi-series (i.e. Series ID column)\n* Backtest partitioning\n* Feature Derivation Window\n* Forecast Window\n* Known-in-advance (KA) Variables\n* Do not derive (DND) Variables\n* Calendars\n\nWe will set a number of these parameters. Additional information can be referenced in the [Time series modeling documentation](https://docs.datarobot.com/en/docs/modeling/time/ts-flow-overview.html). Here are some initial parameters to get started:\n\n**What rolling window should DataRobot use to derive features?** \n\nThe Feature Derivation window (FDW) represents the rolling window that is used to derive time series features and lags, relative to the Forecast Point (i.e. date of prediction). For example, we will build features within the rolling window of last 35 days in this accelerator.\n\n**Which future values do you want to forecast?**\n\nThe Forecast Window (FW) represents the rolling window of future values to predict, relative to the Forecast Point. For example, to predict sales for next 7 days, we enter 1 to 7 days. ([Click here for additional documentation on FDW and FW.](https://docs.datarobot.com/en/docs/modeling/time/ts-adv-modeling/ts-customization.html#set-window-values))\n\n**What are [\"Known-in-advance\" (KA) features](https://docs.datarobot.com/en/docs/modeling/time/ts-adv-modeling/ts-adv-opt.html#set-known-in-advance-ka)?**\n\nKA features are variables that you know in advance and can use as inputs for that row at the time of prediction. For example, our marketing team will already set the marketing campaign ahead of each date, so we do not need to lag the Marketing feature (i.e. Marketing is a KA feature).\n\n**How can we include special calendar events during feature engineering?**\n\nYou can [generate or upload a customized calendar](https://docs.datarobot.com/en/docs/modeling/time/ts-adv-modeling/ts-adv-opt.html#calendar-files) to the project. DataRobot will include additional feature engineering based on the calendar as part of the time-series feature engineering process.\n\n```python\n# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-35, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 7)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = [\"Store_Size\", \"Marketing\", \"TouristEvent\"]\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Date\"]),\n)  # Last date in calendar\n```\n\nWe pass all our settings to a [DatetimePartitioningSpecification](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/spec/datetime_partition.html?highlight=datetimepartitioningspecification#setting-up-a-datetime-partitioned-project) object which will then be passed to our Autopilot process.\n\n```python\n# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Date\",  # Date column\n    multiseries_id_columns=[\"Store\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)\n```\n\n### Start modeling with autopilot\n\nTo start the Autopilot process, call the `analyze_and_model` function. Provide the prediction target and our DatetimePartitioningSpecification as part of the function call. We have several modes to spin up Autopilot - in this demo, we will use the default \"Quick\" mode.\n\n```python\n# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)\n```\n\n```python\n# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)\n```\n\n## Retrieve and evaluate model performances and insights\n\nAfter Autopilot completes, you can easily evaluate your model results. Evaluation can include compiling the Leaderboard as a dataframe, measuring performances across different backtest partitions with different metrics, visualizing the accuracy across series, analyzing Feature Impact and Feature Effects to understand each models' behaviors, and more. This can be done for every single model created by DataRobot.\n\nAs a simple example in this notebook, **we identify the best model created by Autopilot and evaluate:**\n\n* RMSE performance\n* MASE performance\n* Accuracy for Time for various Forecast Distance and Series combinations\n* Feature Impact of Top 10 features\n* Compare Accuracy across Series\n\n```python\n# # For convenience, we can access the project directly with the project ID:\n# project = dr.Project.get(project_id='65021b8e737ea778d21a78d8')\n```\n\n```python\n# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    scores = scores.append(model_performances, sort=False).reset_index(drop=True)\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores\n```\n\n```python\n# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)\n```\n\n### Model performance by metric\n\nAs a demonstration, we will get the performance of our model for:\n\n* **RMSE**: The optimization metric used by Autopilot in this project. We could have used another optimization metric when we kicked off the autopilot process with \"analyze_and_model\").\n* [**MASE (Mean Absolute Scaled Error)**](https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html#mase): Autopilot automatically uses a naive baseline model (e.g. using most recent value as the forecasted value) and scales the error to the naive model. MASE < 1 means an improvement over baseline, whereas MASE > 1 means the model performs worse than the naive approach (e.g. just taking the most recent value).\n\n```python\nprint(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)\n```\n\n### Get Accuracy Over Time\n\nDataRobot provides two helpful views of our forecasts out-of-the-box:\n\n* [**Accuracy Over Time**](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/aot.html) fixes the forecast distance and visualizes the corresponding forecast for each forecasted date.\n* [**Forecast vs Actual**](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/fore-act.html) sets a specific forecast point and visualizes the corresponding forecasts for the entire forecast window.\n\nWe can pull the results out for either analysis. As a demonstration, we will generate the Accuracy Over Time plots for forecast distances of 1 day and 7 day.\n\n```python\n# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()\n```\n\n```python\n# Get Accuracy over Time for FD=7, For just the Baltimore store series\nacc_plot_FD7_Baltimore = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=2, series_id=\"Baltimore\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD7_Baltimore.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()\n```\n\n### Retrieve Feature Impact\n\nAs an example of model explainability, calculate the Feature Impact values of the model using the `get_or_request_feature_impact` function.\n\n```python\n# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:10]\n\n# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()\n```\n\n### Analyze Accuracy for each Series\n\nThe [**Series Insight**](https://docs.datarobot.com/en/docs/modeling/analyze-models/evaluate/series-insights-multi.html) tool provides the ability to compute the accuracy for each indivudal series. This is especially powerful to help us identify which series the model is doing particularly better or worse in forecasting.\n\nIn this demonstration, we see that the model has particularly high RMSE for the Savannah and Louisville store forecasts. We may consider refining our model by splitting those two series into a separate model as a future modeling experiment.\n\n```python\n# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing\n```\n\n```python\n# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight\n```\n\n```python\n# Visualize the performance by stores\nplt.scatter(model_series_insight[\"Series\"], model_series_insight[\"backtestingScore\"])\np = plt.xticks(rotation=45, horizontalalignment=\"right\")\np = plt.ylabel(\"RMSE\")\n```\n\n## Make new predictions with a test dataset\n\nWe can make new predictions directly on the leaderboard by uploading new test datasets to the project. We can then score the test dataset with any model on the leaderboard and retrieve the results.\n\nIn this notebook, we will load the data into the notebook and upload to the project. As with the training dataset, you can also use the JDBC connector to ingest the test dataset into the AI catalog and then directly use the dataset ID in the `request_predictions` call. The JDBC path provides more efficient upload of large datasets to DataRobot.\n\nFor time-series predictions, DataRobot expects new prediction datasets to have rows for each new date we want to forecast. These rows should have values for the known-in-advance features and NA everywhere else. For example, since the model is forecasting 1-7 days out from each forecast point, we will have 7 new rows (corresponding from June 15 to June 21, 2014).\n\n**Note: We assume the prediction data is already available within Databricks. If not, please refer to the optional setup section to download and create a temporary view of the prediction dataset.**\n\n```python\n# Query and prepare data for ingest\ntest_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_prediction \"\"\")\n\n# Convert to pandas df\ntest_df = test_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# We prepare our dataframe accordingly:\ntest_df[[\"Date\"]] = test_df[[\"Date\"]].apply(pd.to_datetime)  # Convert Date to datetime\ntest_df = test_df.sort_values(by=[\"Store\", \"Date\"])  # Order\n```\n\n```python\n# Upload data to modeling project\ndataset = project.upload_dataset(test_df)\n\n# Make test predictions on the top model\npred_job = top_model.request_predictions(\n    dataset_id=dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=80,\n)\n\n# Get prediction results\npreds = pred_job.get_result_when_complete()\npreds.head(10)\n```\n\n## Deploy a model with monitoring in MLOps\n\nWith a single function call or click on the UI, DataRobot can quickly deploy models into production while fully reproducing the entire modeling pipeline including the necessary data preprocessing steps utilized by the blueprints and any advanced feature engineering that are part of the project. Once deployed, you can call the DataRobot REST or Python API to make batch and real-time predictions. You can also configure and schedule recurring batch prediction jobs that write back into a database.\n\nOnce a model is deployed, you can access MLOps monitoring capabilities such as:\n\n* Service health\n* Data drift\n* Prediction accuracy\n* Model retraining\n\nTo deploy a model, call the `create_from_learning_model` function and provide the ID of the model you want to deploy and the ID of the prediction server you want to deploy into.\n\nFor additional information, please see documentation for:\n\n1. [**MLOps monitoring**](https://docs.datarobot.com/en/docs/mlops/mlops-overview.html) \n2. [**Available prediction methods**](https://docs.datarobot.com/en/docs/predictions/index.html)\n3. [**Other deployment workflows with DataRobot**](https://docs.datarobot.com/en/docs/mlops/deployment/deploy-workflows/index.html)\n\n**Note: This demo assumes you have a prediction server available, such as in the DataRobot managed cloud instance. Please check that you have an accessible prediction server for your account and that you have available deployment slots before continuing.**\n\n```python\n# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    label=\"[AIA] Multi-Store Forecasting Deployment\",\n    description=\"[AIA] Multi Store Forecasting Deployment\",\n    default_prediction_server_id=prediction_server_id,\n)\n```\n\nEvery deployment in DataRobot is assigned a **deployment ID** which you can use to reference/retrieve the deployment via the API.\n\n```python\n# Quick link to the deployment you just created\nprint(\"DataRobot Deployment URL: \" + deployment.get_uri())\nprint(\"Deployment ID: \" + deployment.id)\n```\n\n### Configure model monitoring\n\nIn this example, we set up the deployment to monitor data drift. We use the following API call:\n\n* Data drift: `update_drift_tracking_settings`\n\nWe additionally enable and set the deployment to return (alongside the forecast):\n* Prediction intervals: `update_prediction_intervals_settings`\n* Prediction explanations: `PredictionExplanationsInitialization.create`\n\n```python\n# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n```\n\n```python\n# For time-series deployments, we can also set the prediction intervals for each forecast.\ndeployment.update_prediction_intervals_settings(percentiles=[95], enabled=True)\n```\n\n```python\n# In order to compute prediction explanations at time of predictions, initialize it for the best model\ndr.PredictionExplanationsInitialization.create(\n    project_id=project.id, model_id=top_model.id\n)\n```\n\n## Forecast predictions via the Prediction API\n\nThe [Batch Prediction API](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/index.html) provides flexible options for intake and output when scoring large datasets using the prediction servers you have already deployed. The API is exposed through the DataRobot Public API and can be consumed using  any REST-enabled client or the DataRobot Python Public API bindings.\n\nTo ensure the Batch Prediction API can process your time series dataset, you must configure the following:\n* Sort prediction rows by their timestamps, with the earliest row first.\n* If using multiseries, the prediction rows must be sorted by series ID then timestamp.\n* There is no limit on the number of series DataRobot supports. For more information, see [Time series data requirements for batch predictions via API](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/batch-pred-ts.html).\n\nIn this notebook, we will perform batch predictions with the 'test_df' dataset by calling the API endpoint and getting the results out as a pandas dataframe.\n\nAdditional methods to score DataRobot models include:\n* Reading from and writing to different data sources ([General Batch Prediction documentation](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/index.html)). For example, we can leverage JDBC connectors to read from Databricks and write to S3 with [score](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/batch_predictions.html?highlight=score#wiring-a-batch-prediction-job-manually).\n* Using the model's scoring code in a Databricks notebook for distributed spark scoring ([AI Accelerator demonstrating spark scoring here](https://community.datarobot.com/t5/ai-accelerators-library/end-to-end-workflows-with-datarobot-and-databricks/ta-p/16461)). Note that not every model will have scoring code available. For example, the top model in this project does not. You can identify scoring-code-capable models on the leaderboard by whether there is a Scoring Code tag.\n\n```python\njob, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, test_df, max_explanations=3  # Deployment ID  # Scoring dataset\n)  # If prediction explanations are required\n\nprint(\"Started scoring...\", job)\njob.wait_for_completion()\n```\n\n```python\n# Take a look at our prediction results\n# Format the prediction output. Select specific columns we want to view\nselect_predictions_df = predictions[\n    [\n        \"Store_y\",\n        \"FORECAST_POINT\",\n        \"Date_y\",\n        \"Sales (actual)_PREDICTION\",\n        \"PREDICTION_95_PERCENTILE_LOW\",\n        \"PREDICTION_95_PERCENTILE_HIGH\",\n        \"Marketing\",\n        \"EXPLANATION_1_FEATURE_NAME\",\n        \"EXPLANATION_2_FEATURE_NAME\",\n    ]\n]\nselect_predictions_df = select_predictions_df.rename(\n    columns={\"Store_y\": \"Store\", \"Date_y\": \"Date\"}\n)\n\n# Output predictions for the next seven days for two stores\nselect_predictions_df.head(14)\n```\n\n## Clean up\n\nTo remove everything added in the DataRobot platform as part of this notebook, run the following cell. If you want to delete any tables created in Databricks as a result of running this notebook, you will need to manually do so in Databricks.\n\n```python\n# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# deployment.delete()\n# project.delete()\n# CALENDAR.delete(CALENDAR.id)\n# new_dataset.delete(new_dataset.id)\n```\n\n## Additional Resources\n\n* The [DataRobot AI Accelerator Library](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library) has similar accelerators for other [Ecosystem Integrations](https://community.datarobot.com/t5/ai-accelerators-library/tkb-p/ai-accelerators-library/label-name/ecosystem%20integration%20templates) to use DataRobot with other tools (e.g. AWS, GCP, Azure, etc) as well as accelerators for more advanced time-series applications.\n* The [DataRobot API user guide](https://docs.datarobot.com/en/docs/api/guide/python/index.html) provides code examples covering topics such as model factories, classification problems, feature impact rank ensembling, and more.\n* Learn how to [schedule notebooks in Databricks](https://docs.databricks.com/notebooks/schedule-notebook-jobs.html).\n\nTo learn more about advanced workflows for handling complex and large scale time series problems:\n* [Time series clustering](https://docs.datarobot.com/en/docs/modeling/time/ts-clustering.html#time-series-clustering)\n* [Segmented modeling](https://docs.datarobot.com/en/docs/modeling/time/ts-segmented.html)",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Databricks & Datarobot - Large Scale Forecasting/Databricks & Datarobot - Large Scale Forecasting.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "openai",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "time-series",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Databricks & Datarobot - Large Scale Forecasting",
        "file_type": "notebook",
        "size": 972162
      },
      "code_examples": [
        "# DataRobot Python library\nimport datetime as dt\n\nimport datarobot as dr\nimport matplotlib.pyplot as plt\n\n# The following are libraries used in this notebook for data and model evaluation\nimport pandas as pd",
        "# Pull data from public DataRobot datasets\n# Training Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_training.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_training\")\n\n\n# Test Dataset\ndata_path = \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_Sales_Multiseries_prediction.csv\"\npd_df = pd.read_csv(data_path, infer_datetime_format=True, engine=\"c\")\n\n# Convert into Spark DataFrame\n# For demo purposes, we'll create a local temporary view\nspark_df = spark.createDataFrame(pd_df)\nspark_df.createOrReplaceTempView(\"Sales_Multiseries_prediction\")",
        "# Connect to the DataRobot client\n# API Token\nDATAROBOT_API_TOKEN = \"INSERT YOUR DATAROBOT API TOKEN\"  # You can find the API token under the Developer Tools in the UI\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"  # If you have another endpoint (e.g. on-prem installs), change this accordingly\n\n# Connect to client\ndr.Client(token=DATAROBOT_API_TOKEN, endpoint=DATAROBOT_ENDPOINT)",
        "# Query and prepare data for ingest\ntraining_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_training \"\"\")\n\n# Convert to pandas df\ntraining_df = training_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# Prepare our dataframe accordingly:\ntraining_df[[\"Date\"]] = training_df[[\"Date\"]].apply(\n    pd.to_datetime\n)  # Convert Date to datetime\ntraining_df = training_df.sort_values(by=[\"Store\", \"Date\"])  # Order\n\ntraining_df.head(5)",
        "# Let's also visualize our data:\ntraining_df.pivot(index=\"Date\", columns=\"Store\", values=\"Sales\").plot(figsize=(18, 8))\nplt.show()",
        "# Upload data to DataRobot AI Catalog\nnew_dataset = dr.Dataset.upload(source=training_df)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Sales_Multiseries_training\")",
        "# Quick link to the AI Catalog dataset you just created\nprint(\"AI Catalog URL: \" + new_dataset.get_uri())\nprint(\"Dataset ID: \" + new_dataset.id)",
        "# Create a new DataRobot project\nproject = dr.Project.create_from_dataset(\n    project_name=\"[AIA] Multi_Store_Sales_Forecast\", dataset_id=new_dataset.id\n)",
        "# Quick link to the DataRobot project you just created\n# Note: the get_uri for projects goes to the Model tab. This won't be populated yet since we haven't run Autopilot.\n# Switch to the Data tab in the UI after following the url to get to the project setup section.\nprint(\"DataRobot Project URL: \" + project.get_uri())\nprint(\"Project ID: \" + project.id)",
        "# Set Time Series Parameters\n# Feature Derivation Window\n# What rolling window should DataRobot use to derive features?\nFDW = [(-35, 0)]\n\n# Forecast Window\n# Which future values do you want to forecast? (i.e. Forecast Distances)\nFW = [(1, 7)]\n\n# Known In Advance features\n# Features that will be known at prediction time - all other features will go through an iterative feature engineering and selection process to create time-series features.\nFEATURE_SETTINGS = []\nKA_VARS = [\"Store_Size\", \"Marketing\", \"TouristEvent\"]\nfor column in KA_VARS:\n    FEATURE_SETTINGS.append(\n        dr.FeatureSettings(column, known_in_advance=True, do_not_derive=False)\n    )\n\n# Calendar\n# Create a calendar file from a dataset to see how specific events by date contribute to better model performance\nCALENDAR = dr.CalendarFile.create_calendar_from_country_code(\n    country_code=\"US\",\n    start_date=min(training_df[\"Date\"]),  # Earliest date in calendar\n    end_date=max(training_df[\"Date\"]),\n)  # Last date in calendar",
        "# Create DatetimePartitioningSpecification\n# The DatetimePartitioningSpecification object is how we pass our settings to the project\ntime_partition = dr.DatetimePartitioningSpecification(\n    # General TS settings\n    use_time_series=True,\n    datetime_partition_column=\"Date\",  # Date column\n    multiseries_id_columns=[\"Store\"],  # Multi-series ID column\n    # FDW and FD\n    forecast_window_start=FW[0][0],\n    forecast_window_end=FW[0][1],\n    feature_derivation_window_start=FDW[0][0],\n    feature_derivation_window_end=FDW[0][1],\n    # Advanced settings\n    feature_settings=FEATURE_SETTINGS,\n    calendar_id=CALENDAR.id,\n)",
        "# Start Autopilot\nproject.analyze_and_model(\n    # General parameters\n    target=\"Sales\",  # Target to predict\n    worker_count=-1,  # Use all available modeling workers for faster processing\n    # TS options\n    partitioning_method=time_partition,  # Feature settings\n)",
        "# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)",
        "# # For convenience, we can access the project directly with the project ID:\n# project = dr.Project.get(project_id='65021b8e737ea778d21a78d8')",
        "# Identify the best model by the optimization metric\nmetric_of_interest = project.metric\n\n# Get all models\nall_models = project.get_datetime_models()\n\n# Extract models that have a \"All Backtests\" performance evaluation for our metric\nbest_models = sorted(\n    [model for model in all_models if model.metrics[project.metric][\"backtesting\"]],\n    key=lambda m: m.metrics[project.metric][\"backtesting\"],\n)\n\n# Iterate through the models and extract model metadata and performance\nscores = pd.DataFrame()\n\nfor m in best_models:\n    model_performances = pd.DataFrame(\n        [\n            {\n                \"Project_Name\": project.project_name,\n                \"Project_ID\": project.id,\n                \"Model_ID\": m.id,\n                \"Model_Type\": m.model_type,\n                \"Featurelist\": m.featurelist_name,\n                \"Optimization_Metric\": project.metric,\n                \"Partition\": \"All backtests\",\n                \"Value\": m.metrics[project.metric][\"backtesting\"],\n            }\n        ]\n    )\n    scores = scores.append(model_performances, sort=False).reset_index(drop=True)\n\n# Sort by performance value\nscores = scores.sort_values(\n    by=\"Value\", ascending=True\n)  # Sort ascending so best model (lowest RMSE) is first\nscores",
        "# Select the top model in our project for further evaluation\ntop_model = dr.Model.get(project=project.id, model_id=scores[\"Model_ID\"][0])\n\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)",
        "print(\n    \"Top Model RMSE performance (All Backtests): \"\n    + str(top_model.metrics[\"RMSE\"][\"backtesting\"])\n)\nprint(\n    \"Top Model MASE performance (All Backtests): \"\n    + str(top_model.metrics[\"MASE\"][\"backtesting\"])\n)",
        "# Get Accuracy over Time for FD=1, Averaged for all series\nacc_plot_FD1_Avg = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=1, series_id=None\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD1_Avg.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()",
        "# Get Accuracy over Time for FD=7, For just the Baltimore store series\nacc_plot_FD7_Baltimore = top_model.get_accuracy_over_time_plot(\n    backtest=0, forecast_distance=2, series_id=\"Baltimore\"\n)\n\n# Convert to dataframe\ndf = pd.DataFrame.from_dict(acc_plot_FD7_Baltimore.bins)\n\n# Plot\nfigure = df.plot(\"start_date\", [\"predicted\", \"actual\"]).get_figure()",
        "# Request and retrieve feature impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n\n# Take top 10\nFI_df = FI_df[0:10]\n\n# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()",
        "# Trigger the Series Insight computation\nseries_insight_job = top_model.compute_series_accuracy()\nseries_insight_job.wait_for_completion()  # Complete job before progressing",
        "# Retrieve Series Accuracy\nmodel_series_insight = top_model.get_series_accuracy_as_dataframe(\n    metric=\"RMSE\", order_by=\"backtestingScore\"\n)\n\n# Unlist 'multiseriesValues' to 'Series' column\nmodel_series_insight[\"multiseriesValues\"] = model_series_insight[\n    \"multiseriesValues\"\n].apply(lambda x: x[0])\nmodel_series_insight.rename(columns={\"multiseriesValues\": \"Series\"}, inplace=True)\n\n# View\nmodel_series_insight",
        "# Visualize the performance by stores\nplt.scatter(model_series_insight[\"Series\"], model_series_insight[\"backtestingScore\"])\np = plt.xticks(rotation=45, horizontalalignment=\"right\")\np = plt.ylabel(\"RMSE\")",
        "# Query and prepare data for ingest\ntest_df = spark.sql(\"\"\" SELECT * FROM Sales_Multiseries_prediction \"\"\")\n\n# Convert to pandas df\ntest_df = test_df.toPandas()\n\n# For time-series projects, DataRobot expects a dataset that is ordered by our Multi-series ID (e.g. Store) and Date\n# We prepare our dataframe accordingly:\ntest_df[[\"Date\"]] = test_df[[\"Date\"]].apply(pd.to_datetime)  # Convert Date to datetime\ntest_df = test_df.sort_values(by=[\"Store\", \"Date\"])  # Order",
        "# Upload data to modeling project\ndataset = project.upload_dataset(test_df)\n\n# Make test predictions on the top model\npred_job = top_model.request_predictions(\n    dataset_id=dataset.id,\n    include_prediction_intervals=True,\n    prediction_intervals_size=80,\n)\n\n# Get prediction results\npreds = pred_job.get_result_when_complete()\npreds.head(10)",
        "# Set the prediction server to deploy to\nprediction_server_id = dr.PredictionServer.list()[\n    0\n].id  # EDIT THIS BASED ON THE PREDICTION SERVERS AVAILABLE TO YOU\n\n# Set deployment details\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    label=\"[AIA] Multi-Store Forecasting Deployment\",\n    description=\"[AIA] Multi Store Forecasting Deployment\",\n    default_prediction_server_id=prediction_server_id,\n)",
        "# Quick link to the deployment you just created\nprint(\"DataRobot Deployment URL: \" + deployment.get_uri())\nprint(\"Deployment ID: \" + deployment.id)",
        "# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)",
        "# For time-series deployments, we can also set the prediction intervals for each forecast.\ndeployment.update_prediction_intervals_settings(percentiles=[95], enabled=True)",
        "# In order to compute prediction explanations at time of predictions, initialize it for the best model\ndr.PredictionExplanationsInitialization.create(\n    project_id=project.id, model_id=top_model.id\n)",
        "job, predictions = dr.BatchPredictionJob.score_pandas(\n    deployment.id, test_df, max_explanations=3  # Deployment ID  # Scoring dataset\n)  # If prediction explanations are required\n\nprint(\"Started scoring...\", job)\njob.wait_for_completion()",
        "# Take a look at our prediction results\n# Format the prediction output. Select specific columns we want to view\nselect_predictions_df = predictions[\n    [\n        \"Store_y\",\n        \"FORECAST_POINT\",\n        \"Date_y\",\n        \"Sales (actual)_PREDICTION\",\n        \"PREDICTION_95_PERCENTILE_LOW\",\n        \"PREDICTION_95_PERCENTILE_HIGH\",\n        \"Marketing\",\n        \"EXPLANATION_1_FEATURE_NAME\",\n        \"EXPLANATION_2_FEATURE_NAME\",\n    ]\n]\nselect_predictions_df = select_predictions_df.rename(\n    columns={\"Store_y\": \"Store\", \"Date_y\": \"Date\"}\n)\n\n# Output predictions for the next seven days for two stores\nselect_predictions_df.head(14)",
        "# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# deployment.delete()\n# project.delete()\n# CALENDAR.delete(CALENDAR.id)\n# new_dataset.delete(new_dataset.id)"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "project.wait_for_autopilot",
        "model.metrics",
        "project.get_datetime_models",
        "model.get_series_accuracy_as_dataframe",
        "deployment.create_from_learning_model",
        "deployment.update_drift_tracking_settings",
        "dr.dataset.upload",
        "model.request_predictions",
        "project.get_uri",
        "model.id",
        "dr.predictionserver.list",
        "project.project_name",
        "dr.predictionexplanationsinitialization.create",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "dr.batchpredictionjob.score_pandas",
        "model.compute_series_accuracy",
        "project.get",
        "project.upload_dataset",
        "deployment.delete",
        "deployment.id",
        "deployment.get_uri",
        "project.delete",
        "model.get_uri",
        "model.get",
        "dr.calendarfile.create_calendar_from_country_code",
        "deployment.update_prediction_intervals_settings",
        "dr.model.get",
        "dr.project.get",
        "model.model_type",
        "project.analyze_and_model",
        "project.metric",
        "model.get_accuracy_over_time_plot",
        "project.create_from_dataset",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_4591876942021719127",
      "title": "Databricks_template: Databricks_End_To_End.config.yaml",
      "content": "---\ndescription: \"This notebook illustrates and end-to-end data science workflow using features of both DataRobot\\\n  \\ and Databricks.\\nYou will leverage DataRobot for model training, selection, and MLOps monitoring while\\\n  \\ using Databricks to facilitate datasource access and utilize the Spark backbone to perform distributed\\\n  \\ scoring to support large-scale use cases.\"\nfile_name: Databricks_End_To_End.ipynb\nlanguages:\n  - python\nmaintainers:\n  - João Gomes\nmaintainers_email:\n  - joao@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: End to end ML workflow with Databricks\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Databricks_template/Databricks_End_To_End.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Databricks_template",
        "file_type": "yaml",
        "size": 601
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.8,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-4662679503130224263",
      "title": "Databricks_template: Databricks_End_To_End.ipynb",
      "content": "# End to end ML workflow with Databricks\n\nThis notebook illustrates and end-to-end data science workflow using features of both DataRobot and Databricks. You will leverage DataRobot for model training, selection, and MLOps monitoring while using Databricks to facilitate datasource access and utilize the Spark backbone to perform distributed scoring to support large-scale use cases.\n\nThis notebook covers the following steps:\n- Acquiring a training dataset from a data table\n- Building a new DataRobot project\n- Deploying a recommended model\n- Scoring via Spark using DataRobot's exportable Java scoring code\n- Scoring via prediction API\n- Reporting monitoring data to DataRobot's MLOps agent framework\n- Writing results back to a new table\n\nPrior to execution, you need to install a few dependencies to the Databricks cluster:\n- **datarobot**, provided via PyPI (Python library used to communicate with the DataRobot platform)\n- **com.datarobot:datarobot-prediction:2.2.1**, provided via Maven Central (Java library used to establish interface with DataRobot scoring code)\n- **com.datarobot:scoring-code-spark-api_3.0.0:0.0.4**, provided via Maven Central (Java library used to wrap scoring code with Spark functionality)\n- **mlops_utils_for_spark_3_2_0_8_1_0-4c992.jar**, provided via downloadable MLOps package which is available on the Developer Tools page in the DataRobot UI (Java library used to report monitoring statistics to MLOps Agent)\n\n## Setup\n\n### Import libraries\n\nThe first cell of the notebook imports necessary packages, and sets up the connection to the DataRobot platform. There are also optional values that can be provided to use an existing project and deployment - if they are omitted then a new Autopilot session will be kicked off and a new deployment will be created using DataRobot's recommended model.\n\n```python\nfrom io import StringIO\nimport time\n\nimport datarobot as dr\nimport pandas as pd\nfrom py4j.java_gateway import java_import\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import col\nimport requests\n\napi_key = \"\"  # Get this from the Developer Tools page in the DataRobot UI\nendpoint = \"https://app.datarobot.com/\"  # This should be the URL you use to access the DataRobot UI\n\nclient = dr.Client(\n    token=api_key,\n    endpoint=endpoint,\n    user_agent_suffix=\"AIA-E2E-DBX-8\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\n# Set these to empty strings to create a new project and/or deployment\nproject_id = \"\"\ndeployment_id = \"\"\n```\n\n### Connect to DataRobot\n\n```python\ndr.Client()\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')\n```\n\nRead more about different options for [connecting to DataRobot from the client](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html).\n\n### Import data\n\nHere you'll pull in some data to work with. If a data table is available, you can provide the input table name, destination table name, and target feature in this cell. If none of those are provided, load the sample dataset provided by Databricks. This is also where any necessary data preparation would occur before sending the dataset to DataRobot. Note that DataRobot does not currently ingest Spark dataframes directly, so the dataframe will need to be converted to a Pandas dataframe prior to upload.\n\n```python\ntraining_table = \"\"\nscoring_table = \"\"\ntarget = \"\"\n\nif training_table == \"\":\n    scoring_table = \"white_wine_scored\"\n    target = \"quality\"\n    input_df = (\n        spark.read.option(\"header\", True)\n        .option(\"delimiter\", \";\")\n        .csv(\"dbfs:/databricks-datasets/wine-quality/winequality-white.csv\")\n    )\n    input_df = input_df.select(\n        [col(column).alias(column.replace(\" \", \"_\")) for column in input_df.columns]\n    )\nelse:\n    input_df = sql(\"select * from %s\" % (training_table))\n\ndf = input_df.toPandas()\ndisplay(input_df)\n```\n\n### Create a project\n\nThe Pandas dataframe is uploaded to the DataRobot platform and a name is given to the project.\n\n```python\n# Create a project wothout setting the target\nif project_id == \"\":\n    project = dr.Project.create(\n        project_name=\"New Test Project (Databricks)\", sourcedata=df\n    )\n    print(project.id)\n```\n\n## Modeling\n\n### Set the target feature\n\nHere you can define any advanced options needed for your project, including the Autopilot mode you wish to run (Standard Autopilot, Quick Mode, Comprehensive Mode, Manual). This API call will set our desired target feature and then kick off the EDA2 process, followed immediately by model training.\n\n```python\nif project_id == \"\":\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=target,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight\n```\n\n### Start Autopilot\n\nThis optional API call will block execution of the notebook until the full autopilot process has completed. This can take several minutes or hours, depending on the autopilot mode selected, the size of the dataset, and the type of problem we're trying to solve.\n\n```python\nif project_id == \"\":\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current autopilot status:\n    # project.stage\n    # project.get_model_jobs()\n```\n\n### List models\n\nThis API call outputs a list of all the models trained in the project, sorted by the selected validation metric.\n\n```python\n# Optionally, skip Autopilot and start here:\nif project_id != \"\":\n    project = dr.Project.get(project_id)\n\n# Pull the list of all models. You can iterate over these and examine them.\nproject.get_models()\n```\n\n### Retrieve the recommended model\n\nDataRobot provides a recommendation for an accurate and performant model at the end of Autopilot. This API call will fetch that recommendation.\n\n```python\nprint(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()\n```\n\n## Deploy a model\n\nIf no deployment ID was specified during setup, deploy DataRobot's recommended model. This will make the model available via the dedicated prediction API, and will wrap the model in our MLOps monitoring framework. Optional monitoring features are also enabled here, including accuracy tracking and data drift monitoring.\n\n```python\n# When you are happy with your model you can automate deployment\nif deployment_id == \"\":\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)\n```\n\n## Score a Spark Dataframe\n\nThe Spark wrapper that you imported into your cluster allows you to use the distributed power of the Spark cluster to quickly score large datasets. The following cells provide examples of scoring a Spark dataframe using Python or Scala.\n\n### Score with Python\n\nPython can be used to invoke the Java methods you provide to score with DataRobot models. The method call on **line 7** dynamically reaches out to the DataRobot platform to download the Scoring Code and make it available in your classpath. To avoid waiting for the network transfer, the scoring code can be downloaded ahead of time and imported as a new library in the Databricks cluster.\n\nIn order to perform the scoring transformation on the Spark dataframe, you must convert it to a Java dataframe and then back to a PySpark dataframe after scoring. You also capture the overall time it took to score in order to report that metric back to DataRobot MLOps in a later step.\n\n```python\njava_import(spark._jvm, \"com.datarobot.prediction.Predictors\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Model\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Predictors\")\n\nstart_time = (\n    time.time()\n)  # Grab timestamps before and after scoring to provide MLOps with an estimated execution time.\n# This next method call will use the endpoint, API token, and Deployment ID that were defined in previous cells to fetch our Scoring Code.\ndr_model = (\n    spark._jvm.com.datarobot.prediction.spark30.Predictors.getPredictorFromDeployment(\n        endpoint, deployment.id, api_key\n    )\n)\n\noutput_df = DataFrame(\n    dr_model.transform(input_df._jdf), spark\n)  # Apply the scoring transformation\nscore_time = (\n    time.time() - start_time\n)  # Get the total runtime of the fetching and scoring process\n\ndisplay(output_df)\n```\n\n### Score with Scala\n\nThe following cell performs the same scoring action as the previous one, only using Scala instead of Python.\n\nThis cell is commented out by default since variable values aren't shared between language contexts.\n\n```python\n%scala\n/**\nimport com.datarobot.prediction.spark30.Predictors\n\nval apiKey = \"\" //Provide DataRobot API token here\nval endpoint = \"https://app.datarobot.com/\" //This is the URL that you use to access the DataRobot UI\nval deploymentId = \"\" //The ID oif the deployment you'd like to use for scoring\nval inputDf = sql(\"select * from loans\") //Substitute a table name here\n\nval javaModel = Predictors.getPredictorFromDeployment(endpoint,deploymentId,apiKey)\n\nval outputDf = javaModel.transform(inputDf)\ndisplay(outputDf)\n**/\n```\n\n### Score with the Prediction API\n\nThis cell demonstrates scoring using a Pandas dataframe and the native DataRobot prediction API. This scoring method is limited to payloads under 50MB, so is not ideal for large datasets. An advantage to using this method would be easier access to monitoring data, since it does not require setup of the agent-based external monitoring framework.\n\n```python\nhost = \"https://example.dynamic.orm.datarobot.com\"  # This should be the URL of your prediction server, which you can find in the Deployment Overview page of the UI\nheaders = {\n    \"Content-Type\": \"application/json; charset=utf-8\",\n    \"Accept\": \"text/csv\",\n    \"datarobot-key\": \"\",  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    \"Authorization\": \"Bearer %s\" % (api_key),\n}\n\nparams = {\n    \"passthroughColumnsSet\": \"all\"  # This line tells the API to reflect back the input data along with the predictions\n}\n\ndata = df.to_json(orient=\"records\")\nresponse = requests.post(\n    \"{:}/predApi/v1.0/deployments/{:}/predictions\".format(host, deployment.id),\n    data=data,\n    headers=headers,\n    params=params,\n)\n\napi_df = pd.read_csv(\n    StringIO(response.text)\n)  # Here you read the API's CSV output into a Pandas dataframe\ndisplay(api_df)\n```\n\n### Report monitoring data\n\nPass monitoring data to the appropriate message channel - a Kafka topic, in this case. From there our external monitoring agent will pick up this data and pass it back to the DataRobot platform for display in the MLOps dashboard.\n\nNote that **this cell is provided as an example only**, and will not be executable without completing the full setup of the MLOps monitoring agent. More information regarding MLOps Monitoring Agent setup can be found [in the DataRobot documentation](https://docs.datarobot.com/en/docs/mlops/deployment/mlops-agent/monitoring-agent/index.html). This code cell illustrates the client library invocation that will push monitoring data to a message queue. More services need to be setup external to this notebook to complete the transfer of monitoring data to DataRobot.\n\n```python\n# java_import(spark._jvm, \"com.datarobot.mlops_spark_utils.MLOpsSparkUtils\")\n# channelConfig = \"spooler_type=kafka;kafka_topic_name=monitoring-agent-topic\"\n\n# spark._jvm.com.datarobot.mlops_spark_utils.MLOpsSparkUtils.reportPredictions(\n#     output_df._jdf, # scoring data\n#     deployment.id, # DeploymentId\n#     selection.id, # ModelId\n#     channelConfig, # MLOps channel configuration\n#     float(score_time), # scoring time\n#     ['target_1_PREDICTION','target_0_PREDICTION'] # target columns\n# )\n```\n\n### Write Results\nYou can now write our results back to a table. In this case you'll create a new table since the original source table's schema doesn't include columns to hold the scores or prediction explanations. In this example you are converting the results from the DataRobot Prediction API back to a Spark dataframe to facilitate writing to a table.\n\n```python\napi_spark_df = spark.createDataFrame(api_df)\napi_spark_df.write.mode(\"overwrite\").saveAsTable(scoring_table)\n```",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Databricks_template/Databricks_End_To_End.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Databricks_template",
        "file_type": "notebook",
        "size": 24827
      },
      "code_examples": [
        "from io import StringIO\nimport time\n\nimport datarobot as dr\nimport pandas as pd\nfrom py4j.java_gateway import java_import\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import col\nimport requests\n\napi_key = \"\"  # Get this from the Developer Tools page in the DataRobot UI\nendpoint = \"https://app.datarobot.com/\"  # This should be the URL you use to access the DataRobot UI\n\nclient = dr.Client(\n    token=api_key,\n    endpoint=endpoint,\n    user_agent_suffix=\"AIA-E2E-DBX-8\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n\n# Set these to empty strings to create a new project and/or deployment\nproject_id = \"\"\ndeployment_id = \"\"",
        "dr.Client()\n# The `config_path` should only be specified if the config file is not in the default location described in the API Quickstart guide\n# dr.Client(config_path = 'path-to-drconfig.yaml')",
        "training_table = \"\"\nscoring_table = \"\"\ntarget = \"\"\n\nif training_table == \"\":\n    scoring_table = \"white_wine_scored\"\n    target = \"quality\"\n    input_df = (\n        spark.read.option(\"header\", True)\n        .option(\"delimiter\", \";\")\n        .csv(\"dbfs:/databricks-datasets/wine-quality/winequality-white.csv\")\n    )\n    input_df = input_df.select(\n        [col(column).alias(column.replace(\" \", \"_\")) for column in input_df.columns]\n    )\nelse:\n    input_df = sql(\"select * from %s\" % (training_table))\n\ndf = input_df.toPandas()\ndisplay(input_df)",
        "# Create a project wothout setting the target\nif project_id == \"\":\n    project = dr.Project.create(\n        project_name=\"New Test Project (Databricks)\", sourcedata=df\n    )\n    print(project.id)",
        "if project_id == \"\":\n    mode = dr.enums.AUTOPILOT_MODE.QUICK\n\n    project.analyze_and_model(\n        target=target,\n        mode=mode,\n        worker_count=-1,  # Setting the worker count to -1 will ensure that you use the maximum number of modeling workers available to your account\n        max_wait=600,\n    )\n    # When you get control back, that means EDA is finished and model jobs are in flight",
        "if project_id == \"\":\n    # This is helpful if you want to keep execution serial:\n    project.wait_for_autopilot()\n\n    # Otherwise you can periodically ask the project for its current autopilot status:\n    # project.stage\n    # project.get_model_jobs()",
        "# Optionally, skip Autopilot and start here:\nif project_id != \"\":\n    project = dr.Project.get(project_id)\n\n# Pull the list of all models. You can iterate over these and examine them.\nproject.get_models()",
        "print(dr.ModelRecommendation.get_all(project.id))\nrec = dr.ModelRecommendation.get(\n    project_id=project.id,\n    recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT,\n)\nselection = rec.get_model()",
        "# When you are happy with your model you can automate deployment\nif deployment_id == \"\":\n    prediction_server = dr.PredictionServer.list()[\n        0\n    ]  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    deployment = dr.Deployment.create_from_learning_model(\n        model_id=selection.id,\n        label=\"New Test Deployment\",\n        description=\"Some extra data that I can use to search later.\",\n        default_prediction_server_id=prediction_server.id,  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    )\n    deployment.update_association_id_settings(\n        column_names=[\"id\"], required_in_prediction_requests=False\n    )\n    deployment.update_drift_tracking_settings(\n        target_drift_enabled=True, feature_drift_enabled=True\n    )\nelse:\n    deployment = dr.Deployment.get(deployment_id)\n\nprint(deployment.id)",
        "java_import(spark._jvm, \"com.datarobot.prediction.Predictors\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Model\")\njava_import(spark._jvm, \"com.datarobot.prediction.spark30.Predictors\")\n\nstart_time = (\n    time.time()\n)  # Grab timestamps before and after scoring to provide MLOps with an estimated execution time.\n# This next method call will use the endpoint, API token, and Deployment ID that were defined in previous cells to fetch our Scoring Code.\ndr_model = (\n    spark._jvm.com.datarobot.prediction.spark30.Predictors.getPredictorFromDeployment(\n        endpoint, deployment.id, api_key\n    )\n)\n\noutput_df = DataFrame(\n    dr_model.transform(input_df._jdf), spark\n)  # Apply the scoring transformation\nscore_time = (\n    time.time() - start_time\n)  # Get the total runtime of the fetching and scoring process\n\ndisplay(output_df)",
        "%scala\n/**\nimport com.datarobot.prediction.spark30.Predictors\n\nval apiKey = \"\" //Provide DataRobot API token here\nval endpoint = \"https://app.datarobot.com/\" //This is the URL that you use to access the DataRobot UI\nval deploymentId = \"\" //The ID oif the deployment you'd like to use for scoring\nval inputDf = sql(\"select * from loans\") //Substitute a table name here\n\nval javaModel = Predictors.getPredictorFromDeployment(endpoint,deploymentId,apiKey)\n\nval outputDf = javaModel.transform(inputDf)\ndisplay(outputDf)\n**/",
        "host = \"https://example.dynamic.orm.datarobot.com\"  # This should be the URL of your prediction server, which you can find in the Deployment Overview page of the UI\nheaders = {\n    \"Content-Type\": \"application/json; charset=utf-8\",\n    \"Accept\": \"text/csv\",\n    \"datarobot-key\": \"\",  # This line of code is only needed if you are using the DataRobot multi-tenant SaaS environment.\n    \"Authorization\": \"Bearer %s\" % (api_key),\n}\n\nparams = {\n    \"passthroughColumnsSet\": \"all\"  # This line tells the API to reflect back the input data along with the predictions\n}\n\ndata = df.to_json(orient=\"records\")\nresponse = requests.post(\n    \"{:}/predApi/v1.0/deployments/{:}/predictions\".format(host, deployment.id),\n    data=data,\n    headers=headers,\n    params=params,\n)\n\napi_df = pd.read_csv(\n    StringIO(response.text)\n)  # Here you read the API's CSV output into a Pandas dataframe\ndisplay(api_df)",
        "# java_import(spark._jvm, \"com.datarobot.mlops_spark_utils.MLOpsSparkUtils\")\n# channelConfig = \"spooler_type=kafka;kafka_topic_name=monitoring-agent-topic\"\n\n# spark._jvm.com.datarobot.mlops_spark_utils.MLOpsSparkUtils.reportPredictions(\n#     output_df._jdf, # scoring data\n#     deployment.id, # DeploymentId\n#     selection.id, # ModelId\n#     channelConfig, # MLOps channel configuration\n#     float(score_time), # scoring time\n#     ['target_1_PREDICTION','target_0_PREDICTION'] # target columns\n# )",
        "api_spark_df = spark.createDataFrame(api_df)\napi_spark_df.write.mode(\"overwrite\").saveAsTable(scoring_table)"
      ],
      "api_methods": [
        "deployment.get",
        "project.wait_for_autopilot",
        "deployment.create_from_learning_model",
        "dr.project.create",
        "deployment.update_drift_tracking_settings",
        "datarobot.prediction.spark30",
        "dr.client._global_client",
        "project.stage",
        "dr.enums.autopilot_mode",
        "dr.deployment.get",
        "project.get_model_jobs",
        "dr.enums.recommended_model_type",
        "dr.predictionserver.list",
        "model.transform",
        "dr.deployment.create_from_learning_model",
        "datarobot.mlops_spark_utils.mlopssparkutils",
        "project.id",
        "datarobot.prediction.predictors",
        "dr.modelrecommendation.get_all",
        "project.get",
        "deployment.update_association_id_settings",
        "deployment.id",
        "project.create",
        "dr.modelrecommendation.get",
        "dr.project.get",
        "project.analyze_and_model",
        "project.get_models"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-8068176820174279966",
      "title": "GCP_template: GCP DataRobot End To End.config.yaml",
      "content": "---\ndescription: \"In this notebook, you will build an ML model using a combination of GCP services and DataRobot.\\n\\\n  It covers an end-to-end workflow that includes sourcing the data through exploratory data analysis,\\\n  \\ model development, and deployment.\"\nfile_name: GCP DataRobot End To End.ipynb\nlanguages:\n  - python\nmaintainers:\n  - Luke Shulman\nmaintainers_email:\n  - luke.shulman@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: Foo Bar Example\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/GCP_template/GCP DataRobot End To End.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "GCP_template",
        "file_type": "yaml",
        "size": 472
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-5768410762685547527",
      "title": "GCP_template: GCP DataRobot End To End.ipynb",
      "content": "Author: Luke Shulman \n\nVersion Date: 12/22/2022 \n# Build a DataRobot ML model and deploy from Google Cloud Platform\n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/DR%20and%20GCP%20Better%20Together.svg\" width=200 />\n\nIn this notebook, you will build an ML model using a combination of GCP services and DataRobot. It covers an end-to-end workflow that includes sourcing the data through exploratory data analysis, model development, and deployment.\n\nDataRobot recommends running this notebook in Google Colaboratory or Vertex AI Workbench, which both provide hosted notebooks with automatic configuration of Google services. Everything else in this notebook should work in any Jupyter environment with properly configured authentication. \n\n### Import libraries\n\n```python\nimport os\n\nimport pandas as pd\n\n# The Google Cloud Notebook product has specific requirements\nIS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/google\")\n\n# Google Cloud Notebook requires dependencies to be installed with '--user'\nUSER_FLAG = \"\"\nif IS_GOOGLE_CLOUD_NOTEBOOK:\n    USER_FLAG = \"--user\"\n```\n\n```python\n!pip install {USER_FLAG} --upgrade  google-cloud-resource-manager google-cloud-bigquery google-cloud-storage datarobot pandas altair google-cloud-secret-manager google-auth\n```\n\n### Configure a Google Cloud project\n\n**The following steps are required, regardless of your notebook environment.**\n\n1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute and storage costs.\n\n2. [Ensure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n\n3. [Enable the Big Query API, Secrets Manager, and Cloud Storage APIs](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com,storage_component,secretmanager.googleapis.com).\n\n4. If you are running this notebook locally, then install the [Cloud SDK](https://cloud.google.com/sdk).\n\n5. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the correct project for all the commands in this notebook.\n\n**Note**: Jupyter runs lines prefixed with `!` as shell commands and it interpolates Python variables prefixed with `$` into these commands.\n\n```python\n# Set project constants for Google\ntry:\n    import google.colab\n\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    credentials, _ = google.auth.default()\n    # @title Enter GCP/BigQuery Project ID\n    PROJECT_ID = \"datarobot-vertex-pipelines\"  # @param{type:\"string\"}\nelif IS_GOOGLE_CLOUD_NOTEBOOK:  # Likely using vertex or dataproc\n    import google\n\n    credentials, project = google.auth.default()\n    PROJECT_ID = project\nelse:  # Project running locally\n    from google import auth\n\n    credentials, project = auth.default()\n    PROJECT_ID = project\n\nif IN_COLAB:\n    # @title Enter GCP/BigQuery Project Number\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # @param{type:\"string\"}\nelse:\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # The ID number for you project\n```\n\n## Import data\n\nThis example uses loan data from a public dataset. To facilitate this demonstration, you will first load the data into a BigQuery table that will be used as the DataSource for DataRobot modeling.\n\n```python\nfrom tempfile import TemporaryFile\n\nfrom google.cloud import bigquery\nimport pandas as pd\nimport requests\n\n# Construct a BigQuery client object\nclient = bigquery.Client(project=PROJECT_ID)\n\n# Create the dataset if needed\ndataset_name = \"dr_sample_data\"\n\nclient.create_dataset(dataset_name, exists_ok=True)\n\nfull_table_name = client.project + \".\" + dataset_name + \".\" + \"lending_club\"\n\nprint(f\"\"\"Data will be written to {full_table_name}\"\"\")\n\njob_config = bigquery.LoadJobConfig(\n    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n\nwith TemporaryFile() as tmpfile:\n    r = requests.get(\n        \"https://s3.amazonaws.com/datarobot_public_datasets/10K_Lending_Club_Loans.csv\"\n    )\n    tmpfile.write(r.content)\n    tmpfile.seek(0)\n    load_job = client.load_table_from_file(\n        tmpfile, full_table_name, job_config=job_config\n    )  # Make an API request\n\n\nload_job.result()\n\ndestination_table = client.get_table(full_table_name)\nprint(\"Loaded {} rows.\".format(destination_table.num_rows))\n```\n\n```python\nimport datarobot as dr\nfrom google.cloud import secretmanager\n\napi_secret = f\"projects/{PROJECT_NUMBER}/secrets/DR_API_KEY/versions/1\"\nendpoint = f\"projects/{PROJECT_NUMBER}/secrets/DR_ENDPOINT/versions/1\"\nsecrets = secretmanager.SecretManagerServiceClient(credentials=credentials)\n\nDR_API_KEY = secrets.access_secret_version(name=api_secret).payload.data.decode(\"UTF-8\")\nDR_ENDPOINT = secrets.access_secret_version(name=endpoint).payload.data.decode(\"UTF-8\")\n\n\nclient = dr.Client(\n    token=DR_API_KEY,\n    endpoint=DR_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-GCP-6\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n```\n\n### Register data in the AI Catalog\n\nTo register the data with DataRobot, you will need to authorize DataRobot to access BigQuery data. As this requires user authorization, it must be enabled via the GUI. To authorize DataRobot to access data in BigQuery, follow these steps: \n\n1. In the AI Catalog, select **Add New Data Connection** and choose BigQuery.\n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/dr_new_data_connection.jpg\" width=300 />\n\n<span style=\"font-size:7;font-weight:100;\"><i>Create a new Data connection in DataRobot</i></span>\n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/BigQueryEnjoy.jpg\" width=300 />\n\n<span style=\"font-size:7;font-weight:100;\"><i>Select the BigQuery connection</i></span>\n\n2. Name the connection \"BigQuery,\" select the driver, and then enter your GCP project ID (saved in this notebook) in the text field as shown: \n\n<img src=\"https://storage.googleapis.com/public-artifacts-datarobot/e2e_logos/BigQuery.jpg\" width=300 />\n\n3. Once the connection is saved, select **Test Data Connection**. This prompts you to authorize the DataRobot connection to BigQuery using your GCP Account. \n\nMore information on this process can be found in the [DataRobot BigQuery Documentation](https://app.datarobot.com/docs/data/connect-data/data-sources/dc-bigquery.html).\n\nOnce this process is complete, you can use the DataRobot API to access BigQuery datasets. \n\nTo facilitate data access, DataRobot defines the following entities:\n    \n- *Data store:* The system with the data in this case BigQuery. You created this in the previous step. \n- *Data source:* The query or table with the data. In this case, `dr_sample_data.lending_club`. \n- *Dataset:* A registered dataset for ML projects.\n\nThe following snippet creates all three of these assets.\n\n```python\n# Access the newly created DataStore that was named \"BigQuery\"\nfrom IPython.display import display, HTML\n\nDATA_STORE_NAME = \"DataRobot BigQuery Vertex\"\ndata_store = [ds for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][\n    0\n]\ncredential = [cred for cred in dr.Credential.list() if cred.name == \"bigquery-oauth\"][0]\n# now we will register the table as a data soruce.\n\n\nparams = dr.DataSourceParameters(\n    table=full_table_name, data_store_id=data_store.id  # from creating the table above\n)\n\ndata_source = dr.DataSource.create(\n    data_source_type=\"jdbc\", canonical_name=\"Test BigQuery\", params=params\n)\n\ndata_set = dr.Dataset.create_from_data_source(\n    data_source_id=data_source.id, credential_id=credential.credential_id\n)\n\nHTML(\n    f\"\"\"<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{data_set.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Dataset in DataRobot</a>\n</div>\"\"\"\n)\n```\n\nWith the dataset logged in the AI Catalog, you can quickly see key statistics about all of the features. \n\n```python\nfeatures_from_dr = data_set.get_all_features()\n\npd.DataFrame(\n    [\n        {\n            \"Feature Name\": f.name,\n            \"Feature Type\": f.feature_type,\n            \"Unique Count\": f.unique_count,\n            \"NA Count\": f.na_count,\n            \"Mean\": f.mean,\n            \"Median\": f.median,\n        }\n        for f in features_from_dr\n    ]\n)\n```\n\n## Initiate Autopilot\n\nWith the dataset logged in the AI Catalog, you can go ahead and kick off a project to predict `is_bad`, an indicator that the loan was not paid.\n\n```python\nproject = dr.Project.create_from_dataset(\n    dataset_id=data_set.id,\n)\n\n\ntry:\n    project.analyze_and_model(target=\"is_bad\")\nexcept dr.errors.AsyncTimeoutError:\n    print(\"Don't worry if it times out, the process is async and will continue to run\")\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{project.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Project in DataRobot</a>\n</div>\"\"\"\n)\n```\n\n## Evaluate the model \n\nAs DataRobot runs Autopilot, you can access the models on the Leaderboard using the `get_models` method. By default, this function returns models sorted by their performance so it is easy to find the top performing model. You can also call the `get_top_model` helper.   \n\n```python\ntop_model = project.get_top_model()\n\ndisplay(\n    HTML(\n        f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{top_model.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">{top_model.model_type}</a>\n</div>\"\"\"\n    )\n)\n\n\npd.DataFrame(top_model.metrics)\n```\n\n### Build an ROC curve\n\nBeyond the Leaderboard, you can access any analysis DataRobot does out-of-the-box for every model. In the following cell, reproduce the ROC curve by calling the `get_roc_curve` function from the top model.\n\n```python\nimport altair as alt\n\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)\n\n\nbase_line = pd.DataFrame({\"x\": [0, 1], \"y\": [0, 1]})\n\ncurve = (\n    alt.Chart(roc, title=\"ROC Curve For DataRobot Top Model\")\n    .mark_line()\n    .encode(x=\"false_positive_rate:Q\", y=\"true_positive_rate:Q\")\n)\n\nref_line = (\n    alt.Chart(base_line)\n    .mark_line(color=\"black\", strokeDash=[8, 4])\n    .encode(x=\"x:Q\", y=\"y:Q\")\n)\n\ncurve + ref_line\n```\n\n### Feature Impact \n\nTo demonstrate model explainability, you can trigger and get the feature impact values of any model with the `get_or_request_feature_impact` function.\n\n```python\n#### Retrieve Feature Impact ####\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False).head(\n    10\n)  # fist ten features\n\nalt.Chart(\n    FI_df, title=\"Feature Impact Chart for Top DataRobot Model\"\n).mark_bar().encode(x=\"impactNormalized:Q\", y=alt.X(\"featureName:N\", sort=\"-x\"))\n```\n\n## Deploy a model\n\nOnce selected, your top model can be easily deployed. \n\n```python\nprediction_server = dr.PredictionServer.list()[\n    0\n]  # Deploy to the first prediction server\n\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    description=\"Test Google End to End Deployment\",\n    prediction_threshold=0.5,\n    label=\"Test Google End to End\",\n    default_prediction_server_id=prediction_server.id,\n)\n\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{deployment.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Deployment in DataRobot</a>\n</div>\"\"\"\n)\n```\n\n### Run batch predictions\n\nThere are two ways of making batch predictions with the deployment. The first is to use the User OAuth JDBC connection you created in previous steps. The data will be saved to DataRobot and it can be accessed directly. \n\n```python\nfrom tempfile import TemporaryFile\n\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"query\": f\"\"\"SELECT * from {full_table_name};\"\"\",\n    \"data_store_id\": data_store.id,\n    \"credential_id\": credential.credential_id,\n}\n\n\njob = dr.BatchPredictionJob.score(deployment.id, intake_settings=intake_settings)\n\n\nwith TemporaryFile() as tmpfile:\n    job_csv = job.get_result_when_complete()\n    tmpfile.write(job_csv)\n    tmpfile.seek(0)\n    result = pd.read_csv(tmpfile)\n\nresult\n```\n\nYou can also use a service account to write data back to GCP directly. A service account is preferred here because it allows these jobs to be scheduled to happen automatically server to server. \n\n```python\nimport json\nfrom pathlib import Path\n\njson_credential = json.loads(\n    Path(\"PATH TO YOUR JSON SERVICE CREDENTIAL\").read_text()\n)  # You can obtain your service credentials in a number of ways\n\n# google_cloud_credential = dr.Credential.create_gcp(name='GCP Key Credential Test', gcp_key=json_credential, description=\"For GCP Batch Access\")\n\njob = dr.BatchPredictionJob.score(\n    deployment.id,  # this is the deployment id\n    intake_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club_predictions\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n)\n\njob.get_result_when_complete()\n```\n\n```python\nquery = f\"\"\"\n    SELECT count(*) as total_rows, avg(cast(is_bad_PREDICTION as numeric)) as avg_prediction from dr_sample_data.lending_club_predictions\n\"\"\"\nquery_job = client.query(query)  # Make an API request\n\nprint(\"The query data:\")\nfor row in query_job:\n    # Row values can be accessed by field name or index\n    print(f\"Result: {row['total_rows']} rows with avg of {row['avg_prediction']}\")\n```\n\n```python\n# CLEAN UP\n# Uncomment and run this cell to remove everything you added during this session\n\ndata_set.delete(data_set.id)\ndata_source.delete()\n# deployment.delete()\n# project.delete()\n# google_cloud_credential.delete()\n```",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/GCP_template/GCP DataRobot End To End.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "GCP_template",
        "file_type": "notebook",
        "size": 137659
      },
      "code_examples": [
        "import os\n\nimport pandas as pd\n\n# The Google Cloud Notebook product has specific requirements\nIS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/google\")\n\n# Google Cloud Notebook requires dependencies to be installed with '--user'\nUSER_FLAG = \"\"\nif IS_GOOGLE_CLOUD_NOTEBOOK:\n    USER_FLAG = \"--user\"",
        "!pip install {USER_FLAG} --upgrade  google-cloud-resource-manager google-cloud-bigquery google-cloud-storage datarobot pandas altair google-cloud-secret-manager google-auth",
        "# Set project constants for Google\ntry:\n    import google.colab\n\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    credentials, _ = google.auth.default()\n    # @title Enter GCP/BigQuery Project ID\n    PROJECT_ID = \"datarobot-vertex-pipelines\"  # @param{type:\"string\"}\nelif IS_GOOGLE_CLOUD_NOTEBOOK:  # Likely using vertex or dataproc\n    import google\n\n    credentials, project = google.auth.default()\n    PROJECT_ID = project\nelse:  # Project running locally\n    from google import auth\n\n    credentials, project = auth.default()\n    PROJECT_ID = project\n\nif IN_COLAB:\n    # @title Enter GCP/BigQuery Project Number\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # @param{type:\"string\"}\nelse:\n    PROJECT_NUMBER = \"ENTER YOUR PROJECT NUMBER HERE\"  # The ID number for you project",
        "from tempfile import TemporaryFile\n\nfrom google.cloud import bigquery\nimport pandas as pd\nimport requests\n\n# Construct a BigQuery client object\nclient = bigquery.Client(project=PROJECT_ID)\n\n# Create the dataset if needed\ndataset_name = \"dr_sample_data\"\n\nclient.create_dataset(dataset_name, exists_ok=True)\n\nfull_table_name = client.project + \".\" + dataset_name + \".\" + \"lending_club\"\n\nprint(f\"\"\"Data will be written to {full_table_name}\"\"\")\n\njob_config = bigquery.LoadJobConfig(\n    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n    source_format=bigquery.SourceFormat.CSV,\n    skip_leading_rows=1,\n    autodetect=True,\n)\n\n\nwith TemporaryFile() as tmpfile:\n    r = requests.get(\n        \"https://s3.amazonaws.com/datarobot_public_datasets/10K_Lending_Club_Loans.csv\"\n    )\n    tmpfile.write(r.content)\n    tmpfile.seek(0)\n    load_job = client.load_table_from_file(\n        tmpfile, full_table_name, job_config=job_config\n    )  # Make an API request\n\n\nload_job.result()\n\ndestination_table = client.get_table(full_table_name)\nprint(\"Loaded {} rows.\".format(destination_table.num_rows))",
        "import datarobot as dr\nfrom google.cloud import secretmanager\n\napi_secret = f\"projects/{PROJECT_NUMBER}/secrets/DR_API_KEY/versions/1\"\nendpoint = f\"projects/{PROJECT_NUMBER}/secrets/DR_ENDPOINT/versions/1\"\nsecrets = secretmanager.SecretManagerServiceClient(credentials=credentials)\n\nDR_API_KEY = secrets.access_secret_version(name=api_secret).payload.data.decode(\"UTF-8\")\nDR_ENDPOINT = secrets.access_secret_version(name=endpoint).payload.data.decode(\"UTF-8\")\n\n\nclient = dr.Client(\n    token=DR_API_KEY,\n    endpoint=DR_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-GCP-6\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client",
        "# Access the newly created DataStore that was named \"BigQuery\"\nfrom IPython.display import display, HTML\n\nDATA_STORE_NAME = \"DataRobot BigQuery Vertex\"\ndata_store = [ds for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][\n    0\n]\ncredential = [cred for cred in dr.Credential.list() if cred.name == \"bigquery-oauth\"][0]\n# now we will register the table as a data soruce.\n\n\nparams = dr.DataSourceParameters(\n    table=full_table_name, data_store_id=data_store.id  # from creating the table above\n)\n\ndata_source = dr.DataSource.create(\n    data_source_type=\"jdbc\", canonical_name=\"Test BigQuery\", params=params\n)\n\ndata_set = dr.Dataset.create_from_data_source(\n    data_source_id=data_source.id, credential_id=credential.credential_id\n)\n\nHTML(\n    f\"\"\"<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{data_set.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Dataset in DataRobot</a>\n</div>\"\"\"\n)",
        "features_from_dr = data_set.get_all_features()\n\npd.DataFrame(\n    [\n        {\n            \"Feature Name\": f.name,\n            \"Feature Type\": f.feature_type,\n            \"Unique Count\": f.unique_count,\n            \"NA Count\": f.na_count,\n            \"Mean\": f.mean,\n            \"Median\": f.median,\n        }\n        for f in features_from_dr\n    ]\n)",
        "project = dr.Project.create_from_dataset(\n    dataset_id=data_set.id,\n)\n\n\ntry:\n    project.analyze_and_model(target=\"is_bad\")\nexcept dr.errors.AsyncTimeoutError:\n    print(\"Don't worry if it times out, the process is async and will continue to run\")\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{project.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Project in DataRobot</a>\n</div>\"\"\"\n)",
        "top_model = project.get_top_model()\n\ndisplay(\n    HTML(\n        f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{top_model.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">{top_model.model_type}</a>\n</div>\"\"\"\n    )\n)\n\n\npd.DataFrame(top_model.metrics)",
        "import altair as alt\n\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)\n\n\nbase_line = pd.DataFrame({\"x\": [0, 1], \"y\": [0, 1]})\n\ncurve = (\n    alt.Chart(roc, title=\"ROC Curve For DataRobot Top Model\")\n    .mark_line()\n    .encode(x=\"false_positive_rate:Q\", y=\"true_positive_rate:Q\")\n)\n\nref_line = (\n    alt.Chart(base_line)\n    .mark_line(color=\"black\", strokeDash=[8, 4])\n    .encode(x=\"x:Q\", y=\"y:Q\")\n)\n\ncurve + ref_line",
        "#### Retrieve Feature Impact ####\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False).head(\n    10\n)  # fist ten features\n\nalt.Chart(\n    FI_df, title=\"Feature Impact Chart for Top DataRobot Model\"\n).mark_bar().encode(x=\"impactNormalized:Q\", y=alt.X(\"featureName:N\", sort=\"-x\"))",
        "prediction_server = dr.PredictionServer.list()[\n    0\n]  # Deploy to the first prediction server\n\ndeployment = dr.Deployment.create_from_learning_model(\n    model_id=top_model.id,\n    description=\"Test Google End to End Deployment\",\n    prediction_threshold=0.5,\n    label=\"Test Google End to End\",\n    default_prediction_server_id=prediction_server.id,\n)\n\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n\n\nHTML(\n    f\"\"\"\n<div style=\"text-aligh:center;padding:.75rem;\"> \n    <a href=\"{deployment.get_uri()}\" target=\"_blank\" style=\"background-color:#5371BF;color:white;padding:.66rem .75rem;border-radius:5px;cursor: pointer;\">Open Deployment in DataRobot</a>\n</div>\"\"\"\n)",
        "from tempfile import TemporaryFile\n\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"query\": f\"\"\"SELECT * from {full_table_name};\"\"\",\n    \"data_store_id\": data_store.id,\n    \"credential_id\": credential.credential_id,\n}\n\n\njob = dr.BatchPredictionJob.score(deployment.id, intake_settings=intake_settings)\n\n\nwith TemporaryFile() as tmpfile:\n    job_csv = job.get_result_when_complete()\n    tmpfile.write(job_csv)\n    tmpfile.seek(0)\n    result = pd.read_csv(tmpfile)\n\nresult",
        "import json\nfrom pathlib import Path\n\njson_credential = json.loads(\n    Path(\"PATH TO YOUR JSON SERVICE CREDENTIAL\").read_text()\n)  # You can obtain your service credentials in a number of ways\n\n# google_cloud_credential = dr.Credential.create_gcp(name='GCP Key Credential Test', gcp_key=json_credential, description=\"For GCP Batch Access\")\n\njob = dr.BatchPredictionJob.score(\n    deployment.id,  # this is the deployment id\n    intake_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n    output_settings={\n        \"type\": \"bigquery\",\n        \"dataset\": \"dr_sample_data\",\n        \"table\": \"lending_club_predictions\",\n        \"bucket\": \"model-staging-dr-demo\",  # a bucket is required\n        \"credential_id\": google_cloud_credential.credential_id,\n    },\n)\n\njob.get_result_when_complete()",
        "query = f\"\"\"\n    SELECT count(*) as total_rows, avg(cast(is_bad_PREDICTION as numeric)) as avg_prediction from dr_sample_data.lending_club_predictions\n\"\"\"\nquery_job = client.query(query)  # Make an API request\n\nprint(\"The query data:\")\nfor row in query_job:\n    # Row values can be accessed by field name or index\n    print(f\"Result: {row['total_rows']} rows with avg of {row['avg_prediction']}\")",
        "# CLEAN UP\n# Uncomment and run this cell to remove everything you added during this session\n\ndata_set.delete(data_set.id)\ndata_source.delete()\n# deployment.delete()\n# project.delete()\n# google_cloud_credential.delete()"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "model.metrics",
        "dr.datasource.create",
        "deployment.create_from_learning_model",
        "model.get_roc_curve",
        "deployment.update_drift_tracking_settings",
        "dr.client._global_client",
        "project.get_uri",
        "dr.credential.list",
        "model.id",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "dr.dataset.create_from_data_source",
        "deployment.delete",
        "deployment.id",
        "dr.datastore.list",
        "project.get_top_model",
        "deployment.get_uri",
        "project.delete",
        "model.get_uri",
        "dr.credential.create_gcp",
        "dr.errors.asynctimeouterror",
        "dr.batchpredictionjob.score",
        "model.model_type",
        "project.analyze_and_model",
        "project.create_from_dataset",
        "dr.project.create_from_dataset"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-4961228225506958995",
      "title": "root: README.md",
      "content": "## Ecosystem Integration Templates\nThis section contains templates for connecting to data sources, building, deploying and monitoring models, and writing predictions back to data sources for common integration infrastructures. These templates are designed to be used as a starting point for building out your own solution pipelines and are solely focused on integration, not domain-specific applications. \n\nYou can combine these with use-case focused accelerators in the [use_cases_and_horizontal_approaches](https://github.com/datarobot-community/ai-accelerators/tree/main/use_cases_and_horizontal_approaches) or [generative_ai](https://github.com/datarobot-community/ai-accelerators/main/update-structure/generative_ai) folders to build out your own custom AI/ML solutions or as a baseline for your projects. If something doesn't work or you have a suggestion, please open a [GitHub issue](https://github.com/datarobot-community/ai-accelerators/issues)\n\n## 💥 What's in here?\n| Title | Primary Label | What it's good for | Other Labels| Extensibility to other Integrations |\n|---|---|---|---|---|\n| [AWS_Athena_template](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/AWS_Athena_template) | AWS| End-to-end workflow using S3 from data to batch predictions | AutoML | Low, AWS focused |\n| [AWS_S3_template](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/AWS_Athena_template) | AWS| End-to-end workflow using Athena from data to batch predictions | AutoML | Low, AWS focused |\n| [AWS_monitor_sagemaker_model_in_DataRobot](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/AWS_monitor_sagemaker_model_in_DataRobot)| AWS| Workflow for monitornig Sagemaker models with DataRobot ML Ops. Covers building DataRobot compatible Sagemaker containers to host, train, and monitor models | Sagemaker, ML Ops | Low, AWS focused, but broadly applies to non-AutoML Sagemaker deployments |\n| [AWS_sagemaker_deployment](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/AWS_sagemaker_deployment)  | AWS | Building models within DataRobot which will then be deployed and hosted within AWS SageMaker | AutoML, Sagemaker | Low, AWS focused |\n| [Azure_template](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/Azure_template) | Azure  | End-to-end workflow in Azure from data to batch predictions | AutoML | Low, Azure focused |\n| [Databricks_template](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/Databricks_template) | Databricks | End-to-end workflow in DBX from data to batch predictions | AutoML, Spark | Low, Databricks focused |\n| [GCP_template](https://github.com/datarobot-community/ai-accelerators/tree/alevan/update-structure/ecosystem_integration_templates/GCP_template) | GCP | End-to-end workflow in GCP from data to batch predictions in BigQuery | AutoML, BigQuery | Low, GCP focused |\n| [SAP_template](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/SAP_template) | SAP | End-to-end workflow in SAP from data to batch predictions. Includes building out SAP HANA environment | AutoML | Low, SAP Focused |\n| [Snowflake_snowpark_template](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/Snowflake_snowpark_template) |Snowflake| Snowpark integration, Java scoring code deployment and monitoring with PPS in Snowflake | Snowpark, AutoML | Low, Snowflake focused |\n| [Snowflake_template](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/Snowflake_template) | Snowflake  | Repeatable end-to-end workflow in Snowflake from data to batch predictions | AutoML | Low, Snowflake focused |\n| [Scoring-code-as-a-microservice-w_docker](https://github.com/datarobot-community/ai-accelerators/tree/main/ecosystem_integration_templates/scoring-code-as-microservice-w_docker) | Docker | Deploy scoring code on self-hosted or hyperscaler Kubernetes | Kubernetes | High, runs on CLI, AWS Sagemaker, Apache Spark, Snowflake, Java project|\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/README.md",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "root",
        "file_type": "markdown",
        "size": 4256
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-6607900112616270071",
      "title": "SAP_template: SAP_End_to_End.config.yaml",
      "content": "---\ndescription: his notebook demonstrates an end-to-end workflow using DataRobot with SAP as the remote data\n  source.\nfile_name: SAP_End_to_End.ipynb\nlanguages:\n  - python\nmaintainers:\n  - Dennis Whitney\nmaintainers_email:\n  - dennis.whitney@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: End-to-End modeling workflow with SAP\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/SAP_template/SAP_End_to_End.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "SAP_template",
        "file_type": "yaml",
        "size": 348
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.15,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-8108630507779539202",
      "title": "SAP_template: SAP_End_to_End.ipynb",
      "content": "# End-to-End modeling workflow with SAP\n\nAuthor: Dennis Whitney\n\nVersion Date: 27/April/2023\n\n[Reference DataRobot's API documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n![datarobotandsaphana.png](attachment:datarobotandsaphana.png)\n\n# Overview\n\nThis notebook demonstrates an end-to-end workflow using DataRobot with SAP as the remote data source.\n\nThe data set used for this notebook is the [Auto MPG eample in the Quickstart example](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html) and an example method for loading this data into the SAP Hana database is given in the [Sample SAP HANA data load appendix](#appendix-sample-sap-hana-data-load) at the end of this document.\n\nThe workflow consists of the following steps:\n\n- Use the SAP HANA JDBC driver\n- [Create DataRobot credentials](https://docs.datarobot.com/en/docs/data/connect-data/stored-creds.html#stored-data-credentials)\n- Read a SAP HANA schema.table into DataRobot's AI Catalog\n- Start a project with the dataset\n- Deploy the recomended model\n- [Score via batch with various writeback options](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/predictions/batch_predictions.html?highlight=dr.BatchPredictionJob.score#jdbc-output)\n- Real-time prediction requests\n- Show service statistics\n- Clean up the created artifacts\n\n### Reference documentation\n\n- [DataRobot Python API documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/)\n- [DataRobot API quickstart guide](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n\n### Prerequisites\n\n- If your DataRobot version is <= 9.0, use Python SDK version 2.28.1\n- If your DataRobot version is later than 9.0, use Python SDK version 3.0 or later\n- You must provide a [DataRobot API key](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management) for the DataRobot cluster in use\n- [The SAP HANA JDBC Driver must be installed](https://repo1.maven.org/maven2/com/sap/cloud/db/jdbc/ngdbc/2.16.11/)\n- Use a working SAP HANA installation\n  - [This example uses Red Hat Enterprise Linux 9.0](https://cloud.redhat.com/blog/containerizing-sap-hana-express-on-red-hat-enterprise-linux)\n- [Valid DataRobot credentials for the data source](https://docs.datarobot.com/en/docs/data/connect-data/stored-creds.html#stored-data-credentials) \n- Networking must allow for port 22 for SSH and 39041 for the SAP HANA server\n\n# Notes\n\nBelow are the assorted notes found while using this notebook that may be useful\n\n**Note 1:** The jdbc connector, credentials and the data connection do not allow for finding with an id, so this notebook will use the name instead.\n\n**Note 2:** A future release of this notebook will demonstrate prediction write back.\n\n# Install Libraries\n\nIf you are using VS Code, you can install the required DataRobot packages with the cell below. Otherwise, use the cell that follows it.\n\n```python\n!{sys.executable} -m pip install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !{sys.executable} -m pip install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!{sys.executable} -m pip install datarobot\n```\n\n```python\n!pip3 install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !pip3 install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!pip3 install datarobot\n```\n\n# Import libraries and set parameters\n\nThe following code block will import the required python packages and provides a single place to set the parameters required to make the notebook work properly.\n\n**Please note:** These parameters are aligned to use the [Sample SAP HANA data load appendix](#appendix-sample-sap-hana-data-load) at the end of this document and you will need to update to suit your needs.\n\n```python\nimport csv\nimport json\nimport pprint\nfrom urllib.parse import urlparse\n\nimport datarobot as dr\nfrom datarobot import AUTOPILOT_MODE\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.models import Deployment\nimport requests\n\n# Provide the URL protocol, address (IP or FQDN)\n# Example: https://app.datarobot.com or https://datarobot.example.com or http://10.1.2.3\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com\"\n\n# # Provide an API key from a user with permission from this cluster\nDATAROBOT_API_TOKEN = \"<A valid DataRobot API key goes here>\"\n\n# Required by app.datarobot.com\nDATAROBOT_KEY = \"<A valid DataRobot App Key goes here>\"\n\n# Define the SAP Hana deployment\n# The settings below assumes the usage of the example data load defined at the end of this AI Accelerator\n# # If you are using a custom database, you must update as required\nSAP_JDBC_URL = \"jdbc:sap://<the SAP Hana IP goes here>:39041?databaseName=HXE\"\n\n# # When using https://www.sap.com/products/erp/s4hana.html, your jdbc url will look like:\n# SAP_JDBC_URL = \"jdbc:sap://<SAP Hana cloud ID goes here>.hanacloud.ondemand.com:443\"\n\n### JDBC Connectivity settings\nJDBC_USERNAME = \"JDBC_DR_WORKER\"\nJDBC_PASSWORD = \"His_Password_1\"\n\nJDBC_WRITE_SCHEMA = \"SAP_DEMO\"\nJDBC_WRITE_TABLE = \"scored_data_from_notebook\"\n\n### JDBC Driver Settings\nJDBC_CREDENTIAL_NAME = \"SAP_HANA_CREDENTIAL\"\nSAP_JDBC_DRIVER_NAME = \"2.15.10 recommended\"\nSAP_DATASTORE_NAME = \"SAP_HANA_DataStore\"\n\n### SQL Data Extraction statements\nTRAIN_DATASOURCE_NAME = \"SAP_HANA_DataSource\"\nTRAIN_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG;\"\n\nPREDICT_DATASOURCE_NAME = \"SAP_HANA_DataSource4Prediction\"\nPREDICT_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG LIMIT 10;\"\n\n### DataRobot project settings\nPROJECT_NAME = \"AutomobileMpG-burn-notice\"\nPROJECT_WORKERS = -1\nTARGET = \"MPG\"\nMETRIC = \"RMSE\"\n\n# Turn on DEBUG statements\nDEBUG = False  # True #\n\n# Set Autopilot mode\nOMODE = AUTOPILOT_MODE.QUICK\n\n# These values are mostly constant\n# Create the shared DataRobot client\nmy_client = dr.Client(\n    token=\"%s\" % (DATAROBOT_API_TOKEN),\n    endpoint=\"%s/api/v2\" % (DATAROBOT_ENDPOINT),\n    ssl_verify=True if (urlparse(DATAROBOT_ENDPOINT)).scheme == \"https\" else False,\n    trace_context=\"AIA-E2E-SAPWORKFLOW-125\",  # Optional. Helps DataRobot improve this workflow\n)\n\n# Find the dedicated prediction engine URL\nPREDICTION_SERVER = dr.PredictionServer.list()[0]\n\n# Verbose settings statement\nprint(\"# -----------------------------------------\")\nprint(\"DataRobot client version: %s\" % dr.__version__)\nprint(\"# -----------------------------------------\")\nprint(\"DATAROBOT_ENDPOINT: %s | %s\" % (DATAROBOT_ENDPOINT, PREDICTION_SERVER))\nprint(\"DATAROBOT_API_TOKEN: %s\" % (len(DATAROBOT_API_TOKEN)))\nprint(\"# Credentials settings -------------------\")\nprint(\"JDBC_CREDENTIAL_NAME: %s\" % (JDBC_CREDENTIAL_NAME))\nprint(\"JDBC_USERNAME: %s\" % (JDBC_USERNAME))\nprint(\"JDBC_PASSWORD: %s\" % (len(JDBC_PASSWORD)))\nprint(\"JDBC_WRITE_SCHEMA: %s\" % (JDBC_WRITE_SCHEMA))\nprint(\"JDBC_WRITE_TABLE: %s\" % (JDBC_WRITE_TABLE))\nprint(\"# DataStore settings ---------------------\")\nprint(\"SAP_JDBC_DRIVER_NAME: %s\" % (SAP_JDBC_DRIVER_NAME))\nprint(\"SAP_DATASTORE_NAME: %s\" % (SAP_DATASTORE_NAME))\nprint(\"SAP_JDBC_URL: %s\" % (len(SAP_JDBC_URL)))\nprint(\"TRAIN_DATASOURCE_NAME: %s\" % (TRAIN_DATASOURCE_NAME))\nprint(\"TRAIN_EXTRACTION_SQL: %s\" % (TRAIN_EXTRACTION_SQL))\nprint(\"PREDICT_DATASOURCE_NAME: %s\" % (PREDICT_DATASOURCE_NAME))\nprint(\"PREDICT_EXTRACTION_SQL: %s\" % (PREDICT_EXTRACTION_SQL))\nprint(\"# Project settings ------------------------\")\nprint(\"PROJECT_NAME: %s\" % (PROJECT_NAME))\nprint(\"TARGET: %s | METRIC: %s\" % (TARGET, METRIC))\n```\n\n# Create the Basic Credentials\n\nThe API will now build a Credentials set that will be used for Authentication when data is extracted, using the JDBC username and password\n\nFor more information on credentials, reference the [Python SDK documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/credentials.html?highlight=Credential#basic-credentials).\n\n**Please Note:** This notebook shares a set of credentials across the train and predict data sources.\n\n```python\nCREDENTIAL = dr.Credential.create_basic(\n    name=JDBC_CREDENTIAL_NAME,\n    user=JDBC_USERNAME,\n    password=JDBC_PASSWORD,\n)\n\nprint(\"Created CREDENTIAL: %s\" % (CREDENTIAL))\n```\n\n# Create a DataStore\n\nThe API code block below will find the existing SAP Hana JDBC driver ([See the Prerequisites above](#prerequisites)) and build a DataStore (or Data Connection) based on it, passing the jdbc url used to conenct to the source.\n\nFor more information on data connections, reference the [Python SDK documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/database_connectivity.html#creating-datastores).\n\nTo find a suitable JDBC driver, please refer to the [SAP Hana Maven repo](https://mvnrepository.com/artifact/com.sap.cloud.db.jdbc/ngdbc)\n\n```python\n# Set the driver name in the SAP Hana JDBC driver name temaplate\njdbc_driver_name = \"SAP HANA (%s)\" % SAP_JDBC_DRIVER_NAME\n\nJDBC_DRIVER = [\n    drs for drs in dr.DataDriver.list() if drs.canonical_name == jdbc_driver_name\n][-1]\nprint(\"Using JDBC_DRIVER: %s\" % (JDBC_DRIVER))\n\nDATASTORE = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=SAP_DATASTORE_NAME,\n    driver_id=JDBC_DRIVER.id,\n    jdbc_url=SAP_JDBC_URL,\n)\nprint(\"Created DATASTORE: %s\" % (DATASTORE))\n```\n\n# Create the DataSources\n\nThis section of the notebook will create the Training and Prediction DataSources used in the AI Catalog.\n\n**Please Note:** The Training DataSource is used during the Model Creation and Training phase and the Prediction DataSource is used to make predictions.\n\nFor more information on data sources, reference the [Python SDK documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/database_connectivity.html#creating-datasources).\n\n```python\nTRAIN_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=TRAIN_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=TRAIN_EXTRACTION_SQL,\n    ),\n)\nprint(\"Created TRAIN_DATASOURCE id: %s | %s\" % (TRAIN_DATASOURCE.id, TRAIN_DATASOURCE))\n\nPREDICT_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=PREDICT_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=PREDICT_EXTRACTION_SQL,\n    ),\n)\nprint(\n    \"Created PREDICT_DATASOURCE id: %s | %s\"\n    % (PREDICT_DATASOURCE.id, PREDICT_DATASOURCE)\n)\n```\n\n# Create the Datasets\n\nThis is where the notebook will push the 2 DataSources to the AI Catalog as a DataSet, with:\n1. The Train Extraction SQL statement\n2. The Prediction Extraction SQL statement\n\nFor more information on creating a dataset, reference the [Python SDK referenece documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html?highlight=create_dataset#datarobot.DataSource.create_dataset) and [its user guide](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/data/dataset.html?highlight=find%20dataset). \n\n```python\n# Create for the Training DataSet\nTRAIN_DATASET = TRAIN_DATASOURCE.create_dataset(\n    do_snapshot=False, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created TRAIN_DATASET id: %s | %s\" % (TRAIN_DATASET.id, TRAIN_DATASET))\n\n# Create for the Prediction Source\n# Please note the \"do_snapshot=True\" which is required for prediction datasets\nPREDICT_DATASET = PREDICT_DATASOURCE.create_dataset(\n    do_snapshot=True, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PREDICT_DATASET id: %s | %s\" % (PREDICT_DATASET.id, PREDICT_DATASET))\n\n# Handy debug statement to open a browser tab pointing to the Training DataSet in the AI Catalog\nif str(dr.__version__) > str(3) and DEBUG:\n    TRAIN_DATASET.open_in_browser()\n```\n\n# Create a project\n\nThe notebook will now create a Project based off the Training DataSet in the AI Catalog. This is where the API sets the name, credentails to use, the target and the other required parameters.\n\nThe code set below will show the existing worker and job state to show the current usage, the Project status as it processes the models and data and finally the exit worker state.\n\nFor more information on DataRobot Projects, please [Reference documentation for the Python SDK](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/project.html)\n\n```python\n# Find the current worker and job status, which is a good measure of system usage\nWORKERS_BEFORE = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers before: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_BEFORE[\"total\"],\n        WORKERS_BEFORE[\"inUse\"],\n        WORKERS_BEFORE[\"usersRunning\"],\n        WORKERS_BEFORE[\"jobsWaiting\"],\n        WORKERS_BEFORE[\"usersWaiting\"],\n    )\n)\n\nPROJECT = TRAIN_DATASET.create_project(\n    project_name=PROJECT_NAME, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PROJECT: %s\" % (PROJECT))\n\nPROJECT.analyze_and_model(\n    target=TARGET, metric=METRIC, mode=OMODE, worker_count=PROJECT_WORKERS\n)\n\nPROJECT.wait_for_autopilot()\n\nprint(\"PROJECT %s info: %s\" % (PROJECT.id, PROJECT))\n\n# An easy way to confirm the worker autoscaling is working as expected\nWORKERS_AFTER = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers after: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_AFTER[\"total\"],\n        WORKERS_AFTER[\"inUse\"],\n        WORKERS_AFTER[\"usersRunning\"],\n        WORKERS_AFTER[\"jobsWaiting\"],\n        WORKERS_AFTER[\"usersWaiting\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    PROJECT.open_in_browser()\n```\n\n# Get the model recomended for deployment\n\nThe code set below will get what DataRobot has determined to be the recomended model for deployment and display the models validation and crossValidation metrics.\n\n[Reference documentation for model recommendation](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/modeling/model_recommendation.html?highlight=dr.ModelRecommendation.get#get-recommended-model)\n\n```python\nTOP_MODEL = dr.ModelRecommendation.get(\n    PROJECT.id, dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT\n).get_model()\n\nprint(\"Using Recommended Model id: %s | %s\" % (TOP_MODEL.id, TOP_MODEL))\nprint(\n    \"validation: %s | crossValidation: %s\"\n    % (\n        TOP_MODEL.metrics[PROJECT.metric][\"validation\"],\n        TOP_MODEL.metrics[PROJECT.metric][\"crossValidation\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    TOP_MODEL.open_model_browser()\n```\n\n# Deploy the top model\n\nThis code block will push the given model out to the DataRobot Prediction Server such that it can now serve requests with predictions. For more information, please [Reference the documentation for deployments](https://datarobot-public-api-client.readthedocs-hosted.com/en/v3.0.2/reference/mlops/deployment.html#create-a-deployment)\n\n```python\nDEPLOYMENT = dr.Deployment.create_from_learning_model(\n    TOP_MODEL.id,\n    label=\"%s RECOMMENDED_MODEL\" % PROJECT.project_name,\n    description=\"API %s deployment of: %s\"\n    % (PROJECT.project_name, TOP_MODEL.model_type),\n    default_prediction_server_id=PREDICTION_SERVER.id,\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    DEPLOYMENT.open_in_browser()\n\nprint(\"Created DEPLOYMENT: %s | %s\" % (DEPLOYMENT.id, DEPLOYMENT))\n```\n\n# Predictions\n\nNow that the heavy lifting of Training and Deploying the model is complete, we can start making predictions with that model via the Dedicated Prediction Server.\n\nThis note book will demonstate the 2 types of Predictions: Batch and Real-Time\n\n## Make Batch Predictions with SQL Statement\n\nThis example shows how to pull data via a fresh JDBC select statement and saves the results to a local file.\n\n[Reference documentation for batch predictions](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.1/reference/predictions/batch_predictions.html)\n\n```python\nJDBC_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,  #   \"select * from new_york_taxi.input limit 1000\",\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"localFile\",\n        \"path\": \"jdbc-batch-predictions.csv\",\n    },\n)\n\nJDBC_BATCH_PREDICTION_JOB.wait_for_completion()\n\nwith open(\"jdbc-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)\n```\n\n## Batch Predictions using the AI Catalog\n\nThe following cell reads from the latest version of the PREDICT_DATASOURCE_NAME in the AI Catalog and outputs to a local file. \n\n**Please Note:** Becuase this method uses an AI Catalog entry to score, it is expected to be faster than the method above that makes a fresh resqust to the SAP Hana database.\n\nFor further information, please see the [Reference documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.1/reference/predictions/batch_predictions.html#local-file-output)\n\n```python\nAICATALOG_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"dataset\",\n        \"dataset\": PREDICT_DATASET,\n        \"dataset_version_id\": PREDICT_DATASET.version_id,\n    },\n    output_settings={\"type\": \"localFile\", \"path\": \"aicatalog-batch-predictions.csv\"},\n)\nAICATALOG_BATCH_PREDICTION_JOB.wait_for_completion()\nAICATALOG_BATCH_PREDICTION_JOB.get_status()\n\nwith open(\"aicatalog-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)\n```\n\n# Make Batch Predictions\n\nThis section will read from the latest version in the AI Catalog and outputs to the SAP Hana database using the JDBC connector\n\n**Please Note:** this is a work in progress and will be ready soon after this page says its available: [Data sources supported for batch predictions](https://docs.datarobot.com/en/docs/api/reference/batch-prediction-api/index.html#data-sources-supported-for-batch-predictions)\n\nFor further information, please see the [Reference documentation](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.1/reference/predictions/batch_predictions.html#jdbc-output)\n\n```python\nAICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"jdbc\",\n        \"statement_type\": \"insert\",\n        \"schema\": JDBC_WRITE_SCHEMA,\n        \"table\": JDBC_WRITE_TABLE,\n        \"data_store_id\": DATASTORE.id,\n        \"credential_id\": CREDENTIAL.credential_id,\n        \"create_table_if_not_exists\": True,\n    },\n)\nAICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK.wait_for_completion()\n```\n\n# Make real-time predictions\n\nThe notebook will now make a small set of real time predictions where speed is king\n\n[Reference documentation](https://docs.datarobot.com/en/docs/api/reference/predapi/dr-predapi.html#datarobot-prediction-api)\n\n```python\n# JSON records for example automobiles for which to predict mpg\nautos = [\n    {\n        \"CYLINDERS\": 4,\n        \"DISPLACEMENT\": 119.0,\n        \"HORSEPOWER\": 82.00,\n        \"WEIGHT\": 2720.0,\n        \"ACCELERATION\": 19.4,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n    {\n        \"CYLINDERS\": 8,\n        \"DISPLACEMENT\": 120.0,\n        \"HORSEPOWER\": 79.00,\n        \"WEIGHT\": 2625.0,\n        \"ACCELERATION\": 18.6,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n]\n\n# Create REST request for prediction API\nprediction_server = DEPLOYMENT.default_prediction_server\n\nprint(\"prediction_server: %s\" % prediction_server)\n\nprediction_headers = {\n    \"Authorization\": \"Bearer {}\".format(DATAROBOT_API_TOKEN),\n    \"User-Agent\": \"AIA-E2E-SAPWORKFLOW-125\",  # Optional but helps DataRobot improve this workflow\n    \"DataRobot-Key\": DATAROBOT_KEY,\n    \"Content-Type\": \"application/json\",\n}\n\nREALTINE_PREDICTIONS = requests.post(\n    \"%s/predApi/v1.0/deployments/%s/predictions?passthroughColumns=CYLINDERS&passthroughColumns=DISPLACEMENT&passthroughColumns=HORSEPOWER&passthroughColumns=WEIGHT&passthroughColumns=ACCELERATION&passthroughColumns=MODELYEAR&passthroughColumns=ORIGIN\"\n    % (prediction_server[\"url\"], DEPLOYMENT.id),\n    data=json.dumps(autos),\n    headers=prediction_headers,\n)\n\npprint.pprint(REALTINE_PREDICTIONS.json())\n```\n\n# Show Deployment Prediction Information\n\n[Reference documentation for service stats](https://datarobot-public-api-client.readthedocs-hosted.com/en/v3.0.2/reference/mlops/deployment.html#service-stats)\n\n```python\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\nservice_stats = DEPLOYMENT.get_service_stats()\nprint(\"Service Statistics:\")\nprint(service_stats)\n\nfor item in service_stats.metrics:\n    print(\"%s => %s\" % (item, service_stats.metrics[item]))\n```\n\n## Cleanup\n\nThis section below will remove everything that was created above.\n\n```python\nitems_deleted = 0\n\n# Delete the elemets created above\nDR_ELEMENTS = [\n    TRAIN_DATASET,\n    PREDICT_DATASET,\n    DEPLOYMENT,\n    PROJECT,\n    TRAIN_DATASOURCE,\n    PREDICT_DATASOURCE,\n    DATASTORE,\n    CREDENTIAL,\n]\n\nfor item in DR_ELEMENTS:\n    if \"Credential\" in str(type(item)):\n        my_id = item.credential_id\n    else:\n        my_id = item.id\n\n    try:\n        print(\"Deleting: %s\" % (item.name))\n        item.delete()\n        items_deleted += 1\n\n        # my_id = item.credential_id if credential_id in item else item.id\n        print(\"Deleted %s: %s id: %s | %s\" % (items_deleted, item.name, my_id, item))\n\n    except Exception as ex:\n        print(\"id not found: %s (%s)\" % (my_id, type(item)))\n        pass\n    except NameError as n:\n        print(\"name %s not found\" % (n))\n        pass\n\nprint(\"Cleaned up %s items\" % (items_deleted))\n```\n\n# Appendix: Sample SAP Hana data load\n\nThis is the process used to get the SAP Hana database into a place where it is usable via the DataRobot JDBC connector.\n\n[This sample uses a varient of the data file referenced API Quickstart example](https://docs.datarobot.com/en/docs/api/api-quickstart/index.html)\n\nThe directions below are a packaged version of the [the RedHat example](https://cloud.redhat.com/blog/containerizing-sap-hana-express-on-red-hat-enterprise-linux).\n\n## System size\n\n- OS: RHEL 9.1\n- Instance Type: R6i.large (2cpu/16GB RAM)\n- Storage:\n  - root disk = 10GB\n  - second disk = 40GB for filesystem “/containerstorage”\n\n## Example command for USER_DATA\n\nCopy paste the script below to the USER_DATA text box given during system creation.\n\n```bash\n#!/bin/bash\n\n# Set volume name\ndisk=nvme1n1\necho \"y\" | mkfs -t xfs /dev/${disk}\nmkdir -p /containerstorage\nmount -t xfs /dev/${disk} /containerstorage\n# Add entry to the fstab\necho \"/dev/${disk} /containerstorage xfs defaults,nofail 0 0\" | tee -a /etc/fstab\n\nsetenforce 0\nsed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=permissive/' /etc/sysconfig/selinux\n\necho \"export TMPDIR='/containerstorage/tmp'\" | tee -a /root/.bashrc\n\necho -e \"fs.file-max=20000000\\nfs.aio-max-nr=262144\\nvm.memory_failure_early_kill=1\\nvm.max_map_count=135217728\\nnet.ipv4.ip_local_port_range=60000 65535\" | tee -a /etc/sysctl.conf\n\nmkdir -p /data/hxe\nchown 12000:79 /data/hxe\necho -e '{\\n\"master_password\" : \"HXEHana1\"}' | tee -a /data/hxe/hxepasswd.json\n\nyum install tmux container-tools -y\nsed -i 's|/var/lib/containers/storage|/containerstorage|' /etc/containers/storage.conf\n```\n\n## Finish Docker Image Pull\n\nThis portion will run pulled SAP Hana container in a tmux'ed process.\n\n**Please note:** this notebook uses `docker.io` for the container location and to update as needed and the user will need to supply a valid docker username and password\n\n```bash\n# Connect\nssh -i <Your Key> ec2-user@<yourhost>\n\n# become root\nsudo su - \n\n# Set your docker username and password\nD_USER=''\nD_PASSWORD=''\n\npodman login --username ${D_USER} --password ${D_PASSWORD} docker.io/store/saplabs\n\npodman pull docker.io/saplabs/hanaexpress:2.00.061.00.20220519.1\n\ntmux\n\npodman run -p 39013:39013 -p 39017:39017 -p 39041-39045:39041-39045 -p 1128-1129:1128-1129 -p 59013-59014:59013-59014 -v /data/hxe:/hana/mounts --ulimit nofile=1048576:1048576 --sysctl kernel.shmmax=1073741824 --sysctl net.ipv4.ip_local_port_range='40000 60999' --sysctl kernel.shmmni=4096 --sysctl kernel.shmall=8388608 --name hxecont docker.io/saplabs/hanaexpress:2.00.061.00.20220519.1 --passwords-url file:///hana/mounts/hxepasswd.json --agree-to-sap-license\n```\n\n## Load the Sample Data\n\nBelow find the \n\nOpen a second SSH window, create the data sources, the user, grant permissions, and test.\n\n```bash\n# Connect\nssh -i <Your Key> ec2-user@<yourhost>\n\n# become root\nsudo su - \n\n# Download and prep the source file\n\ncurl -o /tmp/auto-mpg.csv https://raw.githubusercontent.com/datarobot-community/quickstart-guide/master/data/auto-mpg.csv\nsed -i 's/[a-z]/\\U&/g' /tmp/auto-mpg.csv\nsed -i '1s/ //' /tmp/auto-mpg.csv\n\npodman cp /tmp/auto-mpg.csv hxecont:/usr/sap/HXE/HDB90/work\npodman exec -it hxecont /usr/sap/HXE/HDB90/exe/hdbsql -i 90 -d HXE -u system -p HXEHana1\n\n# Make the schma, table and Load the data\nALTER SYSTEM ALTER CONFIGURATION ('indexserver.ini', 'system') set ('import_export', 'enable_csv_import_path_filter') = 'false' with reconfigure;\nCREATE SCHEMA DATAINPUT\n\nCREATE TABLE DATAINPUT.AUTOMPG(MPG NUMERIC, CYLINDERS INT, DISPLACEMENT NUMERIC, HORSEPOWER NUMERIC, WEIGHT NUMERIC, ACCELERATION NUMERIC, MODELYEAR INT, ORIGIN INT );\n\nIMPORT FROM CSV FILE '/usr/sap/HXE/HDB90/work/auto-mpg.csv' INTO DATAINPUT.AUTOMPG WITH COLUMN LIST IN FIRST ROW FIELD DELIMITED BY ',';\nSELECT COUNT(*) FROM DATAINPUT.AUTOMPG;\n\nCREATE USER JDBC_DR_WORKER PASSWORD His_Password_1 NO FORCE_FIRST_PASSWORD_CHANGE\nGRANT SELECT, INSERT, UPDATE, DELETE, EXECUTE ON SCHEMA DATAINPUT TO JDBC_DR_WORKER\nGRANT SELECT, INSERT, UPDATE, DELETE ON DATAINPUT.AUTOMPG TO JDBC_DR_WORKER\n\n# Log in to the client as the JDBC_DR_WORKER user to test\npodman exec -it hxecont /usr/sap/HXE/HDB90/exe/hdbsql -i 90 -d HXE -u JDBC_DR_WORKER -p His_Password_1\nSELECT * FROM DATAINPUT.AUTOMPG LIMIT 10\n```\n\n---\n## Copyright 2023 DataRobot Inc. All Rights Reserved.\n\n**This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied**\n\n---",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/SAP_template/SAP_End_to_End.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "SAP_template",
        "file_type": "notebook",
        "size": 56455
      },
      "code_examples": [
        "!{sys.executable} -m pip install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !{sys.executable} -m pip install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!{sys.executable} -m pip install datarobot",
        "!pip3 install --upgrade pip\n\n# # Use when using DatgaRobot 8.x\n# !pip3 install \"datarobot>=2.28,<2.29\"\n\n# Use for DataRobot 9.x or app.datarobot.com\n!pip3 install datarobot",
        "import csv\nimport json\nimport pprint\nfrom urllib.parse import urlparse\n\nimport datarobot as dr\nfrom datarobot import AUTOPILOT_MODE\nfrom datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.models import Deployment\nimport requests\n\n# Provide the URL protocol, address (IP or FQDN)\n# Example: https://app.datarobot.com or https://datarobot.example.com or http://10.1.2.3\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com\"\n\n# # Provide an API key from a user with permission from this cluster\nDATAROBOT_API_TOKEN = \"<A valid DataRobot API key goes here>\"\n\n# Required by app.datarobot.com\nDATAROBOT_KEY = \"<A valid DataRobot App Key goes here>\"\n\n# Define the SAP Hana deployment\n# The settings below assumes the usage of the example data load defined at the end of this AI Accelerator\n# # If you are using a custom database, you must update as required\nSAP_JDBC_URL = \"jdbc:sap://<the SAP Hana IP goes here>:39041?databaseName=HXE\"\n\n# # When using https://www.sap.com/products/erp/s4hana.html, your jdbc url will look like:\n# SAP_JDBC_URL = \"jdbc:sap://<SAP Hana cloud ID goes here>.hanacloud.ondemand.com:443\"\n\n### JDBC Connectivity settings\nJDBC_USERNAME = \"JDBC_DR_WORKER\"\nJDBC_PASSWORD = \"His_Password_1\"\n\nJDBC_WRITE_SCHEMA = \"SAP_DEMO\"\nJDBC_WRITE_TABLE = \"scored_data_from_notebook\"\n\n### JDBC Driver Settings\nJDBC_CREDENTIAL_NAME = \"SAP_HANA_CREDENTIAL\"\nSAP_JDBC_DRIVER_NAME = \"2.15.10 recommended\"\nSAP_DATASTORE_NAME = \"SAP_HANA_DataStore\"\n\n### SQL Data Extraction statements\nTRAIN_DATASOURCE_NAME = \"SAP_HANA_DataSource\"\nTRAIN_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG;\"\n\nPREDICT_DATASOURCE_NAME = \"SAP_HANA_DataSource4Prediction\"\nPREDICT_EXTRACTION_SQL = \"SELECT MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,ORIGIN FROM DATAINPUT.AUTOMPG LIMIT 10;\"\n\n### DataRobot project settings\nPROJECT_NAME = \"AutomobileMpG-burn-notice\"\nPROJECT_WORKERS = -1\nTARGET = \"MPG\"\nMETRIC = \"RMSE\"\n\n# Turn on DEBUG statements\nDEBUG = False  # True #\n\n# Set Autopilot mode\nOMODE = AUTOPILOT_MODE.QUICK\n\n# These values are mostly constant\n# Create the shared DataRobot client\nmy_client = dr.Client(\n    token=\"%s\" % (DATAROBOT_API_TOKEN),\n    endpoint=\"%s/api/v2\" % (DATAROBOT_ENDPOINT),\n    ssl_verify=True if (urlparse(DATAROBOT_ENDPOINT)).scheme == \"https\" else False,\n    trace_context=\"AIA-E2E-SAPWORKFLOW-125\",  # Optional. Helps DataRobot improve this workflow\n)\n\n# Find the dedicated prediction engine URL\nPREDICTION_SERVER = dr.PredictionServer.list()[0]\n\n# Verbose settings statement\nprint(\"# -----------------------------------------\")\nprint(\"DataRobot client version: %s\" % dr.__version__)\nprint(\"# -----------------------------------------\")\nprint(\"DATAROBOT_ENDPOINT: %s | %s\" % (DATAROBOT_ENDPOINT, PREDICTION_SERVER))\nprint(\"DATAROBOT_API_TOKEN: %s\" % (len(DATAROBOT_API_TOKEN)))\nprint(\"# Credentials settings -------------------\")\nprint(\"JDBC_CREDENTIAL_NAME: %s\" % (JDBC_CREDENTIAL_NAME))\nprint(\"JDBC_USERNAME: %s\" % (JDBC_USERNAME))\nprint(\"JDBC_PASSWORD: %s\" % (len(JDBC_PASSWORD)))\nprint(\"JDBC_WRITE_SCHEMA: %s\" % (JDBC_WRITE_SCHEMA))\nprint(\"JDBC_WRITE_TABLE: %s\" % (JDBC_WRITE_TABLE))\nprint(\"# DataStore settings ---------------------\")\nprint(\"SAP_JDBC_DRIVER_NAME: %s\" % (SAP_JDBC_DRIVER_NAME))\nprint(\"SAP_DATASTORE_NAME: %s\" % (SAP_DATASTORE_NAME))\nprint(\"SAP_JDBC_URL: %s\" % (len(SAP_JDBC_URL)))\nprint(\"TRAIN_DATASOURCE_NAME: %s\" % (TRAIN_DATASOURCE_NAME))\nprint(\"TRAIN_EXTRACTION_SQL: %s\" % (TRAIN_EXTRACTION_SQL))\nprint(\"PREDICT_DATASOURCE_NAME: %s\" % (PREDICT_DATASOURCE_NAME))\nprint(\"PREDICT_EXTRACTION_SQL: %s\" % (PREDICT_EXTRACTION_SQL))\nprint(\"# Project settings ------------------------\")\nprint(\"PROJECT_NAME: %s\" % (PROJECT_NAME))\nprint(\"TARGET: %s | METRIC: %s\" % (TARGET, METRIC))",
        "CREDENTIAL = dr.Credential.create_basic(\n    name=JDBC_CREDENTIAL_NAME,\n    user=JDBC_USERNAME,\n    password=JDBC_PASSWORD,\n)\n\nprint(\"Created CREDENTIAL: %s\" % (CREDENTIAL))",
        "# Set the driver name in the SAP Hana JDBC driver name temaplate\njdbc_driver_name = \"SAP HANA (%s)\" % SAP_JDBC_DRIVER_NAME\n\nJDBC_DRIVER = [\n    drs for drs in dr.DataDriver.list() if drs.canonical_name == jdbc_driver_name\n][-1]\nprint(\"Using JDBC_DRIVER: %s\" % (JDBC_DRIVER))\n\nDATASTORE = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=SAP_DATASTORE_NAME,\n    driver_id=JDBC_DRIVER.id,\n    jdbc_url=SAP_JDBC_URL,\n)\nprint(\"Created DATASTORE: %s\" % (DATASTORE))",
        "TRAIN_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=TRAIN_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=TRAIN_EXTRACTION_SQL,\n    ),\n)\nprint(\"Created TRAIN_DATASOURCE id: %s | %s\" % (TRAIN_DATASOURCE.id, TRAIN_DATASOURCE))\n\nPREDICT_DATASOURCE = dr.DataSource.create(\n    data_source_type=\"jdbc\",\n    canonical_name=PREDICT_DATASOURCE_NAME,\n    params=dr.DataSourceParameters(\n        data_store_id=DATASTORE.id,\n        query=PREDICT_EXTRACTION_SQL,\n    ),\n)\nprint(\n    \"Created PREDICT_DATASOURCE id: %s | %s\"\n    % (PREDICT_DATASOURCE.id, PREDICT_DATASOURCE)\n)",
        "# Create for the Training DataSet\nTRAIN_DATASET = TRAIN_DATASOURCE.create_dataset(\n    do_snapshot=False, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created TRAIN_DATASET id: %s | %s\" % (TRAIN_DATASET.id, TRAIN_DATASET))\n\n# Create for the Prediction Source\n# Please note the \"do_snapshot=True\" which is required for prediction datasets\nPREDICT_DATASET = PREDICT_DATASOURCE.create_dataset(\n    do_snapshot=True, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PREDICT_DATASET id: %s | %s\" % (PREDICT_DATASET.id, PREDICT_DATASET))\n\n# Handy debug statement to open a browser tab pointing to the Training DataSet in the AI Catalog\nif str(dr.__version__) > str(3) and DEBUG:\n    TRAIN_DATASET.open_in_browser()",
        "# Find the current worker and job status, which is a good measure of system usage\nWORKERS_BEFORE = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers before: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_BEFORE[\"total\"],\n        WORKERS_BEFORE[\"inUse\"],\n        WORKERS_BEFORE[\"usersRunning\"],\n        WORKERS_BEFORE[\"jobsWaiting\"],\n        WORKERS_BEFORE[\"usersWaiting\"],\n    )\n)\n\nPROJECT = TRAIN_DATASET.create_project(\n    project_name=PROJECT_NAME, credential_id=CREDENTIAL.credential_id\n)\nprint(\"Created PROJECT: %s\" % (PROJECT))\n\nPROJECT.analyze_and_model(\n    target=TARGET, metric=METRIC, mode=OMODE, worker_count=PROJECT_WORKERS\n)\n\nPROJECT.wait_for_autopilot()\n\nprint(\"PROJECT %s info: %s\" % (PROJECT.id, PROJECT))\n\n# An easy way to confirm the worker autoscaling is working as expected\nWORKERS_AFTER = (json.loads(my_client.get(\"resourceMonitor\").text))[\"workers\"]\nprint(\n    \"workers after: %s | inUse: %s | usersRunning: %s | jobsWaiting: %s | usersWaiting: %s\"\n    % (\n        WORKERS_AFTER[\"total\"],\n        WORKERS_AFTER[\"inUse\"],\n        WORKERS_AFTER[\"usersRunning\"],\n        WORKERS_AFTER[\"jobsWaiting\"],\n        WORKERS_AFTER[\"usersWaiting\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    PROJECT.open_in_browser()",
        "TOP_MODEL = dr.ModelRecommendation.get(\n    PROJECT.id, dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT\n).get_model()\n\nprint(\"Using Recommended Model id: %s | %s\" % (TOP_MODEL.id, TOP_MODEL))\nprint(\n    \"validation: %s | crossValidation: %s\"\n    % (\n        TOP_MODEL.metrics[PROJECT.metric][\"validation\"],\n        TOP_MODEL.metrics[PROJECT.metric][\"crossValidation\"],\n    )\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    TOP_MODEL.open_model_browser()",
        "DEPLOYMENT = dr.Deployment.create_from_learning_model(\n    TOP_MODEL.id,\n    label=\"%s RECOMMENDED_MODEL\" % PROJECT.project_name,\n    description=\"API %s deployment of: %s\"\n    % (PROJECT.project_name, TOP_MODEL.model_type),\n    default_prediction_server_id=PREDICTION_SERVER.id,\n)\n\nif str(dr.__version__) > str(3) and DEBUG:\n    DEPLOYMENT.open_in_browser()\n\nprint(\"Created DEPLOYMENT: %s | %s\" % (DEPLOYMENT.id, DEPLOYMENT))",
        "JDBC_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,  #   \"select * from new_york_taxi.input limit 1000\",\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"localFile\",\n        \"path\": \"jdbc-batch-predictions.csv\",\n    },\n)\n\nJDBC_BATCH_PREDICTION_JOB.wait_for_completion()\n\nwith open(\"jdbc-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)",
        "AICATALOG_BATCH_PREDICTION_JOB = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"dataset\",\n        \"dataset\": PREDICT_DATASET,\n        \"dataset_version_id\": PREDICT_DATASET.version_id,\n    },\n    output_settings={\"type\": \"localFile\", \"path\": \"aicatalog-batch-predictions.csv\"},\n)\nAICATALOG_BATCH_PREDICTION_JOB.wait_for_completion()\nAICATALOG_BATCH_PREDICTION_JOB.get_status()\n\nwith open(\"aicatalog-batch-predictions.csv\", \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file)\n    for line in csv_reader:\n        print(line)",
        "AICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK = dr.BatchPredictionJob.score(\n    deployment=DEPLOYMENT.id,\n    passthrough_columns=[\n        \"MPG\",\n        \"CYLINDERS\",\n        \"DISPLACEMENT\",\n        \"HORSEPOWER\",\n        \"WEIGHT\",\n        \"ACCELERATION\",\n        \"MODELYEAR\",\n        \"ORIGIN\",\n    ],\n    intake_settings={\n        \"type\": \"jdbc\",\n        \"query\": PREDICT_EXTRACTION_SQL,\n        \"data_store_id\": DATASTORE.id,  # The ID of the data store you want\n        \"credential_id\": CREDENTIAL.credential_id,  # The credentialid of the credentials stored in your credentials manager\n    },\n    output_settings={\n        \"type\": \"jdbc\",\n        \"statement_type\": \"insert\",\n        \"schema\": JDBC_WRITE_SCHEMA,\n        \"table\": JDBC_WRITE_TABLE,\n        \"data_store_id\": DATASTORE.id,\n        \"credential_id\": CREDENTIAL.credential_id,\n        \"create_table_if_not_exists\": True,\n    },\n)\nAICATALOG_BATCH_PREDICTION_JOB_WITH_WRITEBACK.wait_for_completion()",
        "# JSON records for example automobiles for which to predict mpg\nautos = [\n    {\n        \"CYLINDERS\": 4,\n        \"DISPLACEMENT\": 119.0,\n        \"HORSEPOWER\": 82.00,\n        \"WEIGHT\": 2720.0,\n        \"ACCELERATION\": 19.4,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n    {\n        \"CYLINDERS\": 8,\n        \"DISPLACEMENT\": 120.0,\n        \"HORSEPOWER\": 79.00,\n        \"WEIGHT\": 2625.0,\n        \"ACCELERATION\": 18.6,\n        \"MODELYEAR\": 82,\n        \"ORIGIN\": 1,\n    },\n]\n\n# Create REST request for prediction API\nprediction_server = DEPLOYMENT.default_prediction_server\n\nprint(\"prediction_server: %s\" % prediction_server)\n\nprediction_headers = {\n    \"Authorization\": \"Bearer {}\".format(DATAROBOT_API_TOKEN),\n    \"User-Agent\": \"AIA-E2E-SAPWORKFLOW-125\",  # Optional but helps DataRobot improve this workflow\n    \"DataRobot-Key\": DATAROBOT_KEY,\n    \"Content-Type\": \"application/json\",\n}\n\nREALTINE_PREDICTIONS = requests.post(\n    \"%s/predApi/v1.0/deployments/%s/predictions?passthroughColumns=CYLINDERS&passthroughColumns=DISPLACEMENT&passthroughColumns=HORSEPOWER&passthroughColumns=WEIGHT&passthroughColumns=ACCELERATION&passthroughColumns=MODELYEAR&passthroughColumns=ORIGIN\"\n    % (prediction_server[\"url\"], DEPLOYMENT.id),\n    data=json.dumps(autos),\n    headers=prediction_headers,\n)\n\npprint.pprint(REALTINE_PREDICTIONS.json())",
        "from datarobot.enums import SERVICE_STAT_METRIC\nfrom datarobot.helpers.partitioning_methods import construct_duration_string\nfrom datarobot.models import Deployment\n\nservice_stats = DEPLOYMENT.get_service_stats()\nprint(\"Service Statistics:\")\nprint(service_stats)\n\nfor item in service_stats.metrics:\n    print(\"%s => %s\" % (item, service_stats.metrics[item]))",
        "items_deleted = 0\n\n# Delete the elemets created above\nDR_ELEMENTS = [\n    TRAIN_DATASET,\n    PREDICT_DATASET,\n    DEPLOYMENT,\n    PROJECT,\n    TRAIN_DATASOURCE,\n    PREDICT_DATASOURCE,\n    DATASTORE,\n    CREDENTIAL,\n]\n\nfor item in DR_ELEMENTS:\n    if \"Credential\" in str(type(item)):\n        my_id = item.credential_id\n    else:\n        my_id = item.id\n\n    try:\n        print(\"Deleting: %s\" % (item.name))\n        item.delete()\n        items_deleted += 1\n\n        # my_id = item.credential_id if credential_id in item else item.id\n        print(\"Deleted %s: %s id: %s | %s\" % (items_deleted, item.name, my_id, item))\n\n    except Exception as ex:\n        print(\"id not found: %s (%s)\" % (my_id, type(item)))\n        pass\n    except NameError as n:\n        print(\"name %s not found\" % (n))\n        pass\n\nprint(\"Cleaned up %s items\" % (items_deleted))"
      ],
      "api_methods": [
        "project.wait_for_autopilot",
        "model.metrics",
        "dr.datasource.create",
        "dr.datastore.create",
        "datarobot.example.com",
        "deployment.create_from_learning_model",
        "datarobot.datasource.create_dataset",
        "dr.enums.recommended_model_type",
        "model.id",
        "dr.predictionserver.list",
        "project.html",
        "project.project_name",
        "dr.deployment.create_from_learning_model",
        "deployment.get_service_stats",
        "project.id",
        "model.open_model_browser",
        "datarobot.helpers.partitioning_methods",
        "deployment.default_prediction_server",
        "deployment.open_in_browser",
        "deployment.id",
        "dr.datadriver.list",
        "deployment.html",
        "dr.credential.create_basic",
        "dr.modelrecommendation.get",
        "dr.batchpredictionjob.score",
        "project.open_in_browser",
        "model.model_type",
        "project.analyze_and_model",
        "project.metric"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-8890208582284115318",
      "title": "Snowflake_snowpark_template: Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud.config.yaml",
      "content": "---\ndescription: This notebook showcases the native integration between DataRobot and Snowflake's data cloud,\n  leveraging DataRobot notebooks and Snowflake Snowpark (with Python and Java).\nfile_name: Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud.ipynb\nlanguages:\n  - python\nmaintainers:\n  - Atalia Horenshtien\nmaintainers_email:\n  - atalia.horenshtien@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: Integrate DataRobot and Snowpark by maximizing the data cloud\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Snowflake_snowpark_template/Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Snowflake_snowpark_template",
        "file_type": "yaml",
        "size": 513
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.25,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_7427299798524897450",
      "title": "Snowflake_snowpark_template: Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud.ipynb",
      "content": "# Integrate DataRobot and Snowpark by maximizing the data cloud\n\nAuthor: Atalia Horenshtien\n\nVersion Date: April 6 2023\n\n## Overview\n\nThis notebook showcases the native integration between DataRobot and Snowflake's data cloud, leveraging DataRobot notebooks and Snowflake Snowpark (with Python and Java).\n\n### DataRobot\nThe notebook leverages DataRobot ML experimentation. DataRobot hosts notebooks, AutoML, model selection, insights and explainability, and ML production as a single shop for deployment and model monitoring.\n\n### Snowflake\nYou will leverage Snowflake for data storage and Snowpark for deployment, feature engineering, and model scoring. They facilitate data source access and utilize the Spark's backbone to perform distributed scoring to support large-scale use cases.\n\nThe dataset for this project is a fraud detection use case and the notebook was created from a DataRobot notebook. DataRobot notebooks are Jupyter compatible and supporting preloaded packages for Python and R. DataRobot notebooks can also be used for code-snippets, versioning, and simple environment management for rapid AI experimentation to increase your productivity.\n\n### Summary\n\nThis notebook covers the following steps:\n\n1. Load data to Snowflake from an S3 file\n2. Acquire a training dataset from a Snowflake table using Snowpark for Python\n3. Feature engineering: analyze data and create new features using Snowpark\n4. Build a new DataRobot project\n5. Analyze and evaluate model performance and explainability using DataRobot AutoML\n6. Deploy the recommended model to Snowflake using DataRobot MLOps\n7. Score the model via Snowpark for Java\n8. Monitor the model with MLOps\n\n## Setup\n\nBefore executing the cells in the notebook, ensure the following:\n\n* Make sure the environment is Python 3.8\n* Configure the following environmental variables (with your own Snowflake credentials):\n    'snowflake_user' - Snowflake user name\n    'snowflake_password' - Snowflake password\n    'snowflake_account' - Snowflake account name\n\nMost of this workflow is executed by the notebook, but you will create a deployment and monitor it via the DataRobot GUI. Supporting documentation for these steps are included the notebook.\n\n```python\n# If needed, install the following:\n!pip install Flask-SQLAlchemy\n!pip install --upgrade snowflake-sqlalchemy\n```\n\n### Import libraries\n\n```python\nimport datetime as datetime\nfrom datetime import datetime\nimport json\nimport os\nfrom pathlib import Path\n\nfrom datarobot.enums import CHART_DATA_SOURCE, DATA_DRIFT_METRIC\nfrom datarobot.models.deployment import Deployment, FeatureDrift\nfrom datarobot.models.prediction_server import PredictionServer\nfrom datarobot.models.project import Project\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import pd_writer\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import call_udf, col, udf\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.dialects import registry\n\n# from datarobot.models import FeatureDrift\n\n\nregistry.register(\"snowflake\", \"snowflake.sqlalchemy\", \"dialect\")\nimport urllib.parse\n\nfrom snowflake.sqlalchemy import URL\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n```\n\n### Bind data source credentials\n\n```python\nsnowflake_password = os.environ[\"snowflake_password\"]\nsnowflake_user = os.environ[\"snowflake_user\"]\nsnowflake_account = os.environ[\"snowflake_account\"]\n```\n\n```python\n# No need to change these bindings as they are for the demo workflow\n\ndatabase_name = \"SANDBOX\"\nschema_name = \"FINANCE\"\nwarehouse_name = \"DEMO_WH\"\n```\n\n### Import data\n\nUse the following cell to load training data for the sample workflow: importing the \"Fraud Detection\" dataset from S3 to Snowflake.\n\n```python\n# load training data - 'Fraud Detection' from S3 to Snowflake\nsafe_password_string = urllib.parse.quote_plus(snowflake_password)\n\nconn_string = f\"snowflake://{snowflake_user}:{safe_password_string}@{snowflake_account}/{database_name}\"\n\nengine = create_engine(conn_string)\n\n# %%\n\nengine.execute(\n    \"\"\"\n    CREATE SCHEMA IF NOT EXISTS FINANCE;\n    \n    \n    \"\"\"\n)\n\nrenamer = {\"date\": \"DATE_COLUMNS\"}\npd.read_csv(\n    \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/FRAUD_DETECTION_TRAINING.csv\"\n).rename(columns=renamer).to_sql(\n    name=\"fraud_detection_training\",\n    schema=schema_name,\n    con=engine,\n    if_exists=\"replace\",\n    method=pd_writer,\n    index=False,\n)\n\n\n# %\n```\n\n```python\nCONNECTION_PARAMETERS = {\n    \"account\": snowflake_account,\n    \"user\": snowflake_user,\n    \"password\": snowflake_password,\n    \"database\": database_name,\n    \"schema\": schema_name,\n    \"warehouse\": warehouse_name,\n}\n```\n\n### Data Preparation \n\nThis workflow uses Snowpark for feature engineering. Snowpark is a developer framework where you can work in a familiar syntax such as Python. Snowpark pushes down processing to Snowflake to run consistently in a highly secure and elastic engine.\n\n```python\n# Create the ‘session’ object that represents Snowpark and a dataframe that references the data in Snowflake\n\nsession = Session.builder.configs(CONNECTION_PARAMETERS).create()\n```\n\n```python\ndf_train = session.table(\"FRAUD_DETECTION_TRAINING\")\n```\n\n```python\ndf_train.show()\n```\n\n```python\n# Snowpark - Create an instance of UserDefinedFunction using the @udf decorator\n\n\n@udf\n# Define the function arguments: WHOLE_WEIGHT, HEIGHT\ndef is_long_session(session_length_in_mins: float) -> bool:\n    if session_length_in_mins > 20:\n        return 1\n    else:\n        return 0\n```\n\n```python\n# Snowpark call UDFs on a dataframe, with calculation in Snowflake\nudf_df_train = df_train.select(\n    col(\"*\"), is_long_session(col(\"SESSION_LENGTH_IN_MINS\")).alias(\"LONG_SESSION\")\n)\n```\n\n```python\nudf_df_train.write.mode(\"overwrite\").save_as_table(\"FRAUD_DETECTION_TRAINING\")\n```\n\n```python\n# Snowpark supports Pandas\ndf_train_pd = df_train.toPandas()\n```\n\n```python\nsns.countplot(data=df_train_pd, x=\"ISFRAUD\")\n```\n\n## Modeling\n\nIn this section you'll leverage DataRobot’s AutoML capabilities, including explainability and compliance documentation to speed up the model’s results and regulatory compliance.\n\n```python\n# Optional - If you've already run Autopilot before, get the project by replacing the ID\n# project = Project.get('63853765cfc79e4555fa2537')\n```\n\n```python\n# Define the project name and current date\nnow = datetime.now().strftime(\"%Y-%m-%dT%H:%M\")\nproject = Project.create(df_train_pd, project_name=\"Fraud_detection_{}\".format(now))\n```\n\n### Start Autopilot\n\n```python\n# Set the target variable and start Autopilot\nproject.analyze_and_model(\n    target=\"ISFRAUD\",  # setting project target\n    mode=\"quick\",  # setting up project run mode, leaving in auto\n    worker_count=-1,  # assigning worker count, -1 will use all available\n    max_wait=24\n    * 60\n    * 60,  # time series projects can take a little more time to get through EDA2 than normal AutoML projects\n)\n```\n\n```python\n# This (optional) will block execution of the notebook until the full autopilot process has completed. This can take several minutes or hours, depending on the autopilot mode selected, the size of the dataset, and the type of problem we're trying to solve.\n\nproject.wait_for_autopilot()\n```\n\n```python\n# This output a list of all the models trained in the project, sorted by the selected validation metric.\n\nproject.get_models()[:10]\n```\n\n### Get the most accurate model\n\nDataRobot provides a recommendation for an accurate and well-performing model at the end of training process.\n\n```python\n# Get the most accurate model based on the project's metric\nmost_accurate_model = project.get_top_model()\n```\n\n```python\n# Evaluate the model using a lift chart\nplot = most_accurate_model.get_lift_chart(\"crossValidation\")\ndf = pd.DataFrame.from_dict(plot.bins)\n\n# Create data\nx = df.index\ny = df[\"actual\"]\ny2 = df[\"predicted\"]\n\n# Plot lines\nplt.plot(x, y, label=\"actuals\")\nplt.plot(x, y2, label=\"predicted\")\nplt.legend()\nplt.title(\"lift chart\")\nplt.show()\n```\n\n```python\n# Evaluate the model using ROC Curve\n\n# Get the ROC curve\nroc = most_accurate_model.get_roc_curve(source=CHART_DATA_SOURCE.VALIDATION)\n\n# Save the result into a Pandas dataframe\nroc_df = pd.DataFrame(roc.roc_points)\nroc_df.head()\n\n# Set chart colors\ndr_roc_green = \"#03c75f\"\nwhite = \"#ffffff\"\ndr_purple = \"#65147D\"\ndr_dense_green = \"#018f4f\"\ndr_dark_blue = \"#08233F\"\n\n# Create chart\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=dr_dark_blue)\nplt.scatter(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot([0, 1], [0, 1], color=white, alpha=0.25)\n\n# Format chart\nplt.title(\"ROC curve\")\nplt.xlabel(\"False Positive Rate (Fallout)\")\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate (Sensitivity)\")\nplt.ylim([0, 1])\n```\n\n```python\n# Get insights on which feature driving model's outcome\n\n# Get Feature Impact\nfeature_impact = most_accurate_model.get_or_request_feature_impact()\n\n# Save Feature Impact in a Pandas dataframe\nfi_df = pd.DataFrame(feature_impact)\n\n# Plot the top five most impactful features\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.barplot(x=\"featureName\", y=\"impactNormalized\", data=fi_df[0:5], color=\"g\")\n```\n\n## Model Deploymnet\n\nTo deploy to Snowflake, you need to do it through the DataRobot GUI. Reference the documentation to [create a snowflake prediction environment and deployment](https://docs.datarobot.com/en/docs/mlops/mlops-preview/pp-snowflake-sc-deploy-replace.html).\n\n```python\n# Get the deployed model and change the ID to your deployment's\ndeployment = Deployment.get(\"6413a07ea0a9ee22d203464b\")\ndeployment\n```\n\n```python\n# Create the scoring data\n# This workflow uses an example for subset data\nquery = (\n    \" create or replace table FRAUD_DETECTION_SCORING as \"\n    + \"  select * from FRAUD_DETECTION_TRAINING limit 10 \"\n)\n\nsession.sql(query).collect()\n```\n\nIn order to score the data in Snowflake, leverage Snowpark for Java.\n\nGo to the created deployment's **Predictions** tab. Copy the signature of the function included there and replace it in the next section. \n\nExample:\nquery = \" SELECT *, scoring_code_udf_schema.replace_function_name_here(OBJECT_CONSTRUCT_KEEP_NULL(*)) \" +\\\n        \" FROM FRAUD_DETECTION_SCORING \"\n\n```python\n# Score new data\n\nquery = (\n    \" CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as\"\n    + \" SELECT *, datarobot_deployment_642adf008b22ed8d5de483ba(OBJECT_CONSTRUCT_KEEP_NULL(*)) as model_score FROM FRAUD_DETECTION_SCORING;\"\n)\n\n\nsession.sql(query).collect()\n```\n\n```python\n# Store results and modify column names\n\nquery = ' CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as select *, model_score:\"0\"::float false_fraud, model_score:\"1\"::float true_fraud  from FRAUD_DETECTION_SCORING; '\n\nsession.sql(query).collect()\n```\n\n```python\n# Convert the session object to a dataframe\n\ndf_scored = session.table(\"FRAUD_DETECTION_SCORING\")\ndf_scored_pd = df_scored.toPandas()\ndf_scored_pd\n```\n\nYou can see the predictions in the last two columns.\n\n## Model monitoring\n\nTo make sure business decisions are aligned with external and internal factors, monitor the model performance and understand if the model needs to be replaced or retrained. To learn how to define a monitoring job, [reference the documentation](https://docs.datarobot.com/en/docs/mlops/mlops-preview/pp-monitoring-jobs.html).\n\nOnce the monitoring job is executed, you can see data drift and accuracy in the UI or retrieve it with the API.\n\n```python\n# Get target drift\n\ntarget_drift = deployment.get_target_drift(\n    start_time=datetime(2023, 4, 1, hour=15), end_time=datetime(2023, 4, 4, hour=15)\n)\ntarget_drift.drift_score\n```\n\n## Conclusion and next steps\n\nDataRobot and Snowflake together offer an end-to-end, enterprise-grade AI experience and expertise to enterprises by reducing complexity and productionizing ML models at scale, unlocking business value.\n\n[Visit the DataRobot website](https://www.datarobot.com/partners/technology-partners/snowflake/) to learn more.",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Snowflake_snowpark_template/Native integration DataRobot and Snowflake Snowpark-Maximizing the Data Cloud.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "openai",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "time-series",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Snowflake_snowpark_template",
        "file_type": "notebook",
        "size": 176228
      },
      "code_examples": [
        "# If needed, install the following:\n!pip install Flask-SQLAlchemy\n!pip install --upgrade snowflake-sqlalchemy",
        "import datetime as datetime\nfrom datetime import datetime\nimport json\nimport os\nfrom pathlib import Path\n\nfrom datarobot.enums import CHART_DATA_SOURCE, DATA_DRIFT_METRIC\nfrom datarobot.models.deployment import Deployment, FeatureDrift\nfrom datarobot.models.prediction_server import PredictionServer\nfrom datarobot.models.project import Project\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import pd_writer\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import call_udf, col, udf\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.dialects import registry\n\n# from datarobot.models import FeatureDrift\n\n\nregistry.register(\"snowflake\", \"snowflake.sqlalchemy\", \"dialect\")\nimport urllib.parse\n\nfrom snowflake.sqlalchemy import URL\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker",
        "snowflake_password = os.environ[\"snowflake_password\"]\nsnowflake_user = os.environ[\"snowflake_user\"]\nsnowflake_account = os.environ[\"snowflake_account\"]",
        "# No need to change these bindings as they are for the demo workflow\n\ndatabase_name = \"SANDBOX\"\nschema_name = \"FINANCE\"\nwarehouse_name = \"DEMO_WH\"",
        "# load training data - 'Fraud Detection' from S3 to Snowflake\nsafe_password_string = urllib.parse.quote_plus(snowflake_password)\n\nconn_string = f\"snowflake://{snowflake_user}:{safe_password_string}@{snowflake_account}/{database_name}\"\n\nengine = create_engine(conn_string)\n\n# %%\n\nengine.execute(\n    \"\"\"\n    CREATE SCHEMA IF NOT EXISTS FINANCE;\n    \n    \n    \"\"\"\n)\n\nrenamer = {\"date\": \"DATE_COLUMNS\"}\npd.read_csv(\n    \"https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/FRAUD_DETECTION_TRAINING.csv\"\n).rename(columns=renamer).to_sql(\n    name=\"fraud_detection_training\",\n    schema=schema_name,\n    con=engine,\n    if_exists=\"replace\",\n    method=pd_writer,\n    index=False,\n)\n\n\n# %",
        "CONNECTION_PARAMETERS = {\n    \"account\": snowflake_account,\n    \"user\": snowflake_user,\n    \"password\": snowflake_password,\n    \"database\": database_name,\n    \"schema\": schema_name,\n    \"warehouse\": warehouse_name,\n}",
        "# Create the ‘session’ object that represents Snowpark and a dataframe that references the data in Snowflake\n\nsession = Session.builder.configs(CONNECTION_PARAMETERS).create()",
        "df_train = session.table(\"FRAUD_DETECTION_TRAINING\")",
        "df_train.show()",
        "# Snowpark - Create an instance of UserDefinedFunction using the @udf decorator\n\n\n@udf\n# Define the function arguments: WHOLE_WEIGHT, HEIGHT\ndef is_long_session(session_length_in_mins: float) -> bool:\n    if session_length_in_mins > 20:\n        return 1\n    else:\n        return 0",
        "# Snowpark call UDFs on a dataframe, with calculation in Snowflake\nudf_df_train = df_train.select(\n    col(\"*\"), is_long_session(col(\"SESSION_LENGTH_IN_MINS\")).alias(\"LONG_SESSION\")\n)",
        "udf_df_train.write.mode(\"overwrite\").save_as_table(\"FRAUD_DETECTION_TRAINING\")",
        "# Snowpark supports Pandas\ndf_train_pd = df_train.toPandas()",
        "sns.countplot(data=df_train_pd, x=\"ISFRAUD\")",
        "# Optional - If you've already run Autopilot before, get the project by replacing the ID\n# project = Project.get('63853765cfc79e4555fa2537')",
        "# Define the project name and current date\nnow = datetime.now().strftime(\"%Y-%m-%dT%H:%M\")\nproject = Project.create(df_train_pd, project_name=\"Fraud_detection_{}\".format(now))",
        "# Set the target variable and start Autopilot\nproject.analyze_and_model(\n    target=\"ISFRAUD\",  # setting project target\n    mode=\"quick\",  # setting up project run mode, leaving in auto\n    worker_count=-1,  # assigning worker count, -1 will use all available\n    max_wait=24\n    * 60\n    * 60,  # time series projects can take a little more time to get through EDA2 than normal AutoML projects\n)",
        "# This (optional) will block execution of the notebook until the full autopilot process has completed. This can take several minutes or hours, depending on the autopilot mode selected, the size of the dataset, and the type of problem we're trying to solve.\n\nproject.wait_for_autopilot()",
        "# This output a list of all the models trained in the project, sorted by the selected validation metric.\n\nproject.get_models()[:10]",
        "# Get the most accurate model based on the project's metric\nmost_accurate_model = project.get_top_model()",
        "# Evaluate the model using a lift chart\nplot = most_accurate_model.get_lift_chart(\"crossValidation\")\ndf = pd.DataFrame.from_dict(plot.bins)\n\n# Create data\nx = df.index\ny = df[\"actual\"]\ny2 = df[\"predicted\"]\n\n# Plot lines\nplt.plot(x, y, label=\"actuals\")\nplt.plot(x, y2, label=\"predicted\")\nplt.legend()\nplt.title(\"lift chart\")\nplt.show()",
        "# Evaluate the model using ROC Curve\n\n# Get the ROC curve\nroc = most_accurate_model.get_roc_curve(source=CHART_DATA_SOURCE.VALIDATION)\n\n# Save the result into a Pandas dataframe\nroc_df = pd.DataFrame(roc.roc_points)\nroc_df.head()\n\n# Set chart colors\ndr_roc_green = \"#03c75f\"\nwhite = \"#ffffff\"\ndr_purple = \"#65147D\"\ndr_dense_green = \"#018f4f\"\ndr_dark_blue = \"#08233F\"\n\n# Create chart\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=dr_dark_blue)\nplt.scatter(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot(roc_df.false_positive_rate, roc_df.true_positive_rate, color=dr_roc_green)\nplt.plot([0, 1], [0, 1], color=white, alpha=0.25)\n\n# Format chart\nplt.title(\"ROC curve\")\nplt.xlabel(\"False Positive Rate (Fallout)\")\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate (Sensitivity)\")\nplt.ylim([0, 1])",
        "# Get insights on which feature driving model's outcome\n\n# Get Feature Impact\nfeature_impact = most_accurate_model.get_or_request_feature_impact()\n\n# Save Feature Impact in a Pandas dataframe\nfi_df = pd.DataFrame(feature_impact)\n\n# Plot the top five most impactful features\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.barplot(x=\"featureName\", y=\"impactNormalized\", data=fi_df[0:5], color=\"g\")",
        "# Get the deployed model and change the ID to your deployment's\ndeployment = Deployment.get(\"6413a07ea0a9ee22d203464b\")\ndeployment",
        "# Create the scoring data\n# This workflow uses an example for subset data\nquery = (\n    \" create or replace table FRAUD_DETECTION_SCORING as \"\n    + \"  select * from FRAUD_DETECTION_TRAINING limit 10 \"\n)\n\nsession.sql(query).collect()",
        "# Score new data\n\nquery = (\n    \" CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as\"\n    + \" SELECT *, datarobot_deployment_642adf008b22ed8d5de483ba(OBJECT_CONSTRUCT_KEEP_NULL(*)) as model_score FROM FRAUD_DETECTION_SCORING;\"\n)\n\n\nsession.sql(query).collect()",
        "# Store results and modify column names\n\nquery = ' CREATE OR REPLACE TABLE FRAUD_DETECTION_SCORING as select *, model_score:\"0\"::float false_fraud, model_score:\"1\"::float true_fraud  from FRAUD_DETECTION_SCORING; '\n\nsession.sql(query).collect()",
        "# Convert the session object to a dataframe\n\ndf_scored = session.table(\"FRAUD_DETECTION_SCORING\")\ndf_scored_pd = df_scored.toPandas()\ndf_scored_pd",
        "# Get target drift\n\ntarget_drift = deployment.get_target_drift(\n    start_time=datetime(2023, 4, 1, hour=15), end_time=datetime(2023, 4, 4, hour=15)\n)\ntarget_drift.drift_score"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "project.create",
        "project.get_top_model",
        "project.wait_for_autopilot",
        "deployment.get",
        "model.get_roc_curve",
        "deployment.get_target_drift",
        "model.get_lift_chart",
        "datarobot.models.project",
        "project.get",
        "datarobot.models.deployment",
        "datarobot.models.prediction_server",
        "project.analyze_and_model",
        "project.get_models"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_586858157610840341",
      "title": "Snowflake_template: Snowflake - End-to-end Ecommerce Churn.config.yaml",
      "content": "---\ndescription: \"This notebook focuses on working with Snowflake as a data source and walks through how to\\\n  \\ use the Python API client to connect to DataRobot, import data from Snowflake into the AI Catalog,\\\n  \\ and create a project and run Autopilot.\\nNext, this notebook walks through how to select and evaluate\\\n  \\ the top recommended model from a project, deploy a model in a Docker container with a REST API endpoint,\\\n  \\ and orchestrate schedule predictions with an MLOps job definition.\"\nfile_name: Snowflake - End-to-end Ecommerce Churn.ipynb\nlanguages:\n  - python\nauthors:\n  - Austin Chou\n  - Arjun Arora\nauthors_email:\n  - austin.chou@datarobot.com\n  - arjun.arora@datarobot.com\nsmoke_test:\n  run_smoke_test: false\nmaintainers:\n  - Austin Chou\nmaintainers_email:\n  - austin.chou@datarobot.com\ntags: []\ntitle: End-to-end modeling and production workflow with DataRobot and Snowflake\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Snowflake_template/Snowflake - End-to-end Ecommerce Churn.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Snowflake_template",
        "file_type": "yaml",
        "size": 899
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.85,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_7593584042684844248",
      "title": "Snowflake_template: Snowflake - End-to-end Ecommerce Churn.ipynb",
      "content": "# End-to-end modeling and production workflow with DataRobot and Snowflake\n\nAuthors: Austin Chou, Arjun Arora\n\nVersion Date: 01/11/2023\n\n[Reference DataRobot's API documentation](https://docs.datarobot.com/en/docs/api/reference/index.html)\n\n![DR_and_Snowflake_logo.png](attachment:DR_and_Snowflake_logo.png)\n\n## Notebook overview\n\nThis notebook focuses on working with **Snowflake** as a data source and walks through how to use the Python API client to:\n\n1. [Connect to DataRobot](#Setup)\n2. [Import data from Snowflake into the AI catalog](#Import)\n3. [Create a project and run Autopilot](#Project)\n4. [Select and evaluate the top recommended model from a project)](#Eval)\n5. [Deploy a model in a Docker container with a REST API endpoint](#Deploy)\n6. [Orchestrate scheduled predictions with an MLOps job definition](#Schedule)\n\n### Libraries used in this notebook\n\n* The [**DataRobot Python client**](https://pypi.org/project/datarobot/).\n* [**Snowflake Python Connector**](https://docs.snowflake.com/en/user-guide/python-connector.html) if pulling the demo dataset to follow along with this notebook.\n* **Matplotlib** for model evaluation.\n* **Pandas** for model evaluation.\n\n#### Optional:  Import public demo data\n\nFor this walkthrough, you can use a publicly available dataset (`DR_Demo_E-commerce_churn_train.csv`) from DataRobot's S3 bucket to create a table in your Snowflake instance. This will let you run the cells in this notebook and follow along.\n\nYou will need to update the fields below with your Snowflake information. You will also need the following files found in the same repo as this notebook:\n\n* utils.py\n* datasets.yaml\n\nOnce you are done with this walkthrough, remember to delete the data from your Snowflake instance.\n\n**If you have already established a Snowflake data connection in DataRobot, make sure the following information matches your credentials and data connection details.**\n\n```python\nfrom utils import prepare_demo_tables_in_db\n```\n\n```python\n# Fill out the credentials for your Snowflake instance. You will need write access to a database\ndb_user = \"your_username\"  # Username to access Snowflake database\ndb_password = \"your_password\"  # Password\naccount = \"eg:datarobotap_partner.ap-southeast-2\"  # Snowflake account identifier, can be found in the db_url\ndb = \"YOUR_DB_NAME\"  # Database to Write_To\nwarehouse = \"YOUR_WAREHOUSE\"  # Warehouse\nschema = \"YOUR_SCHEMA\"  # Schema\n\ndb_url = \"jdbc:snowflake://{account}.snowflakecomputing.com/?warehouse={warehouse}&db={db}\".format(\n    account=account, db=db, warehouse=warehouse\n)\n```\n\n```python\n# Use the util function to pull the data from DataRobot's public S3 and import into your Snowflake instance\nreponse = prepare_demo_tables_in_db(\n    db_user=db_user,\n    db_password=db_password,\n    account=account,\n    db=db,\n    warehouse=warehouse,\n    schema=schema,\n)\n```\n\n## Setup <a id='Setup'></a>\n\n### Import libraries\n\n```python\nimport datarobot as dr\n\n# The following are libraries used in this notebook during model evaluation\nimport matplotlib.pyplot as plt\nimport pandas as pd\n```\n\n### Connect to DataRobot\n\n**To connect to DataRobot,** you need to provide your **API Token** and the **endpoint**. For more information, please refer to the following documentation:\n\n* [**Create/Manage API keys via Developer tools in the GUI**](https://docs.datarobot.com/en/docs/platform/account-mgmt/acct-settings/api-key-mgmt.html#api-key-management)\n* [**Different options to connect to DataRobot from the API client**](https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html)\n\n```python\n# API Token\ntoken = open(\n    \"..\\\\..\\\\API token.txt\"\n).read()  # Load the API token from a .txt file for ease of repeatable access\nDATAROBOT_API_TOKEN = (\n    token  # You can also find the API token under the Developer Tools in the UI\n)\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\n# To connect from Jupyter notebook:\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-SNF-17\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client\n```\n\n## Import data from Snowflake to DataRobot AI Catalog  <a id='Import'></a>\n\nYou can upload data to Datarobot in a variety of ways:\n\n* Data Sources (e.g., through a Snowflake data connection)\n* Local Files\n* Links\n\nIn this notebook, you will upload a dataset from a **Snowflake** data source. To do so:\n\n1. Retrieve the Snowflake data connection and credentials.\n2. Create a new data source (i.e., setting up the query to a Snowflake data table) if needed.\n3. Import the dataset into the AI Catalog.\n\n### Retrieve credentials\n\nYou will need to provide your Snowflake credentials to DataRobot to ingest the data from Snowflake to the AI Catalog.\n\nYou can easily [create and configure your credentials in the DataRobot UI](https://docs.datarobot.com/en/docs/data/connect-data/stored-creds.html). DataRobot will store all your credentials in the Credentials Management tab under your profile on the platform.\n\nYou can also create a new set of credentials by using the `dr.Credential.create_basic` function:\n\n```python\n# If you need to create the credentials, run this cell\n# If you already have Snowflake credentials in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_user and db_password from the Setup section\n\ncred = dr.Credential.create_basic(\n    name=\"[AIA] Your_Snowflake_Credentials\",  # Rename\n    user=db_user,\n    password=db_password,\n)\n\ncredential_id = cred.credential_id\n```\n\nIf you have already created credentials on DataRobot, you can retrieve them via the API. This notebook uses the credentials named \"Snowflake Sandbox Demo Connection\" - **replace it with the name you've given your Snowflake credentials in DataRobot's Credentials Management.**\n\n```python\n# # If you already have Snowflake credentials in DataRobot, you can retrieve and reuse the same credentials\n# # Uncomment and run this cell instead of creating new credentials\n\n# CREDS_NAME = 'Snowflake Sandbox Demo Connection' # Replace with your credentials name given during Snowflake credentials creation\n\n# credential_id = [cr.credential_id for cr in dr.Credential.list() if cr.name == CREDS_NAME][0]\n\n# # You can verify that you have the right credential by showing the ID and name\n# dr.Credential.get(credential_id)\n```\n\n### Retrieve a Snowflake data connection\n\nYou will also need to create a data connection from DataRobot to Snowflake. This will allow DataRobot to pull data directly from your Snowflake database.\n\nYou can easily [create a new data connection in the DataRobot UI](https://docs.datarobot.com/en/docs/data/connect-data/data-conn.html). You can find all your established data connections in the **Data Connections** tab under your profile on the platform.\n\n\nYou can also create a new data connection by finding the Snowflake driver and using the `dr.DataStore.create` function via the API.\n\n```python\n# Find a Snowflake driver ID by name. This can be skipped if you have the ID - code shown here for completeness\nfor d in dr.DataDriver.list():\n    if d.canonical_name in \"Snowflake (3.13.9 - recommended)\":\n        print((d.id, d.canonical_name))\n```\n\n```python\n# Create a datastore\n# If you already have the Snowflake data connection to the right database in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_url from the \"Setup\" section\n\ndata_store = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=\"[AIA] Your New Snowflake Data Connection\",  # Rename\n    driver_id=\"626bae0a98b54f9ba70b4122\",  # Snowflake driver id\n    jdbc_url=db_url,\n)\n\n# Test new data connection; access with your Snowflake credentials\ndata_store.test(username=db_user, password=db_password)\n\ndata_store_id = data_store.id\n```\n\nIf you have already created the data connection on DataRobot, you can retrieve it via API. This notebook uses a data connection named \"Snowflake - TEST_DB\" - **please replace with the name you've given your appropriate Snowflake data connection in DataRobot's Data Connections tab.**\n\n```python\n# # If you already have a Snowflake data connection to your Snowflake database in DataRobot, you can\n# # retrieve and reuse the same connection. Uncomment and run this cell instead of creating a new connection.\n\n# # Once the connection is established via the UI, you can retrieve your data connection by name\n# DATA_STORE_NAME = 'Snowflake - TEST_DB' # Replace with your Data Store name given during Snowflake data connection creation\n\n# data_store_id = [ds.id for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][0]\n\n# # You can verify that you have the right DataStore by name\n# dr.DataStore.get(data_store_id)\n```\n\nNext, create a data source specifying the query that you will use to pull data from the Snowflake data connection and into the DataRobot AI Catalog. For more information, please refer to the [**DataRobot documentation on data sources**](https://docs.datarobot.com/en/docs/data/connect-data/data-conn.html#add-data-sources).\n\nIf you already set up an existing data source, you can search for it by name.\n\n```python\n# Pick the data source\n# If you had already established a data source, just pick the existing data source\n# Otherwise, create a new data source\n\nDATA_SOURCE_NAME = \"[AIA] Snowflake ECommerce Churn\"  # Rename\nquery_train = (\n    \"SELECT * FROM \" + schema + '.\"DR_Demo_E-commerce_churn_train\";'\n)  # Edit this query to pull the appropriate table\n\ndata_sources = [\n    ds for ds in dr.DataSource.list() if ds.canonical_name == DATA_SOURCE_NAME\n]\n\nif len(data_sources) > 0:\n    data_source_train = data_sources[0].id\n    print(\"Existing data source ID:\", data_source_train)\nelse:\n    query_train = query_train\n    ds_params = dr.DataSourceParameters(data_store_id=data_store_id, query=query_train)\n    data_source_train = dr.DataSource.create(\n        data_source_type=\"jdbc\", canonical_name=DATA_SOURCE_NAME, params=ds_params\n    ).id\n    print(\"New data source created (ID):\", data_source_train)\n```\n\n### Import dataset to AI Catalog\n\nTo **upload a dataset to AI Catalog**, use the `dr.create_from_` family of functions. In this case, we use `create_from_data_source`.\n\n```python\nnew_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=data_source_train, do_snapshot=True, credential_id=credential_id\n)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Ecommerce Customer Churn Data\")\n```\n\nDatasets in the AI Catalog are assigned a **dataset ID** which you can use to reference/get the dataset via the API.\n\n```python\n# Quick link to the AI Catalog dataset you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\"AI Catalog URL: \" + \"https://app.datarobot.com/ai-catalog/\" + new_dataset.id)\nprint(\"Dataset ID: \" + new_dataset.id)\n```\n\n## Create a project and run Autopilot <a id='Project'></a>\n\nYou can **create DataRobot projects** directly from:\n\n* A dataset in AI Catalog (using the dataset's ID in DataRobot)\n* A pandas dataframe (you do not need to write back to a data source or disk)\n* Directly from data sources\n\nOnce a project is created, you can pass in DataRobot options and start modeling. At a minimum, you need to provide the prediction **Target**. With just two function calls (`create_from_` and `analyze_and_model`), you can go from data to optimizing 10 to 100 models with data science best practices.\n\nYou can actively watch the project in action in the DataRobot UI once Autopilot is triggered - the API and UI are parallel gateways to working on the same project which allows for **cross-functional collaboration.**\n\nNote that each created project is associated with a unique **project ID**. You can use the project ID to retrieve the project of interest via the API.\n\n### Create a project\n\nCreate a DataRobot project by uploading the dataset.\n\n```python\nproject = dr.Project.create_from_dataset(\n    dataset_id=new_dataset.id, project_name=\"[AIA] Ecommerce Churn Project\"\n)\n```\n\n```python\n# Quick link to the DataRobot project you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Project URL: \"\n    + \"https://app.datarobot.com/projects/\"\n    + project.id\n    + \"/eda\"\n)\nprint(\"Project ID: \" + project.id)\n```\n\n### Initiate Autopilot\n\nTo start the Autopilot process, call the `analyze_and_model` function. Provide the prediction target as part of the function call and use the default \"Quick\" Autopilot mode.\n\n```python\nproject.analyze_and_model(\n    target=\"Is_dormant\",\n    worker_count=-1,  # Setting worker count to -1 will use your maximum available workers\n)\n```\n\n```python\n# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)\n```\n\nWhile out of scope for this notebook, you can set advanced options to further configure Autopilot with the `set_advanced_options` function before triggering `analyze_and_model`. For visibility, you can view the advanced options used by the current project (which would be the default options):\n\n```python\n# View advanced options used by Autopilot for the project\nproject = dr.Project.get(project.id)  # Update the project object after Autopilot\n\nprint(\"Advanced Settings used:\")\nproject.list_advanced_options()\n```\n\n## Select and evaluate the top recommended model <a id='Eval'></a>\n\nAfter Autopilot completes, you can evaluate the results. Evaluation can include compiling the Leaderboard as a dataframe, measuring performances across different partitions with different metrics, visualizing the ROC curve, analyzing Feature Impact and Feature Effects to understand each models' behaviors, and more. This can be done for every single model created by DataRobot.\n\nAs a simple example in this notebook, **select the recommended model created by Autopilot and evaluate**:\n\n* LogLoss performance\n* AUC performance\n* ROC curve\n* Feature Impact of Top 10 features\n\nNote: The recommended model is the best performing model that DataRobot identifies during Autopilot and then effectively retrains the blueprint on 100% of the data. For more information, refer to the [Model recommendation process documentation](https://docs.datarobot.com/en/docs/modeling/reference/model-detail/model-rec-process.html#model-recommendation-process).\n\n```python\n# Select the model recommended by DataRobot AutoML\ntop_model = project.recommended_model()\n```\n\n```python\n# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)\n```\n\n### Model performance by metric\n\n```python\nprint(\n    \"Top Model LogLoss performance: \"\n    + str(top_model.metrics[\"LogLoss\"][\"crossValidation\"])\n)\nprint(\"Top Model AUC performance: \" + str(top_model.metrics[\"AUC\"][\"crossValidation\"]))\n```\n\n### Create an ROC Curve\n\nBeyond the Leaderboard, you can access any analysis DataRobot does out-of-the-box for every single model. The example below reproduces the ROC curve by calling the `get_roc_curve` function from the top model.\n\n```python\n# Retrieve ROC Curve\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)\n```\n\n```python\n# Plot the ROC Curve\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.plot(roc.false_positive_rate, roc.true_positive_rate, color=\"b\")\nplt.plot([0, 1], [0, 1], color=\"r\", linestyle=\"dashed\", alpha=0.25)\nplt.title(\"ROC curve\", size=16, fontweight=\"bold\")\nplt.xlabel(\"False Positive Rate\", size=14)\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate\", size=14)\nplt.ylim([0, 1])\nplt.show()\n```\n\n### Feature Impact\n\nAs an example of model explainability, we calculate the Feature Impact values of the model using the `get_or_request_feature_impact` function.\n\n```python\n# Retrieve Feature Impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)\n```\n\n```python\n# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()\n```\n\n## Deploy a model with monitoring in MLOps <a id='Deploy'></a>\n\nWith a single function call or click on the UI, DataRobot can quickly deploy models into production while fully reproducing the entire modeling pipeline including the necessary data preprocessing steps utilized by the blueprints and any advanced feature engineering that are part of the project (such as in [Feature Discovery projects](https://www.datarobot.com/platform/feature-discovery/)). Once deployed, you can call DataRobot's REST or Python API to make batch and real-time predictions. You can also configure and schedule recurring batch prediction jobs that write back into a database.\n\nOnce a model is deployed, you can access MLOps monitoring capabilities such as:\n\n- Service health\n- Data drift\n- Prediction accuracy\n- Model retraining\n\nTo **deploy a model from the Leaderboard**, call the `create_from_learning_model` function and provide the **ID of the model** you want to deploy and the **ID of the prediction server** you want to deploy into.\n\nFor additional information, please see documentation for:\n\n1. [**MLOps monitoring**](https://docs.datarobot.com/en/docs/mlops/mlops-overview.html) \n2. [**Available prediction methods**](https://docs.datarobot.com/en/docs/predictions/index.html)\n3. [**Other deployment workflows with DataRobot**](https://docs.datarobot.com/en/docs/mlops/deployment/deploy-workflows/index.html)\n\n```python\n# Prepare new deployment\ndeployment_model_id = top_model.id\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    deployment_model_id,\n    label=\"[AIA] Customer Churn Deployment\",\n    description=\"[AIA] Customer Churn Prediction\",\n    default_prediction_server_id=prediction_server.id,\n)\n```\n\nEvery deployment in DataRobot is assigned a **deployment ID** which you can use to reference/retrieve the deployment via the API.\n\n```python\n# Quick link to the deployment you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Deployment URL: \"\n    + \"https://app.datarobot.com/deployments/\"\n    + deployment.id\n    + \"/overview\"\n)\nprint(\"Deployment ID: \" + deployment.id)\n```\n\n### Configure model monitoring\n\nIn this example, you can set up the deployment to monitor data drift and accuracy using the following functions:\n\n* Data drift: `update_drift_tracking_settings`\n* Accuracy: `update_association_id_settings`\n\n```python\n# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)\n```\n\n```python\n# Turn on Accuracy tracking by providing an association ID\ndeployment.update_association_id_settings(\n    column_names=[\n        \"customerID\"\n    ],  # Association ID to associate prediction results to actuals for accuracy monitoring\n    required_in_prediction_requests=True,\n)\n```\n\n## Orchestrate scheduled prediction jobs <a id='Schedule'></a>\n\nYou can **schedule a recurring batch prediction job** that looks up data from a data source (e.g.,  **Snowflake**), score predictions, and write back the results to a data table in a data source (e.g., **Snowflake**).\n\nSimilar to [**working in the UI**](https://docs.datarobot.com/en/docs/predictions/batch/batch-dep/batch-pred-jobs.html), you need to provide settings to set up a prediction job. In a [**code-first approach**](https://datarobot-public-api-client.readthedocs-hosted.com/en/v3.0.2/reference/predictions/batch_predictions.html#batch-prediction-job-definitions-1), this is done by passing dictionaries containing settings for:\n\n* Intake Settings (input data table or data source)\n* Output Settings (output data table or data source)\n* Schedule\n\nYou also need to provide the intake and output with IDs for:\n\n* Data Store (i.e., data connection as set up in DataRobot)\n* Credentials for the data store (as set up in DataRobot)\n\n```python\n# Select the data store and credentials needed\n# This notebook uses the same Snowflake data store and credentials as the ones used\n# at the start of this notebook\nwriteback_data_store_id = data_store_id\nwriteback_cred_id = credential_id\n\n# Set the Snowflake catalog (database) and schema for intake and output\n# This notebook uses the same Snowflake database and schema as the ones\n# at the start of this notebook\nintake_catalog = db\nintake_schema = schema\noutput_catalog = db\noutput_schema = schema\n```\n\n```python\n# Set up settings dictionaries - edit this to your data and prediction pipeline\n\n# Intake settings\n# This example uses the training dataset as if it was also used for scoring\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"DR_Demo_E-commerce_churn_train\",  # Data table in data source\n    \"schema\": intake_schema,\n    \"catalog\": intake_catalog,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Output settings\noutput_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"E-commerce_churn_prediction_demo_output\",  # Rename; Data table to write out to\n    \"schema\": output_schema,\n    \"catalog\": output_catalog,\n    \"statement_type\": \"insert\",\n    \"create_table_if_not_exists\": True,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Schedule settings\nschedule = {\n    \"minute\": [59],\n    \"hour\": [7],\n    \"month\": [\"*\"],\n    \"dayOfWeek\": [\"*\"],\n    \"dayOfMonth\": [1],\n}\n```\n\n```python\n# Set up the job dictionary\n# Includes intake and output settings within the job dictionary\njob = {\n    \"deployment_id\": deployment.id,\n    \"num_concurrent\": 4,\n    \"intake_settings\": intake_settings,\n    \"output_settings\": output_settings,\n    \"passthroughColumnsSet\": \"all\",\n}\n\n# Create the Batch Prediction job definition\njob_definition = dr.BatchPredictionJobDefinition.create(\n    enabled=True,\n    batch_prediction_job=job,\n    name=\"[AIA] ECommerce Churn - Monthly Prediction Job JDBC\",  # Make sure you use a unique name from other existing JobDefinitions in your organization\n    schedule=schedule,\n)\n```\n\nThe job definition should now be listed in the deployment's **Job Definitions** tab.\n\nAs a test, you can trigger the job manually to see DataRobot MLOps in action. **The next snippet will manually trigger the Job Definition to run.**\n\n```python\n# Trigger the job manually\njob_definition.run_once()\n```\n\nWhen the job completes successfully, you will find the output written to the corresponding Snowflake table. The MLOps deployment will also update accordingly and you should see service health and data drift metrics update.\n\n## Clean up\n\nTo remove everything added in the DataRobot platform as part of this notebook, run the following cell. If you want to delete any tables created in Snowflake as a result of running this notebook, you will need to manually do so in Snowflake.\n\n```python\n# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# job_definition.delete()\n# deployment.delete()\n# project.delete()\n# new_dataset.delete(new_dataset.id)\n# dr.DataSource.get(data_source_train).delete()\n# data_store.delete()\n# cred.delete()\n```\n\n## Additional Resources\n\n* Check the [**DataRobot Community AI Accelerator page**](https://community.datarobot.com/t5/ai-accelerators/bd-p/ai_accelerators) for similar walkthroughs using DataRobot with other data sources (e.g. AWS, GCP, Azure, etc).\n* Check the [**DataRobot API user guide for other code examples**](https://docs.datarobot.com/en/docs/api/guide/python/index.html) covering various topics such as model factories, feature impact rank ensembling, and more.",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Snowflake_template/Snowflake - End-to-end Ecommerce Churn.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "openai",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Snowflake_template",
        "file_type": "notebook",
        "size": 127058
      },
      "code_examples": [
        "from utils import prepare_demo_tables_in_db",
        "# Fill out the credentials for your Snowflake instance. You will need write access to a database\ndb_user = \"your_username\"  # Username to access Snowflake database\ndb_password = \"your_password\"  # Password\naccount = \"eg:datarobotap_partner.ap-southeast-2\"  # Snowflake account identifier, can be found in the db_url\ndb = \"YOUR_DB_NAME\"  # Database to Write_To\nwarehouse = \"YOUR_WAREHOUSE\"  # Warehouse\nschema = \"YOUR_SCHEMA\"  # Schema\n\ndb_url = \"jdbc:snowflake://{account}.snowflakecomputing.com/?warehouse={warehouse}&db={db}\".format(\n    account=account, db=db, warehouse=warehouse\n)",
        "# Use the util function to pull the data from DataRobot's public S3 and import into your Snowflake instance\nreponse = prepare_demo_tables_in_db(\n    db_user=db_user,\n    db_password=db_password,\n    account=account,\n    db=db,\n    warehouse=warehouse,\n    schema=schema,\n)",
        "import datarobot as dr\n\n# The following are libraries used in this notebook during model evaluation\nimport matplotlib.pyplot as plt\nimport pandas as pd",
        "# API Token\ntoken = open(\n    \"..\\\\..\\\\API token.txt\"\n).read()  # Load the API token from a .txt file for ease of repeatable access\nDATAROBOT_API_TOKEN = (\n    token  # You can also find the API token under the Developer Tools in the UI\n)\n\n# Endpoint - This notebook uses the default endpoint for DataRobot Managed AI Cloud (US)\nDATAROBOT_ENDPOINT = \"https://app.datarobot.com/api/v2\"\n\n# To connect from Jupyter notebook:\nclient = dr.Client(\n    token=DATAROBOT_API_TOKEN,\n    endpoint=DATAROBOT_ENDPOINT,\n    user_agent_suffix=\"AIA-E2E-SNF-17\",  # Optional but helps DataRobot improve this workflow\n)\n\ndr.client._global_client = client",
        "# If you need to create the credentials, run this cell\n# If you already have Snowflake credentials in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_user and db_password from the Setup section\n\ncred = dr.Credential.create_basic(\n    name=\"[AIA] Your_Snowflake_Credentials\",  # Rename\n    user=db_user,\n    password=db_password,\n)\n\ncredential_id = cred.credential_id",
        "# # If you already have Snowflake credentials in DataRobot, you can retrieve and reuse the same credentials\n# # Uncomment and run this cell instead of creating new credentials\n\n# CREDS_NAME = 'Snowflake Sandbox Demo Connection' # Replace with your credentials name given during Snowflake credentials creation\n\n# credential_id = [cr.credential_id for cr in dr.Credential.list() if cr.name == CREDS_NAME][0]\n\n# # You can verify that you have the right credential by showing the ID and name\n# dr.Credential.get(credential_id)",
        "# Find a Snowflake driver ID by name. This can be skipped if you have the ID - code shown here for completeness\nfor d in dr.DataDriver.list():\n    if d.canonical_name in \"Snowflake (3.13.9 - recommended)\":\n        print((d.id, d.canonical_name))",
        "# Create a datastore\n# If you already have the Snowflake data connection to the right database in DataRobot, DO NOT RUN THIS CELL\n# Instead, run the next cell (needs to be uncommented) to retrieve your existing credentials\n\n# Use the db_url from the \"Setup\" section\n\ndata_store = dr.DataStore.create(\n    data_store_type=\"jdbc\",\n    canonical_name=\"[AIA] Your New Snowflake Data Connection\",  # Rename\n    driver_id=\"626bae0a98b54f9ba70b4122\",  # Snowflake driver id\n    jdbc_url=db_url,\n)\n\n# Test new data connection; access with your Snowflake credentials\ndata_store.test(username=db_user, password=db_password)\n\ndata_store_id = data_store.id",
        "# # If you already have a Snowflake data connection to your Snowflake database in DataRobot, you can\n# # retrieve and reuse the same connection. Uncomment and run this cell instead of creating a new connection.\n\n# # Once the connection is established via the UI, you can retrieve your data connection by name\n# DATA_STORE_NAME = 'Snowflake - TEST_DB' # Replace with your Data Store name given during Snowflake data connection creation\n\n# data_store_id = [ds.id for ds in dr.DataStore.list() if ds.canonical_name == DATA_STORE_NAME][0]\n\n# # You can verify that you have the right DataStore by name\n# dr.DataStore.get(data_store_id)",
        "# Pick the data source\n# If you had already established a data source, just pick the existing data source\n# Otherwise, create a new data source\n\nDATA_SOURCE_NAME = \"[AIA] Snowflake ECommerce Churn\"  # Rename\nquery_train = (\n    \"SELECT * FROM \" + schema + '.\"DR_Demo_E-commerce_churn_train\";'\n)  # Edit this query to pull the appropriate table\n\ndata_sources = [\n    ds for ds in dr.DataSource.list() if ds.canonical_name == DATA_SOURCE_NAME\n]\n\nif len(data_sources) > 0:\n    data_source_train = data_sources[0].id\n    print(\"Existing data source ID:\", data_source_train)\nelse:\n    query_train = query_train\n    ds_params = dr.DataSourceParameters(data_store_id=data_store_id, query=query_train)\n    data_source_train = dr.DataSource.create(\n        data_source_type=\"jdbc\", canonical_name=DATA_SOURCE_NAME, params=ds_params\n    ).id\n    print(\"New data source created (ID):\", data_source_train)",
        "new_dataset = dr.Dataset.create_from_data_source(\n    data_source_id=data_source_train, do_snapshot=True, credential_id=credential_id\n)\n\n# Update the dataset name in the AI Catalog\nnew_dataset.modify(name=\"[AIA] Ecommerce Customer Churn Data\")",
        "# Quick link to the AI Catalog dataset you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\"AI Catalog URL: \" + \"https://app.datarobot.com/ai-catalog/\" + new_dataset.id)\nprint(\"Dataset ID: \" + new_dataset.id)",
        "project = dr.Project.create_from_dataset(\n    dataset_id=new_dataset.id, project_name=\"[AIA] Ecommerce Churn Project\"\n)",
        "# Quick link to the DataRobot project you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Project URL: \"\n    + \"https://app.datarobot.com/projects/\"\n    + project.id\n    + \"/eda\"\n)\nprint(\"Project ID: \" + project.id)",
        "project.analyze_and_model(\n    target=\"Is_dormant\",\n    worker_count=-1,  # Setting worker count to -1 will use your maximum available workers\n)",
        "# If you want to wait for Autopilot to finish, run this code\n# You can set verbosity to 1 if you want to print progress updates as Autopilot runs\nproject.wait_for_autopilot(verbosity=0)",
        "# View advanced options used by Autopilot for the project\nproject = dr.Project.get(project.id)  # Update the project object after Autopilot\n\nprint(\"Advanced Settings used:\")\nproject.list_advanced_options()",
        "# Select the model recommended by DataRobot AutoML\ntop_model = project.recommended_model()",
        "# Quick link to the recommended model built by Autopilot\nprint(\"Top Model URL: \" + top_model.get_uri())\nprint(\"Top Model Type: \" + top_model.model_type)",
        "print(\n    \"Top Model LogLoss performance: \"\n    + str(top_model.metrics[\"LogLoss\"][\"crossValidation\"])\n)\nprint(\"Top Model AUC performance: \" + str(top_model.metrics[\"AUC\"][\"crossValidation\"]))",
        "# Retrieve ROC Curve\nroc_object = top_model.get_roc_curve(source=\"crossValidation\")\nroc = pd.DataFrame(roc_object.roc_points)",
        "# Plot the ROC Curve\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.plot(roc.false_positive_rate, roc.true_positive_rate, color=\"b\")\nplt.plot([0, 1], [0, 1], color=\"r\", linestyle=\"dashed\", alpha=0.25)\nplt.title(\"ROC curve\", size=16, fontweight=\"bold\")\nplt.xlabel(\"False Positive Rate\", size=14)\nplt.xlim([0, 1])\nplt.ylabel(\"True Positive Rate\", size=14)\nplt.ylim([0, 1])\nplt.show()",
        "# Retrieve Feature Impact\nfeature_impacts = (\n    top_model.get_or_request_feature_impact()\n)  # Will trigger Feature Impact calculations if not done.\nFI_df = pd.DataFrame(feature_impacts)  # Convert to dataframe\n\n# Sort features by Normalized Feature Impact\nFI_df = FI_df.sort_values(by=\"impactNormalized\", ascending=False)",
        "# Plot Feature Impact\nFI_df[\"X axis\"] = FI_df.index\n\n# Plot\nfig = plt.figure(figsize=(8, 8))\naxes = fig.add_subplot(1, 1, 1, facecolor=\"w\")\n\nplt.barh(FI_df.featureName, FI_df.impactNormalized)\naxes.invert_yaxis()\nplt.title(\"Feature Impact\", size=16, fontweight=\"bold\")\nplt.xlabel(\"Normalized Feature Impact\", size=14)\nplt.xlim([0, 1.1])\nplt.ylabel(\"Feature\", size=14)\nplt.show()",
        "# Prepare new deployment\ndeployment_model_id = top_model.id\nprediction_server = dr.PredictionServer.list()[0]\n\n# Create deployment\ndeployment = dr.Deployment.create_from_learning_model(\n    deployment_model_id,\n    label=\"[AIA] Customer Churn Deployment\",\n    description=\"[AIA] Customer Churn Prediction\",\n    default_prediction_server_id=prediction_server.id,\n)",
        "# Quick link to the deployment you just created\n# The base link assumes you are on the DataRobot Managed AI Cloud\nprint(\n    \"DataRobot Deployment URL: \"\n    + \"https://app.datarobot.com/deployments/\"\n    + deployment.id\n    + \"/overview\"\n)\nprint(\"Deployment ID: \" + deployment.id)",
        "# Turn on Data Drift tracking for features and the target\ndeployment.update_drift_tracking_settings(\n    target_drift_enabled=True, feature_drift_enabled=True\n)",
        "# Turn on Accuracy tracking by providing an association ID\ndeployment.update_association_id_settings(\n    column_names=[\n        \"customerID\"\n    ],  # Association ID to associate prediction results to actuals for accuracy monitoring\n    required_in_prediction_requests=True,\n)",
        "# Select the data store and credentials needed\n# This notebook uses the same Snowflake data store and credentials as the ones used\n# at the start of this notebook\nwriteback_data_store_id = data_store_id\nwriteback_cred_id = credential_id\n\n# Set the Snowflake catalog (database) and schema for intake and output\n# This notebook uses the same Snowflake database and schema as the ones\n# at the start of this notebook\nintake_catalog = db\nintake_schema = schema\noutput_catalog = db\noutput_schema = schema",
        "# Set up settings dictionaries - edit this to your data and prediction pipeline\n\n# Intake settings\n# This example uses the training dataset as if it was also used for scoring\nintake_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"DR_Demo_E-commerce_churn_train\",  # Data table in data source\n    \"schema\": intake_schema,\n    \"catalog\": intake_catalog,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Output settings\noutput_settings = {\n    \"type\": \"jdbc\",\n    \"table\": \"E-commerce_churn_prediction_demo_output\",  # Rename; Data table to write out to\n    \"schema\": output_schema,\n    \"catalog\": output_catalog,\n    \"statement_type\": \"insert\",\n    \"create_table_if_not_exists\": True,\n    \"data_store_id\": writeback_data_store_id,\n    \"credential_id\": writeback_cred_id,\n}\n\n# Schedule settings\nschedule = {\n    \"minute\": [59],\n    \"hour\": [7],\n    \"month\": [\"*\"],\n    \"dayOfWeek\": [\"*\"],\n    \"dayOfMonth\": [1],\n}",
        "# Set up the job dictionary\n# Includes intake and output settings within the job dictionary\njob = {\n    \"deployment_id\": deployment.id,\n    \"num_concurrent\": 4,\n    \"intake_settings\": intake_settings,\n    \"output_settings\": output_settings,\n    \"passthroughColumnsSet\": \"all\",\n}\n\n# Create the Batch Prediction job definition\njob_definition = dr.BatchPredictionJobDefinition.create(\n    enabled=True,\n    batch_prediction_job=job,\n    name=\"[AIA] ECommerce Churn - Monthly Prediction Job JDBC\",  # Make sure you use a unique name from other existing JobDefinitions in your organization\n    schedule=schedule,\n)",
        "# Trigger the job manually\njob_definition.run_once()",
        "# # CLEAN UP - Uncomment and run this cell to remove everything you added during this session\n\n# job_definition.delete()\n# deployment.delete()\n# project.delete()\n# new_dataset.delete(new_dataset.id)\n# dr.DataSource.get(data_source_train).delete()\n# data_store.delete()\n# cred.delete()"
      ],
      "api_methods": [
        "model.get_or_request_feature_impact",
        "dr.credential.get",
        "dr.datasource.list",
        "project.wait_for_autopilot",
        "model.metrics",
        "dr.datasource.create",
        "dr.datastore.create",
        "deployment.create_from_learning_model",
        "model.get_roc_curve",
        "deployment.update_drift_tracking_settings",
        "dr.client._global_client",
        "project.list_advanced_options",
        "dr.project.create_from_dataset",
        "dr.credential.list",
        "model.id",
        "dr.predictionserver.list",
        "dr.deployment.create_from_learning_model",
        "project.id",
        "project.get",
        "deployment.update_association_id_settings",
        "dr.dataset.create_from_data_source",
        "deployment.delete",
        "deployment.id",
        "dr.datadriver.list",
        "dr.datastore.list",
        "dr.batchpredictionjobdefinition.create",
        "project.delete",
        "model.get_uri",
        "dr.credential.create_basic",
        "project.recommended_model",
        "dr.project.get",
        "model.model_type",
        "project.analyze_and_model",
        "project.create_from_dataset",
        "dr.datasource.get",
        "dr.datastore.get"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-8889792402619034933",
      "title": "Snowflake_template: datasets.yaml",
      "content": "---\ndatasets:\n  dataset1:\n    table_name: DR_Demo_E-commerce_churn_train\n    url: https://s3.amazonaws.com/datarobot_public_datasets/ai_accelerators/DR_Demo_E-commerce_churn_train.csv\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Snowflake_template/datasets.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Snowflake_template",
        "file_type": "yaml",
        "size": 184
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.15,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_496293582788157643",
      "title": "Snowflake_template: requirements.txt",
      "content": "datarobot==3.0.2\nmatplotlib==3.4.3\npandas==1.5.2\nsnowflake-connector-python==2.9.0\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Snowflake_template/requirements.txt",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Snowflake_template",
        "file_type": "text",
        "size": 83
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-4017511268363821972",
      "title": "Snowflake_template: utils.py",
      "content": "#!/usr/bin/env python3\n# -*- mode: python; python-indent-offset: 4 -*-\n\nfrom IPython.display import display\nimport pandas as pd\nimport snowflake\nfrom snowflake.connector.pandas_tools import write_pandas\nimport yaml\n\n# we can skip the import snowfalke step entirely https://stephenallwright.com/python-connector-write-pandas-snowflake/\n# this causes issues if they install snowflake connector and \"snowflake\" - https://stackoverflow.com/questions/74223900/snowflake-connector-python-package-not-recognized\n\n\ndef prepare_demo_tables_in_db(\n    db_user=None,  # username to access snowflake database\n    db_password=None,  # password\n    account=None,  # Snowflake Account Identifier can be found in the db_url\n    db=None,  # Database to Write_To\n    warehouse=None,  # Warehouse\n    schema=None,  # schema\n):\n    \"\"\"description: method to prepare demo table in snowflake database\n    reads from datasets.yaml\n\n    by: gongoraj, demidov91 and jpgomes\n        date: 12/22/2022\n    \"\"\"\n\n    with snowflake.connector.connect(\n        user=db_user,\n        password=db_password,\n        account=account,\n        warehouse=warehouse,\n        database=db,\n        schema=schema,\n    ) as con:\n        with open(\"datasets.yaml\") as f:\n            config = yaml.safe_load(f)\n            for key, value in config[\"datasets\"].items():\n                print(\"*\" * 30)\n                print(\"table:\", value[\"table_name\"])\n                try:\n                    df = pd.read_csv(value[\"url\"], encoding=\"utf8\")\n                except:\n                    df = pd.read_csv(value[\"url\"], encoding=\"cp850\")\n                display(df.head())\n                print(\"info for \", value[\"table_name\"])\n                print(df.info())\n                print(\n                    \"writing\", value[\"table_name\"], \"to snowflake from: \", value[\"url\"]\n                )\n                write_pandas(\n                    con, df, value[\"table_name\"], auto_create_table=True, overwrite=True\n                )\n                con.commit()\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/Snowflake_template/utils.py",
      "tags": [
        "templates",
        "integration",
        "ai-accelerators",
        "ecosystem",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "Snowflake_template",
        "file_type": "python",
        "size": 2009
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.15,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_7611887459714631313",
      "title": "scoring-code-as-microservice-w_docker: Application.config.yaml",
      "content": "---\ndescription: This AI Accelerator explains the step-by-step procedure to embed Scoring Code in a microservice\n  and prepare it as the Docker container for a deployment on customer infrastructure (it can be self-\n  or hyperscaler-managed K8s).\nfile_name: Application.java\nlanguages:\n  - java\nmaintainers:\n  - Pavel Ustinov\nmaintainers_email:\n  - pavel.ustinov@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: Deploy Scoring Code as a microservice\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/scoring-code-as-microservice-w_docker/Application.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "scoring-code-as-microservice-w_docker",
        "file_type": "yaml",
        "size": 466
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.4,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_379971739085711350",
      "title": "scoring-code-as-microservice-w_docker: README.md",
      "content": "# Deploy Scoring Code as a microservice\n\n**Author:** Pavel Ustinov\n\n**Date:** June 26th, 2023\n\n## Problem framing\n\n[Scoring Code](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/sc-overview.html) allows you to export DataRobot models as [JAR files](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/sc-jar-integrations.html#scoring-code-jar-integrations) that can be used outside of the platform. You can do so without the need to introduce an additional level of customization on top of the model, while respecting company compliance requirements. Alternatively, you can embed the model into a low-latency application. \n\nThere are multiple options on how to use exported Scoring Code for the inference: it can be used \n* On the [command line](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/scoring-cli.html) \n* Integrated with [Amazon SageMaker](https://docs.datarobot.com/en/docs/more-info/how-to/aws/sagemaker/sc-sagemaker.html)\n* Integrated with [Apache Spark](https://docs.datarobot.com/en/docs/more-info/how-to/sc-apache-spark.html)\n* Integrated with [Snowflake](https://docs.datarobot.com/en/docs/more-info/how-to/snowflake/snowflake-sc.html)\n* Embedded into an [existing Java project](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/quickstart-api.html#java-api-example)\n\nThis AI Accelerator explains the step-by-step procedure to embed Scoring Code in a microservice and prepare it as the Docker container for a deployment on customer infrastructure (it can be self- or hyperscaler-managed K8s). The [Micronaut framework](https://micronaut.io/download/) is used for the implementation as it is modular, efficient, and easy to use. Deployment is out of scope for this accelerator.\n\n## Accelerator overview\n\nThis accelerator requires:\n\n- The Maven project (the example is provided in this repository).\n- Scoring JAR of the model trained and exported from DataRobot.\n- A CSV file with scoring data for testing purposes.\n\nThe following steps outline the accelerator workflow.\n\n1. Download Scoring Code from the DataRobot [Leaderboard](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/sc-download-leaderboard.html) or from a [deployment](https://docs.datarobot.com/en/docs/predictions/port-pred/scoring-code/sc-download-deployment.html).\n\n2. Install the [Micronaut framework](https://micronaut.io/download/).\n\n3. [Create an app](https://guides.micronaut.io/latest/creating-your-first-micronaut-app-maven-java.html) using the following command: \n\n    `mn create-app com.datarobot.micronaut.scoring --build=maven --lang=java`\n\n4. Implement test and controller classes (see the attached Maven project that is ready to be used).\n\n5. [Generate Docker](https://guides.micronaut.io/latest/micronaut-docker-image-maven-java.html) or an [executable JAR file](https://guides.micronaut.io/latest/executable-jar-maven-java.html) with Micronaut.\n\n6. Create the folder that contains scoring file JAR downloaded in step 1.\n\n7. Run Docker container with the following command: \n\n    `docker run -it --rm -p 8080:8080 -e MODEL_FOLDER=home --name scoring-container -v /Users/pavel.ustinov/Documents/customers/micronaut-scoring/scoring/model:/home scoring`\n\nThe environment variable `MODEL_FOLDER` points to the directory inside the Docker container (`home` directory in this example) that contains the scoring JAR. If its value is not provided, the default directory `model` will be used.\n\nThe mounted directory `-v /Users/pavel.ustinov/Documents/customers/micronaut-scoring/scoring/model:/home` should referenced to the local directory (`/Users/pavel.ustinov/Documents/customers/micronaut-scoring/scoring/model` in this example) that contains the scoring JAR.\n\n8. Send data for inference using curl or Postman using the following command:\n\n    `curl --location 'http://localhost:8080/score' --header 'Content-Type: text/plain' --data '@/Users/pavel.ustinov/Downloads/for_predictions100.csv'`\n\n## Code structure\n\nThis is a classical Maven project with a `pom.xml` file generated by Micronaut in step 3.\n\nThere are five main files in the project: \n\n* `pom.xml`: a description of the Java project and its dependencies.\n* `Application.java`\n* `Predictions.java`\n* `ScoringController.java`\n* `ScoringControllerTest.java`\n\n### pom.xml\nThis file is a description of the Java project and its dependencies.\n\n### Application.java\nThis file is an entry point of the application.\n\n### Predictions.java\nThis file is a class used to create the object that contains predictions.\n\n### ScoringController.java\nThis file is a controller class that exposes the `/score` endpoint of the microservice. It contains 4 methods:\n\n    - `public Predictions score(@Body String body) throws Exception {...}`\n    This method reads the body of the incoming HTTP request, loads scoring JAR into the memory, and triggers scoring.\n    \n    - `private List<Map<String, Object>> readCsv(String body) {...}`\n    This method deserializes the HTTP body and saves it into internal data structure.\n    \n    - `private void loadModel() throws Exception {...}`\n    This method loads the JAR file with the model into RAM.\n    \n    - `private List<Double> makePredictions(List<Map<String, Object>> rowsToScore) {...}`\n    This method runs inference itself.\n\n### ScoringControllerTest.java\n\nThis file is a unit test that runs the local `HttpClient` in order to test the end-to-end process.\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/scoring-code-as-microservice-w_docker/README.md",
      "tags": [
        "integration",
        "ai-accelerators",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "scoring-code-as-microservice-w_docker",
        "file_type": "markdown",
        "size": 5439
      },
      "code_examples": [],
      "api_methods": [
        "deployment.html",
        "datarobot.micronaut.scoring"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-1888526219233504695",
      "title": "teams_datarobot: README.md",
      "content": "# Use DataRobot generative AI with Microsoft Teams\n\nMicrosoft Teams offers workspace chat and video conferencing, file storage, and application integration to organizations. Workspace chat feature allows you to interact with other users and bots in their day-to-day activities. This feature is useful for deploying Generative AI agents to improve employee productivity. With DataRobot's Generative AI offerings, organizations can deploy chatbots without the need for an additional front-end or consumption layers. \n\n## How it works\n\nMost messenger/communication apps support [bots](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/what-are-bots). A bot is a program that interacts with the users of the messenger application by ingesting the user message and providing responses. Bot can be static or dynamic depending on the logic encoded. A bot in most cases is a service exposing Rest Endpoints which recieve user text and respond back with text. Instead of developers starting from scratch, Microsoft provides an [SDK](https://learn.microsoft.com/en-us/azure/bot-service/index-bf-sdk?view=azure-bot-service-4.0) which can be used as building blocks or boilerplate. This SDK supports different languages including [python](https://github.com/microsoft/botbuilder-python). The code demonstrated here uses this [code](https://github.com/microsoft/BotBuilder-Samples/tree/main/samples/python/02.echo-bot) as boilerplate.\n\nNote: Python support is deprecated. A Typescript version of the code will be released.\n\n## Prerequisites\n\n1. **Developer Account:** Developers need access to the [Teams Developer Portal](https://dev.teams.microsoft.com/). This portal allows developers to register and configure apps and bots that will be visible in their organization's Teams Portal.\n\n2. **Hosting:** As mentioned before, the bot is an always on service which needs to be hosted somewhere. Microsoft provides different options like Power Virtual Agents and Azure AI Bot Service. Organizations can also host their bots if the bot architecture is complex.\n\n3. **HTTPS:** The bot's REST endpoints should be HTTPS secure. To enable data encryption Teams requires https endpoints and this doesn't include self-signed certificates.  \n\n## Bot backend\n\n1. Install the required libraries using ``` pip install -r requirements.txt ```.\n2. Ensure your DataRobot LLM deployment is working and [deployed](https://app.datarobot.com/deployments/list). Use ```create_llm_deployment.ipynb``` for reference.\n3. Update the appropriate identifiers in the configuration file ```config.py``` . \n    - PORT: Port to run the bot service on.\n    - SERVER: Server IP. Accepts localhost.\n    - APP_ID: Bot Identifier from ``` https://dev.teams.microsoft.com/bots/<<IDENTIFIER>>/configure ```.\n    - APP_PASSWORD: Client Secret from Bot Configure page. Point 4 in Bot Publish section below.\n    - DATAROBOT_TOKEN: DataRobot API token.\n    - DATAROBOT_ENDPOINT: DataRobot endpoint.\n    - DATAROBOT_DEPLOYMENT: DataRobot LLM deployment identifier.\n4. Activate the bot backend by running ``` python app.py  ```.\n5. If your bot service is not HTTPS, you can use port forwarding services like [ngrok](https://ngrok.com/). If you are using Azure VMs, please find the associated documentation to secure the VM using HTTPS [here](https://learn.microsoft.com/en-us/azure/virtual-machines/windows/tutorial-secure-web-server).\n6. The bot service is now online. \n\n## Bot testing\n\nDevelopers can use the [Bot Emulator Framework](https://github.com/microsoft/BotFramework-Emulator) for testing the bot locally. The emulator offers quality of life features like restarting or replaying conversations with the bot. \n\n## Bot publishing\n\n1. Log into [Teams Developer Portal](https://dev.teams.microsoft.com/) and navigate to the **Apps** tab.\n2. Click the **Tools** tab in the sidebar and then click **Bot management**.\n3. Click **New Bot**. In the **Configure** section, add the secure endpoint(\"/api/messages\") in **Endpoint Address**. This is the URL from step 4 in the \"Bot backend\" section above. \n4. In the Client secrets section, create a new secret. Note the secret key for the next steps.\n5. Click **New App** and fill in the basic information fields.\n6. In the **Configure** Tab, click **App features > Bot**. Select your bot from step 3 from the drodown.\n7. Publish your bot by clicking **Submit app update**. Upon approval from your Teams organization admin, which is handled by your IT support team, your bot will be available in the Teams homepage.\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/README.md",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "markdown",
        "size": 4520
      },
      "code_examples": [],
      "api_methods": [
        "deployment.ipynb"
      ],
      "complexity_score": 0.75,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_3673691968750738178",
      "title": "teams_datarobot: app.py",
      "content": "# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License.\r\n\r\nfrom datetime import datetime\r\nimport sys\r\nimport traceback\r\n\r\nfrom aiohttp import web\r\nfrom aiohttp.web import json_response, Request, Response\r\nfrom bot import MyBot\r\nfrom botbuilder.core import (\r\n    BotFrameworkAdapter,\r\n    BotFrameworkAdapterSettings,\r\n    TurnContext,\r\n)\r\nfrom botbuilder.core.integration import aiohttp_error_middleware\r\nfrom botbuilder.schema import Activity, ActivityTypes\r\nfrom config import DefaultConfig\r\n\r\n# Configure HTTPS here\r\n# import ssl\r\n# ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\r\n# ssl_context.load_cert_chain('./selfsigned.crt', './private.key')\r\n\r\nCONFIG = DefaultConfig()\r\n\r\n# Create adapter.\r\n# See https://aka.ms/about-bot-adapter to learn more about how bots work.\r\nSETTINGS = BotFrameworkAdapterSettings(CONFIG.APP_ID, CONFIG.APP_PASSWORD)\r\nADAPTER = BotFrameworkAdapter(SETTINGS)\r\nprint(CONFIG.APP_ID, CONFIG.APP_PASSWORD)\r\n\r\n\r\n# Catch-all for errors.\r\nasync def on_error(context: TurnContext, error: Exception):\r\n    # This check writes out errors to console log .vs. app insights.\r\n    # NOTE: In production environment, you should consider logging this to Azure\r\n    #       application insights.\r\n    print(f\"\\n [on_turn_error] unhandled error: {error}\", file=sys.stderr)\r\n    traceback.print_exc()\r\n\r\n    # Send a message to the user\r\n    await context.send_activity(\"The bot encountered an error or bug.\")\r\n    await context.send_activity(\r\n        \"To continue to run this bot, please fix the bot source code.\"\r\n    )\r\n    # Send a trace activity if we're talking to the Bot Framework Emulator\r\n    if context.activity.channel_id == \"emulator\":\r\n        # Create a trace activity that contains the error object\r\n        trace_activity = Activity(\r\n            label=\"TurnError\",\r\n            name=\"on_turn_error Trace\",\r\n            timestamp=datetime.utcnow(),\r\n            type=ActivityTypes.trace,\r\n            value=f\"{error}\",\r\n            value_type=\"https://www.botframework.com/schemas/error\",\r\n        )\r\n        # Send a trace activity, which will be displayed in Bot Framework Emulator\r\n        await context.send_activity(trace_activity)\r\n\r\n\r\nADAPTER.on_turn_error = on_error\r\n\r\n# Create the Bot\r\nBOT = MyBot(CONFIG.APP_ID, CONFIG.APP_PASSWORD)\r\n\r\n\r\ndef home(req: Request) -> Response:\r\n    return json_response(data=\"success\", status=Response(status=201).status)\r\n\r\n\r\n# Listen for incoming requests on /api/messages\r\nasync def messages(req: Request) -> Response:\r\n    # Main bot message handler.\r\n    if \"application/json\" in req.headers[\"Content-Type\"]:\r\n        body = await req.json()\r\n    else:\r\n        return Response(status=415)\r\n    activity = Activity().deserialize(body)\r\n    auth_header = req.headers[\"Authorization\"] if \"Authorization\" in req.headers else \"\"\r\n    response = await ADAPTER.process_activity(activity, auth_header, BOT.on_turn)\r\n    if response:\r\n        return json_response(data=response.body, status=response.status)\r\n    return Response(status=201)\r\n\r\n\r\nAPP = web.Application(middlewares=[aiohttp_error_middleware])\r\nAPP.router.add_get(\"/\", home)\r\nAPP.router.add_post(\"/api/messages\", messages)\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Bot backend is Active!\")\r\n    web.run_app(\r\n        APP,\r\n        host=CONFIG.SERVER,\r\n        port=CONFIG.PORT,\r\n        # ssl_context=ssl_context Set SSL\r\n    )\r\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/app.py",
      "tags": [
        "templates",
        "integration",
        "ai-accelerators",
        "ecosystem",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "python",
        "size": 3422
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.5,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_3707081185272891676",
      "title": "teams_datarobot: bot.py",
      "content": "# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License.\r\n\r\nfrom datetime import datetime\r\nimport json\r\n\r\nfrom botbuilder.core import (\r\n    ActivityHandler,\r\n    CardFactory,\r\n    ConversationState,\r\n    MemoryStorage,\r\n    MessageFactory,\r\n    TurnContext,\r\n)\r\nfrom botbuilder.core.teams import TeamsActivityHandler\r\nfrom botbuilder.schema import Activity, ActivityTypes, Attachment, ChannelAccount\r\nfrom config import DefaultConfig\r\nimport datarobotx as drx\r\nfrom utilities import generate_links\r\n\r\nadaptive_card_json = json.load(open(\"./card_template.json\"))\r\n\r\nCONFIG = DefaultConfig()\r\n# DataRobot Connect\r\ndrx.Context(token=CONFIG.DATAROBOT_TOKEN, endpoint=CONFIG.DATAROBOT_ENDPOINT)\r\ndeployment_id = CONFIG.DATAROBOT_DEPLOYMENT\r\ndeployment = drx.Deployment(deployment_id)\r\n\r\n# store chat history\r\nmemstore = MemoryStorage()\r\nconvstate = ConversationState(memstore)\r\n\r\n\r\nclass ConversationData:\r\n    def __init__(self, chat_history=[]):\r\n        self.chat_history = chat_history\r\n\r\n\r\nclass MyBot(TeamsActivityHandler):\r\n    # See https://aka.ms/about-bot-activity-message to learn more about the message and other activity types.\r\n    def __init__(self, app_id: str, app_password: str):\r\n        self._app_id = app_id\r\n        self._app_password = app_password\r\n        self.convstate_accessor = convstate.create_property(\"convdata\")\r\n\r\n    async def on_message_activity(self, turn_context: TurnContext):\r\n        time_start = datetime.now()\r\n        convdata = await self.convstate_accessor.get(turn_context, ConversationData)\r\n        chat_history = convdata.chat_history\r\n        TurnContext.remove_recipient_mention(turn_context.activity)\r\n        answer = deployment.predict_unstructured(\r\n            {\"question\": turn_context.activity.text, \"chat_history\": chat_history}\r\n        )\r\n        print(\"deployment response\")\r\n        print(answer)\r\n        answer_formatted = answer[\"answer\"]\r\n        references = generate_links(answer[\"references\"])\r\n        print(json.dumps(answer))\r\n        chat_history.append((turn_context.activity.text, answer_formatted))\r\n        time_end = datetime.now()\r\n        link_block = []\r\n        if len(references) > 0:\r\n            link_block.append(\r\n                {\"type\": \"TextBlock\", \"size\": \"medium\", \"text\": \"References:\"}\r\n            )\r\n            fs = {\"type\": \"FactSet\", \"facts\": []}\r\n            for link in references:\r\n                # [Adaptive cards!](https://adaptivecards.io)\r\n                fs[\"facts\"].append({\"value\": \"[\" + link + \"](\" + link + \")\"})\r\n            link_block.append(fs)\r\n        adaptive_card_json[\"body\"] = link_block\r\n        message = Activity(\r\n            text=answer_formatted,\r\n            type=ActivityTypes.message,\r\n            attachments=[CardFactory.adaptive_card(adaptive_card_json)],\r\n        )\r\n        await turn_context.send_activity(message)\r\n\r\n    async def on_members_added_activity(\r\n        self, members_added: ChannelAccount, turn_context: TurnContext\r\n    ):\r\n        for member_added in members_added:\r\n            if member_added.id != turn_context.activity.recipient.id:\r\n                await turn_context.send_activity(\r\n                    \"Hello!. I am DataRobot's Generative AI Bot, how can I help you?\"\r\n                )\r\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/bot.py",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "python",
        "size": 3284
      },
      "code_examples": [],
      "api_methods": [
        "deployment.predict_unstructured"
      ],
      "complexity_score": 0.6,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-1688573673994015496",
      "title": "teams_datarobot: card_template.json",
      "content": "{\n  \"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\",\n  \"type\": \"AdaptiveCard\",\n  \"version\": \"1.0\",\n  \"body\": [\n    {\n      \"type\": \"TextBlock\",\n      \"text\": \"Mention a user by User Principle Name: \"\n    },\n    {\n      \"type\": \"TextBlock\",\n      \"text\": \"Mention a user by AAD Object Id: \"\n    }\n  ]\n}",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/card_template.json",
      "tags": [
        "templates",
        "integration",
        "ai-accelerators",
        "ecosystem",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "json",
        "size": 317
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_4586538826597028379",
      "title": "teams_datarobot: config.py",
      "content": "#!/usr/bin/env python3\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License.\r\n\r\nimport os\r\n\r\n\r\nclass DefaultConfig:\r\n    \"\"\"Bot Configuration\"\"\"\r\n\r\n    PORT = os.environ.get(\"PORT\", 443)  # Port to run the bot service on\r\n    SERVER = os.environ.get(\"SERVER\", \"0.0.0.0\")  # Server IP. Accepts localhost\r\n    APP_ID = os.environ.get(\r\n        \"MicrosoftAppId\", \"\"\r\n    )  # Bot Identifier from https://dev.teams.microsoft.com/bots/<<IDENTIFIER>>/configure\r\n    APP_PASSWORD = os.environ.get(\r\n        \"MicrosoftAppPassword\", \"\"\r\n    )  # Client Secret from Bot Configure page.\r\n    DATAROBOT_TOKEN = os.environ.get(\r\n        \"apiToken\", \"\"\r\n    )  # DataRobot API Token for authorization\r\n    DATAROBOT_ENDPOINT = os.environ.get(\r\n        \"DATAROBOT_ENDPOINT\", \"https://app.datarobot.com/api/v2\"\r\n    )  # DataRobot Endpoint\r\n    DATAROBOT_DEPLOYMENT = os.environ.get(\r\n        \"deploymentId\", \"65cd8cb6481e3be33dcd194c\"\r\n    )  # DataRobot LLM Deployment Identifier\r\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/config.py",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "python",
        "size": 1009
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.2,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_8079107376272394679",
      "title": "teams_datarobot: create_llm_deployment.config.yaml",
      "content": "---\ndescription: Microsoft Teams offers workspace chat and video conferencing, file storage, and application\n  integration to organizations. Workspace chat feature aloow you to interact with other users and bots\n  in their day-to-day activities. This feature is useful for deploying Generative AI agents to improve\n  employee productivity. With DataRobot's Generative AI offerings, organizations can deploy chatbots without\n  the need for an additional front-end or consumption layers.\nfile_name: create_llm_deployment.ipynb\nlanguages:\n  - python\nmaintainers:\n  - Abdul Jilani\nmaintainers_email:\n  - abdul.jilani@datarobot.com\nsmoke_test:\n  run_smoke_test: false\ntags: []\ntitle: Use DataRobot generative AI with Microsoft Teams\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/create_llm_deployment.config.yaml",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "yaml",
        "size": 728
      },
      "code_examples": [],
      "api_methods": [
        "deployment.ipynb"
      ],
      "complexity_score": 0.65,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-2263436976975892963",
      "title": "teams_datarobot: create_llm_deployment.ipynb",
      "content": "```python\n!pip install \"langchain==0.1.0\" \\\n             \"faiss-cpu==1.7.4\" \\\n             \"sentence-transformers==2.2.2\" \\\n             \"unstructured==0.8.4\" \\\n             \"openai==0.27.8\" \\\n             \"datarobotx\" \\\n             \"pydantic\"\n```\n\n```python\n# Decompress the documents\n!tar -xf ./storage/dr_docs.tar -C ./storage/\n```\n\n```python\nimport re\n\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import MarkdownTextSplitter\n\nSOURCE_DOCUMENTS_DIR = \"storage/datarobot_docs/\"\nSOURCE_DOCUMENTS_FILTER = \"**/*.md\"\n\nloader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\nsplitter = MarkdownTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=1000,\n)\n\nprint(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\ndata = loader.load()\nprint(f\"Splitting {len(data)} documents\")\ndocs = splitter.split_documents(data)\nfor doc in docs:\n    doc.metadata[\"source\"] = re.sub(\n        r\"storage/datarobot_docs/en/(.+)\\.md\",\n        r\"https://docs.datarobot.com/en/docs/\\1.html\",\n        doc.metadata[\"source\"],\n    )\nprint(f\"Created {len(docs)} documents\")\n```\n\n```python\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain.vectorstores.faiss import FAISS\nimport torch\n\nif not torch.cuda.is_available():\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\nelse:\n    EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n\n# Will download the model the first time it runs\nembedding_function = SentenceTransformerEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    cache_folder=\"storage/deploy/sentencetransformers\",\n)\ntry:\n    # Load existing db from disk if previously built\n    db = FAISS.load_local(\"storage/deploy/faiss-db\", embedding_function)\nexcept:\n    texts = [doc.page_content for doc in docs]\n    metadatas = [doc.metadata for doc in docs]\n    # Build and save the FAISS db to persistent notebook storage; this can take some time w/o GPUs\n    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)\n    db.save_local(\"storage/deploy/faiss-db\")\n\nprint(f\"FAISS VectorDB has {db.index.ntotal} documents\")\n```\n\n```python\nimport os\n\nOPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\nOPENAI_ORGANIZATION = os.environ[\"OPENAI_ORGANIZATION\"]\nOPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nOPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\nOPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\nOPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n\n\ndef load_model(input_dir):\n    \"\"\"Custom model hook for loading our knowledge base.\"\"\"\n    import os\n\n    from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n    from langchain.vectorstores.faiss import FAISS\n\n    os.environ[\"OPENAI_API_TYPE\"] = OPENAI_API_TYPE\n    os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n    embedding_function = SentenceTransformerEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        cache_folder=input_dir + \"/\" + \"storage/deploy/sentencetransformers\",\n    )\n    db = FAISS.load_local(\n        input_dir + \"/\" + \"storage/deploy/faiss-db\", embedding_function\n    )\n    return OPENAI_DEPLOYMENT_NAME, db\n\n\ndef score_unstructured(model, data, query, **kwargs) -> str:\n    \"\"\"Custom model hook for making completions with our knowledge base.\"\"\"\n\n    import json\n\n    from langchain.chains import ConversationalRetrievalChain\n    from langchain.chat_models import AzureChatOpenAI\n    from langchain.vectorstores.base import VectorStoreRetriever\n\n    try:\n        deployment_name, db = model\n        data_dict = json.loads(data)\n        llm = AzureChatOpenAI(\n            deployment_name=OPENAI_DEPLOYMENT_NAME,\n            openai_api_type=OPENAI_API_TYPE,\n            openai_api_base=OPENAI_API_BASE,\n            openai_api_version=OPENAI_API_VERSION,\n            openai_api_key=OPENAI_API_KEY,\n            openai_organization=OPENAI_ORGANIZATION,\n            model_name=OPENAI_DEPLOYMENT_NAME,\n            temperature=0,\n            verbose=True,\n            max_retries=0,\n            request_timeout=20,\n        )\n        retriever = VectorStoreRetriever(vectorstore=db)\n        chain = ConversationalRetrievalChain.from_llm(\n            llm, retriever=retriever, return_source_documents=True\n        )\n        if \"chat_history\" in data_dict:\n            chat_history = [\n                (\n                    human,\n                    ai,\n                )\n                for human, ai in data_dict[\"chat_history\"]\n            ]\n        else:\n            chat_history = []\n        rv = chain(\n            inputs={\n                \"question\": data_dict[\"question\"],\n                \"chat_history\": chat_history,\n            },\n        )\n        rv[\"references\"] = [\n            doc.metadata[\"source\"] for doc in rv.pop(\"source_documents\")\n        ]\n    except Exception as e:\n        rv = {\"error\": f\"{e.__class__.__name__}: {str(e)}\"}\n    return json.dumps(rv)\n```\n\n```python\nimport datarobotx as drx\n\ndeployment = drx.deploy(\n    \"storage/deploy/\",\n    name=\"Teams Bot\",\n    hooks={\"score_unstructured\": score_unstructured, \"load_model\": load_model},\n    extra_requirements=[\n        \"langchain==0.1.0\",\n        #                    \"faiss-cpu==1.7.4\",\n        #                    \"sentence-transformers==2.2.2\",\n        #                    \"unstructured==0.8.4\",\n        #                    \"openai==0.27.8\",\n        #                    \"datarobotx\",\n        #                    \"pydantic\"\n    ],\n    # Re-use existing environment if you want to change the hook code,\n    # and not requirements\n    environment_id=\"64c964448dd3f0c07f47d040\",\n)\n# enable storing prediction data, necessary for Data Export for monitoring purposes\ndeployment.dr_deployment.update_predictions_data_collection_settings(enabled=True)\n```\n\n```python\ndeployment.predict_unstructured({\"question\": \"Does datarobot support Azure AD?\"})\n```",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/create_llm_deployment.ipynb",
      "tags": [
        "integration",
        "ai-accelerators",
        "openai",
        "predictions",
        "datarobot",
        "deployment",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "notebook",
        "size": 48908
      },
      "code_examples": [
        "!pip install \"langchain==0.1.0\" \\\n             \"faiss-cpu==1.7.4\" \\\n             \"sentence-transformers==2.2.2\" \\\n             \"unstructured==0.8.4\" \\\n             \"openai==0.27.8\" \\\n             \"datarobotx\" \\\n             \"pydantic\"",
        "# Decompress the documents\n!tar -xf ./storage/dr_docs.tar -C ./storage/",
        "import re\n\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import MarkdownTextSplitter\n\nSOURCE_DOCUMENTS_DIR = \"storage/datarobot_docs/\"\nSOURCE_DOCUMENTS_FILTER = \"**/*.md\"\n\nloader = DirectoryLoader(f\"{SOURCE_DOCUMENTS_DIR}\", glob=SOURCE_DOCUMENTS_FILTER)\nsplitter = MarkdownTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=1000,\n)\n\nprint(f\"Loading {SOURCE_DOCUMENTS_DIR} directory\")\ndata = loader.load()\nprint(f\"Splitting {len(data)} documents\")\ndocs = splitter.split_documents(data)\nfor doc in docs:\n    doc.metadata[\"source\"] = re.sub(\n        r\"storage/datarobot_docs/en/(.+)\\.md\",\n        r\"https://docs.datarobot.com/en/docs/\\1.html\",\n        doc.metadata[\"source\"],\n    )\nprint(f\"Created {len(docs)} documents\")",
        "from langchain.docstore.document import Document\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain.vectorstores.faiss import FAISS\nimport torch\n\nif not torch.cuda.is_available():\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\nelse:\n    EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n\n# Will download the model the first time it runs\nembedding_function = SentenceTransformerEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    cache_folder=\"storage/deploy/sentencetransformers\",\n)\ntry:\n    # Load existing db from disk if previously built\n    db = FAISS.load_local(\"storage/deploy/faiss-db\", embedding_function)\nexcept:\n    texts = [doc.page_content for doc in docs]\n    metadatas = [doc.metadata for doc in docs]\n    # Build and save the FAISS db to persistent notebook storage; this can take some time w/o GPUs\n    db = FAISS.from_texts(texts, embedding_function, metadatas=metadatas)\n    db.save_local(\"storage/deploy/faiss-db\")\n\nprint(f\"FAISS VectorDB has {db.index.ntotal} documents\")",
        "import os\n\nOPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\nOPENAI_ORGANIZATION = os.environ[\"OPENAI_ORGANIZATION\"]\nOPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nOPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\nOPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\nOPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n\n\ndef load_model(input_dir):\n    \"\"\"Custom model hook for loading our knowledge base.\"\"\"\n    import os\n\n    from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n    from langchain.vectorstores.faiss import FAISS\n\n    os.environ[\"OPENAI_API_TYPE\"] = OPENAI_API_TYPE\n    os.environ[\"OPENAI_API_BASE\"] = OPENAI_API_BASE\n    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n    embedding_function = SentenceTransformerEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        cache_folder=input_dir + \"/\" + \"storage/deploy/sentencetransformers\",\n    )\n    db = FAISS.load_local(\n        input_dir + \"/\" + \"storage/deploy/faiss-db\", embedding_function\n    )\n    return OPENAI_DEPLOYMENT_NAME, db\n\n\ndef score_unstructured(model, data, query, **kwargs) -> str:\n    \"\"\"Custom model hook for making completions with our knowledge base.\"\"\"\n\n    import json\n\n    from langchain.chains import ConversationalRetrievalChain\n    from langchain.chat_models import AzureChatOpenAI\n    from langchain.vectorstores.base import VectorStoreRetriever\n\n    try:\n        deployment_name, db = model\n        data_dict = json.loads(data)\n        llm = AzureChatOpenAI(\n            deployment_name=OPENAI_DEPLOYMENT_NAME,\n            openai_api_type=OPENAI_API_TYPE,\n            openai_api_base=OPENAI_API_BASE,\n            openai_api_version=OPENAI_API_VERSION,\n            openai_api_key=OPENAI_API_KEY,\n            openai_organization=OPENAI_ORGANIZATION,\n            model_name=OPENAI_DEPLOYMENT_NAME,\n            temperature=0,\n            verbose=True,\n            max_retries=0,\n            request_timeout=20,\n        )\n        retriever = VectorStoreRetriever(vectorstore=db)\n        chain = ConversationalRetrievalChain.from_llm(\n            llm, retriever=retriever, return_source_documents=True\n        )\n        if \"chat_history\" in data_dict:\n            chat_history = [\n                (\n                    human,\n                    ai,\n                )\n                for human, ai in data_dict[\"chat_history\"]\n            ]\n        else:\n            chat_history = []\n        rv = chain(\n            inputs={\n                \"question\": data_dict[\"question\"],\n                \"chat_history\": chat_history,\n            },\n        )\n        rv[\"references\"] = [\n            doc.metadata[\"source\"] for doc in rv.pop(\"source_documents\")\n        ]\n    except Exception as e:\n        rv = {\"error\": f\"{e.__class__.__name__}: {str(e)}\"}\n    return json.dumps(rv)",
        "import datarobotx as drx\n\ndeployment = drx.deploy(\n    \"storage/deploy/\",\n    name=\"Teams Bot\",\n    hooks={\"score_unstructured\": score_unstructured, \"load_model\": load_model},\n    extra_requirements=[\n        \"langchain==0.1.0\",\n        #                    \"faiss-cpu==1.7.4\",\n        #                    \"sentence-transformers==2.2.2\",\n        #                    \"unstructured==0.8.4\",\n        #                    \"openai==0.27.8\",\n        #                    \"datarobotx\",\n        #                    \"pydantic\"\n    ],\n    # Re-use existing environment if you want to change the hook code,\n    # and not requirements\n    environment_id=\"64c964448dd3f0c07f47d040\",\n)\n# enable storing prediction data, necessary for Data Export for monitoring purposes\ndeployment.dr_deployment.update_predictions_data_collection_settings(enabled=True)",
        "deployment.predict_unstructured({\"question\": \"Does datarobot support Azure AD?\"})"
      ],
      "api_methods": [
        "deployment.predict_unstructured",
        "deployment.dr_deployment"
      ],
      "complexity_score": 1.0,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_4280340093736143742",
      "title": "teams_datarobot: requirements.txt",
      "content": "botbuilder-integration-aiohttp>=4.14.0\r\ndatarobotx\r\ndatarobot\r\ntextstat\r\ntiktoken\r\nnltk\r\npy-readability-metrics\r\ndatarobot-mlops\r\ndatarobot-mlops-connected-client",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/requirements.txt",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "text",
        "size": 162
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.5,
      "use_case_category": "integration"
    },
    {
      "id": "ai_accelerator_-6181530093734661067",
      "title": "teams_datarobot: utilities.py",
      "content": "import re\n\n\ndef generate_links(locs, max_links=None):\n    dr_docs_dir = \"storage/vectordb_training/datarobot_docs/en/\"\n    dr_security_dir = \"storage/vectordb_training/datarobot_docs/\"\n    rfp_docs_dir = \"storage/vectordb_training/reprocessed_data/\"\n    links = []\n    for link in locs:\n        if link.startswith(dr_docs_dir):\n            link = link[len(dr_docs_dir) :]\n            link = \"https://docs.datarobot.com/en/docs/\" + link\n            link = link[:-2] + \"html\"  # Remove \"md\", add html\n        elif link.startswith(dr_security_dir):\n            link = \"https://www.datarobot.com/trustcenter/\"\n        elif link.startswith(rfp_docs_dir):\n            try:\n                file = open(link, \"r\")\n                file_contents = file.read()\n                link = re.search(r\"https://\\S+\", file_contents)\n                file.close()\n                link = link.group()\n            except AttributeError:\n                print(\"Attribute error\")\n                link = None\n                pass\n            except FileNotFoundError:\n                link = None\n                pass\n        links.append(link)\n    links = list(set([link for link in links if link != None]))\n    return links\n",
      "content_type": "template",
      "source_type": "ai_accelerator",
      "source_file": "ecosystem_integration_templates/teams_datarobot/utilities.py",
      "tags": [
        "integration",
        "ai-accelerators",
        "datarobot",
        "ecosystem",
        "templates",
        "ai-accelerator",
        "ecosystem_integration_templates"
      ],
      "metadata": {
        "section": "ecosystem_integration_templates",
        "subsection": "teams_datarobot",
        "file_type": "python",
        "size": 1199
      },
      "code_examples": [],
      "api_methods": [],
      "complexity_score": 0.2,
      "use_case_category": "integration"
    },
    {
      "id": "openapi_4215647039321661223",
      "title": "OpenAPI: List assistants",
      "content": "import requests\n\n# List assistants\n# Returns a list of assistants.\n\nurl = f'{base_url}/assistants'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /assistants",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /assistants",
        "summary": "List assistants",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List assistants\n# Returns a list of assistants.\n\nurl = f'{base_url}/assistants'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6426696433964960306",
      "title": "OpenAPI: Create assistant",
      "content": "import requests\n\n# Create assistant\n# Create an assistant with a model and instructions.\n\nurl = f'{base_url}/assistants'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /assistants",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /assistants",
        "summary": "Create assistant",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create assistant\n# Create an assistant with a model and instructions.\n\nurl = f'{base_url}/assistants'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1001133602878190895",
      "title": "OpenAPI: Retrieve assistant",
      "content": "import requests\n\n# Retrieve assistant\n# Retrieves an assistant.\n\nurl = f'{base_url}/assistants/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /assistants/{assistant_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /assistants/{assistant_id}",
        "summary": "Retrieve assistant",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve assistant\n# Retrieves an assistant.\n\nurl = f'{base_url}/assistants/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2962448700012722010",
      "title": "OpenAPI: Modify assistant",
      "content": "import requests\n\n# Modify assistant\n# Modifies an assistant.\n\nurl = f'{base_url}/assistants/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /assistants/{assistant_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /assistants/{assistant_id}",
        "summary": "Modify assistant",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify assistant\n# Modifies an assistant.\n\nurl = f'{base_url}/assistants/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8190173057628177323",
      "title": "OpenAPI: Delete assistant",
      "content": "import requests\n\n# Delete assistant\n# Delete an assistant.\n\nurl = f'{base_url}/assistants/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /assistants/{assistant_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /assistants/{assistant_id}",
        "summary": "Delete assistant",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete assistant\n# Delete an assistant.\n\nurl = f'{base_url}/assistants/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-841332674614270924",
      "title": "OpenAPI: Create speech",
      "content": "import requests\n\n# Create speech\n# Generates audio from the input text.\n\nurl = f'{base_url}/audio/speech'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /audio/speech",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /audio/speech",
        "summary": "Create speech",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create speech\n# Generates audio from the input text.\n\nurl = f'{base_url}/audio/speech'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2137050258973699445",
      "title": "OpenAPI: Create transcription",
      "content": "import requests\n\n# Create transcription\n# Transcribes audio into the input language.\n\nurl = f'{base_url}/audio/transcriptions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /audio/transcriptions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /audio/transcriptions",
        "summary": "Create transcription",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create transcription\n# Transcribes audio into the input language.\n\nurl = f'{base_url}/audio/transcriptions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1532808646518132584",
      "title": "OpenAPI: Create translation",
      "content": "import requests\n\n# Create translation\n# Translates audio into English.\n\nurl = f'{base_url}/audio/translations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /audio/translations",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /audio/translations",
        "summary": "Create translation",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create translation\n# Translates audio into English.\n\nurl = f'{base_url}/audio/translations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4753013696429660025",
      "title": "OpenAPI: Create batch",
      "content": "import requests\n\n# Create batch\n# Creates and executes a batch from an uploaded file of requests\n\nurl = f'{base_url}/batches'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /batches",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /batches",
        "summary": "Create batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create batch\n# Creates and executes a batch from an uploaded file of requests\n\nurl = f'{base_url}/batches'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-8810268406957840864",
      "title": "OpenAPI: List batch",
      "content": "import requests\n\n# List batch\n# List your organization's batches.\n\nurl = f'{base_url}/batches'\nparams = {\n    'after': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /batches",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /batches",
        "summary": "List batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List batch\n# List your organization's batches.\n\nurl = f'{base_url}/batches'\nparams = {\n    'after': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5293981071038411930",
      "title": "OpenAPI: Retrieve batch",
      "content": "import requests\n\n# Retrieve batch\n# Retrieves a batch.\n\nurl = f'{base_url}/batches/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /batches/{batch_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /batches/{batch_id}",
        "summary": "Retrieve batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve batch\n# Retrieves a batch.\n\nurl = f'{base_url}/batches/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7641095206081689552",
      "title": "OpenAPI: Cancel batch",
      "content": "import requests\n\n# Cancel batch\n# Cancels an in-progress batch. The batch will be in status `cancelling` for up to 10 minutes, before changing to `cancelled`, where it will have partial results (if any) available in the output file.\n\nurl = f'{base_url}/batches/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /batches/{batch_id}/cancel",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /batches/{batch_id}/cancel",
        "summary": "Cancel batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Cancel batch\n# Cancels an in-progress batch. The batch will be in status `cancelling` for up to 10 minutes, before changing to `cancelled`, where it will have partial results (if any) available in the output file.\n\nurl = f'{base_url}/batches/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_5013211781553690945",
      "title": "OpenAPI: List Chat Completions",
      "content": "import requests\n\n# List Chat Completions\n# List stored Chat Completions. Only Chat Completions that have been stored\nwith the `store` parameter set to `true` will be returned.\n\n\nurl = f'{base_url}/chat/completions'\nparams = {\n    'model': None,\n    'metadata': None,\n    'after': None,\n    'limit': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /chat/completions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /chat/completions",
        "summary": "List Chat Completions",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List Chat Completions\n# List stored Chat Completions. Only Chat Completions that have been stored\nwith the `store` parameter set to `true` will be returned.\n\n\nurl = f'{base_url}/chat/completions'\nparams = {\n    'model': None,\n    'metadata': None,\n    'after': None,\n    'limit': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5960660446134092123",
      "title": "OpenAPI: Create chat completion",
      "content": "import requests\n\n# Create chat completion\n# **Starting a new project?** We recommend trying [Responses](https://platform.openai.com/docs/api-reference/responses) \nto take advantage of the latest OpenAI platform features. Compare\n[Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).\n\n---\n\nCreates a model response for the given chat conversation. Learn more in the\n[text generation](https://platform.openai.com/docs/guides/text-generation), [vision](https://platform.openai.com/docs/guides/vision),\nand [audio](https://platform.openai.com/docs/guides/audio) guides.\n\nParameter support can differ depending on the model used to generate the\nresponse, particularly for newer reasoning models. Parameters that are only\nsupported for reasoning models are noted below. For the current state of \nunsupported parameters in reasoning models, \n[refer to the reasoning guide](https://platform.openai.com/docs/guides/reasoning).\n\n\nurl = f'{base_url}/chat/completions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /chat/completions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /chat/completions",
        "summary": "Create chat completion",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create chat completion\n# **Starting a new project?** We recommend trying [Responses](https://platform.openai.com/docs/api-reference/responses) \nto take advantage of the latest OpenAI platform features. Compare\n[Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).\n\n---\n\nCreates a model response for the given chat conversation. Learn more in the\n[text generation](https://platform.openai.com/docs/guides/text-generation), [vision](https://platform.openai.com/docs/guides/vision),\nand [audio](https://platform.openai.com/docs/guides/audio) guides.\n\nParameter support can differ depending on the model used to generate the\nresponse, particularly for newer reasoning models. Parameters that are only\nsupported for reasoning models are noted below. For the current state of \nunsupported parameters in reasoning models, \n[refer to the reasoning guide](https://platform.openai.com/docs/guides/reasoning).\n\n\nurl = f'{base_url}/chat/completions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3170258743413470526",
      "title": "OpenAPI: Get chat completion",
      "content": "import requests\n\n# Get chat completion\n# Get a stored chat completion. Only Chat Completions that have been created\nwith the `store` parameter set to `true` will be returned.\n\n\nurl = f'{base_url}/chat/completions/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /chat/completions/{completion_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /chat/completions/{completion_id}",
        "summary": "Get chat completion",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get chat completion\n# Get a stored chat completion. Only Chat Completions that have been created\nwith the `store` parameter set to `true` will be returned.\n\n\nurl = f'{base_url}/chat/completions/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-8061077563740515318",
      "title": "OpenAPI: Update chat completion",
      "content": "import requests\n\n# Update chat completion\n# Modify a stored chat completion. Only Chat Completions that have been\ncreated with the `store` parameter set to `true` can be modified. Currently,\nthe only supported modification is to update the `metadata` field.\n\n\nurl = f'{base_url}/chat/completions/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /chat/completions/{completion_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /chat/completions/{completion_id}",
        "summary": "Update chat completion",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Update chat completion\n# Modify a stored chat completion. Only Chat Completions that have been\ncreated with the `store` parameter set to `true` can be modified. Currently,\nthe only supported modification is to update the `metadata` field.\n\n\nurl = f'{base_url}/chat/completions/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_5515926675788014981",
      "title": "OpenAPI: Delete chat completion",
      "content": "import requests\n\n# Delete chat completion\n# Delete a stored chat completion. Only Chat Completions that have been\ncreated with the `store` parameter set to `true` can be deleted.\n\n\nurl = f'{base_url}/chat/completions/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /chat/completions/{completion_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /chat/completions/{completion_id}",
        "summary": "Delete chat completion",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete chat completion\n# Delete a stored chat completion. Only Chat Completions that have been\ncreated with the `store` parameter set to `true` can be deleted.\n\n\nurl = f'{base_url}/chat/completions/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2563997621284683765",
      "title": "OpenAPI: Get chat messages",
      "content": "import requests\n\n# Get chat messages\n# Get the messages in a stored chat completion. Only Chat Completions that\nhave been created with the `store` parameter set to `true` will be\nreturned.\n\n\nurl = f'{base_url}/chat/completions/{None}/messages'\nparams = {\n    'after': None,\n    'limit': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /chat/completions/{completion_id}/messages",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /chat/completions/{completion_id}/messages",
        "summary": "Get chat messages",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get chat messages\n# Get the messages in a stored chat completion. Only Chat Completions that\nhave been created with the `store` parameter set to `true` will be\nreturned.\n\n\nurl = f'{base_url}/chat/completions/{None}/messages'\nparams = {\n    'after': None,\n    'limit': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-222574426796205011",
      "title": "OpenAPI: Create completion",
      "content": "import requests\n\n# Create completion\n# Creates a completion for the provided prompt and parameters.\n\nurl = f'{base_url}/completions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /completions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /completions",
        "summary": "Create completion",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create completion\n# Creates a completion for the provided prompt and parameters.\n\nurl = f'{base_url}/completions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5816869331262145444",
      "title": "OpenAPI: List containers",
      "content": "import requests\n\n# List containers\n# List Containers\n\nurl = f'{base_url}/containers'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /containers",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /containers",
        "summary": "List containers",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List containers\n# List Containers\n\nurl = f'{base_url}/containers'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8891536351571786817",
      "title": "OpenAPI: Create container",
      "content": "import requests\n\n# Create container\n# Create Container\n\nurl = f'{base_url}/containers'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /containers",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /containers",
        "summary": "Create container",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create container\n# Create Container\n\nurl = f'{base_url}/containers'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-3980721146810606659",
      "title": "OpenAPI: Retrieve container",
      "content": "import requests\n\n# Retrieve container\n# Retrieve Container\n\nurl = f'{base_url}/containers/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /containers/{container_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /containers/{container_id}",
        "summary": "Retrieve container",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve container\n# Retrieve Container\n\nurl = f'{base_url}/containers/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-321911512379658775",
      "title": "OpenAPI: Delete a container",
      "content": "import requests\n\n# Delete a container\n# Delete Container\n\nurl = f'{base_url}/containers/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /containers/{container_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /containers/{container_id}",
        "summary": "Delete a container",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete a container\n# Delete Container\n\nurl = f'{base_url}/containers/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-8297925472441731250",
      "title": "OpenAPI: Create container file",
      "content": "import requests\n\n# Create container file\n# Create a Container File\n\nYou can send either a multipart/form-data request with the raw file content, or a JSON request with a file ID.\n\n\nurl = f'{base_url}/containers/{None}/files'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /containers/{container_id}/files",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /containers/{container_id}/files",
        "summary": "Create container file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create container file\n# Create a Container File\n\nYou can send either a multipart/form-data request with the raw file content, or a JSON request with a file ID.\n\n\nurl = f'{base_url}/containers/{None}/files'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5773529031169769934",
      "title": "OpenAPI: List container files",
      "content": "import requests\n\n# List container files\n# List Container files\n\nurl = f'{base_url}/containers/{None}/files'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /containers/{container_id}/files",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /containers/{container_id}/files",
        "summary": "List container files",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List container files\n# List Container files\n\nurl = f'{base_url}/containers/{None}/files'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1309062132418736021",
      "title": "OpenAPI: Retrieve container file",
      "content": "import requests\n\n# Retrieve container file\n# Retrieve Container File\n\nurl = f'{base_url}/containers/{None}/files/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /containers/{container_id}/files/{file_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /containers/{container_id}/files/{file_id}",
        "summary": "Retrieve container file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve container file\n# Retrieve Container File\n\nurl = f'{base_url}/containers/{None}/files/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4746670770069412459",
      "title": "OpenAPI: Delete a container file",
      "content": "import requests\n\n# Delete a container file\n# Delete Container File\n\nurl = f'{base_url}/containers/{None}/files/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /containers/{container_id}/files/{file_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /containers/{container_id}/files/{file_id}",
        "summary": "Delete a container file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete a container file\n# Delete Container File\n\nurl = f'{base_url}/containers/{None}/files/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-85651132164106981",
      "title": "OpenAPI: Retrieve container file content",
      "content": "import requests\n\n# Retrieve container file content\n# Retrieve Container File Content\n\nurl = f'{base_url}/containers/{None}/files/{None}/content'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /containers/{container_id}/files/{file_id}/content",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /containers/{container_id}/files/{file_id}/content",
        "summary": "Retrieve container file content",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve container file content\n# Retrieve Container File Content\n\nurl = f'{base_url}/containers/{None}/files/{None}/content'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-7855954624521506947",
      "title": "OpenAPI: Create a conversation",
      "content": "import requests\n\n# Create a conversation\n# Create a conversation with the given ID.\n\nurl = f'{base_url}/conversations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /conversations",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /conversations",
        "summary": "Create a conversation",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create a conversation\n# Create a conversation with the given ID.\n\nurl = f'{base_url}/conversations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6821626313550610077",
      "title": "OpenAPI: Retrieve a conversation",
      "content": "import requests\n\n# Retrieve a conversation\n# Get a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /conversations/{conversation_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /conversations/{conversation_id}",
        "summary": "Retrieve a conversation",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve a conversation\n# Get a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4505276824239863063",
      "title": "OpenAPI: Update a conversation",
      "content": "import requests\n\n# Update a conversation\n# Update a conversation's metadata with the given ID.\n\nurl = f'{base_url}/conversations/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /conversations/{conversation_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /conversations/{conversation_id}",
        "summary": "Update a conversation",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Update a conversation\n# Update a conversation's metadata with the given ID.\n\nurl = f'{base_url}/conversations/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_995136088557131407",
      "title": "OpenAPI: Delete a conversation",
      "content": "import requests\n\n# Delete a conversation\n# Delete a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /conversations/{conversation_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /conversations/{conversation_id}",
        "summary": "Delete a conversation",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete a conversation\n# Delete a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-104477017467658799",
      "title": "OpenAPI: Create items",
      "content": "import requests\n\n# Create items\n# Create items in a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}/items'\nparams = {\n    'include': None\n}\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, params=params, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /conversations/{conversation_id}/items",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /conversations/{conversation_id}/items",
        "summary": "Create items",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create items\n# Create items in a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}/items'\nparams = {\n    'include': None\n}\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, params=params, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8957664848629742469",
      "title": "OpenAPI: List items",
      "content": "import requests\n\n# List items\n# List all items for a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}/items'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /conversations/{conversation_id}/items",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /conversations/{conversation_id}/items",
        "summary": "List items",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List items\n# List all items for a conversation with the given ID.\n\nurl = f'{base_url}/conversations/{None}/items'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-4024068412197865307",
      "title": "OpenAPI: Retrieve an item",
      "content": "import requests\n\n# Retrieve an item\n# Get a single item from a conversation with the given IDs.\n\nurl = f'{base_url}/conversations/{None}/items/{None}'\nparams = {\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /conversations/{conversation_id}/items/{item_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /conversations/{conversation_id}/items/{item_id}",
        "summary": "Retrieve an item",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve an item\n# Get a single item from a conversation with the given IDs.\n\nurl = f'{base_url}/conversations/{None}/items/{None}'\nparams = {\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-3650377165862978414",
      "title": "OpenAPI: Delete an item",
      "content": "import requests\n\n# Delete an item\n# Delete an item from a conversation with the given IDs.\n\nurl = f'{base_url}/conversations/{None}/items/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /conversations/{conversation_id}/items/{item_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /conversations/{conversation_id}/items/{item_id}",
        "summary": "Delete an item",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete an item\n# Delete an item from a conversation with the given IDs.\n\nurl = f'{base_url}/conversations/{None}/items/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-7934299781444962283",
      "title": "OpenAPI: Create embeddings",
      "content": "import requests\n\n# Create embeddings\n# Creates an embedding vector representing the input text.\n\nurl = f'{base_url}/embeddings'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /embeddings",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /embeddings",
        "summary": "Create embeddings",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create embeddings\n# Creates an embedding vector representing the input text.\n\nurl = f'{base_url}/embeddings'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3410477633233380250",
      "title": "OpenAPI: List evals",
      "content": "import requests\n\n# List evals\n# List evaluations for a project.\n\n\nurl = f'{base_url}/evals'\nparams = {\n    'after': None,\n    'limit': None,\n    'order': None,\n    'order_by': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /evals",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /evals",
        "summary": "List evals",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List evals\n# List evaluations for a project.\n\n\nurl = f'{base_url}/evals'\nparams = {\n    'after': None,\n    'limit': None,\n    'order': None,\n    'order_by': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-7941877088120902678",
      "title": "OpenAPI: Create eval",
      "content": "import requests\n\n# Create eval\n# Create the structure of an evaluation that can be used to test a model's performance.\nAn evaluation is a set of testing criteria and the config for a data source, which dictates the schema of the data used in the evaluation. After creating an evaluation, you can run it on different models and model parameters. We support several types of graders and datasources.\nFor more information, see the [Evals guide](https://platform.openai.com/docs/guides/evals).\n\n\nurl = f'{base_url}/evals'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /evals",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /evals",
        "summary": "Create eval",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create eval\n# Create the structure of an evaluation that can be used to test a model's performance.\nAn evaluation is a set of testing criteria and the config for a data source, which dictates the schema of the data used in the evaluation. After creating an evaluation, you can run it on different models and model parameters. We support several types of graders and datasources.\nFor more information, see the [Evals guide](https://platform.openai.com/docs/guides/evals).\n\n\nurl = f'{base_url}/evals'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4040960616758951172",
      "title": "OpenAPI: Get an eval",
      "content": "import requests\n\n# Get an eval\n# Get an evaluation by ID.\n\n\nurl = f'{base_url}/evals/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /evals/{eval_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /evals/{eval_id}",
        "summary": "Get an eval",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get an eval\n# Get an evaluation by ID.\n\n\nurl = f'{base_url}/evals/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-398614439894096674",
      "title": "OpenAPI: Update an eval",
      "content": "import requests\n\n# Update an eval\n# Update certain properties of an evaluation.\n\n\nurl = f'{base_url}/evals/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /evals/{eval_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /evals/{eval_id}",
        "summary": "Update an eval",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Update an eval\n# Update certain properties of an evaluation.\n\n\nurl = f'{base_url}/evals/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_199711271482577921",
      "title": "OpenAPI: Delete an eval",
      "content": "import requests\n\n# Delete an eval\n# Delete an evaluation.\n\n\nurl = f'{base_url}/evals/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /evals/{eval_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /evals/{eval_id}",
        "summary": "Delete an eval",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete an eval\n# Delete an evaluation.\n\n\nurl = f'{base_url}/evals/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2421660946175271626",
      "title": "OpenAPI: Get eval runs",
      "content": "import requests\n\n# Get eval runs\n# Get a list of runs for an evaluation.\n\n\nurl = f'{base_url}/evals/{None}/runs'\nparams = {\n    'after': None,\n    'limit': None,\n    'order': None,\n    'status': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /evals/{eval_id}/runs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /evals/{eval_id}/runs",
        "summary": "Get eval runs",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get eval runs\n# Get a list of runs for an evaluation.\n\n\nurl = f'{base_url}/evals/{None}/runs'\nparams = {\n    'after': None,\n    'limit': None,\n    'order': None,\n    'status': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-7990259540511171700",
      "title": "OpenAPI: Create eval run",
      "content": "import requests\n\n# Create eval run\n# Kicks off a new run for a given evaluation, specifying the data source, and what model configuration to use to test. The datasource will be validated against the schema specified in the config of the evaluation.\n\n\nurl = f'{base_url}/evals/{None}/runs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /evals/{eval_id}/runs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /evals/{eval_id}/runs",
        "summary": "Create eval run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create eval run\n# Kicks off a new run for a given evaluation, specifying the data source, and what model configuration to use to test. The datasource will be validated against the schema specified in the config of the evaluation.\n\n\nurl = f'{base_url}/evals/{None}/runs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-4269406285352629853",
      "title": "OpenAPI: Get an eval run",
      "content": "import requests\n\n# Get an eval run\n# Get an evaluation run by ID.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /evals/{eval_id}/runs/{run_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /evals/{eval_id}/runs/{run_id}",
        "summary": "Get an eval run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get an eval run\n# Get an evaluation run by ID.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4862949174313071689",
      "title": "OpenAPI: Cancel eval run",
      "content": "import requests\n\n# Cancel eval run\n# Cancel an ongoing evaluation run.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /evals/{eval_id}/runs/{run_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /evals/{eval_id}/runs/{run_id}",
        "summary": "Cancel eval run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Cancel eval run\n# Cancel an ongoing evaluation run.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4980333531437747454",
      "title": "OpenAPI: Delete eval run",
      "content": "import requests\n\n# Delete eval run\n# Delete an eval run.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /evals/{eval_id}/runs/{run_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /evals/{eval_id}/runs/{run_id}",
        "summary": "Delete eval run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete eval run\n# Delete an eval run.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6830192240248500093",
      "title": "OpenAPI: Get eval run output items",
      "content": "import requests\n\n# Get eval run output items\n# Get a list of output items for an evaluation run.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}/output_items'\nparams = {\n    'after': None,\n    'limit': None,\n    'status': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /evals/{eval_id}/runs/{run_id}/output_items",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /evals/{eval_id}/runs/{run_id}/output_items",
        "summary": "Get eval run output items",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get eval run output items\n# Get a list of output items for an evaluation run.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}/output_items'\nparams = {\n    'after': None,\n    'limit': None,\n    'status': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8057171787241943457",
      "title": "OpenAPI: Get an output item of an eval run",
      "content": "import requests\n\n# Get an output item of an eval run\n# Get an evaluation run output item by ID.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}/output_items/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /evals/{eval_id}/runs/{run_id}/output_items/{output_item_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /evals/{eval_id}/runs/{run_id}/output_items/{output_item_id}",
        "summary": "Get an output item of an eval run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get an output item of an eval run\n# Get an evaluation run output item by ID.\n\n\nurl = f'{base_url}/evals/{None}/runs/{None}/output_items/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2963534130453696104",
      "title": "OpenAPI: List files",
      "content": "import requests\n\n# List files\n# Returns a list of files.\n\nurl = f'{base_url}/files'\nparams = {\n    'purpose': None,\n    'limit': None,\n    'order': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /files",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /files",
        "summary": "List files",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List files\n# Returns a list of files.\n\nurl = f'{base_url}/files'\nparams = {\n    'purpose': None,\n    'limit': None,\n    'order': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3924340466242826364",
      "title": "OpenAPI: Upload file",
      "content": "import requests\n\n# Upload file\n# Upload a file that can be used across various endpoints. Individual files can be up to 512 MB, and the size of all files uploaded by one organization can be up to 1 TB.\n\nThe Assistants API supports files up to 2 million tokens and of specific file types. See the [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools) for details.\n\nThe Fine-tuning API only supports `.jsonl` files. The input also has certain required formats for fine-tuning [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input) or [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input) models.\n\nThe Batch API only supports `.jsonl` files up to 200 MB in size. The input also has a specific required [format](https://platform.openai.com/docs/api-reference/batch/request-input).\n\nPlease [contact us](https://help.openai.com/) if you need to increase these storage limits.\n\n\nurl = f'{base_url}/files'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /files",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /files",
        "summary": "Upload file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Upload file\n# Upload a file that can be used across various endpoints. Individual files can be up to 512 MB, and the size of all files uploaded by one organization can be up to 1 TB.\n\nThe Assistants API supports files up to 2 million tokens and of specific file types. See the [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools) for details.\n\nThe Fine-tuning API only supports `.jsonl` files. The input also has certain required formats for fine-tuning [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input) or [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input) models.\n\nThe Batch API only supports `.jsonl` files up to 200 MB in size. The input also has a specific required [format](https://platform.openai.com/docs/api-reference/batch/request-input).\n\nPlease [contact us](https://help.openai.com/) if you need to increase these storage limits.\n\n\nurl = f'{base_url}/files'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7908640464394203893",
      "title": "OpenAPI: Delete file",
      "content": "import requests\n\n# Delete file\n# Delete a file.\n\nurl = f'{base_url}/files/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /files/{file_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /files/{file_id}",
        "summary": "Delete file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete file\n# Delete a file.\n\nurl = f'{base_url}/files/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1134663967141540473",
      "title": "OpenAPI: Retrieve file",
      "content": "import requests\n\n# Retrieve file\n# Returns information about a specific file.\n\nurl = f'{base_url}/files/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /files/{file_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /files/{file_id}",
        "summary": "Retrieve file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve file\n# Returns information about a specific file.\n\nurl = f'{base_url}/files/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_97606234734657920",
      "title": "OpenAPI: Retrieve file content",
      "content": "import requests\n\n# Retrieve file content\n# Returns the contents of the specified file.\n\nurl = f'{base_url}/files/{None}/content'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /files/{file_id}/content",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /files/{file_id}/content",
        "summary": "Retrieve file content",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve file content\n# Returns the contents of the specified file.\n\nurl = f'{base_url}/files/{None}/content'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3993899317534995042",
      "title": "OpenAPI: Run grader",
      "content": "import requests\n\n# Run grader\n# Run a grader.\n\n\nurl = f'{base_url}/fine_tuning/alpha/graders/run'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /fine_tuning/alpha/graders/run",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /fine_tuning/alpha/graders/run",
        "summary": "Run grader",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Run grader\n# Run a grader.\n\n\nurl = f'{base_url}/fine_tuning/alpha/graders/run'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6049593702614131133",
      "title": "OpenAPI: Validate grader",
      "content": "import requests\n\n# Validate grader\n# Validate a grader.\n\n\nurl = f'{base_url}/fine_tuning/alpha/graders/validate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /fine_tuning/alpha/graders/validate",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /fine_tuning/alpha/graders/validate",
        "summary": "Validate grader",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Validate grader\n# Validate a grader.\n\n\nurl = f'{base_url}/fine_tuning/alpha/graders/validate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-4973928707178177571",
      "title": "OpenAPI: List checkpoint permissions",
      "content": "import requests\n\n# List checkpoint permissions\n# **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).\n\nOrganization owners can use this endpoint to view all permissions for a fine-tuned model checkpoint.\n\n\nurl = f'{base_url}/fine_tuning/checkpoints/{None}/permissions'\nparams = {\n    'project_id': None,\n    'after': None,\n    'limit': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /fine_tuning/checkpoints/{fine_tuned_model_checkpoint}/permissions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /fine_tuning/checkpoints/{fine_tuned_model_checkpoint}/permissions",
        "summary": "List checkpoint permissions",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List checkpoint permissions\n# **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).\n\nOrganization owners can use this endpoint to view all permissions for a fine-tuned model checkpoint.\n\n\nurl = f'{base_url}/fine_tuning/checkpoints/{None}/permissions'\nparams = {\n    'project_id': None,\n    'after': None,\n    'limit': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4828888709305683927",
      "title": "OpenAPI: Create checkpoint permissions",
      "content": "import requests\n\n# Create checkpoint permissions\n# **NOTE:** Calling this endpoint requires an [admin API key](../admin-api-keys).\n\nThis enables organization owners to share fine-tuned models with other projects in their organization.\n\n\nurl = f'{base_url}/fine_tuning/checkpoints/{None}/permissions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /fine_tuning/checkpoints/{fine_tuned_model_checkpoint}/permissions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /fine_tuning/checkpoints/{fine_tuned_model_checkpoint}/permissions",
        "summary": "Create checkpoint permissions",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create checkpoint permissions\n# **NOTE:** Calling this endpoint requires an [admin API key](../admin-api-keys).\n\nThis enables organization owners to share fine-tuned models with other projects in their organization.\n\n\nurl = f'{base_url}/fine_tuning/checkpoints/{None}/permissions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4608502482034920853",
      "title": "OpenAPI: Delete checkpoint permission",
      "content": "import requests\n\n# Delete checkpoint permission\n# **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).\n\nOrganization owners can use this endpoint to delete a permission for a fine-tuned model checkpoint.\n\n\nurl = f'{base_url}/fine_tuning/checkpoints/{None}/permissions/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /fine_tuning/checkpoints/{fine_tuned_model_checkpoint}/permissions/{permission_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /fine_tuning/checkpoints/{fine_tuned_model_checkpoint}/permissions/{permission_id}",
        "summary": "Delete checkpoint permission",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete checkpoint permission\n# **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).\n\nOrganization owners can use this endpoint to delete a permission for a fine-tuned model checkpoint.\n\n\nurl = f'{base_url}/fine_tuning/checkpoints/{None}/permissions/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1175395456736637246",
      "title": "OpenAPI: Create fine-tuning job",
      "content": "import requests\n\n# Create fine-tuning job\n# Creates a fine-tuning job which begins the process of creating a new model from a given dataset.\n\nResponse includes details of the enqueued job including job status and the name of the fine-tuned models once complete.\n\n[Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)\n\n\nurl = f'{base_url}/fine_tuning/jobs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /fine_tuning/jobs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /fine_tuning/jobs",
        "summary": "Create fine-tuning job",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create fine-tuning job\n# Creates a fine-tuning job which begins the process of creating a new model from a given dataset.\n\nResponse includes details of the enqueued job including job status and the name of the fine-tuned models once complete.\n\n[Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)\n\n\nurl = f'{base_url}/fine_tuning/jobs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_29509983215018417",
      "title": "OpenAPI: List fine-tuning jobs",
      "content": "import requests\n\n# List fine-tuning jobs\n# List your organization's fine-tuning jobs\n\n\nurl = f'{base_url}/fine_tuning/jobs'\nparams = {\n    'after': None,\n    'limit': None,\n    'metadata': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /fine_tuning/jobs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /fine_tuning/jobs",
        "summary": "List fine-tuning jobs",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List fine-tuning jobs\n# List your organization's fine-tuning jobs\n\n\nurl = f'{base_url}/fine_tuning/jobs'\nparams = {\n    'after': None,\n    'limit': None,\n    'metadata': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1467554151532663348",
      "title": "OpenAPI: Retrieve fine-tuning job",
      "content": "import requests\n\n# Retrieve fine-tuning job\n# Get info about a fine-tuning job.\n\n[Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /fine_tuning/jobs/{fine_tuning_job_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /fine_tuning/jobs/{fine_tuning_job_id}",
        "summary": "Retrieve fine-tuning job",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve fine-tuning job\n# Get info about a fine-tuning job.\n\n[Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5026180297586547673",
      "title": "OpenAPI: Cancel fine-tuning",
      "content": "import requests\n\n# Cancel fine-tuning\n# Immediately cancel a fine-tune job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /fine_tuning/jobs/{fine_tuning_job_id}/cancel",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /fine_tuning/jobs/{fine_tuning_job_id}/cancel",
        "summary": "Cancel fine-tuning",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Cancel fine-tuning\n# Immediately cancel a fine-tune job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3640594937066079126",
      "title": "OpenAPI: List fine-tuning checkpoints",
      "content": "import requests\n\n# List fine-tuning checkpoints\n# List checkpoints for a fine-tuning job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/checkpoints'\nparams = {\n    'after': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /fine_tuning/jobs/{fine_tuning_job_id}/checkpoints",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /fine_tuning/jobs/{fine_tuning_job_id}/checkpoints",
        "summary": "List fine-tuning checkpoints",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List fine-tuning checkpoints\n# List checkpoints for a fine-tuning job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/checkpoints'\nparams = {\n    'after': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1168738023935014411",
      "title": "OpenAPI: List fine-tuning events",
      "content": "import requests\n\n# List fine-tuning events\n# Get status updates for a fine-tuning job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/events'\nparams = {\n    'after': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /fine_tuning/jobs/{fine_tuning_job_id}/events",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /fine_tuning/jobs/{fine_tuning_job_id}/events",
        "summary": "List fine-tuning events",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List fine-tuning events\n# Get status updates for a fine-tuning job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/events'\nparams = {\n    'after': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5706202472170710145",
      "title": "OpenAPI: Pause fine-tuning",
      "content": "import requests\n\n# Pause fine-tuning\n# Pause a fine-tune job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/pause'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /fine_tuning/jobs/{fine_tuning_job_id}/pause",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /fine_tuning/jobs/{fine_tuning_job_id}/pause",
        "summary": "Pause fine-tuning",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Pause fine-tuning\n# Pause a fine-tune job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/pause'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3890925105318531083",
      "title": "OpenAPI: Resume fine-tuning",
      "content": "import requests\n\n# Resume fine-tuning\n# Resume a fine-tune job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/resume'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /fine_tuning/jobs/{fine_tuning_job_id}/resume",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /fine_tuning/jobs/{fine_tuning_job_id}/resume",
        "summary": "Resume fine-tuning",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Resume fine-tuning\n# Resume a fine-tune job.\n\n\nurl = f'{base_url}/fine_tuning/jobs/{None}/resume'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_220676823466413407",
      "title": "OpenAPI: Create image edit",
      "content": "import requests\n\n# Create image edit\n# Creates an edited or extended image given one or more source images and a prompt. This endpoint only supports `gpt-image-1` and `dall-e-2`.\n\nurl = f'{base_url}/images/edits'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /images/edits",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /images/edits",
        "summary": "Create image edit",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create image edit\n# Creates an edited or extended image given one or more source images and a prompt. This endpoint only supports `gpt-image-1` and `dall-e-2`.\n\nurl = f'{base_url}/images/edits'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6347049072548955866",
      "title": "OpenAPI: Create image",
      "content": "import requests\n\n# Create image\n# Creates an image given a prompt. [Learn more](https://platform.openai.com/docs/guides/images).\n\n\nurl = f'{base_url}/images/generations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /images/generations",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /images/generations",
        "summary": "Create image",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create image\n# Creates an image given a prompt. [Learn more](https://platform.openai.com/docs/guides/images).\n\n\nurl = f'{base_url}/images/generations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-912065347065384674",
      "title": "OpenAPI: Create image variation",
      "content": "import requests\n\n# Create image variation\n# Creates a variation of a given image. This endpoint only supports `dall-e-2`.\n\nurl = f'{base_url}/images/variations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /images/variations",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /images/variations",
        "summary": "Create image variation",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create image variation\n# Creates a variation of a given image. This endpoint only supports `dall-e-2`.\n\nurl = f'{base_url}/images/variations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1515638596120362182",
      "title": "OpenAPI: List models",
      "content": "import requests\n\n# List models\n# Lists the currently available models, and provides basic information about each one such as the owner and availability.\n\nurl = f'{base_url}/models'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /models",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /models",
        "summary": "List models",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List models\n# Lists the currently available models, and provides basic information about each one such as the owner and availability.\n\nurl = f'{base_url}/models'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6195968236904627733",
      "title": "OpenAPI: Retrieve model",
      "content": "import requests\n\n# Retrieve model\n# Retrieves a model instance, providing basic information about the model such as the owner and permissioning.\n\nurl = f'{base_url}/models/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /models/{model}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /models/{model}",
        "summary": "Retrieve model",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve model\n# Retrieves a model instance, providing basic information about the model such as the owner and permissioning.\n\nurl = f'{base_url}/models/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7947498153779946300",
      "title": "OpenAPI: Delete a fine-tuned model",
      "content": "import requests\n\n# Delete a fine-tuned model\n# Delete a fine-tuned model. You must have the Owner role in your organization to delete a model.\n\nurl = f'{base_url}/models/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /models/{model}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /models/{model}",
        "summary": "Delete a fine-tuned model",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete a fine-tuned model\n# Delete a fine-tuned model. You must have the Owner role in your organization to delete a model.\n\nurl = f'{base_url}/models/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6203960943225476305",
      "title": "OpenAPI: Create moderation",
      "content": "import requests\n\n# Create moderation\n# Classifies if text and/or image inputs are potentially harmful. Learn\nmore in the [moderation guide](https://platform.openai.com/docs/guides/moderation).\n\n\nurl = f'{base_url}/moderations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /moderations",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /moderations",
        "summary": "Create moderation",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create moderation\n# Classifies if text and/or image inputs are potentially harmful. Learn\nmore in the [moderation guide](https://platform.openai.com/docs/guides/moderation).\n\n\nurl = f'{base_url}/moderations'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2971085799206861165",
      "title": "OpenAPI: List all organization and project API keys.",
      "content": "import requests\n\n# List all organization and project API keys.\n# List organization API keys\n\nurl = f'{base_url}/organization/admin_api_keys'\nparams = {\n    'after': None,\n    'order': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/admin_api_keys",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/admin_api_keys",
        "summary": "List all organization and project API keys.",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List all organization and project API keys.\n# List organization API keys\n\nurl = f'{base_url}/organization/admin_api_keys'\nparams = {\n    'after': None,\n    'order': None,\n    'limit': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7615075262761083117",
      "title": "OpenAPI: Create admin API key",
      "content": "import requests\n\n# Create admin API key\n# Create an organization admin API key\n\nurl = f'{base_url}/organization/admin_api_keys'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/admin_api_keys",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/admin_api_keys",
        "summary": "Create admin API key",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create admin API key\n# Create an organization admin API key\n\nurl = f'{base_url}/organization/admin_api_keys'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-4835053794261780085",
      "title": "OpenAPI: Retrieve admin API key",
      "content": "import requests\n\n# Retrieve admin API key\n# Retrieve a single organization API key\n\nurl = f'{base_url}/organization/admin_api_keys/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/admin_api_keys/{key_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/admin_api_keys/{key_id}",
        "summary": "Retrieve admin API key",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve admin API key\n# Retrieve a single organization API key\n\nurl = f'{base_url}/organization/admin_api_keys/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8993788552019562584",
      "title": "OpenAPI: Delete admin API key",
      "content": "import requests\n\n# Delete admin API key\n# Delete an organization admin API key\n\nurl = f'{base_url}/organization/admin_api_keys/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /organization/admin_api_keys/{key_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /organization/admin_api_keys/{key_id}",
        "summary": "Delete admin API key",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete admin API key\n# Delete an organization admin API key\n\nurl = f'{base_url}/organization/admin_api_keys/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5191592349440412939",
      "title": "OpenAPI: List audit logs",
      "content": "import requests\n\n# List audit logs\n# List user actions and configuration changes within this organization.\n\nurl = f'{base_url}/organization/audit_logs'\nparams = {\n    'effective_at': None,\n    'project_ids[]': None,\n    'event_types[]': None,\n    'actor_ids[]': None,\n    'actor_emails[]': None,\n    'resource_ids[]': None,\n    'limit': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/audit_logs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/audit_logs",
        "summary": "List audit logs",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List audit logs\n# List user actions and configuration changes within this organization.\n\nurl = f'{base_url}/organization/audit_logs'\nparams = {\n    'effective_at': None,\n    'project_ids[]': None,\n    'event_types[]': None,\n    'actor_ids[]': None,\n    'actor_emails[]': None,\n    'resource_ids[]': None,\n    'limit': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3620339031689866522",
      "title": "OpenAPI: List organization certificates",
      "content": "import requests\n\n# List organization certificates\n# List uploaded certificates for this organization.\n\nurl = f'{base_url}/organization/certificates'\nparams = {\n    'limit': None,\n    'after': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/certificates",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/certificates",
        "summary": "List organization certificates",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List organization certificates\n# List uploaded certificates for this organization.\n\nurl = f'{base_url}/organization/certificates'\nparams = {\n    'limit': None,\n    'after': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7501576950143157428",
      "title": "OpenAPI: Upload certificate",
      "content": "import requests\n\n# Upload certificate\n# Upload a certificate to the organization. This does **not** automatically activate the certificate.\n\nOrganizations can upload up to 50 certificates.\n\n\nurl = f'{base_url}/organization/certificates'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/certificates",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/certificates",
        "summary": "Upload certificate",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Upload certificate\n# Upload a certificate to the organization. This does **not** automatically activate the certificate.\n\nOrganizations can upload up to 50 certificates.\n\n\nurl = f'{base_url}/organization/certificates'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1093924387007125110",
      "title": "OpenAPI: Activate certificates for organization",
      "content": "import requests\n\n# Activate certificates for organization\n# Activate certificates at the organization level.\n\nYou can atomically and idempotently activate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/certificates/activate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/certificates/activate",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/certificates/activate",
        "summary": "Activate certificates for organization",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Activate certificates for organization\n# Activate certificates at the organization level.\n\nYou can atomically and idempotently activate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/certificates/activate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-8228529466502870580",
      "title": "OpenAPI: Deactivate certificates for organization",
      "content": "import requests\n\n# Deactivate certificates for organization\n# Deactivate certificates at the organization level.\n\nYou can atomically and idempotently deactivate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/certificates/deactivate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/certificates/deactivate",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/certificates/deactivate",
        "summary": "Deactivate certificates for organization",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Deactivate certificates for organization\n# Deactivate certificates at the organization level.\n\nYou can atomically and idempotently deactivate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/certificates/deactivate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6055813989835716941",
      "title": "OpenAPI: Get certificate",
      "content": "import requests\n\n# Get certificate\n# Get a certificate that has been uploaded to the organization.\n\nYou can get a certificate regardless of whether it is active or not.\n\n\nurl = f'{base_url}/organization/certificates/{None}'\nparams = {\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/certificates/{certificate_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/certificates/{certificate_id}",
        "summary": "Get certificate",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get certificate\n# Get a certificate that has been uploaded to the organization.\n\nYou can get a certificate regardless of whether it is active or not.\n\n\nurl = f'{base_url}/organization/certificates/{None}'\nparams = {\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_5005709387512076549",
      "title": "OpenAPI: Modify certificate",
      "content": "import requests\n\n# Modify certificate\n# Modify a certificate. Note that only the name can be modified.\n\n\nurl = f'{base_url}/organization/certificates/{certificate_id}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/certificates/{certificate_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/certificates/{certificate_id}",
        "summary": "Modify certificate",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify certificate\n# Modify a certificate. Note that only the name can be modified.\n\n\nurl = f'{base_url}/organization/certificates/{certificate_id}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6991948521028534303",
      "title": "OpenAPI: Delete certificate",
      "content": "import requests\n\n# Delete certificate\n# Delete a certificate from the organization.\n\nThe certificate must be inactive for the organization and all projects.\n\n\nurl = f'{base_url}/organization/certificates/{certificate_id}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /organization/certificates/{certificate_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /organization/certificates/{certificate_id}",
        "summary": "Delete certificate",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete certificate\n# Delete a certificate from the organization.\n\nThe certificate must be inactive for the organization and all projects.\n\n\nurl = f'{base_url}/organization/certificates/{certificate_id}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1415418175526990060",
      "title": "OpenAPI: Costs",
      "content": "import requests\n\n# Costs\n# Get costs details for the organization.\n\nurl = f'{base_url}/organization/costs'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/costs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/costs",
        "summary": "Costs",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Costs\n# Get costs details for the organization.\n\nurl = f'{base_url}/organization/costs'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_9103832412942694496",
      "title": "OpenAPI: List invites",
      "content": "import requests\n\n# List invites\n# Returns a list of invites in the organization.\n\nurl = f'{base_url}/organization/invites'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/invites",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/invites",
        "summary": "List invites",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List invites\n# Returns a list of invites in the organization.\n\nurl = f'{base_url}/organization/invites'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2218847685618151910",
      "title": "OpenAPI: Create invite",
      "content": "import requests\n\n# Create invite\n# Create an invite for a user to the organization. The invite must be accepted by the user before they have access to the organization.\n\nurl = f'{base_url}/organization/invites'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/invites",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/invites",
        "summary": "Create invite",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create invite\n# Create an invite for a user to the organization. The invite must be accepted by the user before they have access to the organization.\n\nurl = f'{base_url}/organization/invites'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1173360396712494315",
      "title": "OpenAPI: Retrieve invite",
      "content": "import requests\n\n# Retrieve invite\n# Retrieves an invite.\n\nurl = f'{base_url}/organization/invites/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/invites/{invite_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/invites/{invite_id}",
        "summary": "Retrieve invite",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve invite\n# Retrieves an invite.\n\nurl = f'{base_url}/organization/invites/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-8890022968880820897",
      "title": "OpenAPI: Delete invite",
      "content": "import requests\n\n# Delete invite\n# Delete an invite. If the invite has already been accepted, it cannot be deleted.\n\nurl = f'{base_url}/organization/invites/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /organization/invites/{invite_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /organization/invites/{invite_id}",
        "summary": "Delete invite",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete invite\n# Delete an invite. If the invite has already been accepted, it cannot be deleted.\n\nurl = f'{base_url}/organization/invites/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-4771605328806836381",
      "title": "OpenAPI: List projects",
      "content": "import requests\n\n# List projects\n# Returns a list of projects.\n\nurl = f'{base_url}/organization/projects'\nparams = {\n    'limit': None,\n    'after': None,\n    'include_archived': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects",
        "summary": "List projects",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List projects\n# Returns a list of projects.\n\nurl = f'{base_url}/organization/projects'\nparams = {\n    'limit': None,\n    'after': None,\n    'include_archived': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7413156201640889284",
      "title": "OpenAPI: Create project",
      "content": "import requests\n\n# Create project\n# Create a new project in the organization. Projects can be created and archived, but cannot be deleted.\n\nurl = f'{base_url}/organization/projects'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects",
        "summary": "Create project",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create project\n# Create a new project in the organization. Projects can be created and archived, but cannot be deleted.\n\nurl = f'{base_url}/organization/projects'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-213996082475666552",
      "title": "OpenAPI: Retrieve project",
      "content": "import requests\n\n# Retrieve project\n# Retrieves a project.\n\nurl = f'{base_url}/organization/projects/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}",
        "summary": "Retrieve project",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve project\n# Retrieves a project.\n\nurl = f'{base_url}/organization/projects/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_63742439748434817",
      "title": "OpenAPI: Modify project",
      "content": "import requests\n\n# Modify project\n# Modifies a project in the organization.\n\nurl = f'{base_url}/organization/projects/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}",
        "summary": "Modify project",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify project\n# Modifies a project in the organization.\n\nurl = f'{base_url}/organization/projects/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4989878128173939659",
      "title": "OpenAPI: List project API keys",
      "content": "import requests\n\n# List project API keys\n# Returns a list of API keys in the project.\n\nurl = f'{base_url}/organization/projects/{None}/api_keys'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/api_keys",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/api_keys",
        "summary": "List project API keys",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List project API keys\n# Returns a list of API keys in the project.\n\nurl = f'{base_url}/organization/projects/{None}/api_keys'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5499477188697618086",
      "title": "OpenAPI: Retrieve project API key",
      "content": "import requests\n\n# Retrieve project API key\n# Retrieves an API key in the project.\n\nurl = f'{base_url}/organization/projects/{None}/api_keys/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/api_keys/{key_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/api_keys/{key_id}",
        "summary": "Retrieve project API key",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve project API key\n# Retrieves an API key in the project.\n\nurl = f'{base_url}/organization/projects/{None}/api_keys/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6796628860478705628",
      "title": "OpenAPI: Delete project API key",
      "content": "import requests\n\n# Delete project API key\n# Deletes an API key from the project.\n\nurl = f'{base_url}/organization/projects/{None}/api_keys/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /organization/projects/{project_id}/api_keys/{key_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /organization/projects/{project_id}/api_keys/{key_id}",
        "summary": "Delete project API key",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete project API key\n# Deletes an API key from the project.\n\nurl = f'{base_url}/organization/projects/{None}/api_keys/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8436903413480305872",
      "title": "OpenAPI: Archive project",
      "content": "import requests\n\n# Archive project\n# Archives a project in the organization. Archived projects cannot be used or updated.\n\nurl = f'{base_url}/organization/projects/{None}/archive'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}/archive",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}/archive",
        "summary": "Archive project",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Archive project\n# Archives a project in the organization. Archived projects cannot be used or updated.\n\nurl = f'{base_url}/organization/projects/{None}/archive'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6836380682081488194",
      "title": "OpenAPI: List project certificates",
      "content": "import requests\n\n# List project certificates\n# List certificates for this project.\n\nurl = f'{base_url}/organization/projects/{None}/certificates'\nparams = {\n    'limit': None,\n    'after': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/certificates",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/certificates",
        "summary": "List project certificates",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List project certificates\n# List certificates for this project.\n\nurl = f'{base_url}/organization/projects/{None}/certificates'\nparams = {\n    'limit': None,\n    'after': None,\n    'order': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8269562208029436401",
      "title": "OpenAPI: Activate certificates for project",
      "content": "import requests\n\n# Activate certificates for project\n# Activate certificates at the project level.\n\nYou can atomically and idempotently activate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/projects/{None}/certificates/activate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}/certificates/activate",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}/certificates/activate",
        "summary": "Activate certificates for project",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Activate certificates for project\n# Activate certificates at the project level.\n\nYou can atomically and idempotently activate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/projects/{None}/certificates/activate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-4792981047242047938",
      "title": "OpenAPI: Deactivate certificates for project",
      "content": "import requests\n\n# Deactivate certificates for project\n# Deactivate certificates at the project level. You can atomically and \nidempotently deactivate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/projects/{None}/certificates/deactivate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}/certificates/deactivate",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}/certificates/deactivate",
        "summary": "Deactivate certificates for project",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Deactivate certificates for project\n# Deactivate certificates at the project level. You can atomically and \nidempotently deactivate up to 10 certificates at a time.\n\n\nurl = f'{base_url}/organization/projects/{None}/certificates/deactivate'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1313335251483904046",
      "title": "OpenAPI: List project rate limits",
      "content": "import requests\n\n# List project rate limits\n# Returns the rate limits per model for a project.\n\nurl = f'{base_url}/organization/projects/{None}/rate_limits'\nparams = {\n    'limit': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/rate_limits",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/rate_limits",
        "summary": "List project rate limits",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List project rate limits\n# Returns the rate limits per model for a project.\n\nurl = f'{base_url}/organization/projects/{None}/rate_limits'\nparams = {\n    'limit': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-8540214347061052523",
      "title": "OpenAPI: Modify project rate limit",
      "content": "import requests\n\n# Modify project rate limit\n# Updates a project rate limit.\n\nurl = f'{base_url}/organization/projects/{None}/rate_limits/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}/rate_limits/{rate_limit_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}/rate_limits/{rate_limit_id}",
        "summary": "Modify project rate limit",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify project rate limit\n# Updates a project rate limit.\n\nurl = f'{base_url}/organization/projects/{None}/rate_limits/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3741509597124942434",
      "title": "OpenAPI: List project service accounts",
      "content": "import requests\n\n# List project service accounts\n# Returns a list of service accounts in the project.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/service_accounts",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/service_accounts",
        "summary": "List project service accounts",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List project service accounts\n# Returns a list of service accounts in the project.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5812925881236500208",
      "title": "OpenAPI: Create project service account",
      "content": "import requests\n\n# Create project service account\n# Creates a new service account in the project. This also returns an unredacted API key for the service account.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}/service_accounts",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}/service_accounts",
        "summary": "Create project service account",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create project service account\n# Creates a new service account in the project. This also returns an unredacted API key for the service account.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3128033544318613997",
      "title": "OpenAPI: Retrieve project service account",
      "content": "import requests\n\n# Retrieve project service account\n# Retrieves a service account in the project.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/service_accounts/{service_account_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/service_accounts/{service_account_id}",
        "summary": "Retrieve project service account",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve project service account\n# Retrieves a service account in the project.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_9039583717464113456",
      "title": "OpenAPI: Delete project service account",
      "content": "import requests\n\n# Delete project service account\n# Deletes a service account from the project.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /organization/projects/{project_id}/service_accounts/{service_account_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /organization/projects/{project_id}/service_accounts/{service_account_id}",
        "summary": "Delete project service account",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete project service account\n# Deletes a service account from the project.\n\nurl = f'{base_url}/organization/projects/{None}/service_accounts/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1926796857004478829",
      "title": "OpenAPI: List project users",
      "content": "import requests\n\n# List project users\n# Returns a list of users in the project.\n\nurl = f'{base_url}/organization/projects/{None}/users'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/users",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/users",
        "summary": "List project users",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List project users\n# Returns a list of users in the project.\n\nurl = f'{base_url}/organization/projects/{None}/users'\nparams = {\n    'limit': None,\n    'after': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1498894925364818637",
      "title": "OpenAPI: Create project user",
      "content": "import requests\n\n# Create project user\n# Adds a user to the project. Users must already be members of the organization to be added to a project.\n\nurl = f'{base_url}/organization/projects/{None}/users'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}/users",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}/users",
        "summary": "Create project user",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create project user\n# Adds a user to the project. Users must already be members of the organization to be added to a project.\n\nurl = f'{base_url}/organization/projects/{None}/users'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-973485195103956294",
      "title": "OpenAPI: Retrieve project user",
      "content": "import requests\n\n# Retrieve project user\n# Retrieves a user in the project.\n\nurl = f'{base_url}/organization/projects/{None}/users/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/projects/{project_id}/users/{user_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/projects/{project_id}/users/{user_id}",
        "summary": "Retrieve project user",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve project user\n# Retrieves a user in the project.\n\nurl = f'{base_url}/organization/projects/{None}/users/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6540226557052626149",
      "title": "OpenAPI: Modify project user",
      "content": "import requests\n\n# Modify project user\n# Modifies a user's role in the project.\n\nurl = f'{base_url}/organization/projects/{None}/users/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/projects/{project_id}/users/{user_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/projects/{project_id}/users/{user_id}",
        "summary": "Modify project user",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify project user\n# Modifies a user's role in the project.\n\nurl = f'{base_url}/organization/projects/{None}/users/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-4059564310602182253",
      "title": "OpenAPI: Delete project user",
      "content": "import requests\n\n# Delete project user\n# Deletes a user from the project.\n\nurl = f'{base_url}/organization/projects/{None}/users/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /organization/projects/{project_id}/users/{user_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /organization/projects/{project_id}/users/{user_id}",
        "summary": "Delete project user",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete project user\n# Deletes a user from the project.\n\nurl = f'{base_url}/organization/projects/{None}/users/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2435200154901655495",
      "title": "OpenAPI: Audio speeches",
      "content": "import requests\n\n# Audio speeches\n# Get audio speeches usage details for the organization.\n\nurl = f'{base_url}/organization/usage/audio_speeches'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/audio_speeches",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/audio_speeches",
        "summary": "Audio speeches",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Audio speeches\n# Get audio speeches usage details for the organization.\n\nurl = f'{base_url}/organization/usage/audio_speeches'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-575242078221017162",
      "title": "OpenAPI: Audio transcriptions",
      "content": "import requests\n\n# Audio transcriptions\n# Get audio transcriptions usage details for the organization.\n\nurl = f'{base_url}/organization/usage/audio_transcriptions'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/audio_transcriptions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/audio_transcriptions",
        "summary": "Audio transcriptions",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Audio transcriptions\n# Get audio transcriptions usage details for the organization.\n\nurl = f'{base_url}/organization/usage/audio_transcriptions'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2610984944772345220",
      "title": "OpenAPI: Code interpreter sessions",
      "content": "import requests\n\n# Code interpreter sessions\n# Get code interpreter sessions usage details for the organization.\n\nurl = f'{base_url}/organization/usage/code_interpreter_sessions'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/code_interpreter_sessions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/code_interpreter_sessions",
        "summary": "Code interpreter sessions",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Code interpreter sessions\n# Get code interpreter sessions usage details for the organization.\n\nurl = f'{base_url}/organization/usage/code_interpreter_sessions'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4421734207423555654",
      "title": "OpenAPI: Completions",
      "content": "import requests\n\n# Completions\n# Get completions usage details for the organization.\n\nurl = f'{base_url}/organization/usage/completions'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'batch': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/completions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/completions",
        "summary": "Completions",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Completions\n# Get completions usage details for the organization.\n\nurl = f'{base_url}/organization/usage/completions'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'batch': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6511886860512274882",
      "title": "OpenAPI: Embeddings",
      "content": "import requests\n\n# Embeddings\n# Get embeddings usage details for the organization.\n\nurl = f'{base_url}/organization/usage/embeddings'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/embeddings",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/embeddings",
        "summary": "Embeddings",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Embeddings\n# Get embeddings usage details for the organization.\n\nurl = f'{base_url}/organization/usage/embeddings'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-336412397525033133",
      "title": "OpenAPI: Images",
      "content": "import requests\n\n# Images\n# Get images usage details for the organization.\n\nurl = f'{base_url}/organization/usage/images'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'sources': None,\n    'sizes': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/images",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/images",
        "summary": "Images",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Images\n# Get images usage details for the organization.\n\nurl = f'{base_url}/organization/usage/images'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'sources': None,\n    'sizes': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5591987488603505250",
      "title": "OpenAPI: Moderations",
      "content": "import requests\n\n# Moderations\n# Get moderations usage details for the organization.\n\nurl = f'{base_url}/organization/usage/moderations'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/moderations",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/moderations",
        "summary": "Moderations",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Moderations\n# Get moderations usage details for the organization.\n\nurl = f'{base_url}/organization/usage/moderations'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'user_ids': None,\n    'api_key_ids': None,\n    'models': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-991042480038377747",
      "title": "OpenAPI: Vector stores",
      "content": "import requests\n\n# Vector stores\n# Get vector stores usage details for the organization.\n\nurl = f'{base_url}/organization/usage/vector_stores'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/usage/vector_stores",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/usage/vector_stores",
        "summary": "Vector stores",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Vector stores\n# Get vector stores usage details for the organization.\n\nurl = f'{base_url}/organization/usage/vector_stores'\nparams = {\n    'start_time': None,\n    'end_time': None,\n    'bucket_width': None,\n    'project_ids': None,\n    'group_by': None,\n    'limit': None,\n    'page': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6964992807835700185",
      "title": "OpenAPI: List users",
      "content": "import requests\n\n# List users\n# Lists all of the users in the organization.\n\nurl = f'{base_url}/organization/users'\nparams = {\n    'limit': None,\n    'after': None,\n    'emails': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/users",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/users",
        "summary": "List users",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List users\n# Lists all of the users in the organization.\n\nurl = f'{base_url}/organization/users'\nparams = {\n    'limit': None,\n    'after': None,\n    'emails': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8171065197653609305",
      "title": "OpenAPI: Retrieve user",
      "content": "import requests\n\n# Retrieve user\n# Retrieves a user by their identifier.\n\nurl = f'{base_url}/organization/users/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /organization/users/{user_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /organization/users/{user_id}",
        "summary": "Retrieve user",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve user\n# Retrieves a user by their identifier.\n\nurl = f'{base_url}/organization/users/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5301342519195203947",
      "title": "OpenAPI: Modify user",
      "content": "import requests\n\n# Modify user\n# Modifies a user's role in the organization.\n\nurl = f'{base_url}/organization/users/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /organization/users/{user_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /organization/users/{user_id}",
        "summary": "Modify user",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify user\n# Modifies a user's role in the organization.\n\nurl = f'{base_url}/organization/users/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1525784375727872806",
      "title": "OpenAPI: Delete user",
      "content": "import requests\n\n# Delete user\n# Deletes a user from the organization.\n\nurl = f'{base_url}/organization/users/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /organization/users/{user_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /organization/users/{user_id}",
        "summary": "Delete user",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete user\n# Deletes a user from the organization.\n\nurl = f'{base_url}/organization/users/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8518650921799337747",
      "title": "OpenAPI: Create session",
      "content": "import requests\n\n# Create session\n# Create an ephemeral API token for use in client-side applications with the\nRealtime API. Can be configured with the same session parameters as the\n`session.update` client event.\n\nIt responds with a session object, plus a `client_secret` key which contains\na usable ephemeral API token that can be used to authenticate browser clients\nfor the Realtime API.\n\n\nurl = f'{base_url}/realtime/sessions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /realtime/sessions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /realtime/sessions",
        "summary": "Create session",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create session\n# Create an ephemeral API token for use in client-side applications with the\nRealtime API. Can be configured with the same session parameters as the\n`session.update` client event.\n\nIt responds with a session object, plus a `client_secret` key which contains\na usable ephemeral API token that can be used to authenticate browser clients\nfor the Realtime API.\n\n\nurl = f'{base_url}/realtime/sessions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2661855294921076465",
      "title": "OpenAPI: Create transcription session",
      "content": "import requests\n\n# Create transcription session\n# Create an ephemeral API token for use in client-side applications with the\nRealtime API specifically for realtime transcriptions. \nCan be configured with the same session parameters as the `transcription_session.update` client event.\n\nIt responds with a session object, plus a `client_secret` key which contains\na usable ephemeral API token that can be used to authenticate browser clients\nfor the Realtime API.\n\n\nurl = f'{base_url}/realtime/transcription_sessions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /realtime/transcription_sessions",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /realtime/transcription_sessions",
        "summary": "Create transcription session",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create transcription session\n# Create an ephemeral API token for use in client-side applications with the\nRealtime API specifically for realtime transcriptions. \nCan be configured with the same session parameters as the `transcription_session.update` client event.\n\nIt responds with a session object, plus a `client_secret` key which contains\na usable ephemeral API token that can be used to authenticate browser clients\nfor the Realtime API.\n\n\nurl = f'{base_url}/realtime/transcription_sessions'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4109303703562907926",
      "title": "OpenAPI: Create a model response",
      "content": "import requests\n\n# Create a model response\n# Creates a model response. Provide [text](https://platform.openai.com/docs/guides/text) or\n[image](https://platform.openai.com/docs/guides/images) inputs to generate [text](https://platform.openai.com/docs/guides/text)\nor [JSON](https://platform.openai.com/docs/guides/structured-outputs) outputs. Have the model call\nyour own [custom code](https://platform.openai.com/docs/guides/function-calling) or use built-in\n[tools](https://platform.openai.com/docs/guides/tools) like [web search](https://platform.openai.com/docs/guides/tools-web-search)\nor [file search](https://platform.openai.com/docs/guides/tools-file-search) to use your own data\nas input for the model's response.\n\n\nurl = f'{base_url}/responses'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /responses",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /responses",
        "summary": "Create a model response",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create a model response\n# Creates a model response. Provide [text](https://platform.openai.com/docs/guides/text) or\n[image](https://platform.openai.com/docs/guides/images) inputs to generate [text](https://platform.openai.com/docs/guides/text)\nor [JSON](https://platform.openai.com/docs/guides/structured-outputs) outputs. Have the model call\nyour own [custom code](https://platform.openai.com/docs/guides/function-calling) or use built-in\n[tools](https://platform.openai.com/docs/guides/tools) like [web search](https://platform.openai.com/docs/guides/tools-web-search)\nor [file search](https://platform.openai.com/docs/guides/tools-file-search) to use your own data\nas input for the model's response.\n\n\nurl = f'{base_url}/responses'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6004169094991321265",
      "title": "OpenAPI: Get a model response",
      "content": "import requests\n\n# Get a model response\n# Retrieves a model response with the given ID.\n\n\nurl = f'{base_url}/responses/{None}'\nparams = {\n    'include': None,\n    'stream': None,\n    'starting_after': None,\n    'include_obfuscation': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /responses/{response_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /responses/{response_id}",
        "summary": "Get a model response",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Get a model response\n# Retrieves a model response with the given ID.\n\n\nurl = f'{base_url}/responses/{None}'\nparams = {\n    'include': None,\n    'stream': None,\n    'starting_after': None,\n    'include_obfuscation': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7959621233393873102",
      "title": "OpenAPI: Delete a model response",
      "content": "import requests\n\n# Delete a model response\n# Deletes a model response with the given ID.\n\n\nurl = f'{base_url}/responses/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /responses/{response_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /responses/{response_id}",
        "summary": "Delete a model response",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete a model response\n# Deletes a model response with the given ID.\n\n\nurl = f'{base_url}/responses/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6350492989719345817",
      "title": "OpenAPI: Cancel a response",
      "content": "import requests\n\n# Cancel a response\n# Cancels a model response with the given ID. Only responses created with\nthe `background` parameter set to `true` can be cancelled. \n[Learn more](https://platform.openai.com/docs/guides/background).\n\n\nurl = f'{base_url}/responses/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /responses/{response_id}/cancel",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /responses/{response_id}/cancel",
        "summary": "Cancel a response",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Cancel a response\n# Cancels a model response with the given ID. Only responses created with\nthe `background` parameter set to `true` can be cancelled. \n[Learn more](https://platform.openai.com/docs/guides/background).\n\n\nurl = f'{base_url}/responses/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_5327878585425481337",
      "title": "OpenAPI: List input items",
      "content": "import requests\n\n# List input items\n# Returns a list of input items for a given response.\n\nurl = f'{base_url}/responses/{None}/input_items'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /responses/{response_id}/input_items",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /responses/{response_id}/input_items",
        "summary": "List input items",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List input items\n# Returns a list of input items for a given response.\n\nurl = f'{base_url}/responses/{None}/input_items'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'include': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2703524475131405164",
      "title": "OpenAPI: Create thread",
      "content": "import requests\n\n# Create thread\n# Create a thread.\n\nurl = f'{base_url}/threads'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads",
        "summary": "Create thread",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create thread\n# Create a thread.\n\nurl = f'{base_url}/threads'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8593411785921112727",
      "title": "OpenAPI: Create thread and run",
      "content": "import requests\n\n# Create thread and run\n# Create a thread and run it in one request.\n\nurl = f'{base_url}/threads/runs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/runs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/runs",
        "summary": "Create thread and run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create thread and run\n# Create a thread and run it in one request.\n\nurl = f'{base_url}/threads/runs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-619427382581252781",
      "title": "OpenAPI: Retrieve thread",
      "content": "import requests\n\n# Retrieve thread\n# Retrieves a thread.\n\nurl = f'{base_url}/threads/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /threads/{thread_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /threads/{thread_id}",
        "summary": "Retrieve thread",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve thread\n# Retrieves a thread.\n\nurl = f'{base_url}/threads/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-1613726773960353769",
      "title": "OpenAPI: Modify thread",
      "content": "import requests\n\n# Modify thread\n# Modifies a thread.\n\nurl = f'{base_url}/threads/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/{thread_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/{thread_id}",
        "summary": "Modify thread",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify thread\n# Modifies a thread.\n\nurl = f'{base_url}/threads/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2914281595844358391",
      "title": "OpenAPI: Delete thread",
      "content": "import requests\n\n# Delete thread\n# Delete a thread.\n\nurl = f'{base_url}/threads/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /threads/{thread_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /threads/{thread_id}",
        "summary": "Delete thread",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete thread\n# Delete a thread.\n\nurl = f'{base_url}/threads/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2772605490662968824",
      "title": "OpenAPI: List messages",
      "content": "import requests\n\n# List messages\n# Returns a list of messages for a given thread.\n\nurl = f'{base_url}/threads/{None}/messages'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'run_id': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /threads/{thread_id}/messages",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /threads/{thread_id}/messages",
        "summary": "List messages",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List messages\n# Returns a list of messages for a given thread.\n\nurl = f'{base_url}/threads/{None}/messages'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'run_id': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2797002315237426041",
      "title": "OpenAPI: Create message",
      "content": "import requests\n\n# Create message\n# Create a message.\n\nurl = f'{base_url}/threads/{None}/messages'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/{thread_id}/messages",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/{thread_id}/messages",
        "summary": "Create message",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create message\n# Create a message.\n\nurl = f'{base_url}/threads/{None}/messages'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2367392798373706074",
      "title": "OpenAPI: Retrieve message",
      "content": "import requests\n\n# Retrieve message\n# Retrieve a message.\n\nurl = f'{base_url}/threads/{None}/messages/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /threads/{thread_id}/messages/{message_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /threads/{thread_id}/messages/{message_id}",
        "summary": "Retrieve message",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve message\n# Retrieve a message.\n\nurl = f'{base_url}/threads/{None}/messages/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7619556359196596164",
      "title": "OpenAPI: Modify message",
      "content": "import requests\n\n# Modify message\n# Modifies a message.\n\nurl = f'{base_url}/threads/{None}/messages/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/{thread_id}/messages/{message_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/{thread_id}/messages/{message_id}",
        "summary": "Modify message",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify message\n# Modifies a message.\n\nurl = f'{base_url}/threads/{None}/messages/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_5876798675423923975",
      "title": "OpenAPI: Delete message",
      "content": "import requests\n\n# Delete message\n# Deletes a message.\n\nurl = f'{base_url}/threads/{None}/messages/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /threads/{thread_id}/messages/{message_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /threads/{thread_id}/messages/{message_id}",
        "summary": "Delete message",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete message\n# Deletes a message.\n\nurl = f'{base_url}/threads/{None}/messages/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_3839190613801268197",
      "title": "OpenAPI: List runs",
      "content": "import requests\n\n# List runs\n# Returns a list of runs belonging to a thread.\n\nurl = f'{base_url}/threads/{None}/runs'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /threads/{thread_id}/runs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /threads/{thread_id}/runs",
        "summary": "List runs",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List runs\n# Returns a list of runs belonging to a thread.\n\nurl = f'{base_url}/threads/{None}/runs'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-7765829412205771199",
      "title": "OpenAPI: Create run",
      "content": "import requests\n\n# Create run\n# Create a run.\n\nurl = f'{base_url}/threads/{None}/runs'\nparams = {\n    'include[]': None\n}\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, params=params, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/{thread_id}/runs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/{thread_id}/runs",
        "summary": "Create run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create run\n# Create a run.\n\nurl = f'{base_url}/threads/{None}/runs'\nparams = {\n    'include[]': None\n}\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, params=params, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6496912841947202214",
      "title": "OpenAPI: Retrieve run",
      "content": "import requests\n\n# Retrieve run\n# Retrieves a run.\n\nurl = f'{base_url}/threads/{None}/runs/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /threads/{thread_id}/runs/{run_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /threads/{thread_id}/runs/{run_id}",
        "summary": "Retrieve run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve run\n# Retrieves a run.\n\nurl = f'{base_url}/threads/{None}/runs/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_112401359248677810",
      "title": "OpenAPI: Modify run",
      "content": "import requests\n\n# Modify run\n# Modifies a run.\n\nurl = f'{base_url}/threads/{None}/runs/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/{thread_id}/runs/{run_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/{thread_id}/runs/{run_id}",
        "summary": "Modify run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify run\n# Modifies a run.\n\nurl = f'{base_url}/threads/{None}/runs/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8628663159879410072",
      "title": "OpenAPI: Cancel a run",
      "content": "import requests\n\n# Cancel a run\n# Cancels a run that is `in_progress`.\n\nurl = f'{base_url}/threads/{None}/runs/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/{thread_id}/runs/{run_id}/cancel",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/{thread_id}/runs/{run_id}/cancel",
        "summary": "Cancel a run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Cancel a run\n# Cancels a run that is `in_progress`.\n\nurl = f'{base_url}/threads/{None}/runs/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_8371090251544603845",
      "title": "OpenAPI: List run steps",
      "content": "import requests\n\n# List run steps\n# Returns a list of run steps belonging to a run.\n\nurl = f'{base_url}/threads/{None}/runs/{None}/steps'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'include[]': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /threads/{thread_id}/runs/{run_id}/steps",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /threads/{thread_id}/runs/{run_id}/steps",
        "summary": "List run steps",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List run steps\n# Returns a list of run steps belonging to a run.\n\nurl = f'{base_url}/threads/{None}/runs/{None}/steps'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'include[]': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5389705525758625572",
      "title": "OpenAPI: Retrieve run step",
      "content": "import requests\n\n# Retrieve run step\n# Retrieves a run step.\n\nurl = f'{base_url}/threads/{None}/runs/{None}/steps/{None}'\nparams = {\n    'include[]': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /threads/{thread_id}/runs/{run_id}/steps/{step_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /threads/{thread_id}/runs/{run_id}/steps/{step_id}",
        "summary": "Retrieve run step",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve run step\n# Retrieves a run step.\n\nurl = f'{base_url}/threads/{None}/runs/{None}/steps/{None}'\nparams = {\n    'include[]': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-3316136234112887385",
      "title": "OpenAPI: Submit tool outputs to run",
      "content": "import requests\n\n# Submit tool outputs to run\n# When a run has the `status: \"requires_action\"` and `required_action.type` is `submit_tool_outputs`, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.\n\n\nurl = f'{base_url}/threads/{None}/runs/{None}/submit_tool_outputs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /threads/{thread_id}/runs/{run_id}/submit_tool_outputs",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /threads/{thread_id}/runs/{run_id}/submit_tool_outputs",
        "summary": "Submit tool outputs to run",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Submit tool outputs to run\n# When a run has the `status: \"requires_action\"` and `required_action.type` is `submit_tool_outputs`, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.\n\n\nurl = f'{base_url}/threads/{None}/runs/{None}/submit_tool_outputs'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6578760992379639932",
      "title": "OpenAPI: Create upload",
      "content": "import requests\n\n# Create upload\n# Creates an intermediate [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object\nthat you can add [Parts](https://platform.openai.com/docs/api-reference/uploads/part-object) to.\nCurrently, an Upload can accept at most 8 GB in total and expires after an\nhour after you create it.\n\nOnce you complete the Upload, we will create a\n[File](https://platform.openai.com/docs/api-reference/files/object) object that contains all the parts\nyou uploaded. This File is usable in the rest of our platform as a regular\nFile object.\n\nFor certain `purpose` values, the correct `mime_type` must be specified. \nPlease refer to documentation for the \n[supported MIME types for your use case](https://platform.openai.com/docs/assistants/tools/file-search#supported-files).\n\nFor guidance on the proper filename extensions for each purpose, please\nfollow the documentation on [creating a\nFile](https://platform.openai.com/docs/api-reference/files/create).\n\n\nurl = f'{base_url}/uploads'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /uploads",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /uploads",
        "summary": "Create upload",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create upload\n# Creates an intermediate [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object\nthat you can add [Parts](https://platform.openai.com/docs/api-reference/uploads/part-object) to.\nCurrently, an Upload can accept at most 8 GB in total and expires after an\nhour after you create it.\n\nOnce you complete the Upload, we will create a\n[File](https://platform.openai.com/docs/api-reference/files/object) object that contains all the parts\nyou uploaded. This File is usable in the rest of our platform as a regular\nFile object.\n\nFor certain `purpose` values, the correct `mime_type` must be specified. \nPlease refer to documentation for the \n[supported MIME types for your use case](https://platform.openai.com/docs/assistants/tools/file-search#supported-files).\n\nFor guidance on the proper filename extensions for each purpose, please\nfollow the documentation on [creating a\nFile](https://platform.openai.com/docs/api-reference/files/create).\n\n\nurl = f'{base_url}/uploads'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5497333063283374583",
      "title": "OpenAPI: Cancel upload",
      "content": "import requests\n\n# Cancel upload\n# Cancels the Upload. No Parts may be added after an Upload is cancelled.\n\n\nurl = f'{base_url}/uploads/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /uploads/{upload_id}/cancel",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /uploads/{upload_id}/cancel",
        "summary": "Cancel upload",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Cancel upload\n# Cancels the Upload. No Parts may be added after an Upload is cancelled.\n\n\nurl = f'{base_url}/uploads/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1324243630129802188",
      "title": "OpenAPI: Complete upload",
      "content": "import requests\n\n# Complete upload\n# Completes the [Upload](https://platform.openai.com/docs/api-reference/uploads/object). \n\nWithin the returned Upload object, there is a nested [File](https://platform.openai.com/docs/api-reference/files/object) object that is ready to use in the rest of the platform.\n\nYou can specify the order of the Parts by passing in an ordered list of the Part IDs.\n\nThe number of bytes uploaded upon completion must match the number of bytes initially specified when creating the Upload object. No Parts may be added after an Upload is completed.\n\n\nurl = f'{base_url}/uploads/{None}/complete'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /uploads/{upload_id}/complete",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /uploads/{upload_id}/complete",
        "summary": "Complete upload",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Complete upload\n# Completes the [Upload](https://platform.openai.com/docs/api-reference/uploads/object). \n\nWithin the returned Upload object, there is a nested [File](https://platform.openai.com/docs/api-reference/files/object) object that is ready to use in the rest of the platform.\n\nYou can specify the order of the Parts by passing in an ordered list of the Part IDs.\n\nThe number of bytes uploaded upon completion must match the number of bytes initially specified when creating the Upload object. No Parts may be added after an Upload is completed.\n\n\nurl = f'{base_url}/uploads/{None}/complete'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6246743574983894197",
      "title": "OpenAPI: Add upload part",
      "content": "import requests\n\n# Add upload part\n# Adds a [Part](https://platform.openai.com/docs/api-reference/uploads/part-object) to an [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object. A Part represents a chunk of bytes from the file you are trying to upload. \n\nEach Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.\n\nIt is possible to add multiple Parts in parallel. You can decide the intended order of the Parts when you [complete the Upload](https://platform.openai.com/docs/api-reference/uploads/complete).\n\n\nurl = f'{base_url}/uploads/{None}/parts'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /uploads/{upload_id}/parts",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /uploads/{upload_id}/parts",
        "summary": "Add upload part",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Add upload part\n# Adds a [Part](https://platform.openai.com/docs/api-reference/uploads/part-object) to an [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object. A Part represents a chunk of bytes from the file you are trying to upload. \n\nEach Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.\n\nIt is possible to add multiple Parts in parallel. You can decide the intended order of the Parts when you [complete the Upload](https://platform.openai.com/docs/api-reference/uploads/complete).\n\n\nurl = f'{base_url}/uploads/{None}/parts'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7905355366804833302",
      "title": "OpenAPI: List vector stores",
      "content": "import requests\n\n# List vector stores\n# Returns a list of vector stores.\n\nurl = f'{base_url}/vector_stores'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /vector_stores",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /vector_stores",
        "summary": "List vector stores",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List vector stores\n# Returns a list of vector stores.\n\nurl = f'{base_url}/vector_stores'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-2252415587039467428",
      "title": "OpenAPI: Create vector store",
      "content": "import requests\n\n# Create vector store\n# Create a vector store.\n\nurl = f'{base_url}/vector_stores'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /vector_stores",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /vector_stores",
        "summary": "Create vector store",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create vector store\n# Create a vector store.\n\nurl = f'{base_url}/vector_stores'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_2969229545405771399",
      "title": "OpenAPI: Retrieve vector store",
      "content": "import requests\n\n# Retrieve vector store\n# Retrieves a vector store.\n\nurl = f'{base_url}/vector_stores/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /vector_stores/{vector_store_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /vector_stores/{vector_store_id}",
        "summary": "Retrieve vector store",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve vector store\n# Retrieves a vector store.\n\nurl = f'{base_url}/vector_stores/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_9868722781964155",
      "title": "OpenAPI: Modify vector store",
      "content": "import requests\n\n# Modify vector store\n# Modifies a vector store.\n\nurl = f'{base_url}/vector_stores/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /vector_stores/{vector_store_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /vector_stores/{vector_store_id}",
        "summary": "Modify vector store",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Modify vector store\n# Modifies a vector store.\n\nurl = f'{base_url}/vector_stores/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4017104593124550086",
      "title": "OpenAPI: Delete vector store",
      "content": "import requests\n\n# Delete vector store\n# Delete a vector store.\n\nurl = f'{base_url}/vector_stores/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /vector_stores/{vector_store_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /vector_stores/{vector_store_id}",
        "summary": "Delete vector store",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete vector store\n# Delete a vector store.\n\nurl = f'{base_url}/vector_stores/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1510560738692759320",
      "title": "OpenAPI: Create vector store file batch",
      "content": "import requests\n\n# Create vector store file batch\n# Create a vector store file batch.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /vector_stores/{vector_store_id}/file_batches",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /vector_stores/{vector_store_id}/file_batches",
        "summary": "Create vector store file batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create vector store file batch\n# Create a vector store file batch.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1141626608809010149",
      "title": "OpenAPI: Retrieve vector store file batch",
      "content": "import requests\n\n# Retrieve vector store file batch\n# Retrieves a vector store file batch.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /vector_stores/{vector_store_id}/file_batches/{batch_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /vector_stores/{vector_store_id}/file_batches/{batch_id}",
        "summary": "Retrieve vector store file batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve vector store file batch\n# Retrieves a vector store file batch.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-5620129133481124610",
      "title": "OpenAPI: Cancel vector store file batch",
      "content": "import requests\n\n# Cancel vector store file batch\n# Cancel a vector store file batch. This attempts to cancel the processing of files in this batch as soon as possible.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel",
        "summary": "Cancel vector store file batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Cancel vector store file batch\n# Cancel a vector store file batch. This attempts to cancel the processing of files in this batch as soon as possible.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches/{None}/cancel'\n\nresponse = requests.post(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_7099419579384864856",
      "title": "OpenAPI: List vector store files in a batch",
      "content": "import requests\n\n# List vector store files in a batch\n# Returns a list of vector store files in a batch.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches/{None}/files'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'filter': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /vector_stores/{vector_store_id}/file_batches/{batch_id}/files",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /vector_stores/{vector_store_id}/file_batches/{batch_id}/files",
        "summary": "List vector store files in a batch",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List vector store files in a batch\n# Returns a list of vector store files in a batch.\n\nurl = f'{base_url}/vector_stores/{None}/file_batches/{None}/files'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'filter': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_5891796255252150160",
      "title": "OpenAPI: List vector store files",
      "content": "import requests\n\n# List vector store files\n# Returns a list of vector store files.\n\nurl = f'{base_url}/vector_stores/{None}/files'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'filter': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /vector_stores/{vector_store_id}/files",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /vector_stores/{vector_store_id}/files",
        "summary": "List vector store files",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# List vector store files\n# Returns a list of vector store files.\n\nurl = f'{base_url}/vector_stores/{None}/files'\nparams = {\n    'limit': None,\n    'order': None,\n    'after': None,\n    'before': None,\n    'filter': None\n}\n\nresponse = requests.get(url, params=params)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4625989969403498056",
      "title": "OpenAPI: Create vector store file",
      "content": "import requests\n\n# Create vector store file\n# Create a vector store file by attaching a [File](https://platform.openai.com/docs/api-reference/files) to a [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object).\n\nurl = f'{base_url}/vector_stores/{None}/files'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /vector_stores/{vector_store_id}/files",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /vector_stores/{vector_store_id}/files",
        "summary": "Create vector store file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Create vector store file\n# Create a vector store file by attaching a [File](https://platform.openai.com/docs/api-reference/files) to a [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object).\n\nurl = f'{base_url}/vector_stores/{None}/files'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1677485545265010687",
      "title": "OpenAPI: Retrieve vector store file",
      "content": "import requests\n\n# Retrieve vector store file\n# Retrieves a vector store file.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /vector_stores/{vector_store_id}/files/{file_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /vector_stores/{vector_store_id}/files/{file_id}",
        "summary": "Retrieve vector store file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve vector store file\n# Retrieves a vector store file.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_4912104028249955574",
      "title": "OpenAPI: Delete vector store file",
      "content": "import requests\n\n# Delete vector store file\n# Delete a vector store file. This will remove the file from the vector store but the file itself will not be deleted. To delete the file, use the [delete file](https://platform.openai.com/docs/api-reference/files/delete) endpoint.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "DELETE /vector_stores/{vector_store_id}/files/{file_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "DELETE /vector_stores/{vector_store_id}/files/{file_id}",
        "summary": "Delete vector store file",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Delete vector store file\n# Delete a vector store file. This will remove the file from the vector store but the file itself will not be deleted. To delete the file, use the [delete file](https://platform.openai.com/docs/api-reference/files/delete) endpoint.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}'\n\nresponse = requests.delete(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_6043530476274480323",
      "title": "OpenAPI: Update vector store file attributes",
      "content": "import requests\n\n# Update vector store file attributes\n# Update attributes on a vector store file.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /vector_stores/{vector_store_id}/files/{file_id}",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /vector_stores/{vector_store_id}/files/{file_id}",
        "summary": "Update vector store file attributes",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Update vector store file attributes\n# Update attributes on a vector store file.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_-6566764455585875695",
      "title": "OpenAPI: Retrieve vector store file content",
      "content": "import requests\n\n# Retrieve vector store file content\n# Retrieve the parsed contents of a vector store file.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}/content'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "GET /vector_stores/{vector_store_id}/files/{file_id}/content",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "GET /vector_stores/{vector_store_id}/files/{file_id}/content",
        "summary": "Retrieve vector store file content",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Retrieve vector store file content\n# Retrieve the parsed contents of a vector store file.\n\nurl = f'{base_url}/vector_stores/{None}/files/{None}/content'\n\nresponse = requests.get(url)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    },
    {
      "id": "openapi_1385388936669708389",
      "title": "OpenAPI: Search vector store",
      "content": "import requests\n\n# Search vector store\n# Search a vector store for relevant chunks based on a query and file attributes filter.\n\nurl = f'{base_url}/vector_stores/{None}/search'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)",
      "content_type": "code",
      "source_type": "openapi",
      "source_file": "POST /vector_stores/{vector_store_id}/search",
      "tags": [
        "openai-api",
        "api-reference",
        "code-example"
      ],
      "metadata": {
        "endpoint": "POST /vector_stores/{vector_store_id}/search",
        "summary": "Search vector store",
        "language": "python"
      },
      "code_examples": [
        "import requests\n\n# Search vector store\n# Search a vector store for relevant chunks based on a query and file attributes filter.\n\nurl = f'{base_url}/vector_stores/{None}/search'\ndata = {\n    # Add your request data here\n    # Based on the API schema\n}\n\nresponse = requests.post(url, json=data)\nresponse.raise_for_status()\nresult = response.json()\nprint(result)"
      ],
      "api_methods": [],
      "complexity_score": 0.3,
      "use_case_category": "api_integration"
    }
  ]
}